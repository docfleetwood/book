[
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "11  Contribute",
    "section": "",
    "text": "11.1 Audience oriented\nDissemination of research findings is a critical part of the research process. Whether it is through presentations, articles, blog posts, or social media, the ability to effectively communicate the results of research is essential for the impact and dissemination of research. Furthermore, the act of composing a research document can help to develop and refine ideas and conclusions.\nThe two most common forms of research dissemination in academics are presentations and articles. Both share a common goal: to effectively communicate the research to an audience. However, they also have distinct purposes and require different approaches to achieve their goals. These purposes complement each other, with presentations often serving as a means to engage and elicit feedback from an audience, and articles serving as a more comprehensive and permanent record of the research.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Contribute</span>"
    ]
  },
  {
    "objectID": "contributing.html#sec-contr-audience-oriented",
    "href": "contributing.html#sec-contr-audience-oriented",
    "title": "11  Contribute",
    "section": "",
    "text": "11.1.1 Structure\nFirst, let’s focus on the structural elements that appear in both research presentations and articles. The components in Table 11.1 reflect the typical structure for presenting research in the social sciences (S. T. Gries 2016; S. Th. Gries and Paquot 2020; Sternberg and Sternberg 2010). Their combined purpose is to trace the research narrative from the rationale and goals to connecting the findings with the research questions and aims.\n\n\nTable 11.1: Common components of research presentations and articles\n\n\n\n\n\n\n\nComponent\nPurpose\n\n\n\nIntroduction\nProvide context and rationale based on previous research\n\n\nMethods\nDescribe the research design and procedures\n\n\nResults\nPresent the findings including key statistics and table and/ or visual summaries\n\n\nDiscussion\nInterpret the findings and discuss implications\n\n\nConclusion\nSummarize the research and suggest future work\n\n\n\n\n\n\nWhen research is connected to a well-designed plan, as described in Chapter 4, many of these components are already in place. The steps taken to identify an area and a problem or gap in the literature find themselves in the introduction. This section builds the case for the research and provides the context for the research question(s) and aim(s). The methods section describes the research design and procedures, including the data collection and analysis steps that are key to contextualize the findings. In the results section, the findings are presented in the appropriate manner given the research aim and the analysis performed.\nThe discussion and conclusions sections, however, are where the research narrative is brought together. Crafting these sections can be seen as an extension of the research process itself. Instead of elaborating on the planning steps and their implementation, the discussion focus on the interpretation of the findings in the light of the research questions and previous literature. At this stage, the act of articulating the implications of the findings is where deeper insights are developed and refined. The conclusion, for its part, puts a finer point on the research goal and main findings, but also is an opportunity to extend suggestions to where subsequent research might go.\nThe research abstract is another important component of both presentations and articles. I have not included it in Table 11.1 as it is a distillation of the entire research narrative. Furthermore, the purpose of the abstract is distinct between presentations and articles. In presentations, the abstract typically serves to provide conference referees and attendees with a summary of the research. In articles, the abstract helps readers decide whether the article is relevant to their interests and worth the time to read in full.\n\n11.1.2 Purpose\n\nUnderstanding the roles the structural elements play in contributing to the overall narrative is essential for effective research communication. Yet, presentations and articles are not the same. They have distinct goals which are reflected in the emphasis that each communication channel places on particular narrative elements and the level of detail and nuance that is included in the narrative.\nIt is likely not a surprise that articles are more detailed and nuanced than presentations. But what is sometimes overlooked is that presentations should emphasize storytelling and relatability, over nuance and detail. Sure, the research should be accurate and reliable, but the focus is on engaging the audience and making the research relatable. No matter how much detail you provide in a presentation, there will likely be questions and feedback that will require further explanation. A “less is more” approach is a good rule of thumb for presentations.\nTabular and visual summaries are key to convey complex findings, regardless of the mode of communication. However, in presentations, the use of visual aids is especially effective for engaging the audience as the visual modality does not compete with the spoken word for attention. Along these lines, limiting the amount of text on slides and increasing natural discourse with the audience is a good practice. Your presentation will be more engaging leading to more questions and feedback that you can use to refine your current or to seed future research.\nThe purpose of an article is to provide a comprehensive record of the research. In this record, the methods and results sections are particularly significant. The methods section should provide the reader with the necessary information to understand the research design and procedures and to evaluate the findings, as it should in presentations, but, in contrast to presentations, it should also speak to researchers providing the details required to reproduce the research. These details summarize and, ideally, point to the data and code that are used to produce the findings in your reproducible research project.\nThe results section, for its part, should present the findings in a manner that is clear and concise, but also comprehensive. The research aim and the analysis performed will determine the appropriate measures and/ or summaries to use. Table 11.2 outlines the key statistical results, tables, and visualizations that are often included in the results section for exploratory, predictive, and inferential analyses.\n\n\nTable 11.2: Key statistical results, tables, and visualizations for research results\n\n\n\n\n\n\n\n\nResearch aim\nStatistical results\nSummaries\n\n\n\nExploratory\nDescriptive statistics\nExtensive use of tables and/ or visualizations\n\n\nPredictive\nDescriptive statistics, model performance metrics\nTables for model performance comparisons and/ or visualizations for feature importance measures\n\n\nInferential\nDescriptive statistics, hypothesis testing confidence metrics\nTables for hypothesis testing results and/ or visualizations to visualize trends\n\n\n\n\n\n\nBy and large, the results section should be a descriptive and visual summary of the findings as they are, without interpretation. The discussion section is where the interpretation of the findings and their implications are presented. This general distinction between the results and discussion may be less pronounced in exploratory research, as the interpretation of the findings may be more intertwined with the presentation of the findings given the nature of the research.\n\n11.1.3 Strategies\nUp to this point, we have discussed the structural elements and the purpose of research presentations and articles. Now, let’s turn to the practical strategies for creating effective research presentations and articles.\nStrong research write-ups begin with well-framed and well-documented research plans. The steps outlined in Chapter 4 are the foundation for the various elements of the research narrative. Furthermore, you can further prepare for the research write-up by leaving yourself a breadcrumb trail along the way.\nThe introduction includes the rationale, goals, and research questions and aims. These components are directly connected to the primary literature that you consulted. For this reason, it is a good practice to keep a record of the literature that you consulted and the notes that you took. This record will help you to trace the development of your ideas and to provide the necessary context for your research. A reference manager, such as Zotero, Mendeley, or EndNote, is a good tool for this purpose. These tools allow you to manage your ideas and keep notes, organize your references and resources, and integrate your references and resources with your writing in Quarto through BibTeX entry citation keys.\nSimilarly, if you are following best practices, you will have documented your data, processing steps, and analysis choices while conducting your research. The methods section stems directly from these resources. Data origin files provide the necessary context for the data that you used in your research. Data dictionary files clarify variables and values in your datasets. Literate programming, as implemented in Quarto, can further provide process and analysis documentation.\nThe results section can also benefit from some preparation. The key statistical results, tables, and visualizations generated in your process script for the analysis should be saved as outputs. This provides a more convenient way to include these results in your research document(s). If you are using a project structure similiar to the one outlined in Section 4.4.2, you can write statistical results as R objects using saveRDS(), and write tables and visualizations as files using kableExtra::save_kable() and ggplot2::ggsave(), respectively, to the corresponding outputs/ directory. This will allow you to easily access and include these results in your research document(s) to avoid having to recreate the analysis steps from a dataset or manually copy and paste results from the console, which can be error-prone and is not reproducible.\n\n\n\n\n\n\n Tip\nThe qtkit package provides three functions for writing R objects, ggplot2 objects, and kable objects to a given directory. These functions are write_obj(), write_gg(), and write_kbl(), respectively. These functions also provide functionality to automatically name the output files based on the label of the code block in which they are called to make it easier to connect the output to the code that generated it. For more information, see the qtkit documentation.\n\n\n\nAt this point we have our ducks in a row, so to speak. We have a well-documented research plan, a record of the literature that we consulted, and a record of the data, processing steps, and analysis choices that we made. We have also saved the key statistical results, tables, and visualizations that we generated in our process script for the analysis. Now, we can begin to write our research document(s).\n\n\n\n\n\n\n Dive deeper\nQuarto 1.4+ includes a ‘manuscript’ feature which provides a workflow for including source content, computations, and visualizations in a single document which links to these source documents. For more information, see the Quarto documentation.\n\n\n\nAlthough there are many tools and platforms for creating and sharing research presentations and articles, I advocate for using Quarto to create and share both. First, Quarto documents fit squarely into a reproducible workflow, as we have seen throughout this textbook. Secondly, using Quarto for both presentations and articles allows for a seamless transition between the two. This is particularly useful when you are presenting research that you have written about in an article, or vice versa. The statistical results, tables, and visualizations that you saved as outputs can be easily included in both presentations and articles seamlessly. Third, changes in your research process will naturally be reflected in your write-ups. This helps maintain the fidelity of your research across the various stages of the research process, including write-ups. Finally, Quarto provides a variety of output formats, including PDF, HTML, and Word, which are suitable for sharing research presentations and articles. These documents can be shared on various platforms, including GitHub pages, and can be easily converted to other formats. Quarto also provides a styles for citations and bibliographies and a variety of extensions for journal-specific formatting, which can be useful for publishing articles in specific venues.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Contribute</span>"
    ]
  },
  {
    "objectID": "contributing.html#sec-contr-research-oriented",
    "href": "contributing.html#sec-contr-research-oriented",
    "title": "11  Contribute",
    "section": "\n11.2 Research oriented",
    "text": "11.2 Research oriented\nWhether for other researchers or for your future self, creating research that is well-documented and reproducible is a fundamental part of conducting modern scientific inquiry. In this section, we will emphasize the importance of this endeavor and outline strategies for ensuring your research project is reproducible. This will include directory and file structure, key documentation files as well as how to effectively use existing software resources and frameworks for publishing your research.\n\nImportance of well-documented and reproducible research in modern scientific inquiry\n\nEnsuring accuracy and reliability of results\nFacilitating knowledge sharing and collaboration\n\nCombining expertise from different fields.\nPeer review and feedback.\n\n\nEnhance the visibility and impact of research\n\n\n\n\n11.2.1 Structure\n\nResearch compendium\n\nUsing literate programming for transparency and documentation\nDirectory and file structure/ naming conventions\nModularization of scripts and files for specific analysis goals\nKey documentation files: README, data origin, and data dictionaries\nImplementing a ‘main’ script for coordinating project steps\n\n\nComputing environment\n\nDirectory and file management\n\nOpen source formats, software, and tools\nVersion control\n\nTracking changes and collaboration\nEthical considerations (data privacy, security, etc.)\n\n\n.gitignore for sensitive data\n\n\n\n\n\n\nSoftware management\n\nR and package management (renv)\n\n\nSystem-level dependencies\n\nDocker (or rix) for containerization\n\n\n\n\n\n11.2.2 Purpose\n\nResearch compendium\n\nTransparency and documentation\nReproducibility and sharing research findings\nFacilitating collaboration and peer review\nEnhancing the visibility and impact of research\n\n\nComputing environment\n\nEnsuring accuracy and reliability of results\nFacilitating knowledge sharing and collaboration\nEnhancing the visibility and impact of research\n\n\n\n11.2.3 Strategies\n\nResearch compendium\n\nUsing literate programming for transparency and documentation\nDirectory and file structure/ naming conventions\nModularization of scripts and files for specific analysis goals\nKey documentation files: README, data origin, and data dictionaries\nImplementing a ‘main’ script for coordinating project steps\n\n\nComputing environment\n\nDirectory and file management\n\nOpen source formats, software, and tools\nVersion control\n\nTracking changes and collaboration\nEthical considerations (data privacy, security, etc.)\n\n\n.gitignore for sensitive data\n\n\n\n\n\n\nSoftware management\n\nR and package management (renv)\n\n\nSystem-level dependencies\n\nDocker (or rix) for containerization\n\n\n\n\nPublishing research\n\nPopular repositories (e.g., GitHub, Open Science Framework)\nStrategies for uploading and organizing research projects\nCollaboration best practices\n\nGitHub branches and pull requests\nCode review and feedback (Issues, Discussions, etc.)\nLicensing and citation attribution\n\nappropriate licenses for code and data to ensure proper attribution and reuse",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Contribute</span>"
    ]
  },
  {
    "objectID": "contributing.html#activities",
    "href": "contributing.html#activities",
    "title": "11  Contribute",
    "section": "Activities",
    "text": "Activities\n\n\n\n\n\n\n Recipe\n\nWhat: Manage project and computing environmentsHow: Read Recipe 11, complete comprehension check, and prepare for Lab 11.Why: To follow the steps for managing a research project and computing environment for effective communication and reproducibility.\n\n\n\n\n\n\n\n\n\n Lab\n\nWhat: Future-proof researchHow: Clone, fork, and complete the steps in Lab 11.Why: To apply the strategies for ensuring that your research project is reproducible.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Contribute</span>"
    ]
  },
  {
    "objectID": "contributing.html#summary",
    "href": "contributing.html#summary",
    "title": "11  Contribute",
    "section": "Summary",
    "text": "Summary\nIn this chapter, we have discussed the importance of clear and effective communication in research reporting, and the strategies for ensuring that your research project is reproducible. We have discussed the role of research presentations in academic and professional settings, highlighting the benefits of presenting one’s work as a means to develop and refine ideas and conclusions. We then discussed the purpose of a research document, focusing on how its structure effectively conveys the project’s rationale, goals, procedures, results, and findings. We also emphasized the importance of well-documented and reproducible research in modern scientific inquiry and outlined strategies for ensuring your research project is reproducible. This included directory and file structure, key documentation files as well as how to effectively use existing software resources and frameworks for publishing your research.\n\n\n\n\n\n\nGries, Stefan Th. 2016. Quantitative Corpus Linguistics with r: A Practical Introduction. 2nd ed. New York: Routledge. https://doi.org/10.4324/9781315746210.\n\n\nGries, Stefan Th., and Magali Paquot. 2020. “Writing up a Corpus-Linguistic Paper.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Th. Gries, 647–59. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-46216-1_26.\n\n\nSternberg, Robert J., and Karin Sternberg. 2010. The Psychologist’s Companion: A Guide to Writing Scientific Papers for Students and Researchers. 5th ed. Cambridge University Press. https://doi.org/10.1017/CBO9780511762024.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Contribute</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ackoff, Russell L. 1989. “From Data to Wisdom.” Journal\nof Applied Systems Analysis 16 (1): 3–9.\n\n\nÄdel, Annelie. 2020. “Corpus Compilation.” In A\nPractical Handbook of Corpus Linguistics, edited by Magali Paquot\nand Stefan Th. Gries, 3–24. Switzerland: Springer.\n\n\nAlbert, Saul, Laura E. de Ruiter, and J. P. de Ruiter. 2015.\n“CABNC: The Jeffersonian Transcription of the Spoken British\nNational Corpus.” TalkBank.\n\n\nAllaire, JJ, and Christophe Dervieux. 2024. Quarto: R Interface to\nQuarto Markdown Publishing System. https://github.com/quarto-dev/quarto-r.\n\n\nBaayen, R. Harald. 2004. “Statistics in Psycholinguistics: A\nCritique of Some Current Gold Standards.” Mental Lexicon\nWorking Papers 1 (1): 1–47.\n\n\n———. 2008. Analyzing Linguistic Data: A Practical Introduction to\nStatistics Using r. Cambridge Univ Pr.\n\n\n———. 2011. “Corpus Linguistics and Naive Discriminative\nLearning.” Revista Brasileira de\nLingu\\’\\istica Aplicada 11 (2): 295–328.\n\n\nBaayen, R. Harald, L. B. Feldman, and R. Schreuder. 2006.\n“Morphological Influences on the Recognition of Monosyllabic\nMonomorphemic Words.” Journal of Memory and Language 55:\n290–313. https://doi.org/10.1016/j.jml.2006.03.008.\n\n\nBaayen, R. H., and Elnaz Shafaei-Bajestan. 2019. languageR:\nAnalyzing Linguistic Data: A Practical Introduction to Statistics.\nhttps://CRAN.R-project.org/package=languageR.\n\n\nBaker, Monya. 2016. “1,500 Scientists Lift the Lid on\nReproducibility.” Nature 533 (7604): 452–54. https://doi.org/10.1038/533452a.\n\n\nBao, Wang, Ning Lianju, and Kong Yue. 2019. “Integration of\nUnsupervised and Supervised Machine Learning Algorithms for Credit Risk\nAssessment.” Expert Systems with Applications 128\n(August): 301–15. https://doi.org/10.1016/j.eswa.2019.02.033.\n\n\nBengtsson, Henrik. 2023. Future: Unified Parallel and Distributed\nProcessing in r for Everyone. https://future.futureverse.org.\n\n\nBenoit, Kenneth, David Muhr, and Kohei Watanabe. 2021. Stopwords:\nMultilingual Stopword Lists. https://github.com/quanteda/stopwords.\n\n\nBenoit, Kenneth, and Adam Obeng. 2024. Readtext: Import and Handling\nfor Plain and Formatted Text Files. https://github.com/quanteda/readtext.\n\n\nBlischak, John, Peter Carbonetto, and Matthew Stephens. 2023.\nWorkflowr: A Framework for Reproducible and Collaborative Data\nScience. https://github.com/workflowr/workflowr.\n\n\nBraginsky, Mika. 2024. Wordbankr: Accessing the Wordbank\nDatabase. https://langcog.github.io/wordbankr/.\n\n\nBray, Andrew, Chester Ismay, Evgeni Chasnovski, Simon Couch, Ben Baumer,\nand Mine Cetinkaya-Rundel. 2024. Infer: Tidy Statistical\nInference. https://github.com/tidymodels/infer.\n\n\nBresnan, Joan. 2007. “A Few Lessons from Typology.”\nLinguistic Typology 11 (1): 297–306.\n\n\nBresnan, Joan, Anna Cueni, Tatiana Nikitina, and R. Harald Baayen. 2007.\n“Predicting the Dative Alternation.” In Cognitive\nFoundations of Interpretation, edited by G. Bouma, I. Kraemer, and\nJan-Wouter C Zwart, 1–33. Amsterdam: KNAW.\n\n\nBrown, Keith. 2005. Encyclopedia of Language and Linguistics.\nVol. 1. Elsevier.\n\n\nBryan, Jennifer, Jim Hester, David Robinson, Hadley Wickham, and\nChristophe Dervieux. 2024. Reprex: Prepare Reproducible Example Code\nvia the Clipboard. https://reprex.tidyverse.org.\n\n\nBuckheit, Jonathan B., and David L. Donoho. 1995. “Wavelab and\nReproducible Research.” In Wavelets and Statistics,\n55–81. Springer.\n\n\nBychkovska, Tetyana, and Joseph J. Lee. 2017. “At the Same Time:\nLexical Bundles in L1 and L2 University Student Argumentative\nWriting.” Journal of English for Academic Purposes 30\n(November): 38–52. https://doi.org/10.1016/j.jeap.2017.10.008.\n\n\nCampbell, Lyle. 2001. “The History of Linguistics.” In\nThe Handbook of Linguistics, edited by Mark Aronoff and Janie\nRees-Miller, 81–104. Blackwell Handbooks in Linguistics. Blackwell\nPublishers.\n\n\nCarmi, Elinor, Simeon J. Yates, Eleanor Lockley, and Alicja Pawluczuk.\n2020. “Data Citizenship: Rethinking Data Literacy in the Age of\nDisinformation, Misinformation, and Malinformation.” Internet\nPolicy Review 9 (2).\n\n\nChambers, John M. 2020. “S, r, and Data Science.”\nProceedings of the ACM on Programming Languages 4 (HOPL): 1–17.\nhttps://doi.org/10.1145/3386334.\n\n\nChan, Sin-wai. 2014. Routledge Encyclopedia of Translation\nTechnology. Routledge.\n\n\nConway, Lucian Gideon, Laura Janelle Gornick, Chelsea Burfeind, Paul\nMandella, Andrea Kuenzli, Shannon C. Houck, and Deven Theresa Fullerton.\n2012. “Does Complex or Simple Rhetoric Win Elections? An\nIntegrative Complexity Analysis of u.s. Presidential Campaigns.”\nPolitical Psychology 33 (5): 599–618. https://doi.org/10.1111/j.1467-9221.2012.00910.x.\n\n\nCross, Nigel. 2006. “Design as a Discipline.”\nDesignerly Ways of Knowing, 95–103.\n\n\nCsárdi, Gábor, Jim Hester, Hadley Wickham, Winston Chang, Martin Morgan,\nand Dan Tenenbaum. 2023. Remotes: R Package Installation from Remote\nRepositories, Including GitHub. https://remotes.r-lib.org.\n\n\n“Data Never Sleeps 7.0 Infographic.” 2019.\nhttps://www.domo.com/learn/infographic/data-never-sleeps-7.\n\n\nDeshors, Sandra C, and Stefan Th. Gries. 2016. “Profiling Verb\nComplementation Constructions Across New Englishes.”\nInternational Journal of Corpus Linguistics. 21 (2): 192–218.\n\n\nDesjardins, Jeff. 2019. “How Much Data Is Generated Each\nDay?” Visual Capitalist.\n\n\nDonoho, David. 2017. “50 Years of Data Science.”\nJournal of Computational and Graphical Statistics 26 (4):\n745–66. https://doi.org/10.1080/10618600.2017.1384734.\n\n\nDubnjakovic, Ana, and Patrick Tomlin. 2010. A Practical Guide to\nElectronic Resources in the Humanities. Elsevier.\n\n\nDuran, P. 2004. “Developmental Trends in Lexical\nDiversity.” Applied Linguistics 25 (2): 220–42. https://doi.org/10.1093/applin/25.2.220.\n\n\nEisenstein, Jacob, Brendan O’Connor, Noah A Smith, and Eric P Xing.\n2012. “Mapping the Geographical Diffusion of New Words.”\nComputation and Language, 1–13. https://doi.org/10.1371/journal.pone.0113114.\n\n\nFirth, John R. 1957. Papers in Linguistics. Oxford University\nPress.\n\n\nFrancom, Jerid. 2022. “Corpus Studies of Syntax.” In\nThe Cambridge Handbook of Experimental Syntax, edited by Grant\nGoodall, 687–713. Cambridge Handbooks in Language and Linguistics.\nCambridge University Press.\n\n\n———. 2023. Qtalrkit: Quantitative Text Analysis for Linguists\nResource Kit. https://github.com/qtalr/qtalrkit.\n\n\nGandrud, Christopher. 2015. Reproducible\nResearch with r and r Studio. Second edition. CRC Press.\n\n\nGarg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018.\n“Word Embeddings Quantify 100 Years of Gender and Ethnic\nStereotypes.” Proceedings of the National Academy of\nSciences 115 (16): E3635–44. https://doi.org/10.1073/pnas.1720347115.\n\n\nGentleman, Robert, and Duncan Temple Lang. 2007. “Statistical\nAnalyses and Reproducible Research.” Journal of Computational\nand Graphical Statistics 16 (1): 1–23.\n\n\nGilquin, Gaëtanelle, and Stefan Th Gries. 2009. “Corpora and\nExperimental Methods: A State-of-the-Art Review.” Corpus\nLinguistics and Linguistic Theory 5 (1): 1–26. https://doi.org/10.1515/CLLT.2009.001.\n\n\nGomez-Uribe, Carlos A., and Neil Hunt. 2015. “The Netflix\nRecommender System: Algorithms, Business Value, and Innovation.”\nACM Transactions on Management Information Systems (TMIS) 6\n(4): 1–19.\n\n\nGries, Stefan Th. 2016. Quantitative Corpus Linguistics with r: A\nPractical Introduction. 2nd ed. New York: Routledge. https://doi.org/10.4324/9781315746210.\n\n\n———. 2021. Statistics for Linguistics with r. De Gruyter\nMouton.\n\n\n———. 2023. “Statistical Methods in Corpus Linguistics.” In\nReadings in Corpus Linguistics: A Teaching and Research Guide for\nScholars in Nigeria and Beyond, 78–114.\n\n\nGries, Stefan Th. 2013. Statistics for Linguistics with r. A\nPractical Introduction. 2nd revise.\n\n\nGries, Stefan Th., and Sandra C. Deshors. 2014. “Using Regressions\nto Explore Deviations Between Corpus Data and a Standard/Target: Two\nSuggestions.” Corpora 9 (1): 109–36. https://doi.org/10.3366/cor.2014.0053.\n\n\nGries, Stefan Th., and Magali Paquot. 2020. “Writing up a\nCorpus-Linguistic Paper.” In A Practical Handbook of Corpus\nLinguistics, edited by Magali Paquot and Stefan Th. Gries, 647–59.\nCham: Springer International Publishing. https://doi.org/10.1007/978-3-030-46216-1_26.\n\n\nGrieve, Jack, Andrea Nini, and Diansheng Guo. 2018. “Mapping\nLexical Innovation on American Social Media.” Journal of\nEnglish Linguistics 46 (4): 293–319.\n\n\nHarris, Zellig S. 1954. “Distributional Structure.”\nWord 10 (2-3): 146–62. https://doi.org/10.1080/00437956.1954.11659520.\n\n\nHay, Jennifer. 2002. “From Speech Perception to Morphology: Affix\nOrdering Revisited.” Language 78 (3): 527–55.\n\n\nHead, Megan L., Luke Holman, Rob Lanfear, Andrew T. Kahn, and Michael D.\nJennions. 2015. “The Extent and Consequences of p-Hacking in\nScience.” PLOS Biology 13 (3): e1002106. https://doi.org/10.1371/journal.pbio.1002106.\n\n\nHester, Jim, Hadley Wickham, and Gábor Csárdi. 2023. Fs:\nCross-Platform File System Operations Based on Libuv. https://fs.r-lib.org.\n\n\nHicks, Stephanie C., and Roger D. Peng. 2019. “Elements and\nPrinciples for Characterizing Variation Between Data Analyses.”\narXiv. https://doi.org/10.48550/arXiv.1903.07639.\n\n\nHvitfeldt, Emil. 2023. Textrecipes: Extra Recipes for Text\nProcessing. https://github.com/tidymodels/textrecipes.\n\n\nIde, Nancy, Collin Baker, Christiane Fellbaum, Charles Fillmore, and\nRebecca Passonneau. 2008. “MASC: The Manually Annotated Sub-Corpus\nof American English.” In 6th International Conference on\nLanguage Resources and Evaluation, LREC 2008, 2455–60. European\nLanguage Resources Association (ELRA).\n\n\nIgnatow, Gabe, and Rada Mihalcea. 2017. An Introduction to Text\nMining: Research Design, Data Collection, and Analysis. Sage\nPublications.\n\n\nJaeger, T Florian, and Neal Snider. 2007. “Implicit Learning and\nSyntactic Persistence: Surprisal and Cumulativity.”\nUniversity of Rochester Working Papers in the Language Sciences\n3 (1).\n\n\nJohnson, K. 2008. Quantitative Methods in Linguistics.\nBlackwell Pub.\n\n\nKato, Akiru, Shogo Ichinose, and Taku Kudo. 2024. Gibasa: An\nAlternative Rcpp Wrapper of MeCab. https://paithiov909.github.io/gibasa/.\n\n\nKaur, Jashanjot, and P. Kaur Buttar. 2018. “A Systematic Review on\nStopword Removal Algorithms.” International Journal on Future\nRevolution in Computer Science & Communication Engineering 4\n(4): 207–10.\n\n\nKerr, Norbert L. 1998. “HARKing: Hypothesizing After the Results\nAre Known.” Personality and Social Psychology Review 2\n(3): 196–217.\n\n\nKloumann, IM, CM Danforth, KD Harris, and CA Bliss. 2012.\n“Positivity of the English Language.” PloS One.\n\n\nKoehn, P. 2005. “Europarl: A Parallel Corpus for Statistical\nMachine Translation.” MT Summit X, 12–16.\n\n\nKostić, Aleksandar, Tanja Marković, and Aleksandar Baucal. 2003.\n“Inflectional Morphology and Word Meaning: Orthogonal or\nCo-Implicative Cognitive Domains?” In Morphological Structure\nin Language Processing, edited by R. Harald Baayen and Robert\nSchreuder, 1–44. De Gruyter Mouton. https://doi.org/10.1515/9783110910186.1.\n\n\nKowalski, John, and Rob Cavanaugh. 2022. TBDBr: Easy Access to\nTalkBankDB via r API. https://github.com/TalkBank/TalkBankDB-R.\n\n\nKrathwohl, David R. 2002. “A Revision of Bloom’s Taxonomy: An\nOverview.” Theory into Practice 41 (4): 212–18.\n\n\nKross, Sean, Nick Carchedi, Bill Bauer, and Gina Grdina. 2020.\nSwirl: Learn r, in r. http://swirlstats.com.\n\n\nKucera, H, and W N Francis. 1967. Computational Analysis of Present\nDay American English. Brown University Press Providence.\n\n\nLandau, William Michael. 2024. Targets: Dynamic Function-Oriented\nMake-Like Declarative Pipelines. https://docs.ropensci.org/targets/.\n\n\nLeech, Geoffrey. 1992. “100 Million Words of English: The British\nNational Corpus (BNC),” no. 1991: 1–13.\n\n\nLewis, Michael. 2004. Moneyball: The Art of Winning an Unfair\nGame. WW Norton & Company.\n\n\nLiu, Kanglong, and Muhammad Afzaal. 2021. “Syntactic Complexity in\nTranslated and Non-Translated Texts: A Corpus-Based Study of\nSimplification.” Edited by Diego Raphael Amancio. PLOS\nONE 16 (6): e0253454. https://doi.org/10.1371/journal.pone.0253454.\n\n\nLozano, Cristóbal. 2009. “CEDEL2: Corpus Escrito Del\nEspañol L2.” Applied Linguistics Now:\nUnderstanding Language and Mind/La Lingüística\nAplicada Hoy: Comprendiendo El Lenguaje y La Mente.\nAlmería: Universidad de Almería, 197–212.\n\n\nMacwhinney, Brian. 2003. “TalkBank.” Repository. The\nTalkBank System. https://www.talkbank.org/.\n\n\nManning, Christopher. 2003. “Probabilistic Syntax.” In\nProbabilistic Linguistics, edited by Bod, Jennifer Hay, and\nJannedy, 289–341. Cambridge, MA: MIT Press.\n\n\nMarcus, Mitchell P, Beatrice Santorini, and Mary Ann Marcinkiewicz.\n1993. “Building a Large Annotated Corpus of English: The Penn\nTreebank.” Computational Linguistics 19 (2): 313–30.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging\nData Analytical Work Reproducibly Using r (and Friends).” The\nAmerican Statistician 72 (1): 80–88.\n\n\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. “Distributed Representations of Words and Phrases and\nTheir Compositionality.” In Advances in Neural Information\nProcessing Systems, 3111–19.\n\n\nMoroz, George. 2024. Lingtypology: Linguistic Typology and\nMapping. https://CRAN.R-project.org/package=lingtypology.\n\n\nMorris, Tim P., Ian R. White, and Michael J. Crowther. 2019.\n“Using Simulation Studies to Evaluate Statistical Methods.”\nStatistics in Medicine 38 (11): 2074–2102. https://doi.org/10.1002/sim.8086.\n\n\nMosteller, Frederick, and David L Wallace. 1963. “Inference in an\nAuthorship Problem.” Journal of the American Statistical\nAssociation 58 (302): 275–309. https://www.jstor.org/stable/2283270.\n\n\nMullen, Lincoln. 2022. Tokenizers: Fast, Consistent Tokenization of\nNatural Language Text. https://docs.ropensci.org/tokenizers/.\n\n\nMuñoz, Carmen, ed. 2006. Age and the Rate of Foreign Language\nLearning. 1st ed. Vol. 19. Second Language Acquisition Series.\nClevedon: Multilingual Matters.\n\n\nNivre, Joakim, Marie-Catherine De Marneffe, Filip Ginter, Jan Hajič,\nChristopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis\nTyers, and Daniel Zeman. 2020. “Universal Dependencies V2: An\nEvergrowing Multilingual Treebank Collection.” arXiv Preprint\narXiv:2004.10643. https://arxiv.org/abs/2004.10643.\n\n\nOlohan, Maeve. 2008. “Leave It Out! Using a Comparable Corpus to\nInvestigate Aspects of Explicitation in Translation.”\nCadernos de Tradução, 153–69.\n\n\nOoms, Jeroen. 2023. Jsonlite: A Simple and Robust JSON Parser and\nGenerator for r. https://jeroen.r-universe.dev/jsonlite\nhttps://arxiv.org/abs/1403.2805.\n\n\nPaquot, Magali, and Stefan Th. Gries, eds. 2020. A Practical\nHandbook of Corpus Linguistics. Switzerland: Springer.\n\n\nPetrenz, Philipp, and Bonnie Webber. 2011. “Stable Classification\nof Text Genres.” Computational Linguistics 37 (2):\n385–93. https://doi.org/10.1162/COLI_a_00052.\n\n\nR Special Interest Group on Databases (R-SIG-DB), Hadley Wickham, and\nKirill Müller. 2024. DBI: R Database Interface. https://dbi.r-dbi.org.\n\n\nRiehemann, Susanne Z. 2001. “A Constructional Approach to Idioms\nand Word Formation.” PhD thesis, Stanford.\n\n\nRinker, Tyler. 2019. Lexicon: Lexicons for Text Analysis. https://github.com/trinker/lexicon.\n\n\nRobinson, David, and Julia Silge. 2023. Tidytext: Text Mining Using\nDplyr, Ggplot2, and Other Tidy Tools. https://github.com/juliasilge/tidytext.\n\n\nRossman, Allan J., and Beth L. Chance. 2014. “Using\nSimulation-Based Inference for Learning Introductory Statistics.”\nWIREs Computational Statistics 6 (4): 211–21. https://doi.org/10.1002/wics.1302.\n\n\nRowley, Jennifer. 2007. “The Wisdom Hierarchy: Representations of\nthe DIKW Hierarchy.” Journal of Information Science 33\n(2): 163–80. https://doi.org/10.1177/0165551506070706.\n\n\nSaxena, Shweta, and Manasi Gyanchandani. 2020. “Machine Learning\nMethods for Computer-Aided Breast Cancer Diagnosis Using Histopathology:\nA Narrative Review.” Journal of Medical Imaging and Radiation\nSciences 51 (1): 182–93.\n\n\nSedgwick, Philip. 2015. “Units of Sampling, Observation, and\nAnalysis.” BMJ (Online) 351 (October): h5396. https://doi.org/10.1136/bmj.h5396.\n\n\nShriberg, Elizabeth Ellen. 1994. “Preliminaries to a Theory of\nSpeech Disfluencies.” PhD thesis, Citeseer.\n\n\nSilge, Julia. 2022. Janeaustenr: Jane Austen’s Complete Novels.\nhttps://github.com/juliasilge/janeaustenr.\n\n\nSternberg, Robert J., and Karin Sternberg. 2010. The Psychologist’s\nCompanion: A Guide to Writing Scientific Papers for Students and\nResearchers. 5th ed. Cambridge University Press. https://doi.org/10.1017/CBO9780511762024.\n\n\nSzmrecsanyi, Benedikt. 2004. “On Operationalizing Syntactic\nComplexity.” In Le Poids Des Mots. Proceedings of the 7th\nInternational Conference on Textual Data Statistical Analysis.\nLouvain-La-Neuve, 2:1032–39.\n\n\nTottie, Gunnel. 2011. “Uh and Um as Sociolinguistic Markers in\nBritish English.” International Journal of Corpus\nLinguistics 16 (2): 173–97.\n\n\nUniversity of Colorado Boulder. 2008. “Switchboard Dialog Act\nCorpus. Web Download.” Linguistic Data Consortium.\n\n\nUryu, Shinya. 2024. Washoku: Extra ’Recipes’ for Japanese Text, Date\nand Address Processing.\n\n\nVoigt, Rob, Nicholas P. Camp, Vinodkumar Prabhakaran, William L.\nHamilton, Rebecca C. Hetey, Camilla M. Griffiths, David Jurgens, Dan\nJurafsky, and Jennifer L. Eberhardt. 2017. “Language from Police\nBody Camera Footage Shows Racial Disparities in Officer Respect.”\nProceedings of the National Academy of Sciences 114 (25):\n6521–26.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia,\nHao Zhu, and Shannon Ellis. 2022. Skimr: Compact and Flexible\nSummaries of Data. https://docs.ropensci.org/skimr/.\n\n\nWelbers, Kasper, and Wouter van Atteveldt. 2022. Rsyntax: Extract\nSemantic Relations from Text by Querying and Reshaping Syntax.\n\n\nWenfeng, Qin, and Wu Yanyi. 2019. jiebaR: Chinese Text\nSegmentation. https://github.com/qinwf/jiebaR/.\n\n\nWhite, John Myles. 2023. ProjectTemplate: Automates the Creation of\nNew Statistical Analysis Projects. http://projecttemplate.net.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\n———. 2023a. Stringr: Simple, Consistent Wrappers for Common String\nOperations. https://stringr.tidyverse.org.\n\n\n———. 2023b. Tidyverse: Easily Install and Load the Tidyverse.\nhttps://tidyverse.tidyverse.org.\n\n\n———. 2024. Rvest: Easily Harvest (Scrape) Web Pages. https://rvest.tidyverse.org/.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen,\nKohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, Dewey\nDunnington, and Teun van den Brand. 2024. Ggplot2: Create Elegant\nData Visualisations Using the Grammar of Graphics. https://ggplot2.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis\nVaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nWickham, Hadley, Maximilian Girlich, and Edgar Ruiz. 2023. Dbplyr: A\nDplyr Back End for Databases. https://dbplyr.tidyverse.org/.\n\n\nWickham, Hadley, and Lionel Henry. 2023. Purrr: Functional\nProgramming Tools. https://purrr.tidyverse.org/.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. Readr: Read\nRectangular Text Data. https://readr.tidyverse.org.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. Haven: Import\nand Export SPSS, Stata and SAS Files. https://haven.tidyverse.org.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2024. Tidyr:\nTidy Messy Data. https://tidyr.tidyverse.org.\n\n\nWijffels, Jan. 2023. Udpipe: Tokenization, Parts of Speech Tagging,\nLemmatization and Dependency Parsing with the UDPipe ’NLP’ Toolkit.\nhttps://bnosac.github.io/udpipe/en/index.html.\n\n\nWulff, S, A Stefanowitsch, and Stefan Th. Gries. 2007. “Brutal\nBrits and Persuasive Americans.” Aspects of Meaning.\n\n\nXie, Yihui. 2023. Tinytex: Helper Functions to Install and Maintain\nTeX Live, and Compile LaTeX Documents. https://github.com/rstudio/tinytex.\n\n\n———. 2024. Bookdown: Authoring Books and Technical Documents with r\nMarkdown. https://github.com/rstudio/bookdown.\n\n\nZipf, George Kingsley. 1949. Human Behavior and the Principle of\nLeast Effort. Oxford, England: Addison-Wesley Press.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "App A: Data",
    "section": "",
    "text": "A.1 ANC",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-anc",
    "href": "data.html#sec-data-anc",
    "title": "App A: Data",
    "section": "",
    "text": "ANC",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-brown",
    "href": "data.html#sec-data-brown",
    "title": "App A: Data",
    "section": "A.2 BNC",
    "text": "A.2 BNC\n\nBrown Corpus",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-cabnc",
    "href": "data.html#sec-data-cabnc",
    "title": "App A: Data",
    "section": "A.3 CABNC",
    "text": "A.3 CABNC\n\nCABNC",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-cedel2",
    "href": "data.html#sec-data-cedel2",
    "title": "App A: Data",
    "section": "A.4 CEDEL2",
    "text": "A.4 CEDEL2\n\nCEDEL2",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#sec-data-enntt",
    "href": "data.html#sec-data-enntt",
    "title": "App A: Data",
    "section": "A.5 ENNTT",
    "text": "A.5 ENNTT\n\nENNTT\n\n\n\nEuroparl\nFederalist Papers (LOC)\nSOTU\nSWDA\n…",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Quantitative Text Analysis for Linguistics",
    "section": "",
    "text": "About\nBook\nThe goal of this textbook is to provide readers with foundational knowledge and practical skills in quantitative text analysis using the R programming language.\nBy the end of this textbook, readers will be able to identify, interpret and evaluate data analysis procedures and results to support research questions within language science. Additionally, readers will gain experience in designing and implementing research projects that involve processing and analyzing textual data employing modern programming strategies. This textbook aims to instill a strong sense of reproducible research practices, which are critical for promoting transparency, verification, and sharing of research findings.\nThis textbook is geared towards advanced undergraduates, graduate students, and researchers looking to expand their methodological toolbox. It assumes no prior knowledge of programming or quantitative methods and prioritizes practical application and intuitive understanding over technical details.\nAuthor\nDr. Jerid Francom is Associate Professor of Spanish and Linguistics at Wake Forest University. His research focuses on the use of language corpora from a variety of sources (news, social media, and other internet sources) to better understand the linguistic and cultural similarities and differences between language varieties for both scholarly and pedagogical projects. He has published on topics including the development, annotation, and evaluation of linguistic corpora and analyzed corpora through corpus, psycholinguistic, and computational methodologies. He also has experience working with and teaching statistical programming with R.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "An Introduction to Quantitative Text Analysis for Linguistics",
    "section": "License",
    "text": "License\nThis work by Jerid C. Francom is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "An Introduction to Quantitative Text Analysis for Linguistics",
    "section": "Credits",
    "text": "Credits\n\nFont Awesome Icons are SIL OFL 1.1 Licensed",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "An Introduction to Quantitative Text Analysis for Linguistics",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe development of this book has benefited from the generous feedback from the following people: Andrea Bowling, Caroline Brady, Declan Golsen, Asya Little, Claudia Valdez, Laura Aull, Jack Nelson, (add your name here!). As always, any errors or omissions are my own.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#build-information",
    "href": "index.html#build-information",
    "title": "An Introduction to Quantitative Text Analysis for Linguistics",
    "section": "Build information",
    "text": "Build information\nThis textbook was built with the quarto package (Allaire and Dervieux 2024) and the bookdown package (Xie 2024) for R. The source code for this book is available on GitHub.\nThis version of the textbook was built with R version 4.3.3 (2024-02-29) on macOS Ventura 13.6.3 with the following packages:\n\n\n\n\n\n\n\npackage\nversion\nsource\n\n\n\ndplyr\n1.1.4\nCRAN (R 4.3.2)\n\n\nggplot2\n3.5.0\nCRAN (R 4.3.2)\n\n\nhere\n1.0.1\nCRAN (R 4.3.0)\n\n\nkableExtra\n1.4.0\nCRAN (R 4.3.2)\n\n\nknitr\n1.45\nCRAN (R 4.3.2)\n\n\nqtalrkit\n0.9.2\nGithub (qtalr/qtalrkit@e6c0ee871482c503bd8b159c5ea4dd562383fa66)\n\n\nreadr\n2.1.5\nCRAN (R 4.3.2)\n\n\nreprex\n2.1.0\nCRAN (R 4.3.2)\n\n\nrmarkdown\n2.26\nCRAN (R 4.3.3)\n\n\nrstudioapi\n0.15.0\nCRAN (R 4.3.1)\n\n\nscales\n1.3.0\nCRAN (R 4.3.2)\n\n\nstringr\n1.5.1\nCRAN (R 4.3.2)\n\n\ntibble\n3.2.1\nCRAN (R 4.3.2)\n\n\ntidyr\n1.3.1\nCRAN (R 4.3.2)\n\n\ntinytable\n0.0.5\nCRAN (R 4.3.2)\n\n\n\n\n\n\n\nAllaire, JJ, and Christophe Dervieux. 2024. Quarto: R Interface to Quarto Markdown Publishing System. https://github.com/quarto-dev/quarto-r.\n\n\nXie, Yihui. 2024. Bookdown: Authoring Books and Technical Documents with r Markdown. https://github.com/rstudio/bookdown.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "framing-research.html",
    "href": "framing-research.html",
    "title": "4  Research",
    "section": "",
    "text": "4.1 Frame\nTogether a research area, problem, aim and question and the research blueprint that forms the conceptual and practical scaffolding of the project ensure from the outset that the project is solidly grounded in the main characteristics of good research. These characteristics, summarized by Cross (2006), are found in Table 4.1.\nTable 4.1: Characteristics of research (Cross, 2006).\n\n\n\n\n\n\n\n\nCharacteristic\nDescription\n\n\n\nPurposive\nBased on identification of an issue or problem worthy and capable of investigation\n\n\nInquisitive\nSeeking to acquire new knowledge\n\n\nInformed\nConducted from an awareness of previous, related research\n\n\nMethodical\nPlanned and carried out in a disciplined manner\n\n\nCommunicable\nGenerating and reporting results which are feasible and accessible by others\nWith these characteristics in mind, let’s get started with the first component to address –connecting with the literature.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research</span>"
    ]
  },
  {
    "objectID": "framing-research.html#sec-fr-connect",
    "href": "framing-research.html#sec-fr-connect",
    "title": "4  Research",
    "section": "\n4.2 Connect",
    "text": "4.2 Connect\n\n4.2.1 Research area\nThe first decision to make in the research process is to identify a research area. A research area is a general area of interest where a researcher wants to derive insight and make a contribution to understanding. For those with an established research trajectory in language, the area of research to address through text analysis will likely be an extension of their prior work. For others, which include new researchers or researchers that want to explore new areas of language research or approach an area through a language-based lens, the choice of area may be less obvious. In either case, the choice of a research area should be guided by a desire to contribute something relevant to a theoretical, applied, and/ or practical matter of personal interest. Personal relevance goes a long way to developing and carrying out purposive and inquisitive research.\nSo how do we get started? Consider your interests in a language or set of languages, a discipline, a methodology, or some applied area. Language is at the heart of the human experience and therefore found in some fashion anywhere one seeks to find it. But it is a big world and more often than not the general question about what area to explore language use is sometimes the most difficult. To get the ball rolling, it is helpful to peruse disciplinary encyclopedias or handbooks of linguistics and language-related an academic fields (e.g. Encyclopedia of Language and Linguistics (Brown 2005), A Practical Guide to Electronic Resources in the Humanities (Dubnjakovic and Tomlin 2010), Routledge encyclopedia of translation technology (Chan 2014))\nA more personal, less academic, approach is to consult online forums, blogs, etc. that one already frequents or can be accessed via an online search. Through social media you may find particular people that maintain a blog worth browsing. Perusing these resources can help spark ideas and highlight the kinds of questions that interest you.\nRegardless of whether your inquiry stems from academic, professional, or personal interest, try to connect these findings to academic areas of research. Academic research is highly structured and well-documented and making associations with this network will aid in subsequent steps in developing a research project.\n\n4.2.2 Research problem\nOnce you’ve made a rough-cut decision about the area of research, it is now time to take a deeper dive into the subject area and jump into the literature. This is where the rich structure of disciplinary research will provide aid to traverse the vast world of academic knowledge and identify a research problem. A research problem highlights a particular topic of debate or uncertainty in existing knowledge which is worthy of study.\nSurveying the relevant literature is key to ensuring that your research is informed, that is, connected to previous work. Identifying relevant research to consult can be a bit of a ‘chicken or the egg’ problem –some knowledge of the area is necessary to find relevant topics, some knowledge of the topics is necessary to narrow the area of research. Many times the only way forward is to jump into conducting searches. These can be world-accessible resources (e.g. Google Scholar) or limited-access resources that are provided through an academic institution (e.g. Linguistics and Language Behavior Abstracts, ERIC, PsycINFO, etc.). Some organizations and academic institutions provide research guides to help researcher’s access the primary literature. There are even a new breed of search engines that are designed to help researchers aggregate and search academic literature (e.g. Scite, Elicit, etc.).\nAnother avenue to explore are journals and conference proceedings dedicated to linguistics and language-related research. Text analysis is a rapidly expanding methodology which is being applied to a wide range of research areas.\nTo explore research related to text analysis it is helpful to start with the (sub)discipline name(s) you identified in when selecting your research area, more specific terms that occur to you or key terms from the literature, and terms such as ‘corpus study’ or ‘corpus-based’. The results from first searches may not turn out to be sources that end up figuring explicitly in your research, but it is important to skim these results and the publications themselves to mine information that can be useful to formulate better and more targeted searches.\nRelevant information for honing your searches can be found throughout an academic publication. However, pay particular attention to the abstract, in articles, and the table of contents, in books, and the cited references. Abstracts and tables of contents often include discipline-specific jargon that is commonly used in the field. In some articles, there is even a short list of key terms listed below the abstract which can be extremely useful to seed better and more precise search results. The references section will contain relevant and influential research. Scan these references for publications which appear to narrowing in on topic of interest and treat it like a search in its own right.\nOnce your searches begin to show promising results it is time to keep track and organize these references. Whether you plan to collect thousands of references over a lifetime of academic research or your aim is centered around one project, software such as Zotero1, Mendeley, or BibDesk provide powerful, flexible, and easy-to-use tools to collect, organize, annotate, search, and export references. Citation management software is indispensable for modern research –and often free!\nAs your list of relevant references grows, you will want to start the investigation process in earnest. Begin skimming (not reading) the contents of each of these publications, starting with the most relevant first2. Annotate these publications using highlighting features of the citation management software to identify: (1) the stated goal(s) of the research, (2) the data source(s) used, (3) the information drawn from the data source(s), (4) the analysis approach employed, and (5) the main finding(s) of the research as they pertain to the stated goal(s).\nNext, in your own words, summarize these five key areas in prose adding your summary to the notes feature of the citation management software. This process will allow you to efficiently gather and document references with the relevant information to guide the identification of a research problem, to guide the formation of your problem statement, and ultimately, to support the literature review that will figure in your project write-up.\nFrom your preliminary annotated summaries you will undoubtedly start to recognize overlapping and contrasting aspects in the research literature. These aspects may be topical, theoretical, methodological, or appear along other lines. Note these aspects and continue to conduct more refine searches, annotate new references, and monitor for any emerging patterns of uncertainty or debate (gaps) which align with your research interest(s). When a promising pattern takes shape, it is time to engage with a more detailed reading of those references which appear most relevant highlighting the potential gap(s) in the literature.\nAt this point you can focus energy on more nuanced aspects of a particular gap in the literature with the goal to formulate a problem statement. A problem statement directly acknowledges a gap in the literature and puts a finer point on the nature and relevance of this gap for understanding. This statement reflects your first deliberate attempt to establish a line of inquiry. It will be a targeted, but still somewhat general, statement framing the gap in the literature that will guide subsequent research design decisions.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research</span>"
    ]
  },
  {
    "objectID": "framing-research.html#sec-fr-define",
    "href": "framing-research.html#sec-fr-define",
    "title": "4  Research",
    "section": "\n4.3 Define",
    "text": "4.3 Define\n\n4.3.1 Research aim\nWith a problem statement in hand, it is now time to consider the goal(s) of the research. A research aim frames the type of inquiry to be conducted. Will the research aim to explore, examine, or explain? As you can appreciate, the research aim is directly related to the analysis methods we touched upon in Chapter 3.\nTo gauge how to frame your research aim, reflect on the literature that led you to your problem statement and the nature of the problem statement itself. If the gap at the center of the problem statement is a lack of knowledge, your research aim may be exploratory. If the gap concerns a conjecture about a relationship, then your research may take a predictive approach. When the gap points to the validation of a relationship, then your research will likely be inferential in nature. Before selecting your research aim it is also helpful to consult the research aims of the primary literature that led you to your research statement.\nTypically, a problem statement addressing a subtle, specific issue tends to adopt research objectives similar to prior studies. In contrast, a statement focusing on a broader, more distinct issue is likely to have unique research goals. Yet, this is more of a guideline than a strict rule.\nIt’s crucial to understand both the existing literature and the nature of various types of analyses. Being clear about your research goals is important to ensure that your study is well-placed to produce results that add value to the current understanding in an informed manner.\n\n4.3.2 Research question\nThe next step in research design is to craft the research question. A research question is clearly defined statement which identifies an aspect of uncertainty and the particular relationships that this uncertainty concerns. The research question extends and narrows the line of inquiry established in the research statement and research aim. To craft a research question, we can use the research statment for the content and the research aim for the form.\nForm\nThe form of a research question will vary based on the research aim, which as I mentioned, is inimately connected to the analysis approach. For inferential-based research, the research question will actually be a statement, not a question. This statement makes a testable claim about the nature of a particular relationship –i.e. asserts a hypothesis.\nFor illustration, let’s posit a hypothesis (\\(H_1\\)), leaving aside the implicit null hypothesis (\\(H_0\\)), seen in Example 4.1.\n\nExample 4.1 Women use more questions than men in spontaneous conversations.\n\nFor predictive- and exploratory-based research, the research question is in fact a question. A reframing of the example hypothesis for a predictive-based research question might take the form seen in Example 4.2.\n\nExample 4.2 Can the number of questions used in spontaneous conversations predict if a speaker is male or female?\n\nAnd a similar exploratory-based research question might take the form seen in Example 4.3.\n\nExample 4.3 Do men and women differ in terms of the number of questions they use in spontaneous conversations?\n\nThe central research interest behind these hypothetical research questions is, admittedly, quite basic. But from these simplified examples, we are able to appreciate the similarities and differences between the forms of research statements that correspond to distinct research aims.\nContent\nIn terms of content, the research question will make reference to two key components. First, is the unit of analysis. The unit of analysis is the entity which the research aims to investigate. For our three example research aims, the unit of analysis is the same, namely speakers. Note, however, that the current unit of analysis is somewhat vague in the example research questions. A more precise unit of analysis would include more information about the population from which the speakers are drawn (i.e. English speakers, American English speakers, American English speakers of the Southeast, etc.).\nThe second key component is the unit of observation. The unit of observation is the primary element on which the insight into the unit of analysis is derived and in this way constitutes the essential organizational unit of the dataset to be analyzed. In our examples, the unit of observation, again, is unchanged and is spontaneous conversations. Note that while the unit of observation is key to identify as it forms the organizational backbone of the research, it is very common for the research to derive variables from this unit to provide evidence to investigate the research question.\nIn examples 4.1, 4.2, and 4.3, we identified the number of conversations as part of the research question. Later in the research process it will be key to operationalize this variable. For example, will the number of conversations be the total number of conversations in the dataset or will it be the average number of conversations per speaker? These are important questions to consider as they will influence variable selection, statistical choices, and ultimately the interpretation of the results.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research</span>"
    ]
  },
  {
    "objectID": "framing-research.html#sec-fr-blueprint",
    "href": "framing-research.html#sec-fr-blueprint",
    "title": "4  Research",
    "section": "\n4.4 Blueprint",
    "text": "4.4 Blueprint\nThe efforts to develop a research question will produce a clear and focused line of inquiry with the necessary background literature and a well-defined problem statement that forrms the basis of purposeful, inquisitive, and informed research (returning to Cross’s characteristics of research in Table 4.1).\nMoving beyond the research question in the project means developing and laying out the research design in a way such that the research is methodical and communicable. In this textbook, the method to achieve these goals is through the development of a research blueprint. The blueprint includes two components: (1) the conceptual plan and (2) the organizational scaffolding that will support the implementation of the research.\nAs Ignatow and Mihalcea (2017) point out:\n\nResearch design is essentially concerned with the basic architecture of research projects, with designing projects as systems that allow theory, data, and research methods to interface in such a way as to maximize a project’s ability to achieve its goals […]. Research design involves a sequence of decisions that have to be taken in a project’s early stages, when one oversight or poor decision can lead to results that are ultimately trivial or untrustworthy. Thus, it is critically important to think carefully and systematically about research design before committing time and resources to acquiring texts or mastering software packages or programming languages for your text mining project.\n\nIn what follows, I will cover the main aspects of developing a research blueprint. I will start with the conceptual plan and then move on to the organizational scaffolding.\n\n4.4.1 Plan\nImportance of establishing a feasible research design from the outset and documenting the key aspects required to conduct the research cannot be understated. On the one hand, this process links a conceptual plan to a tangible implementation. In doing so, a researcher is better-positioned to conduct research with a clear view of what will be entailed. On the other hand, a promising research question may present unexpected challenges once a researcher sets about to implement the research. This is not uncommon to encounter issues that require modification or reevaluation of the viability of the project. However, a well-documented research plan will help a researcher to identify and address many of these challenges at the conceptual level before expending unnecessary effort during implementation.\nLet’s now consider the subsequent steps to develop a research plan, outlined in Table 4.2.\n\n\nTable 4.2: Research plan checklist\n\n\n\n\n\n\n\n\nStep\nStage\nActivity\n\n\n\n1\nResearch Question or Hypothesis\nFormulate a research question or hypothesis based on a thorough review of existing literature including references. This will guide every subsequent step from data selection to interpretation of results.\n\n\n2\nData Source(s)\nIdentify viable data source(s) and vet the sample data in light of the research question. Consider to what extent the goal is to generalize findings to a target population, and ensure that the corpus aligns as much as feasible with this target.\n\n\n3\nKey Variables\nDetermine the key variables needed for the research, define how they will be operationalized, and ensure they can be derived from the corpus data. Additionally, identify any features that need to be extracted, recoded, generated, or integrated from other data sources.\n\n\n4\nAnalysis Method\nChoose an appropriate method of analysis to interrogate the dataset. This choice should be in line with your research aim (e.g., exploratory, predictive, or inferential). Be aware of what each method can offer and how it addresses your research question.\n\n\n5\nInterpretation & Evaluation\nEstablish criteria to interpret and evaluate the results. This will be a function of the relationship between the research question and the analysis method.\n\n\n\n\n\n\n\nFirst, identify a viable data source. Viability includes the accessibility of the data, availability of the data, and the content of the data. If a purported data source is not accessible and/ or it has stringent restrictions on its use, then it is not a viable data source. If a data source is accessible and available, but does not contain the building blocks needed to address the research question, then it is not a viable data source. A corpus resource should represent, to the extent feasible, with the target population(s).\n\n\nThe second step is to identify the key variables needed to conduct the research and then ensure that this information can be derived from the corpus data. The research question will reference the unit of analysis and the unit of observation, but it is important to pinpoint what the key variables will be. We want to envision what needs to be done to derive these variables. There may be features that need to be extracted, recoded, generated, and/ or integrated from other sources to address the research question, as discussed in Chapter 2.\n\nThe third step is to identify a method of analysis to interrogate the dataset. The selection of the analysis approach that was part of the research aim (i.e. explore, predict, or infer) and then the research question goes a long way to narrowing the methods that a researcher must consider. But there are a number of factors which will make some methods more appropriate than others.\nExploratory research is the least restricted of the three types of analysis approaches. Although it may be the case that a research will not be able to specify from the outset of a project what the exact analysis methods will be, an attempt to consider what types of analysis methods will be most promising to provide results to address the research question goes a long way to steering a project in the right direction and grounding the research. As with the other analysis approaches, it is important to be aware of what the analysis methods available and what type of information they produce in light of the research question.\nFor predictive-based research, the informational value of the outcome variable is key to deciding whether the prediction will be a classification task or a regression task. This has downstream effects when it comes time to evaluate and interpret the results. Although the feature engineering process in predictive analyses means that the features do not need to be specified from the outset and can be tweaked and changed as needed during an analysis, it is a good idea to start with a basic sense of what features most likely will be helpful in developing a robust predictive model.\nIn inferential research, the number and information values of the variables to be analyzed will be of key importance (Gries 2013). The informational value of the response variable will again narrow the search for the appropriate method. The number of explanatory variables also plays an important role. These details need not be nailed down at this point, but it is helpful to have them on your radar to ensure that when the time comes to analyze the data, the appropriate steps are taken to apply the correct test.\nThe last of the main components of the research plan concerns the interpretation and evaluation of the results. This step brings the research plan full circle connecting the research question to the methods employed. It is important to establish from the outset what the criteria will be to evaluate the results. This is in large part a function of the relationship between the research question and the analysis method. For example, in exploratory research, the results will be evaluated qualitatively in terms of the associative patterns that emerge. Predictive and inferential research leans more heavily on quantitative metrics in particular the accuracy of the prediction or the strength of the relationship between the response and explanatory variable(s), respectively. However, these quantitative metrics require qualitative interpretation to determine whether the results are meaningful in light of the research question.\n\nIn addition to addressing the steps outlined in Table 4.2, it is also important to document the strengths and shortcomings of the research plan including the data source(s), the information to be extracted from the data, and the analysis methods. If there are potential shortcomings, which there most often are, sketch out contingency plans to address these shortcomings. This will help buttress your research and ensure that your time and effort is well-spent.\nThe research plan together with the information collected to develop the research question is known as a prospectus. A prospectus is a document that outlines the key aspects of the research plan and is used to guide the research process. It is a living document that will be updated as the research progresses and as new information is collected.\n\n\n\n\n\n\n Dive deeper\nYou may consider pre-registering your prospectus to ensure that your plans are well-documented and to provide a timestamp for your research. Pre-registration can also be a helpful way to get feedback on your research from colleagues and experts in the field. Popular pre-registration platforms include Open Science Framework and Center for Open Science.\n\n\n\n\n4.4.2 Scaffold\nThe next step in developing a research blueprint is to consider how to physically implement your project. This includes how to organize files and directories in a fashion that both provides the researcher a logical and predictable structure to work with but also ensures that the research is Communicable. On the one hand, communicable research includes a strong write-up of the research, but, on the other hand, it is also important that the research is reproducible.\nReproducibility strategies are a benefit to the researcher (in the moment and in the future) as it leads to better work habits and to better teamwork and it makes changes to the project easier. Reproducibility is also of benefit to the scientific community as shared reproducible research enhances replicability and encourages cumulative knowledge development (Gandrud 2015).\nIn Table 4.3, I outline a set of guiding principles that characterize reproducible research (Gentleman and Temple Lang 2007; Marwick, Boettiger, and Mullen 2018).\n\n\nTable 4.3: Reproducible Research Principles\n\n\n\n\n\n\n\n\nNo.\nPrinciple\nDescription\n\n\n\n1\nPlain text\nAll files should be plain text which means they contain no formatting information other than whitespace.\n\n\n2\nClear separation\nThere should be a clear separation between the inputs, process steps, and outputs of research. This should be apparent from the directory structure.\n\n\n3\nOriginal data\nA separation between original data and data created as part of the research process should be made. Original data should be treated as ‘read-only’. Any changes to the original data should be justified, generated by the code, and documented (see point 7).\n\n\n4\nModular scripts\nEach computing file (script) should represent a particular, well-defined step in the research process.\n\n\n5\nModular files\nEach script should be modular –that is, each file should correspond to a specific goal in the analysis procedure with input and output only corresponding to this step.\n\n\n6\nMain script\nThe project should be tied together by a ‘main’ script that is used to coordinate the execution of all the project steps.\n\n\n7\nDocument everything\nEverything should be documented. This includes data collection, data preprocessing, processing steps, script code comments, data description in data dictionaries, information about the computing environment and packages used to conduct the analysis, and detailed instructions on how to reproduce the research.\n\n\n\n\n\n\nThese seven principles in Table 4.3 can be physically implemented in numerous ways. In recent years, there has been a growing number of efforts to create R packages and templates to quickly generate the scaffolding and tools to facilitate reproducible research. Some notable R packages include workflowr (Blischak, Carbonetto, and Stephens 2023), ProjectTemplate (White 2023), and targets (Landau 2024), but there are many other resources for R included on the CRAN Task View for Reproducible Research.\nThere are many advantages to working with pre-existing frameworks for the savvy R programmer including the ability to quickly generate a project scaffold, to efficiently manage changes to the project, and to buy in to a common framework that is supported by a community of developers.\nOn the other hand, these frameworks can be a bit daunting for the novice R programmer. At the most basic level, a project can implement the seven principles outlined above with a directory structure and a set of key files seen in Example 4.4.\n\nExample 4.4 Minimal Project Framework\nproject/\n├── input/\n│   └── ...\n├── code/\n│   └── ...\n├── output/\n│   └── ...\n├── DESCRIPTION\n├── Makefile\n└── README\n\nThe project/ directory is composed of three main sections: input/, code/, and output/ making the destinction between each transparent in the directory structure. The input/ will house the data used and created in the project, ensuring that the original data is kept separate from the data created in the research process. The code/ section will house the scripts that will conduct the project steps including acquiring, curating, transforming, and analyzing the data. These scripts will read and write data and generate output including figures, reports, results, and tables. Lastly, the output/ section will house the resulting output from the project steps.\nAt the root of the project directory are three files which describe, document, and execute the project. The Makefile is used to automate the execution of the project steps. In effect, it is a script that runs scripts. In addition to coordinating the execution of the project steps, a Makefile will often include commands to set up the computing environment and packages. The README and DESCRIPTION files provide on overview of the project from both a conceptual and technical perspective. The README file includes a description of the project rationale, aims, and findings and instructions on how to reproduce the research. The DESCRIPTION file includes technical information about the computing environment and packages used to conduct the analysis.\nThe project structure in Example 4.4 meets the minimal structural requirements for reproducible research and is a good starting point for a project scaffold. However, aspects of this structure can be adjusted in minimal or more sophisticated ways to meet the needs of a particular project while still conforming to the principles outlined in Table 4.3, when we return to this topic in Chapter 11.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research</span>"
    ]
  },
  {
    "objectID": "framing-research.html#activities",
    "href": "framing-research.html#activities",
    "title": "4  Research",
    "section": "Activities",
    "text": "Activities\nThe following activities will build on your experience with R and cloning a GitHub repository, and recent experience with understanding the computing environment. The goal will be to bring you up to speed such that you can begin to work on your own research projects and understand how to use the tools and resources available to you to manage your project.\n\n\n\n\n\n\n Recipe\nWhat: Understanding the computing environmentHow: Read Recipe 4, complete comprehension check, and prepare for Lab 4.Why: To introduce components of the computing environment and how to manage a reproducible research project structure.\n\n\n\n\n\n\n\n\n\n Lab\nWhat: Scaffolding reproducible researchHow: Clone, fork, and complete the steps in Lab 4.Why: To establish a repository and project structure for reproducible research and apply new Git and Github skills to fork, clone, commit, and push changes.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research</span>"
    ]
  },
  {
    "objectID": "framing-research.html#summary",
    "href": "framing-research.html#summary",
    "title": "4  Research",
    "section": "Summary",
    "text": "Summary\nThe aim of this chapter is to provide the key conceptual and practical points to guide the development of a viable research project. Good research is purposive, inquisitive, informed, methodological, and communicable. It is not, however, always a linear process. Exploring your area(s) of interest and connecting with existing work will help couch and refine your research. But practical considerations, such as the existence of viable data, technical skills, and/ or time constrains, sometimes pose challenges and require a researcher to rethink and/ or redirect the research in sometimes small and other times more significant ways. The process of formulating a research question and developing a viable research plan is key to supporting viable, successful, and insightful research. To ensure that the effort to derive insight from data is of most value to the researcher and the research community, the research should strive to be methodological and communicable adopting best practices for reproducible research.\n\n\n\n\n\n\n\nFigure 4.1: Framing research: visual summary\n\n\n\n\nThis chapter concludes the Foundations part of this textbook. At this stage our overview of fundamental characteristics of research are in place to move a project towards implementation, as seen in Figure 4.1. From this point forward we will integrate your conceptual knowledge and emerging R programming skills as we cover common scenarios encountered when conducting reproducible research with real-world data.\nThe next part, Preparation, aims to cover R coding strategies to acquire, curate, and transform data in preparation for analysis. These are the first steps in putting a research blueprint into action and by no coincidence the first components in the Data to Insight Hierarchy. Without further ado, let’s get started!\n\n\n\n\nBlischak, John, Peter Carbonetto, and Matthew Stephens. 2023. Workflowr: A Framework for Reproducible and Collaborative Data Science. https://github.com/workflowr/workflowr.\n\n\nBrown, Keith. 2005. Encyclopedia of Language and Linguistics. Vol. 1. Elsevier.\n\n\nChan, Sin-wai. 2014. Routledge Encyclopedia of Translation Technology. Routledge.\n\n\nCross, Nigel. 2006. “Design as a Discipline.” Designerly Ways of Knowing, 95–103.\n\n\nDubnjakovic, Ana, and Patrick Tomlin. 2010. A Practical Guide to Electronic Resources in the Humanities. Elsevier.\n\n\nGandrud, Christopher. 2015. Reproducible Research with r and r Studio. Second edition. CRC Press.\n\n\nGentleman, Robert, and Duncan Temple Lang. 2007. “Statistical Analyses and Reproducible Research.” Journal of Computational and Graphical Statistics 16 (1): 1–23.\n\n\nGries, Stefan Th. 2013. Statistics for Linguistics with r. A Practical Introduction. 2nd revise.\n\n\nIgnatow, Gabe, and Rada Mihalcea. 2017. An Introduction to Text Mining: Research Design, Data Collection, and Analysis. Sage Publications.\n\n\nLandau, William Michael. 2024. Targets: Dynamic Function-Oriented Make-Like Declarative Pipelines. https://docs.ropensci.org/targets/.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging Data Analytical Work Reproducibly Using r (and Friends).” The American Statistician 72 (1): 80–88.\n\n\nWhite, John Myles. 2023. ProjectTemplate: Automates the Creation of New Statistical Analysis Projects. http://projecttemplate.net.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research</span>"
    ]
  },
  {
    "objectID": "framing-research.html#footnotes",
    "href": "framing-research.html#footnotes",
    "title": "4  Research",
    "section": "",
    "text": "Zotero Guide↩︎\nOr what appears to be most relevant. This may change as you start to take a closer look.↩︎",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Research</span>"
    ]
  }
]