[
  {
    "objectID": "prediction.html#sec-pda-orientation",
    "href": "prediction.html#sec-pda-orientation",
    "title": "9  Prediction",
    "section": "\n9.1 Orientation",
    "text": "9.1 Orientation\nIn this section, I introduce the concept of supervised learning and provide an overview of the workflow for building and evaluating predictive models for text analysis. First we will discuss the research goals that are typically addressed using supervised learning, contrasting them with the goals of exploratory and inferential analysis. Next, we will discuss the approaches that are typically used to address these goals, including the types of data structures and algorithms that are used. Finally, we will discuss the workflow for building predictive models, including the steps for preparing data, training and testing models, and evaluating and reporting results.\n\n9.1.1 Research goal\nPredictive data analysis (PDA) is a powerful analysis method for linguists and other researchers interested in making predictions about new or future data based on patterns in existing data. As discussed in Section 3.2.2 and Section 4.4.1, PDA is a type of supervised learning, which means that it involves training a model on a labeled dataset where the input data and desired output are both provided. The model is able to make predictions or classifications based on the input data by learning the relationships between the input and output data. Supervised machine learning is an important tool for linguists studying language and communication, as it allows us to analyze language data to identify patterns or trends in language use, verify hypotheses, and prescribe actions.\nIn contrast to EDA, PDA does require that we have a particular goal in mind from the outset. This goal is to predict a fixed outcome variable based on a set of predictor variables. However, PDA can be applied with distinct aims in mind: exploratory-oriented or hypothesis-driven prediction. In exploratory-oriented prediction, the goal is to examine potential relationships between an outcome and a mutable set of predictor variables. The results of this type of analysis can be used to generate new insight and questions. In hypothesis-driven prediction, in contrast, the goal is to use predictive data analysis to test a hypothesis about the relationship between the outcome and a pre-defined set of predictor variables. In this case, the results of the analysis is used to explain or infer a relationship between the outcome and predictor variables.\n\n\n\n\n\n\n Dive deeper\n\nMuPDAR approach (Multifactorial Prediction and Deviation Analysis with Regressions) (Deshors and Gries 2016; Gries and Deshors 2014) and\n\nTraining on group\nTesting on another group (or groups)\n\n\nDiscriminant analysis (Baayen 2011)\n\n\n\n\n\nIt is important to establish which aim is being taken, as this will influence the approach that is taken.\n\n9.1.2 Approach\n\nThe approach to conducting predictive analysis shares some commonalities with exploratory data analysis (Section 8.1.2) (as well as inferential analysis Chapter 10), but there are also some key differences. Let’s look at the workflow in Table 9.1 and then discuss these commonalities and differences.\n\n\n\nTable 9.1: Workflow for predictive data analysis\n\n\n\n\n\n\nStep\nName\nDescription\n\n\n\n1\nIdentify\nConsider the research question and aim and identify relevant variables\n\n\n2\nInitial split\nSplit the data into representative training and testing sets\n\n\n3\nIntegrate\nApply variable selection and engineering procedures\n\n\n4\nInspect\nInspect the data to ensure that it is in the correct format and that the training and testing sets are representative of the data\n\n\n5\nInterrogate\nTrain and evaluate the model on the training set, adjusting models or hyperparameters as needed, to produce a final model\n\n\n6\nInterpret\nInterpret the results of the final model in light of the research question or hypothesis\n\n\n7\n(Optional) Iterate\nRepeat steps 3-6 to selecting new variables\n\n\n\n\nFocusing on the overlap with other analysis methods, we can see some fundamentals steps such as identifying relevant variables, inspecting the data, interrogating the data, and interpreting the results. And if our research aim is exploratory in nature, iteration may also be a part of the workflow. These steps highlight the importance conducting methodologic and communicable research, as discussed in Section 4.1.\nThere are two main differences, however, between the PDA and the EDA workflow we discussed in Chapter 8. The first, reflected the majority of the steps in the workflow, is that PDA requires partitioning the data into training and testing sets. As discussed in Section 3.2.2, the training set is used to develop the model, and the testing set is used to evaluate the model’s performance. This strategy is used to ensure that the model is robust and generalizes well to new data. It is well known, and makes intuitive sense, that using the same data to develop and evaluate a model likely will not produce a model that generalizes well to new data. This is because the model will have potentially conflated the nuances of the data (‘the noise’) with any real trends (‘the signal’) and therefore will not be able to generalize well to new data. This is called overfitting and by holding out a portion of the data for testing, we can evaluate the model’s performance on data that it has not seen before and therefore get a more accurate estimate of the generalizable trends in the data.\nAnother procedure to avoid the perils of overfitting, is to use resampling methods as part of the model evaluation on the training set. Resampling is the process of repeatedly drawing samples from the training set and evaluating the model on each sample. The two most common resampling methods are bootstrapping (resampling with replacement) and cross-validation (resampling without replacement). The performance of these multiple models are summarized and the error between them is assessed. The goal is to minimize the performance differences between the models while maximizing the overall performance. These measures go a long way to avoiding overfitting and therefore maximizing the chance that the training phase will produce a model which is robust at the testing phase.\nThe second difference, not reflected in the workflow but inherent in predictive analysis, is that PDA requires a fixed outcome variable. This means that the outcome variable must be defined from the outset and cannot be changed during the analysis. Furthermore, the informational nature of the outcome variable will dictate the what type of algorithm we choose to interrogate the data and how we will evaluate the model’s performance. If the outcome is categorical in nature, we will use a classification algorithm (e.g. logistic regression, naive bayes, etc.). Classification evaluation metrics include accuracy, precision, recall, and F1 score which can be derived from and visualized in a cross-tabulation of the predicted and actual outcome values.\n\nTo understand these measures it is helpful to consider a confusion matrix, which is a table that describes the performance of a classification model on data for which the true values are known. The confusion matrix is a two-by-two matrix that shows the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN), as seen in Table 9.2.\n\n\n\n\nTable 9.2: A labeled confusion matrix\n\n\nActual positive\nActual negative\n\n\n\nPredicted positive\nTP\nFP\n\n\nPredicted negative\nFN\nTN\n\n\n\n\n\n\n\n\nIf the outcome is numeric in nature, we will use a regression algorithm (e.g. linear regression, support vector regression, etc.). Since the difference between prediction and actual values is numeric, metrics that quantify numerical differences, such as root mean square error (RMSE) or \\(R^2\\), are used to evaluate the model’s performance.\n\n\n\n\nFigure 9.1: A plot of the actual and predicted values for a regression model\n\n\n\nThe evaluation of the model is quantitative on the one hand, but it is also qualitative in that we need to consider the implications of the model’s performance in light of the research question or hypothesis. Furthermore, depending on our research question we may be interested in exploring the features that are most important to the model’s performance. This is called feature importance and can be derived from the model’s coefficients or weights. Notably, however, some of the most powerful models in use today, such as deep neural networks, are not easily interpretable and therefore feature importance is not easily derived. This is something to keep in mind when considering the research question and the type of model that will be used to address it."
  },
  {
    "objectID": "prediction.html#sec-pda-analysis",
    "href": "prediction.html#sec-pda-analysis",
    "title": "9  Prediction",
    "section": "\n9.2 Analysis",
    "text": "9.2 Analysis\n\nIn this section we now turn to the practical application of predictive data analysis. The dicussion will be separated into classification and regression tasks, as model selection and evaluation procedures differ between the two. For each task, we will frame a research goal and work through the process of building a predictive model to address that goal. Along the way we will cover concepts and methods that are common to both classification and regression tasks and specific to each.\n\nTo frame our analyses, we will posit research aimed at identifying language usage patterns in second language use, one for a classification task and one for a regression task. Our first research question will be to identify potential salient differences in Spanish language use between natives and L1 English learners (categorical). Our second research question will be to gauge the extent to which the the L1 English learners’ Spanish language placement test scores (numeric) can be predicted based on their language use.\nWe will use data from the CEDEL2 corpus1. We will include a subset of the variables from this data that are relevant to our research questions. The data dictionary for this dataset is seen in Table 9.3.\n\n\n\n\nTable 9.3: Data dictionary for the CEDEL2 corpus\n\nvariable\nname\nvariable_type\ndescription\n\n\n\ndoc_id\nDocument ID\nnumeric\nUnique identifier for each document\n\n\nsubcorpus\nSubcorpus\ncategorical\nThe subcorpus to which the document belongs ('Learner' or 'Native')\n\n\nplacement_score\nPlacement Score\nnumeric\nThe score obtained by the document author in a placement test. Null values indicate missing data (i.e. the document author did not take the placement test)\n\n\nproficiency\nProficiency\nordinal\nThe level of language proficiency of the document author ('Upper intermediate', 'Lower advanced', 'Upper beginner', or 'Native')\n\n\ntext\nText\ncharacter\nThe written text provided by the document author\n\n\n\n\n\n\n\n\nWe will be using the tidymodels framework in R to perform this analysis. tidymodels is a metapackage, much like tidyverse, that provides a consistent interface for modeling and machine learning. Some key packages unique to tidymodels are recipes, parsnip, workflows, and tune. recipes includes functions for preprocessing and engineering features. parsnip provides a consistent interface for specifying modeling algorithms. worflows allows us to combine recipes and models into a single pipeline. Finally, tune give us the ability to evaluate and tune hyperparameters of models.\nSince we are using text data, we will also be using the textrecipes package which makes various functions available for preprocessing text and extracting and engineering features.\nLet’s go ahead and do the setup, loading the necessary packages and data. This is seen in Example 9.1.\n\nExample 9.1  \n\n# Load packages\nlibrary(tidymodels)   # modeling metapackage\nlibrary(textrecipes)  # text preprocessing\nlibrary(janitor)      # data inspection\n\n# Set global options\ntidymodels_prefer()   # prefer tidymodels functions over other functions with the same name\n\n# Read in the dataset\ncedel2_df &lt;- \n  read_csv(\"../data/cedel2/cedel2_df.csv\")\n\n# Preview\ncedel2_df |&gt; glimpse()\n\n\n\n&gt; Rows: 2,957\n&gt; Columns: 5\n&gt; $ doc_id          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n&gt; $ subcorpus       &lt;chr&gt; \"Learner\", \"Learner\", \"Learner\", \"Learner\", \"Learner\",…\n&gt; $ placement_score &lt;dbl&gt; 14.0, 16.3, 16.3, 18.6, 18.6, 18.6, 20.9, 20.9, 20.9, …\n&gt; $ proficiency     &lt;chr&gt; \"Lower beginner\", \"Lower beginner\", \"Lower beginner\", …\n&gt; $ text            &lt;chr&gt; \"Yo vivo es Alanta, Georgia. Atlanta es muy grande ciu…\n\n\n\nThe results from Example 9.1 show that we have five variables and 2957 observations. The primary variables for the classification task are the subcorpus variable and the text variable. The placement_score variable is the outcome variable for the regression task.\n\n\n9.2.1 Text classification\n\n\nThe goal of this analysis is to classify texts as either native or learner based on the writing samples. This is a binary classification problem. We will approach this problem from an exploratory perspective, and therefore our aim is to derive features from the text that best distinguish between the two classes.\n\nLet’s modify the data frame to include only the variables we need for this analysis. In the process, we will rename the subcorpus variable to outcome to reflect that it is the outcome variable and convert it to a factor vector to meet requirements of the modeling functions we will use in our analysis. This is seen in Example 9.2.\n\nExample 9.2  \n\n# Select variables and factor outcome\ncedel2_cls_df &lt;- \n  cedel2_df |&gt; \n  select(outcome = subcorpus, proficiency, text) |&gt; \n  mutate(outcome = factor(outcome))\n\n# Preview\ncedel2_cls_df |&gt; glimpse()\n\n&gt; Rows: 2,957\n&gt; Columns: 3\n&gt; $ outcome     &lt;fct&gt; Learner, Learner, Learner, Learner, Learner, Learner, Lear…\n&gt; $ proficiency &lt;chr&gt; \"Lower beginner\", \"Lower beginner\", \"Lower beginner\", \"Low…\n&gt; $ text        &lt;chr&gt; \"Yo vivo es Alanta, Georgia. Atlanta es muy grande ciudad.…\n\n\n\nTo view the distribution of the outcome variable between the two levels we can use the tabyl() function from the janitor package, as seen in Example 9.3.\n\nExample 9.3  \n\n# View the distribution of the outcome variable\ncedel2_cls_df |&gt; \n  tabyl(outcome) |&gt; \n  adorn_pct_formatting(digits = 1)\n\n&gt;  outcome    n percent\n&gt;  Learner 1906   64.5%\n&gt;   Native 1051   35.5%\n\n\n\nSo a little less than two-thirds of the texts are from learners. It is important to gauge the distribution of the outcome variable to see if it is balanced or imbalanced. The classes need not be perfectly balanced, but if they are wildly imbalanced it can cause problems for the model.\n\nSo in step 1 of our workflow (from Table 9.1), we need to identify the features that we will use to classify the texts. There may be many features that we could use. These could be features derived from raw text (e.g. characters, words, n-grams, etc.), feature vectors (e.g. word embeddings), or meta-linguistic features (e.g. part-of-speech tags, syntactic parses, or semantic features) that have been derived from these through manual or automatic annotation.\n\nLet’s start simple and use a bag-of-words approach to classify texts where words are the features. This is a simple approach that is often used as a baseline for more complex models. If it doesn’t work well, we can try something else.\nThis provides us the linguistic unit we will use but we still need to decide how to represent these words. Do we use raw token counts? Do we use normalized frequencies? Do we use some type of weighting scheme? These are questions that we need to consider as we embark on this analysis. Since we are exploring we can use trial-and-error or consider the implications of each approach and choose the one that best fits our research question –or both.\nAs a first pass, let’s use raw token counts with our word features.\n\nWith our features identified, we can move on to step 2 of our workflow and split the data into training and testing sets. We make the splits to our data at this point to draw a line in the sand between the data we will use to train the model and the data we will use to test the model. A typical approach in supervised machine learning is to allocate around 75-80% of the data to the training set and the remaining 20-25% to the testing set, depending on the number of observations. We have 2957 observations in our data set, so we can allocate 80% of the data to the training set and 20% of the data to the testing set.\nIn Example 9.4, we will use the initial_split() function from the rsample package to split the data into training and testing sets. The initial_split() function takes a data frame and a proportion and returns a split object which contains the training and testing sets. We will use the strata argument to stratify the data by the outcome variable. This will ensure that the training and testing sets have the same proportion of native and learner texts.\n\nExample 9.4  \n\n# Split the data into training and testing sets\ncedel2_cls_split &lt;-\n  initial_split(\n    data = cedel2_cls_df, \n    prop = 0.8, \n    strata = outcome\n    )\n\n# Create training set\ncedel2_cls_train &lt;- training(cedel2_cls_split)  # 80% of data\n\n# Create testing set\ncedel2_cls_test &lt;- testing(cedel2_cls_split)    # 20% of data\n\n\nA confirmation of the distribution of the data across the training and testing sets as well as a break down of the outcome variable can be seen in Example 9.5.\n\nExample 9.5  \n\n# View the distribution of the outcome variables\ncedel2_cls_train |&gt; \n  tabyl(outcome) |&gt; \n  adorn_totals(\"row\") |&gt;\n  adorn_pct_formatting(digits = 1)\n\n&gt;  outcome    n percent\n&gt;  Learner 1524   64.5%\n&gt;   Native  840   35.5%\n&gt;    Total 2364  100.0%\n\ncedel2_cls_test |&gt;\n  tabyl(outcome) |&gt; \n  adorn_totals(\"row\") |&gt;\n  adorn_pct_formatting(digits = 1)\n\n&gt;  outcome   n percent\n&gt;  Learner 382   64.4%\n&gt;   Native 211   35.6%\n&gt;    Total 593  100.0%\n\n\n\nWe can see that the split was successful. The training and testing sets have very similiar proportion of native and learner texts.\n\nWe are now ready to create a ‘recipe’, step 3 in our analysis. A recipe is a set of instructions or blueprint which specify the outcome variable and the feature variable and determines how to preprocess and engineer the feature variables.\nWe will use the recipe() function from the recipes package to create the recipe. The recipe() function minimally takes a formula and a data frame and returns a recipe object. The formula specifies the outcome variable (\\(y\\)) and the feature variable(s) (\\(x_1 .. x_n\\)). For example y ~ x can be read as “y is a function of x”. In our particular case, we will use the formula outcome ~ text to specify that the outcome variable is the outcome variable and the feature variable is the text variable. The code is seen in Example 9.6.\n\nExample 9.6  \n\n# Create a recipe\ncedel2_cls_rec &lt;- \n  recipe(\n    outcome ~ text, \n    data = cedel2_cls_train\n    )\n\n# Preview\ncedel2_cls_rec\n\n\nThe recipe object at this moment contains just one instruction, what the variables are and what their relationship is.\n\n\n\n\n\n\n Tip\nR formulas are a powerful way to specify relationships between variables and are used extensively in data modeling including exploratory, predictive, and inferential analysis. The basic formula syntax is y ~ x where y is the outcome variable and x is the feature variable. The formula syntax can be extended to include multiple feature variables, interactions, and transformations. For more information on R formulas, see R for Data Science.\n\n\n\nThe recipes package provides a wide range of step_*() functions which can be applied to the recipe to specify how to engineer the variables in our recipe call. These include functions to scale (e.g step_center(), step_scale(), etc.) and transform (e.g. step_log(), step_pca(), etc.) numeric variables, and functions to encode (e.g. step_dummy(), step_labelencode(), etc.) categorical variables.\nThese step functions are great when we have selected the variables we want to use in our model and we want to engineer them in a particular way. In our case, however, we need to derive features from the text in the text column of datasets before we engineer them. To ease this process, the textrecipes package provides a number of step functions for preprocessing text data. These include functions to tokenize (e.g. step_tokenize()), remove stop words (e.g. step_stopwords()), and to derive meta-features (e.g. step_lemma(), step_stem(), etc.) 2. Furthermore, there are functions to engineer features in ways that are particularly relevant to text data, such as feature frequencies and weights (e.g. step_tf(), step_tfidf(), etc.) and token filtering (e.g. step_tokenfilter()).\nSo let’s build on our basic recipe cedel2_cls_rec by adding steps relevant to our task. To extract our features, we will use the step_tokenize() function to tokenize the text into words. The default behavior of the step_tokenize() function is to tokenize the text into words, but other token units can be derived and various options can be added to the function call (as the tokenizers package is used under the hood). Adding the step_tokenize() function to our recipe is seen in Example 9.7.\n\nExample 9.7  \n\n# Add step to tokenize the text\ncedel2_cls_rec &lt;- \n  cedel2_cls_rec |&gt; \n  step_tokenize(text)\n\n# Preview\ncedel2_cls_rec\n\n\nThe recipe object now contains two instructions, one for the outcome variable and one for the feature variable. The feature variable instruction specifies that the text should be tokenized into words.\nWe now need to consider how to engineer the word features. If we add step_tf() we will get a matrix of token counts by default. We also have the option to add step_tfidf() to get a matrix of term frequencies weighted by inverse document frequency, which effectively down weights words that are common across all documents.\nWe decided in step 1 that we will start with raw token counts, so we will add step_tf() to our recipe. This is seen in Example 9.8.\n\nExample 9.8  \n\n# Add step to tokenize the text\ncedel2_cls_rec &lt;- \n  cedel2_cls_rec |&gt; \n  step_tf(text)\n\n# Preview\ncedel2_cls_rec\n\n\n\nTo make sure things are in order and that the recipe performs as expected, we can use the functions prep() and bake() to inspect the recipe. The prep() function takes a recipe object and a data frame and returns a prep object. The prep object contains the recipe and the data frame with the feature variables engineered according to the recipe. The bake() function takes a prep object and an optional new dataset to apply the recipe to. If we only want to see the application to the training set, we can use the new_data = NULL argument.\nIn Example 9.9, we use the prep() function to create a prep object and then use the bake() function to create a data frame with the feature variables. We can then inspect the data frame to see if the recipe performed as expected.\n\nExample 9.9  \n\n# Create a prep object\ncedel2_cls_prep &lt;- \n  prep(\n    cedel2_cls_rec, \n    training = cedel2_cls_train\n    )\n\n# Create a data frame with the feature variables\ncedel2_cls_bake &lt;- \n  bake(\n    cedel2_cls_prep, \n    new_data = NULL\n    )\n\n# Preview\ncedel2_cls_bake |&gt; dim()\n\n&gt; [1]  2364 37907\n\ncedel2_cls_bake[\n  c(1000:1001, 2000:2001),     # selected rows\n  c(1, 1000:1001, 20000:20001) # selected columns\n  ]\n\n&gt; # A tibble: 4 × 5\n&gt;   outcome tf_text_actuación tf_text_actuaciones tf_text_inservibles\n&gt;   &lt;fct&gt;               &lt;int&gt;               &lt;int&gt;               &lt;int&gt;\n&gt; 1 Learner                 0                   0                   0\n&gt; 2 Learner                 0                   0                   0\n&gt; 3 Native                  0                   0                   0\n&gt; 4 Native                  0                   0                   0\n&gt; # ℹ 1 more variable: tf_text_inside &lt;int&gt;\n\n\n\nThe resulting engineered features data frame has 2364 observations and 37907 variables. The first variable is the outcome variable and the remaining variables are the engineered features. We can see that the recipe performed as expected and that the feature variables are the token counts for each word in the text.\n\n\n\n\n\n\n Tip\nWhen applying tokenization and feature engineering steps to text data the result is often contained in a matrix object. In the the recipes package a data frame with a matrix-like structure is returned.\n\n\n\nBut we should pause. We have a lot of features! One for every single word in the entire corpus. This is an unweildy number of features for a model and it is likely that many of these features are not useful for our classification task. Furthermore, the more features we have, the more chance these features capture the nuances of these particular writing samples, thus, we are the more likely we are to overfit the model. All in all, we need to reduce the number of features.\nOur domain knowledge about language can be of help us decide on how to approach reducing the feature set. On the one hand, we know that most tokens occur rarely in any sizable corpus. And the lower the frequency, the more likely that the token may reflect nuances of the speaker or the content of particular texts rather than generalizable trends. On the other hand, we know that a relatively small number of the most frequent words quickly account for a large proportion of the tokens in a corpus.\nSo just with these two considerations, we can see that we can filter out many infrequent words and likely have quite a few viable features. Let’s start with an arbitrary threshold of the top 1,000 words by frequency. We can use the step_tokenfilter() function to filter out the top 1,000 words by frequency. This particular step needs to be applied before the step_tf() step, so we will add it to our recipe before the step_tf() step. This is seen in Example 9.10.\n\nExample 9.10  \n\n# Rebuild recipe with tokenfilter step\ncedel2_cls_rec &lt;- \n  recipe(\n    outcome ~ text, \n    data = cedel2_cls_train\n    ) |&gt; \n  step_tokenize(text) |&gt; \n  step_tokenfilter(text, max_tokens = 1000) |&gt; \n  step_tf(text)\n\n# Prep and bake\ncedel2_cls_prep &lt;- \n  prep(\n    cedel2_cls_rec, \n    training = cedel2_cls_train\n    )\n\ncedel2_cls_bake &lt;-\n  bake(\n    cedel2_cls_prep, \n    new_data = NULL\n    )\n\n# Preview\ncedel2_cls_bake |&gt; dim()\n\n&gt; [1] 2364 1001\n\ncedel2_cls_bake[1:5, 1:10]\n\n&gt; # A tibble: 5 × 10\n&gt;   outcome tf_text_10 tf_text_2 tf_text_3 tf_text_4 tf_text_5 tf_text_a\n&gt;   &lt;fct&gt;        &lt;int&gt;     &lt;int&gt;     &lt;int&gt;     &lt;int&gt;     &lt;int&gt;     &lt;int&gt;\n&gt; 1 Learner          0         0         0         0         0         0\n&gt; 2 Learner          0         0         0         0         0         2\n&gt; 3 Learner          0         1         0         0         0         5\n&gt; 4 Learner          0         0         0         0         0         0\n&gt; 5 Learner          0         0         0         0         0         2\n&gt; # ℹ 3 more variables: tf_text_abandonado &lt;int&gt;, tf_text_abuela &lt;int&gt;,\n&gt; #   tf_text_abuelos &lt;int&gt;\n\n\n\nWe now have a manageable set of features. Only during the interrogation step will we know if they are useful.\n\n\n\n\n\n\n Tip\nThe prep() and bake() functions are useful for inspecting the recipe and the engineered features, but they are not required to build a recipe. When a recipe is added to a workflow, the prep() and bake() functions are called automatically as part of the process.\n\n\n\nThere is one last step we should add to our recipe which concerns the skewed nature of word frequency distributions. The magnitude of the token counts will be highly skewed with a few words having very high counts and most words having relatively low counts, even for the top 1,000 words. This skew can be problematic for some models, so we will add a step to log normalize the token counts. This will not change the rank order, only the magnitude of the differences. Note that we use all_predictors() to apply the log transformation to all the word features, as seen in Example 9.11.\n\nExample 9.11  \n\n# Add log step to recipe\ncedel2_cls_rec &lt;- \n  cedel2_cls_rec |&gt; \n  step_log(all_predictors(), offset = 1) # add 1 to avoid log(0)\n\ncedel2_cls_rec\n\n\n\nWe are now ready to turn our attention to step 5 of our workflow, interrogating the data. In this step, we will first select a classification algorithm, then add this algorithm and our recipe to a workflow object. We will then use the workflow object to train and evaluate the model on the training set.\n\nThere are many classification algorithms to choose from with their own strengths and shortcomings. In Table 9.4, we list some of the most common classification algorithms and their characteristics.\n\n\nTable 9.4: Classification algorithms\n\n\n\n\n\n\nAlgorithm\nStrengths\nShortcomings\n\n\n\nLogistic regression\nSimple, interpretable, fast\nAssumes linear relationship between features and outcome\n\n\nNaive Bayes\nSimple, interpretable, fast\nAssumes independence between features\n\n\nDecision trees\nNonlinear relationships, interpretable\nProne to overfitting\n\n\nRandom forest\nNonlinear relationships, interpretable\nProne to overfitting\n\n\nSupport vector machines\nNonlinear relationships, interpretable\nProne to overfitting\n\n\nNeural networks\nNonlinear relationships, fast\nProne to overfitting, difficult to interpret\n\n\n\n\n\n\nIn the process of selecting an algorithm, simple, computationally efficient, and interpretable models are preferred over complex, computationally expensive, and uninterpretable models, all things being equal. Only if the performance of the simple model is not good enough should we move on to a more complex model.\nWith this end mind, we will start with a simple logistic regression model to see how well we can classify the texts in the training set with the features we have engineered. We will use the logistic_reg() function from the parsnip package to specify the logistic regression model. We then select the implementation engine (glm General Linear Model) and the mode of the model (classification). The implementation engine is the software that will be used to fit the model. The mode is the type of model, either classification or regression. The code is seen in Example 9.12.\n\nExample 9.12  \n\n# Create a model specification\ncedel2_cls_spec &lt;- \n  logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt;\n  set_mode(\"classification\")\n\n# Preview\ncedel2_cls_spec\n\n&gt; Logistic Regression Model Specification (classification)\n&gt; \n&gt; Computational engine: glm\n\n\n\nIn the parsnip package, model specifications are separate from model fitting. This allows us to specify the model once and then fit the model with different data sets. Furthermore, different algorithms will have different hyperparameters that can be tuned. The code in Example 9.12 uses the default hyperparameters for the logistic regression model.\n\n\n\n\n\n\n Tip\nYou can retrieve the list of potential engines for a given model specification with the show_engines() function. For example,\nshow_engines(logistic_reg())\n# A tibble: 7 × 2\n  engine    mode          \n  &lt;chr&gt;     &lt;chr&gt;         \n1 glm       classification\n2 glmnet    classification\n3 LiblineaR classification\n4 spark     classification\n5 keras     classification\n6 stan      classification\n7 brulee    classification\n\n\n\nNow we will combine our recipe and model specification into a workflow object. The workflow object will allow us to train and evaluate the model on the training set. We will use the workflow() function from the workflows package to combine the recipe and model specification into a workflow object. The code is seen in Example 9.13.\n\nExample 9.13  \n\n# Create a workflow\ncedel2_cls_wf &lt;- \n  workflow() |&gt; \n  add_recipe(cedel2_cls_rec) |&gt; \n  add_model(cedel2_cls_spec)\n\n# Preview\ncedel2_cls_wf\n\n&gt; ══ Workflow ════════════════════════════════════════════════════════════════════\n&gt; Preprocessor: Recipe\n&gt; Model: logistic_reg()\n&gt; \n&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n&gt; 4 Recipe Steps\n&gt; \n&gt; • step_tokenize()\n&gt; • step_tokenfilter()\n&gt; • step_tf()\n&gt; • step_log()\n&gt; \n&gt; ── Model ───────────────────────────────────────────────────────────────────────\n&gt; Logistic Regression Model Specification (classification)\n&gt; \n&gt; Computational engine: glm\n\n\n\nThe worflow contains all the preprocessing and the model-specific parameters we’ve selected. A workflow object at this stage in the analysis is not trained. It is a blueprint for a model. We can use the fit() function to apply the workflow to the training data to train the model and update our workflow object with the trained model. The fit() function takes a workflow and a data frame and returns an updated workflow object. The code is seen in Example 9.14.\n\nExample 9.14  \n\n# Fit the workflow\ncedel2_cls_wf_fit &lt;- \n  fit(\n    cedel2_cls_wf, \n    data = cedel2_cls_train\n    )\n\n\nThe workflow object now contains the trained model. But things did not go off without a hitch. We receive two warning messages when fitting the workflow to the training data:\nWarning messages:\n1: glm.fit: algorithm did not converge \n2: glm.fit: fitted probabilities numerically 0 or 1 occurred \nWarning messages are a sign that something may be amiss with the features we have choosen, the model we have selected, or both! Warning messages can sometimes be cryptic and steeped in the jargon of the algorithm. When in doubt it is best to consult the documentation of the algorithm and/ or seek help from the R community.\nThe first warning, the ‘algorithm did not converge’, means that given the data and the model, the algorithm could not estimate a stable solution. The reason or reasons for this can be varied, but it typically due to extreme outliers, collinearity, or a small sample size. Collinearity is when two or more features are highly correlated. Given we are using words as features, it is likely that there is collinearity in the data. We will need to address this.\nThe second warning is ‘fitted probabilities numerically 0 or 1 occurred’. This means that the model is overfitting the data, (i.e. there are one or more features that perfectly predict the outcome). This very well could be related to the first warning, as a small set of features may be perfectly predicting the outcome in a way that will likely not generalize to new data.\nThe upshot is that this model specification and/ or the feature engineering needs to be changed. So we step back in the workflow and make changes that will cascade downstream.\nIt turns out that a regularized logistic regression model often addresses the issues this model is experiencing. We will use the glmnet engine to fit a regularized logistic regression model. It is a more complex complex approach, but it can often mitigate the issues of collinearity and overfitting by penalizing features that are highly correlated and by shrinking the coefficients of features that are not useful for predicting the outcome.\nWe will build a new model specification using the logistic_reg() function and the glmnet engine. This time we will use hyperparameters in our logistic_reg() call to specify the penality we want to use and the strength of the penality. The penalty we will use is the lasso (L1). We now need to decide what value to use for the strength of the penalty. We either experiment with different values one by one or we can implement a range of values in tandem and then select the best value. We will use the latter approach.\nThe tune package provides a number of functions for selecting, or ‘tuning’, hyperparameters. The first is the tune() function which we add as the argument of the hyperparameter we want to tune in the model specification, as seen in Example 9.15.\n\nExample 9.15  \n\n# Create a model specification for tuning\ncedel2_cls_spec_tune &lt;- \n  logistic_reg(penalty = tune(), mixture = 1) |&gt; \n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"classification\")\n\n# Create a workflow\ncedel2_cls_wf_tune &lt;- \n  workflow() |&gt; \n  add_recipe(cedel2_cls_rec) |&gt; \n  add_model(cedel2_cls_spec_tune)\n\n\nWe now have a workflow that includes the tune() function as a placeholder for a range of values for the penalty hyperparameter. We use the grid_regular() function from the dials package to specify a grid of values for the penalty hyperparameter. Let’s choose a random set of 10 values, as seen in Example 9.16.\n\n\n\n\n\n\n Dive deeper\nThe hyperparameters penalty and mixture control which type(s) of regularization to apply and if there is a mixing of the types. In the model specification in Example 9.15, the penalty is set to be tuned and the mixture is set to 1, which means that only a lasso penalty will be applied.\nSee the documentation for the parsnip::logistic_reg() function for more information.\n\n\n\n\nExample 9.16  \n\n# Create a grid of values for the penalty hyperparameter\ncedel2_cls_grid &lt;- \n  grid_regular(\n    penalty(), \n    levels = 10\n    )\n\n# Preview\ncedel2_cls_grid\n\n&gt; # A tibble: 10 × 1\n&gt;          penalty\n&gt;            &lt;dbl&gt;\n&gt;  1 0.0000000001 \n&gt;  2 0.00000000129\n&gt;  3 0.0000000167 \n&gt;  4 0.000000215  \n&gt;  5 0.00000278   \n&gt;  6 0.0000359    \n&gt;  7 0.000464     \n&gt;  8 0.00599      \n&gt;  9 0.0774       \n&gt; 10 1\n\n\n\nThe 10 values chosen to be in the grid range from nearly zero to 1, where 0 indicates no penalty and 1 indicates a strong penalty.\nNow to perform the tuning and arrive at an optimal value for penalty we need to create a tuning workflow. We do this by calling the tune_grid() function using our tuning model specification workflow, a resampling object, and our hyperparameter grid and returns a tune_grid object.\n\nNow, a resampling object is not something we’ve seen yet. Resampling is a strategy that allows us to generate multiple training and testing sets from a single data set –in this case the training data we split at the outset. Each variation of the training and testing sets is called a fold. Which is why this type of resampling is called k-fold cross-validation. The vfold_cv() function from the rsample package takes a data frame and a number of folds and returns a vfold_cv object. We will use 10 folds and include the model specification and the hyperparameter grid in the tune_grid() function call. The code is seen in Example 9.17.\n\nExample 9.17  \n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create a resampling object\ncedel2_cls_vfold &lt;- \n  vfold_cv(\n    cedel2_cls_train, \n    v = 10\n    )\n\n# Tune the model\ncedel2_cls_tune &lt;- \n  tune_grid(\n    cedel2_cls_wf_tune, \n    resamples = cedel2_cls_vfold, \n    grid = cedel2_cls_grid\n    )\n\n# Preview\ncedel2_cls_tune\n\n&gt; # Tuning results\n&gt; # 10-fold cross-validation \n&gt; # A tibble: 10 × 4\n&gt;    splits             id     .metrics          .notes          \n&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n&gt;  1 &lt;split [2127/237]&gt; Fold01 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n&gt;  2 &lt;split [2127/237]&gt; Fold02 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n&gt;  3 &lt;split [2127/237]&gt; Fold03 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n&gt;  4 &lt;split [2127/237]&gt; Fold04 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n&gt;  5 &lt;split [2128/236]&gt; Fold05 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n&gt;  6 &lt;split [2128/236]&gt; Fold06 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n&gt;  7 &lt;split [2128/236]&gt; Fold07 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n&gt;  8 &lt;split [2128/236]&gt; Fold08 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n&gt;  9 &lt;split [2128/236]&gt; Fold09 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n&gt; 10 &lt;split [2128/236]&gt; Fold10 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\n\n\nThe cedel2_cls_tune object contains the results of the tuning for each fold. We can see the results of the tuning for each fold by calling the collect_metrics() function on the cedel2_cls_tune object, as seen in Example 9.18.\n\nExample 9.18  \n\n# Collect the results of the tuning\ncedel2_cls_tune_metrics &lt;- \n  collect_metrics(cedel2_cls_tune)\n\n# Visualize metrics\ncedel2_cls_tune |&gt; autoplot()\n\n\n\nFigure 9.2: Metrics for each fold of the tuning process.\n\n\n\n\nThe most common metrics for model performance in classification are accuracy and the area under the receiver operating characteristic curve (ROC-AUC). Accuracy is the proportion of correct predictions. The ROC-AUC is a measure of the trade-off between sensitivity and specificity. Sensitivity is the proportion of true positives that are correctly identified. Specificity is the proportion of true negatives that are correctly identified. The ROC-AUC is a measure of how well the model can distinguish between the two classes.\nIn the plot of the metrics, we can see that the many of the penalty values performed similarly, with a drop off in performance at the higher values. Conveniently, the show_best() function from the tune package takes a tune_grid object and returns the best performing hyperparameter values. The code is seen in Example 9.19.\n\nExample 9.19  \n\n# Show the best performing hyperparameter value\ncedel2_cls_tune |&gt; \n  show_best(metric = \"roc_auc\")\n\n&gt; # A tibble: 5 × 7\n&gt;         penalty .metric .estimator  mean     n std_err .config              \n&gt;           &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n&gt; 1 0.00599       roc_auc binary     0.975    10 0.00395 Preprocessor1_Model08\n&gt; 2 0.000464      roc_auc binary     0.969    10 0.00420 Preprocessor1_Model07\n&gt; 3 0.0000359     roc_auc binary     0.966    10 0.00411 Preprocessor1_Model06\n&gt; 4 0.0000000001  roc_auc binary     0.966    10 0.00415 Preprocessor1_Model01\n&gt; 5 0.00000000129 roc_auc binary     0.966    10 0.00415 Preprocessor1_Model02\n\n\n\nWe can make this selection programmatically by using the select_best() function. This function needs a metric to select by. We will use the ROC-AUC and select the best value for the penalty hyperparameter. The code is seen in Example 9.20.\n\nExample 9.20  \n\n# Select the best performing hyperparameter value\ncedel2_cls_best &lt;- \n  select_best(cedel2_cls_tune, metric = \"roc_auc\")\n\n# Preview\ncedel2_cls_best\n\n&gt; # A tibble: 1 × 2\n&gt;   penalty .config              \n&gt;     &lt;dbl&gt; &lt;chr&gt;                \n&gt; 1 0.00599 Preprocessor1_Model08\n\n\n\nAll of that to tune a hyperparameter! Now we can update the model specification and workflow with the best performing hyperparameter value using the previous cedel2_cls_wf_tune workflow and the finalize_workflow() function. The finalize_workflow() function takes a workflow and the selected parameters and returns an updated workflow object, as seen in Example 9.21.\n\nExample 9.21  \n\n# Update model specification\ncedel2_cls_wf_lasso &lt;-\n  cedel2_cls_wf_tune |&gt;\n  finalize_workflow(cedel2_cls_best)\n\ncedel2_cls_wf_lasso\n\n&gt; ══ Workflow ════════════════════════════════════════════════════════════════════\n&gt; Preprocessor: Recipe\n&gt; Model: logistic_reg()\n&gt; \n&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n&gt; 4 Recipe Steps\n&gt; \n&gt; • step_tokenize()\n&gt; • step_tokenfilter()\n&gt; • step_tf()\n&gt; • step_log()\n&gt; \n&gt; ── Model ───────────────────────────────────────────────────────────────────────\n&gt; Logistic Regression Model Specification (classification)\n&gt; \n&gt; Main Arguments:\n&gt;   penalty = 0.00599484250318942\n&gt;   mixture = 1\n&gt; \n&gt; Computational engine: glmnet\n\n\n\nOur model specification and the worflow are updated with the parameters.\nLet’s now return to fitting the workflow to the training set as we did before for the vanilla logistic regression model. As a reminder we are still working in step 5 of our workflow, interrogating the data. We identified and addressed potential issues in the model specification, leaving the feature selection the same.\nWe again fit the workflow to the training set, as seen in Example 9.22.\n\nExample 9.22  \n\n# Fit the workflow\ncedel2_cls_wf_lasso_fit &lt;- \n  fit(\n    cedel2_cls_wf_lasso, \n    data = cedel2_cls_train\n    )\n\n\nNo warnings, so that is a good sign! –Or at least a better sign than before.\nLet’s evaluate the performance of the model on the training data. The predict() function takes a trained model specification and a data frame and returns a data frame with the predicted outcome. We can join these predicted outcomes with the actual outcomes in the training data. The metrics() function takes a data frame with the actual and predicted outcomes and returns a data frame with the metrics for the model. The code is seen in Example 9.23.\n\nExample 9.23  \n\n# Evaluate workflow\ncedel2_cls_lasso_fit_preds &lt;-\n  bind_cols(\n    cedel2_cls_train,\n    predict(cedel2_cls_wf_lasso_fit, cedel2_cls_train)\n  )\n\n# Calculate accuracy\ncedel2_cls_lasso_fit_preds |&gt;\n  metrics(truth = outcome, estimate = .pred_class)\n\n&gt; # A tibble: 2 × 3\n&gt;   .metric  .estimator .estimate\n&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n&gt; 1 accuracy binary         0.970\n&gt; 2 kap      binary         0.934\n\n\n\nAgain, no warnings, so we have a functioning model. It also has a high accuracy, but we know that this is not the most reliable metric for the robustness of the model on new data. Similar to what we did to tune the hyperparameters, we can use cross-validation to gauge the variability of the model. The fit_resamples() function takes a workflow and a resampling object and returns metrics for each fold. The code is seen in Example 9.24.\n\nExample 9.24  \n\n# Cross-validate workflow\ncedel2_cls_lasso_cv &lt;-\n  cedel2_cls_wf_lasso |&gt;\n  fit_resamples(\n    resamples = cedel2_cls_vfold, \n    control = control_resamples(save_pred = TRUE)\n  )\n\n\nWe want to aggregate the metrics across the folds to get a sense of the variability of the model. The collect_metrics() function takes the results of a cross-validation and returns a data frame with the metrics.\n\nExample 9.25  \n\n# Collect metrics\ncedel2_cls_lasso_cv |&gt; \n  collect_metrics()\n\n&gt; # A tibble: 2 × 6\n&gt;   .metric  .estimator  mean     n std_err .config             \n&gt;   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n&gt; 1 accuracy binary     0.930    10 0.00503 Preprocessor1_Model1\n&gt; 2 roc_auc  binary     0.975    10 0.00395 Preprocessor1_Model1\n\n\n\nThe accuracy has dropped, but is still very high. From these metrics it appears we have a good candidate model. In many cases, however, there may be further room for improvement. A good next step to in these cases is to evaluate the model errors and see if there are any patterns that can be addressed before evaluating the model on the test set.\nFor classification tasks, a good place to start is to visualize the confusion matrix to assess false negatives and false positives. The conf_mat_resampled() function takes a fit_resamples object and returns a table (tidy = FALSE) with the confusion matrix for the aggregated folds. We can pass this to the autoplot() function to plot as in Example 9.26.\n\nExample 9.26  \n\n# Plot confusion matrix\ncedel2_cls_lasso_cv |&gt; \n  conf_mat_resampled(tidy = FALSE) |&gt; \n  autoplot(type = \"heatmap\")\n\n\n\nFigure 9.3: Confusion matrix for the aggregated folds of the cross-validation.\n\n\n\n\nThe top left to bottom right diagonal contains the true positives and true negatives. The top right to bottom left diagonal contains the false positives and false negatives. The convention is speak of one class being the positive class and the other class being the negative class. In our case, we will consider the positive class to be the ‘learner’ class and the negative class to be the ‘natives’ class.\n\n\ninterpretation has changed, with log-transformation\n\nWe can see that there are more learners falsely predicted to be natives than the other way around. This may be due to the fact that there are simply more learners than natives in the data set or this could signal that there are some learners that are more similar to natives than other learners. Clearly this can’t be the entire explanation as the model is not perfect, even some natives are classified falsely as learners! But may be an interesting avenue for further exploration. Maybe these are learners that are more advanced or have a particular style of writing that is more similar to natives.\n\nAnother perspective often applied to evaluate a model is the ROC curve. The ROC curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) for different classification thresholds. To produce the ROC curve, we pass the resampled fit to the collect_preditions() function. We then pass this result to roc_curve() function. But we want the results grouped by each fold, so we use group_by(id). Finally, we can pass this to the autoplot() function to plot as in Example 9.27.\n\nExample 9.27  \n\n# Plot ROC curve\ncedel2_cls_lasso_cv |&gt; \n  collect_predictions() |&gt;\n  group_by(id) |&gt;\n  roc_curve(truth = outcome, .pred_Native) |&gt; \n  autoplot()\n\n\n\nFigure 9.4: ROC curve for the aggregated folds of the cross-validation.\n\n\n\n\n\nAll signs point to a robust model.\n\nWe are now ready to move on to step 6, evaluating the model on the test set. We will use the predict() function to predict the outcome on the test set. The metrics() function takes a data frame with the actual and predicted outcomes and returns a data frame with the metrics for the model. The code is seen in Example 9.28.\n\nExample 9.28  \n\n# Evaluate model on test set\ncedel2_cls_lasso_fit_preds_test &lt;-\n  bind_cols(\n    cedel2_cls_test,\n    predict(cedel2_cls_wf_lasso_fit, cedel2_cls_test)\n  )\n\n# Calculate accuracy\ncedel2_cls_lasso_fit_preds_test |&gt;\n  metrics(truth = outcome, estimate = .pred_class)\n\n&gt; # A tibble: 2 × 3\n&gt;   .metric  .estimator .estimate\n&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n&gt; 1 accuracy binary         0.924\n&gt; 2 kap      binary         0.834\n\n\n\nThe accuracy is as good as the training set, even a tad higher. This is a good sign that the model is robust as it performs well on both training and test sets. We can evaluate the confusion matrix on the test set as well. The code is seen in Example 9.29.\n\nExample 9.29  \n\n# Plot confusion matrix\ncedel2_cls_lasso_fit_preds_test |&gt; \n  conf_mat(truth = outcome, estimate = .pred_class) |&gt; \n  autoplot(type = \"heatmap\")\n\n\n\nFigure 9.5: Confusion matrix for the test set.\n\n\n\n\nOn the test set the false instances look similar to the distribution on the training set, with the exception that learners are more likely to be falsely predicted to be natives than the other way around.\nWe can inspect the errors on the test set by filtering the data frame to only include the false instances. I will then select the columns with the actual outcome, the predicted outcome, the proficiency level, and the text and separate the predicted outcome to inspect them separately, as seen in Example 9.30.\n\nExample 9.30  \n\n# Inspect errors\ncedel2_cls_lasso_fit_preds_test |&gt; \n  filter(outcome != .pred_class) |&gt; \n  select(outcome, .pred_class, proficiency, text)\n\n&gt; # A tibble: 45 × 4\n&gt;    outcome .pred_class proficiency        text                                  \n&gt;    &lt;fct&gt;   &lt;fct&gt;       &lt;chr&gt;              &lt;chr&gt;                                 \n&gt;  1 Learner Native      Upper beginner     \"Un dia, El niño estaba durmiendo cua…\n&gt;  2 Learner Native      Upper beginner     \"Mi opinión de la educación de biling…\n&gt;  3 Learner Native      Lower intermediate \"La película “Solas” contiene muchos …\n&gt;  4 Learner Native      Upper intermediate \"A la semana pasada, yo vi la pelicul…\n&gt;  5 Learner Native      Lower advanced     \"El año pasado fui a EEUU para 3 sema…\n&gt;  6 Learner Native      Lower advanced     \"Es una pregunta que lleva mucho tiem…\n&gt;  7 Learner Native      Lower advanced     \"En el video, podemos ver que el niño…\n&gt;  8 Learner Native      Lower advanced     \"Actualmente estoy viviendo y trabaja…\n&gt;  9 Learner Native      Lower advanced     \"Podria escribir un lilbro de mis pla…\n&gt; 10 Learner Native      Lower advanced     \"Este mes pasado, yo viaje a la bella…\n&gt; # ℹ 35 more rows\n\n# Inspect learners falsely predicted to be natives\ncedel2_cls_lasso_fit_preds_test |&gt; \n  filter(outcome == \"Learner\", .pred_class == \"Native\") |&gt; \n  select(outcome, .pred_class, proficiency, text) |&gt; \n  count(proficiency, sort = TRUE)\n\n&gt; # A tibble: 5 × 2\n&gt;   proficiency            n\n&gt;   &lt;chr&gt;              &lt;int&gt;\n&gt; 1 Lower advanced        10\n&gt; 2 Upper advanced         8\n&gt; 3 Upper beginner         2\n&gt; 4 Lower intermediate     1\n&gt; 5 Upper intermediate     1\n\n\n\n\nMajority of misclassified learners are advanced, which could be expected as they are more similar to natives. There are some beginners that are misclassified as natives, which is interesting, and not expected. But all models are wrong, but some are useful –George Box.\nStill an open question as to why some natives are classified as learners.\n\nWe can inspect the estimates for the features in the model to gain some insight into what features are most important for predicting the outcomes. The extract_fit_parsnip() function takes a trained model specification and returns a data frame with the estimated coefficients for each feature. The code is seen in Example 9.31.\n\nExample 9.31  \n\n# Extract estimates\ncedel2_cls_wf_lasso_fit |&gt; \n  extract_fit_parsnip() |&gt;\n  tidy()\n\n&gt; # A tibble: 1,001 × 3\n&gt;    term               estimate penalty\n&gt;    &lt;chr&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n&gt;  1 (Intercept)          -1.44  0.00599\n&gt;  2 tf_text_10            0     0.00599\n&gt;  3 tf_text_2             0     0.00599\n&gt;  4 tf_text_3             0     0.00599\n&gt;  5 tf_text_4             0     0.00599\n&gt;  6 tf_text_5             0     0.00599\n&gt;  7 tf_text_a             0.332 0.00599\n&gt;  8 tf_text_abandonado    0     0.00599\n&gt;  9 tf_text_abuela        0     0.00599\n&gt; 10 tf_text_abuelos       0     0.00599\n&gt; # ℹ 991 more rows\n\n\n\nThe estimates are the log odds of the outcome. In a binary classification task, the log odds of the outcome is the log of the probability of the outcome divided by the probability of the other outcome. In our case, the reference outcome is “Learner”, so negative log-odds indicate that the feature is associated with the “Learner” outcome and positive log-odds indicate that the feature is associated with the “Native” outcome.\nThe estimates are in log-odds, so we need to exponentiate them to get the odds. The odds are the probability of the outcome divided by the probability of the other outcome. The probability of the outcome is the odds divided by the odds plus one. The code is seen in Example 9.32.\n\nExample 9.32  \n\n# Calculate probability\ncedel2_cls_wf_lasso_fit |&gt; \n  extract_fit_parsnip() |&gt;\n  tidy() |&gt; \n  mutate(probability = exp(estimate) / (exp(estimate) + 1))\n\n&gt; # A tibble: 1,001 × 4\n&gt;    term               estimate penalty probability\n&gt;    &lt;chr&gt;                 &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n&gt;  1 (Intercept)          -1.44  0.00599       0.191\n&gt;  2 tf_text_10            0     0.00599       0.5  \n&gt;  3 tf_text_2             0     0.00599       0.5  \n&gt;  4 tf_text_3             0     0.00599       0.5  \n&gt;  5 tf_text_4             0     0.00599       0.5  \n&gt;  6 tf_text_5             0     0.00599       0.5  \n&gt;  7 tf_text_a             0.332 0.00599       0.582\n&gt;  8 tf_text_abandonado    0     0.00599       0.5  \n&gt;  9 tf_text_abuela        0     0.00599       0.5  \n&gt; 10 tf_text_abuelos       0     0.00599       0.5  \n&gt; # ℹ 991 more rows\n\n\n\nSo just looking at the snippet of the features returned from Example 9.32, we can see that the features ‘a’ and ‘abandonado’ are slightly associated with the “Native” outcome nd the other features are neutral (probability = 0.5).\nA quick way to extract the most important features for predicting the each outcome is to use the vi() function from the vip package. It takes a trained model specification and returns a data frame with the most important features. The code is seen in Example 9.33.\n\nExample 9.33  \n\n# Load package\nlibrary(vip)\n\nconflicted::conflicts_prefer(vip::vi)\n\n# Extract important features\nvar_importance_df &lt;- \n  cedel2_cls_wf_lasso_fit |&gt; \n  vi()\n\n# Preview\nvar_importance_df\n\n&gt; # A tibble: 1,000 × 3\n&gt;    Variable           Importance Sign \n&gt;    &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;\n&gt;  1 tf_text_ahí              8.83 POS  \n&gt;  2 tf_text_sorpresa         8.43 POS  \n&gt;  3 tf_text_juan             7.91 POS  \n&gt;  4 tf_text_todavia          7.45 NEG  \n&gt;  5 tf_text_encuentran       6.72 NEG  \n&gt;  6 tf_text_diferencia       6.64 POS  \n&gt;  7 tf_text_eeuu             6.34 NEG  \n&gt;  8 tf_text_última           6.26 POS  \n&gt;  9 tf_text_favorito         6.25 NEG  \n&gt; 10 tf_text_único            6.07 POS  \n&gt; # ℹ 990 more rows\n\n\n\nThe Variable column contains each feature (with the feature type and corresponding variable tf_text_), Importance provides the absolute log-odds value, and the Sign column indicates whether the feature is associated with the “NEG” (“Learner”) or the “POS” (“Native”) outcome. We can recode the Variable and Sign columns to make them more interpretable and exponentiate the log-odds to get probabilites and then plot them using ggplot(), as in Example 9.34.\n\nExample 9.34  \n\n# Recode variable and sign\nvar_importance_df &lt;- \n  var_importance_df |&gt; \n  mutate(\n    Feature = str_remove(Variable, \"tf_text_\"), \n    Outcome = case_when(Sign == \"NEG\" ~ \"Learner\", Sign == \"POS\" ~ \"Native\"),\n    Importance = exp(Importance) / (exp(Importance) + 1)\n    ) |&gt; \n  select(Outcome, Feature, Importance)\n\n# Plot\nvar_importance_df |&gt; \n  slice_max(Importance, n = 50) |&gt;\n  ggplot(aes(x = reorder(Feature, Importance), y = Importance)) +\n  geom_point() +\n  coord_flip() +\n  facet_wrap(~ Outcome, scales = \"free_y\") +\n  labs(x = NULL, y = \"Importance\", fill = NULL) +\n  theme_minimal()\n\n\n\nFigure 9.6: Most important features for predicting the outcome.\n\n\n\n\nWe can inspect Figure 9.6, and qualitatively assess what these features may be telling us about the differences between the learners and the natives.\n\nIn this section we’ve build a text classifier using a regularized logistic regression model. We’ve tuned the hyperparameters to arrive at a robust model that performs well on both the training and test sets. We’ve also evaluated the model errors and inspected the most important features for predicting the outcome.\nThe tidymodels package provides a framework for building and evaluating supervised machine learning models that is modular in nature. This means, that we can quickly and easily change the model specification, the features, feature engineering, and the hyperparameters to arrive at a robust model. If the model is not robust, we can change models in the model specification. If the features are not robust, we can change the recipe. If the model is overfitting, we can tune the hyperparameters.\n\n9.2.2 Text regression\nWe will now turn our attention to the second task in this section, text regression. In this task, we will use the same original dataset as in the classification task, but we will predict the placement score based on the learner writing samples. Let’s start by extracting the observations (only learners) and the relevant variables from the original data set. The code is seen in Example 9.35.\n\nExample 9.35  \n\n# Extract observations and relevant variables\ncedel2_reg &lt;- \n  cedel2_df |&gt; \n  filter(proficiency != \"Native\") |&gt; \n  select(outcome = placement_score, proficiency, text)\n\n# Preview\ncedel2_reg |&gt; glimpse()\n\n&gt; Rows: 1,906\n&gt; Columns: 3\n&gt; $ outcome     &lt;dbl&gt; 14.0, 16.3, 16.3, 18.6, 18.6, 18.6, 20.9, 20.9, 20.9, 20.9…\n&gt; $ proficiency &lt;chr&gt; \"Lower beginner\", \"Lower beginner\", \"Lower beginner\", \"Low…\n&gt; $ text        &lt;chr&gt; \"Yo vivo es Alanta, Georgia. Atlanta es muy grande ciudad.…\n\n\n\n\nIn this task, our outcome variable is numeric so we do not need, or want, to convert it to a factor. Our predictor variable text is the same as before. We have already weighed the options for feature engineering and decided to use the term frequency method (raw counts) for the top 1,000. Since we are setting up the same recipe as before, essentially, we can use the same code as before.\n\n\nLet’s first move to step 2, initial split. We will use the initial_split() function again, but this time we will not need to stratify the data as we are not predicting a categorical variable. The code is seen in Example 9.36.\n\nExample 9.36  \n\n# Set seed for reproducibility\nset.seed(123)\n\n# Split data\ncedel2_reg_split &lt;- \n  initial_split(cedel2_reg, prop = 0.8)\n\n# Training set\ncedel2_reg_train &lt;- \n  training(cedel2_reg_split)\n\n# Test set\ncedel2_reg_test &lt;- \n  testing(cedel2_reg_split)\n\n\nThe training set has 1524 observations and the test set has 382 observations.\n\nNow we can create the recipe to set up the variable relations, select the features, and engineer the features. The code is seen in Example 9.37.\n\nExample 9.37  \n\n# Create a recipe\ncedel2_reg_rec &lt;- \n  recipe(outcome ~ text, data = cedel2_reg_train) |&gt; \n  step_tokenize(text) |&gt; \n  step_tokenfilter(text, max_tokens = 1000) |&gt;\n  step_tf(text) |&gt; \n  step_log(all_predictors(), offset = 1)\n\n# Preview\ncedel2_reg_rec\n\n\n\nAt this point we would inspect our recipe to make sure that it looks as expected and gauge the number of features. But since are using the same recipe as before, we can skip this step.\n\nWe can now proceed to interrogate the data. As before we will want to start with a simple model and then build up to more complex models. Let’s consider some common algorithms for regression tasks in Table 9.5.\n\n\nTable 9.5: Regression algorithms\n\n\n\n\n\n\nAlgorithm\nStrengths\nShortcomings\n\n\n\nLinear regression\nSimple, interpretable, fast\nAssumes linear relationship between features and outcome\n\n\nDecision trees\nNonlinear relationships, interpretable\nProne to overfitting\n\n\nRandom forest\nNonlinear relationships, interpretable\nProne to overfitting\n\n\nNeural networks\nNonlinear relationships, fast\nProne to overfitting, difficult to interpret\n\n\n\n\n\nLet’s start with a with a linear regression model in mind for our model specification. But let’s also consider what we learned in our first attempt to build a logistic regression model using word frequencies. We learned that the model was overfitting the training data and we needed to use a regularized model to reduce the variance of the model. So let’s start with a regularized linear regression model.\nThe linear_reg() function, just as the logistic_reg() function, provides arguments for the regularization hyperparameters when the glmnet computational engine is used. Let’s tune the penalty hyperparameter in the same way as before. The code for the process is seen in Example 9.38.\n\nExample 9.38  \n\n# Create model specification\ncedel2_reg_spec_lasso &lt;-\n  linear_reg(penalty = tune(), mixture = 1) |&gt; \n  set_engine(\"glmnet\") |&gt; \n  set_mode(\"regression\")\n\n# Create workflow\ncedel2_reg_wf_lasso &lt;-\n  workflow() |&gt; \n  add_recipe(cedel2_reg_rec) |&gt; \n  add_model(cedel2_reg_spec_lasso)\n\n# Create tuning grid\ncedel2_reg_grid &lt;-\n  grid_regular(penalty(), levels = 10)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create tuning workflow\ncedel2_reg_tune &lt;-\n  tune_grid(\n    cedel2_reg_wf_lasso,\n    resamples = vfold_cv(cedel2_reg_train, v = 10),\n    grid = cedel2_reg_grid, \n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Select best parameter\nchosen_penalty &lt;-\n  cedel2_reg_tune |&gt;\n  select_best(metric = \"rmse\")\n\n# Update workflow\ncedel2_reg_final_wf_lasso &lt;-\n  cedel2_reg_wf_lasso |&gt;\n  finalize_workflow(chosen_penalty)\n\ncedel2_reg_final_wf_lasso\n\n&gt; ══ Workflow ════════════════════════════════════════════════════════════════════\n&gt; Preprocessor: Recipe\n&gt; Model: linear_reg()\n&gt; \n&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n&gt; 4 Recipe Steps\n&gt; \n&gt; • step_tokenize()\n&gt; • step_tokenfilter()\n&gt; • step_tf()\n&gt; • step_log()\n&gt; \n&gt; ── Model ───────────────────────────────────────────────────────────────────────\n&gt; Linear Regression Model Specification (regression)\n&gt; \n&gt; Main Arguments:\n&gt;   penalty = 1\n&gt;   mixture = 1\n&gt; \n&gt; Computational engine: glmnet\n\n\n\nNow we fit this model to the training data and evaluate the performance using cross-validation. The code is seen in Example 9.39.\n\nExample 9.39  \n\n# Cross-validated workflow\ncedel2_reg_cv_lasso &lt;-\n  cedel2_reg_final_wf_lasso |&gt;\n  fit_resamples(\n    resamples = vfold_cv(cedel2_reg_train, v = 10),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Collect metrics\ncedel2_reg_cv_lasso |&gt; \n  collect_metrics()\n\n&gt; # A tibble: 2 × 6\n&gt;   .metric .estimator   mean     n std_err .config             \n&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n&gt; 1 rmse    standard   13.3      10  0.348  Preprocessor1_Model1\n&gt; 2 rsq     standard    0.659    10  0.0222 Preprocessor1_Model1\n\n\n\nNow, the RMSE estimate is 13.3. RMSE is expressed in the same units as the outcome variable. In this case, the outcome variable is the placement test score percent. So the RMSE is 13.3 percentage points. The \\(R^2\\) (rsq) is 0.659. This means that the model explains 66% of the variance in the outcome variable. Taken together, this isn’t the best model.\nBut how good or bad is it? This is where we can use the null model to compare the model to. The null model is a model that predicts the mean of the outcome variable. We can use the null_model() function to create a null model and submit it to cross-validation. The code is seen in Example 9.40.\n\nExample 9.40  \n\n# Create null model\nnull_model &lt;- \n  null_model() |&gt; \n  set_engine(\"parsnip\") |&gt; \n  set_mode(\"regression\")\n\n# Cross-validate null model\nnull_cv &lt;- \n  workflow() |&gt; \n  add_recipe(cedel2_reg_rec) |&gt;\n  add_model(null_model) |&gt;\n  fit_resamples(\n    resamples = vfold_cv(cedel2_reg_train, v = 10),\n    metrics = metric_set(rmse)\n  )\n\n# Collect metrics\nnull_cv |&gt; \n  collect_metrics()\n\n&gt; # A tibble: 1 × 6\n&gt;   .metric .estimator  mean     n std_err .config             \n&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n&gt; 1 rmse    standard    22.6    10   0.206 Preprocessor1_Model1\n\n\n\nOur the word features perform better than the null model which means that it is picking up on some signal in the data.\nLet’s visualize the distribution of the predictions and the errors from our word features model to see if there are any patterns of interest. We can use the collect_predictions() function to extract the predictions of the cross-validation and plot the true outcome agains the predicted outcome using ggplot(), as in Example 9.58.\n\nExample 9.41  \n\n# Visualize predictions\ncedel2_reg_cv_lasso |&gt; \n  collect_predictions() |&gt; \n  ggplot(aes(outcome, .pred, shape = id)) +\n  geom_point(alpha = 0.5, position = position_jitter(width = 0.5)) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.5) + # trend for each fold\n  labs(\n    x = \"Truth\",\n    y = \"Predicted score\",\n    shape = \"Fold\"\n  )\n\n\n\nFigure 9.7: Distribution of the RMSE for the cross-validated linear regression model.\n\n\n\n\nFrom this plot, we see data points for each predicted and truth value pair for each of the ten folds. There is a trend line for each fold which shows the linear relationship between the predicted and truth values for each fold. The trend lines are more similar than different, which is a good sign that the model is not wildly overfitting the training data. Looking closer, however, we can see the errors. Some are noticeably distant from the linear trend lines, i.e. outliers, in particular for test scores in the higher and lower ranges. There also seems to be a pattern in that the model predicts lower scores for higher scores and higher scores for lower scores. We can see this by the overplotted points in the higher and lower ranges.\nIf the \\(R^2\\) value is in the ballpark, this means that somewhere around 40% of the variation is not explained by the frequency of the top 1,000 words. This is not surprising, as there are many other factors that contribute to the proficiency level of a text.\nWe have a model that is performing better than the null model, but it is not performing well enough to be very useful. We will need to update the model specification and/ or the features to try to improve the model fit. Let’s start with the model. There are many different model specifications we could try, but we will likely need to use a more complex model specification to capture the complexity that we observed in the errors from the previous model.\nLet’s try a decision tree model. Decision trees are non-linear models that are able to model non-linear relationships and interactions between the features and the outcome and tend to be less influenced by outliers. These are all desirable characteristics. Decision trees, however, can be prone to overfitting\nAlong the spectrum of model complexity, decision trees are more complex than linear regression models, but less complex than other models such as neural networks. Furthermore, decision trees are interpretable, which is a nice feature for an exploratory-oriented analysis.\nTo implement a new model in tidymodels, we need to create a new model specification and a new workflow. We will use the decision_tree() function from the parsnip package to create the model specification. The decision_tree() function takes no arguments and returns a decision_tree object. We create the new model specification in Example 9.42.\n\nExample 9.42  \n\n# Create model specification\ncedel2_reg_spec_tree &lt;-\n  decision_tree() |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"regression\")\n\ncedel2_reg_spec_tree\n\n&gt; Decision Tree Model Specification (regression)\n&gt; \n&gt; Computational engine: rpart\n\n\n\nWe now have a new model specification. We can now create a new workflow. We will use the same recipe as before, but we will change the model specification to cedel2_prof_spec_tree. We add this to a new workflow and fit the workflow to the training data. The code is seen in Example 9.43.\n\nExample 9.43  \n\n# Create workflow\ncedel2_reg_wf_tree &lt;-\n  workflow() |&gt; \n  add_recipe(cedel2_reg_rec) |&gt; \n  add_model(cedel2_reg_spec_tree)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Cross-validated workflow\ncedel2_reg_cv_tree &lt;-\n  cedel2_reg_wf_tree |&gt;\n  fit_resamples(\n    resamples = vfold_cv(cedel2_reg_train, v = 10),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n\nWe can now evaluate the performance of the model using cross-validation. The code is seen in Example 9.44.\n\nExample 9.44  \n\n# Collect metrics\ncedel2_reg_cv_tree |&gt; \n  collect_metrics()\n\n&gt; # A tibble: 2 × 6\n&gt;   .metric .estimator   mean     n std_err .config             \n&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n&gt; 1 rmse    standard   15.1      10  0.404  Preprocessor1_Model1\n&gt; 2 rsq     standard    0.554    10  0.0215 Preprocessor1_Model1\n\n\n\nThe metrics for the vanilla decision tree are similar to the regularized linear regression model. The RSME is 15.1 and the \\(R^2\\) is 0.554. Yet, if we compare the standard error between the two models, we can see that the decision tree model has a larger standard error. This means that the decision tree model is more prone to overfitting the training data.\nTo minimize overfitting, we can tune hyperparameters of the model. Regularization is one way to reduce overfitting, as we saw with the regularized linear regression model. Another way to reduce overfitting is to reduce the complexity of the model. This can be done by reducing the number of features or by reducing the number of splits in the decision tree, known as pruning.\nAnother approach is to implement a random forest model. A random forest is an ensemble model that combines multiple decision trees to make a prediction. A random forest is a type of ensemble model which means that it combines multiple models to make a prediction. In addition to multiple decision trees, random forests also perform random feature selection. This helps to reduce the correlation between the decision trees and thus reduces the variance of the model.\nLet’s try a random forest model. We will use the rand_forest() function from the parsnip package to create the model specification. The rand_forest() function takes no arguments and returns a rand_forest object. We will select the ranger engine and add the importance argument to ensure that we can extract feature importance if this model proves to be useful. We create the new model specification in Example 9.45.\n\nExample 9.45  \n\n# Create model specification\ncedel2_reg_spec_rf &lt;-\n  rand_forest() |&gt; \n  set_engine(\"ranger\", importance = \"impurity\") |&gt; \n  set_mode(\"regression\")\n\ncedel2_reg_spec_rf\n\n&gt; Random Forest Model Specification (regression)\n&gt; \n&gt; Engine-Specific Arguments:\n&gt;   importance = impurity\n&gt; \n&gt; Computational engine: ranger\n\n\n\nWe can now update the workflow to use the new model specification. The code is seen in Example 9.46.\n\nExample 9.46  \n\n# Update workflow\ncedel2_reg_wf_rf &lt;-\n  cedel2_reg_wf_tree |&gt; \n  update_model(cedel2_reg_spec_rf)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Cross-validated workflow\ncedel2_reg_cv_rf &lt;-\n  cedel2_reg_wf_rf |&gt;\n  fit_resamples(\n    resamples = vfold_cv(cedel2_reg_train, v = 10),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n\nWe can now evaluate the performance of the model using cross-validation. The code is seen in Example 9.47.\n\nExample 9.47  \n\n# Collect metrics\ncedel2_reg_cv_rf |&gt; \n  collect_metrics()\n\n&gt; # A tibble: 2 × 6\n&gt;   .metric .estimator   mean     n std_err .config             \n&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n&gt; 1 rmse    standard   13.1      10  0.305  Preprocessor1_Model1\n&gt; 2 rsq     standard    0.680    10  0.0133 Preprocessor1_Model1\n\n\n\nThe random forest model performs better than the decision tree model and the regularized linear regression model. The RSME is 13.1 and the \\(R^2\\) is 0.68. We also see that the standard error is the lowest of the models we have tried so far. This means that the random forest model is less prone to overfitting the training data.\nBefore we settle on this model, let’s try one more model, a support vector machine (SVM). A support vector machine is a supervised machine learning model that can be used for both classification and regression. It is a non-parametric method, which means that it does not make any assumptions about the underlying distribution of the data. It is also a method which can be better suited for high-dimensional data where many values are zero, that is, sparse data, which is often the case with text data.\nLet’s again specify the model. We will use the svm_linear() function from the parsnip package to create the model specification and we will use the LiblineaR computational engine. We create the new model specification in Example 9.48.\n\nExample 9.48  \n\n# Create model specification\ncedel2_reg_spec_svm &lt;-\n  svm_linear() |&gt; \n  set_engine(\"LiblineaR\") |&gt; \n  set_mode(\"regression\")\n\ncedel2_reg_spec_svm\n\n&gt; Linear Support Vector Machine Model Specification (regression)\n&gt; \n&gt; Computational engine: LiblineaR\n\n\n\nNow let’s update the workflow to use the new model specification. The code is seen in Example 9.49.\n\nExample 9.49  \n\n# Update workflow\ncedel2_reg_wf_svm &lt;-\n  cedel2_reg_wf_rf |&gt; \n  update_model(cedel2_reg_spec_svm)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Cross-validated workflow\ncedel2_reg_cv_svm &lt;-\n  cedel2_reg_wf_svm |&gt;\n  fit_resamples(\n    resamples = vfold_cv(cedel2_reg_train, v = 10),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n\nLet’s evaluate the cross-validation performance of the model. The code is seen in Example 9.50.\n\nExample 9.50  \n\n# Collect metrics\ncedel2_reg_cv_svm |&gt; \n  collect_metrics()\n\n&gt; # A tibble: 2 × 6\n&gt;   .metric .estimator   mean     n std_err .config             \n&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n&gt; 1 rmse    standard   14.7      10  0.353  Preprocessor1_Model1\n&gt; 2 rsq     standard    0.625    10  0.0152 Preprocessor1_Model1\n\n\n\nThis SVR is not performing better than the linear regression and random forest models and the standard error is not particularly low. So we will not pursue this model further.\nSo in summary, we’ve tried four different model specifications. The regularized linear regression model, the decision tree model, the random forest model, and the support vector machine model. The random forest model performed the best. But there stands to be improvement, but it looks as if we may need to update the features.\nIn the recipe we have used until this point we have limited the word features to 1,000 and used the raw counts of the words. The rationale for this was that we wanted to limit the number of features to reduce the complexity of the model while still capturing the most important features. But we may have limited the features too much, at least now that we are comparing features between the same population (learners). A potentially more informative feature would be the TF-IDF score. This score is a measure of how important a word is to a document in a collection or corpus. Specifically, it is the product of the term frequency and the inverse document frequency. The more dispersed a feature is across the corpus, the lower the TF-IDF score. The more concentrated a feature is across the corpus, the higher the TF-IDF score, and the more informative the feature is for distinguishing between documents.\nSo we can use the step_tfidf() function to update the recipe. The code is seen in Example 9.51.\n\nExample 9.51  \n\n# Create a recipe\ncedel2_reg_rec &lt;- \n  recipe(outcome ~ text, data = cedel2_reg_train) |&gt; \n  step_tokenize(text) |&gt; \n  step_tokenfilter(text, max_tokens = 1000) |&gt;\n  step_tfidf(text)\n\n# Preview\ncedel2_reg_rec\n\n\n\n\n\n\n\n\n Tip\nNote that the log-transformation is not necessary when using TF-IDF scores as normalization is built into the TF-IDF calculation.\n\n\n\nAnother change we can explore is to increase the number of features. But how many features should we use? We can turn to the tune() function to help us answer this question. Let’s add the tune() function to the recipe and tune the max_tokens hyperparameter. The code is seen in Example 9.52.\n\nExample 9.52  \n\n# Create a recipe\ncedel2_reg_rec &lt;- \n  recipe(outcome ~ text, data = cedel2_reg_train) |&gt; \n  step_tokenize(text) |&gt; \n  step_tokenfilter(text, max_tokens = tune()) |&gt;\n  step_tfidf(text)\n\n# Preview\ncedel2_reg_rec\n\n\nLet’s tune the max_tokens with the random forest model specification that we created earlier. We will create a tuning workflow, which will tune the max_tokens hyperparameter on the training data using cross-validation. The code is seen in Example 9.53.\n\nExample 9.53  \n\n# Create tuning workflow\ncedel2_tune_wf &lt;- \n  workflow() |&gt; \n  add_recipe(cedel2_reg_rec) |&gt; \n  add_model(cedel2_reg_spec_rf)\n\ncedel2_tune_wf\n\n&gt; ══ Workflow ════════════════════════════════════════════════════════════════════\n&gt; Preprocessor: Recipe\n&gt; Model: rand_forest()\n&gt; \n&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n&gt; 3 Recipe Steps\n&gt; \n&gt; • step_tokenize()\n&gt; • step_tokenfilter()\n&gt; • step_tfidf()\n&gt; \n&gt; ── Model ───────────────────────────────────────────────────────────────────────\n&gt; Random Forest Model Specification (regression)\n&gt; \n&gt; Engine-Specific Arguments:\n&gt;   importance = impurity\n&gt; \n&gt; Computational engine: ranger\n\n\n\nThe tune_grid() function takes a workflow, a resampling method, a tuning grid, and a set of metrics. The tuning grid is a set of hyperparameters and their values. The tune_grid() function will tune the hyperparameters on the training data using cross-validation. The code is seen in Example 9.54. This is a computationally intensive process, as it will train a 10 folds, for each hyperparameter value. Each fold in the random forest is 500 trees, so this will train 5,000 trees for each hyperparameter value! So that is 30,000 trees in total. This code will take some time to run.\n\nExample 9.54  \n\n# Create tuning grid\ncedel2_tune_grid &lt;- \n  grid_regular(max_tokens(range = c(500, 3500)), levels = 5)\n\ncedel2_tune_grid\n\n&gt; # A tibble: 5 × 1\n&gt;   max_tokens\n&gt;        &lt;int&gt;\n&gt; 1        500\n&gt; 2       1250\n&gt; 3       2000\n&gt; 4       2750\n&gt; 5       3500\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create tuning workflow\ncedel2_tune &lt;-\n  tune_grid(\n    cedel2_tune_wf,\n    resamples = vfold_cv(cedel2_reg_train, v = 10),\n    grid = cedel2_tune_grid, \n    metrics = metric_set(rmse, rsq),\n    control = control_resamples(save_pred = TRUE)\n  )\n\ncedel2_tune\n\n&gt; # Tuning results\n&gt; # 10-fold cross-validation \n&gt; # A tibble: 10 × 5\n&gt;    splits             id     .metrics          .notes           .predictions\n&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;           &lt;list&gt;      \n&gt;  1 &lt;split [1371/153]&gt; Fold01 &lt;tibble [10 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n&gt;  2 &lt;split [1371/153]&gt; Fold02 &lt;tibble [10 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n&gt;  3 &lt;split [1371/153]&gt; Fold03 &lt;tibble [10 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n&gt;  4 &lt;split [1371/153]&gt; Fold04 &lt;tibble [10 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n&gt;  5 &lt;split [1372/152]&gt; Fold05 &lt;tibble [10 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n&gt;  6 &lt;split [1372/152]&gt; Fold06 &lt;tibble [10 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n&gt;  7 &lt;split [1372/152]&gt; Fold07 &lt;tibble [10 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n&gt;  8 &lt;split [1372/152]&gt; Fold08 &lt;tibble [10 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n&gt;  9 &lt;split [1372/152]&gt; Fold09 &lt;tibble [10 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n&gt; 10 &lt;split [1372/152]&gt; Fold10 &lt;tibble [10 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;\n\n\n\nJust as we did for the penalty hyperparameter, we can visualize the performance of the model as a function of the max_tokens hyperparameter. The code is seen in Example 9.55.\n\nExample 9.55  \n\n# Visualize tuning results\ncedel2_tune |&gt;\n  collect_metrics() |&gt;\n  ggplot(aes(max_tokens, mean)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(\n    breaks = scales::pretty_breaks(n = 10),\n    labels = scales::comma\n  ) +\n  facet_wrap(~ .metric, scales = \"free_y\", ncol = 1) +\n  labs(\n    x = \"Max tokens\"\n  )\n\n\n\nFigure 9.8: Performance of the random forest model as a function of the max_tokens hyperparameter.\n\n\n\n\nWe added a range of maximum token values to the tuning grid to see if there is a point of diminishing returns. We can see that the performance of the model actually appears to improve, not with more tokens, but instead with less. Our lowest value of 500 tokens appears to perform the best. This is interesting, as it suggests that there are a small number of words that are most informative for predicting the placement test score. It also suggests that individual words, on the whole, are not very informative for predicting the placement test score.\nWe can now select the best hyperparameter value. The code is seen in Example 9.56.\n\nExample 9.56  \n\n# Select best parameter\nchosen_max_tokens &lt;-\n  cedel2_tune |&gt;\n  select_best(metric = \"rmse\")\n\n# Update workflow\ncedel2_reg_final_rf &lt;-\n  cedel2_tune_wf |&gt;\n  finalize_workflow(chosen_max_tokens)\n\ncedel2_reg_final_rf\n\n&gt; ══ Workflow ════════════════════════════════════════════════════════════════════\n&gt; Preprocessor: Recipe\n&gt; Model: rand_forest()\n&gt; \n&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n&gt; 3 Recipe Steps\n&gt; \n&gt; • step_tokenize()\n&gt; • step_tokenfilter()\n&gt; • step_tfidf()\n&gt; \n&gt; ── Model ───────────────────────────────────────────────────────────────────────\n&gt; Random Forest Model Specification (regression)\n&gt; \n&gt; Engine-Specific Arguments:\n&gt;   importance = impurity\n&gt; \n&gt; Computational engine: ranger\n\n\n\nWe can now fit the model to the training data and evaluate the performance using cross-validation. The code is seen in Example 9.57.\n\nExample 9.57  \n\n# Cross-validated workflow\ncedel2_reg_cv_rf &lt;-\n  cedel2_reg_final_rf |&gt;\n  fit_resamples(\n    resamples = vfold_cv(cedel2_reg_train, v = 10),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Collect metrics\ncedel2_reg_cv_rf |&gt; \n  collect_metrics()\n\n&gt; # A tibble: 2 × 6\n&gt;   .metric .estimator   mean     n std_err .config             \n&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n&gt; 1 rmse    standard   12.8      10  0.221  Preprocessor1_Model1\n&gt; 2 rsq     standard    0.695    10  0.0137 Preprocessor1_Model1\n\n\n\nWe’ve improved the model, but not very much. We can see that the RMSE is 13.1 and the \\(R^2\\) is 0.68. The standard error is NA. This is the lowest standard error we have seen so far, but it is still not as low as we would like.\nLet’s now visualize the distribution of the predictions and the errors from our word features model to see if there are any patterns of interest. We can use the collect_predictions() function to extract the predictions of the cross-validation and plot the true outcome agains the predicted outcome using ggplot(), as in Example 9.58.\n\nExample 9.58  \n\n# Visualize predictions\ncedel2_reg_cv_rf |&gt; \n  collect_predictions() |&gt; \n  ggplot(aes(outcome, .pred, shape = id)) +\n  geom_point(alpha = 0.5, position = position_jitter(width = 0.5)) +\n  geom_smooth(method = \"lm\", se = FALSE) + # trend for each fold\n  labs(\n    x = \"Truth\",\n    y = \"Predicted score\",\n    shape = \"Fold\"\n  )\n\n\n\nFigure 9.9: Distribution of the RMSE for the cross-validated linear regression model.\n\n\n\n\nThere appears to be more cohesion in the predictions, overall. The errors however seem visually to be larger for the lower scores.\nAt this point we can either consider this model to be good enough or we can try to improve it further. Let’s take one more shot at improving the features by including bigrams. To include words (unigrams) and bigrams, we can modify the step_tokenize() function in our recipe. We add the token = \"ngrams\" argument and specify the options as list(n = 2, n_min = 1). This will create unigrams and bigrams. The code is seen in Example 9.59.\n\nExample 9.59  \n\n# Create a recipe\ncedel2_reg_rec &lt;- \n  recipe(outcome ~ text, data = cedel2_reg_train) |&gt; \n  step_tokenize(text, token = \"ngrams\", options = list(n = 2, n_min = 1)) |&gt; \n  step_tokenfilter(text, max_tokens = tune()) |&gt;\n  step_tfidf(text)\n\n# Preview\ncedel2_reg_rec\n\n\nSince the features have changed, we won’t assume that our tuning of the max_tokens is still valid. So we will tune the max_tokens hyperparameter again, as in Example 9.52 through Example 9.57. For the sake of brevity, we will not show the code here. We will simply show the results.\n\n# Create workflow\ncedel2_reg_wf_rf &lt;-\n  workflow() |&gt; \n  add_recipe(cedel2_reg_rec) |&gt; \n  add_model(cedel2_reg_spec_rf)\n\n# Create tuning grid\ncedel2_tune_grid &lt;- \n  grid_regular(max_tokens(range = c(500, 3500)), levels = 5)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create tuning workflow\ncedel2_tune &lt;-\n  tune_grid(\n    cedel2_reg_wf_rf,\n    resamples = vfold_cv(cedel2_reg_train, v = 10),\n    grid = cedel2_tune_grid, \n    metrics = metric_set(rmse, rsq),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Select best parameter\nchosen_max_tokens &lt;-\n  cedel2_tune |&gt;\n  select_best(metric = \"rmse\")\n\n# Update workflow\ncedel2_reg_final_rf &lt;-\n  cedel2_reg_wf_rf |&gt;\n  finalize_workflow(chosen_max_tokens)\n\n# Cross-validated workflow\ncedel2_reg_cv_rf &lt;-\n  cedel2_reg_final_rf |&gt;\n  fit_resamples(\n    resamples = vfold_cv(cedel2_reg_train, v = 10),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Collect metrics\nestimates &lt;- \n  cedel2_reg_cv_rf |&gt; \n  collect_metrics() |&gt; \n  pull(3)\n\nThe tuning of max_tokens selected 2,000 features as the optimal number. The cross-validation of the training dataset produced an RMSE of 12.8 and an \\(R^2\\) of 0.698. The standard error is NA. The upshot is that by including bigrams we have not improved, or worsened, the model. Including the unigrams and bigrams together may be more informative for the exploration of the features.\nWe could continue to try to improve the model, but at this point we have a model that is performing better than the null model and is performing better than the other models we have tried. So we will consider this model to be good enough.\nLet’s now fit the 1-2 gram model to the testing data and evaluate the performance on the testing set. The code is seen in Example 9.60.\n\n\nExample 9.60  \n\n# Fit model on training data\ncedel2_reg_final_rf_fit &lt;- \n  cedel2_reg_final_rf |&gt;\n  fit(cedel2_reg_train)\n\n# Predict on testing data\ncedel2_reg_final_rf_pred &lt;-\n  bind_cols(\n    cedel2_reg_test,\n    predict(cedel2_reg_final_rf_fit, cedel2_reg_test)\n  )\n\n# Evaluate performance\ncedel2_reg_final_rf_pred |&gt; \n  metrics(truth = outcome, estimate = .pred)\n\n&gt; # A tibble: 3 × 3\n&gt;   .metric .estimator .estimate\n&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n&gt; 1 rmse    standard      12.9  \n&gt; 2 rsq     standard       0.694\n&gt; 3 mae     standard       9.84\n\n\n\nWe can now visualize the feature importance of the model. The code is seen in Example 9.61.\n\nExample 9.61  \n\n# Extract predictions\ncedel2_reg_final_rf_fit |&gt; \n  vip::vi(scale = TRUE)  |&gt; \n  mutate(Variable = str_replace(Variable, \"^tfidf_text_\", \"\")) |&gt; \n  slice_max(Importance, n = 20) |&gt;\n  # reorder variables by importance\n  ggplot(aes(reorder(Variable, Importance), Importance)) +\n  geom_point() +\n  coord_flip() +\n  labs(\n    x = \"Feature\",\n    y = \"Importance\"\n  ) \n\n\n\nFigure 9.10: Feature importance of the random forest model."
  },
  {
    "objectID": "prediction.html#pda-summary",
    "href": "prediction.html#pda-summary",
    "title": "9  Prediction",
    "section": "\n9.3 Summary",
    "text": "9.3 Summary\nIn this chapter we have learned about supervised machine learning. We have learned about the different types of supervised machine learning methods and how they can be used to predict and classify. We have also learned about the different types of data structures that are used in supervised machine learning. Finally, we have learned about the different types of evaluation metrics that are used to evaluate the performance of supervised machine learning models."
  },
  {
    "objectID": "prediction.html#activities",
    "href": "prediction.html#activities",
    "title": "9  Prediction",
    "section": "Activities",
    "text": "Activities\n\n\n\n\n\n\n Recipe\n\nWhat: Predictive models: prep, train, test, and evaluateHow: Read Recipe 9 and participate in the Hypothes.is online social annotation.Why: To illustrate some common coding strategies for preparing datasets for inferential data analysis, as well as the steps conduct descriptive assessment, statistical interrogation, and evaluation and reporting of results.\n\n\n\n\n\n\n\n\n\n Lab\n\nWhat: Predictive Data AnalysisHow: Clone, fork, and complete the steps in Lab 10.Why: To gain experience working with coding strategies to prepare, feature engineer, train and test a predictive model, and evaluate results from a predictive data analysis, practice transforming datasets into new object formats and visualizing relationships, and implement organizational strategies for organizing and reporting results in a reproducible fashion."
  },
  {
    "objectID": "prediction.html#questions",
    "href": "prediction.html#questions",
    "title": "9  Prediction",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\n Conceptual questions\n\nWhat is the difference between a numeric and a categorical variable?\nWhat is the difference between a regression and a classification model?\nWhat is the difference between a training set and a testing set?\nWhat is the difference between a hyperparameter and a parameter?\nWhat is the difference between a supervised and an unsupervised machine learning model?\nWhat advantages and disadvantages do supervised machine learning models have over traditional methods of text analysis?\nWhat are some potential applications of supervised machine learning in linguistics?\n\n\n\n\n\n\n\n\n\n\n Applied questions\n\nWrite a program to build a classification model which uses a set of collected text features to predict a target variable.\nUse the classification model to classify a series of documents and assess the accuracy of the model.\nDevelop a regression model which uses text features to predict a numeric target variable.\nCreate a text mining application to analyze a large body of text and discover correlations between variables.\nUse a clustering algorithm to discover clusters in a large dataset, and create a visualization to present the identified clusters.\nAnalyze the structure of a text corpus and identify patterns in word usage and feature distributions.\nBuild a predictive model using text as an input and binary or categorical outcomes as the target.\nDevelop a natural language processing application which classifies text into predefined categories using a supervised learning algorithm.\nUse a supervised learning algorithm to build a predictive model which classifies a set of unseen texts into predefined categories.\nDevelop a web application which allows users to easily explore a set of text documents, visualize the content of the documents, and generate predictive models from the text.\n\n\n\n\n\n\n\n\nBaayen, R. Harald. 2011. “Corpus Linguistics and Naive Discriminative Learning.” Revista Brasileira de Lingu\\’\\istica Aplicada 11 (2): 295–328.\n\n\nDeshors, Sandra C, and Stefan Th. Gries. 2016. “Profiling Verb Complementation Constructions Across New Englishes.” International Journal of Corpus Linguistics. 21 (2): 192–218.\n\n\nGries, Stefan Th., and Sandra C. Deshors. 2014. “Using Regressions to Explore Deviations Between Corpus Data and a Standard/Target: Two Suggestions.” Corpora 9 (1): 109–36. https://doi.org/10.3366/cor.2014.0053."
  },
  {
    "objectID": "prediction.html#footnotes",
    "href": "prediction.html#footnotes",
    "title": "9  Prediction",
    "section": "",
    "text": "See Appendix A.3 for more information on the CEDEL2 corpus.↩︎\nNote that functions for meta-features require more sophisticated text analysis software to be installed on the computing environment (e.g. spacyr for step_lemma(), step_pos(), etc.). See the textrecipes package documentation for more information.↩︎"
  },
  {
    "objectID": "preface.html#sec-p-rationale",
    "href": "preface.html#sec-p-rationale",
    "title": "Preface",
    "section": "Rationale",
    "text": "Rationale\nData science, an interdisciplinary field that combines knownledge and skills from statistics, computer science, and domain-specific expertise to extract meaningful insight from structured and unstructured data, has emerged as an exciting and rapidly growing field in recent years, driven in large part by the increase in computing power available to the average individual and the abundance of electronic data now available through the internet. These advances have become an integral part of the modern scientific landscape, with data-driven insights now being used to inform decision-making in a wide variety of academic fields, including linguistics and language-related disciplines.\nThis textbook seeks to meet this growing demand by providing an introduction to the fundamental concepts and practical programming skills from data science applied to the task of quantitative text analysis. It is intended primarily for undergraduate students, but may also be useful for graduates and researchers seeking to expand their methodological toolbox. The textbook takes a pedagogical approach which assumes no prior experience with statistics or programming, making it an accessible resource for novices beginning their exploration of quantitative text analysis methods."
  },
  {
    "objectID": "preface.html#sec-p-aims",
    "href": "preface.html#sec-p-aims",
    "title": "Preface",
    "section": "Aims",
    "text": "Aims\nThe overarching goal of this textbook is to provide readers with foundational knowledge and practical skills to conduct and evaluate quantitative text analysis using the R programming language and other open source tools and technologies. The specific aims are to develop the reader’s proficiency in three main areas:\n\n\nData literacy: Identify, interpret and evaluate data analysis procedures and results\n\n\nThroughout this textbook we will explore topics which will help you understand how data analysis methods derive insight from data. In this process you will be encouraged to critically evaluate connections across linguistic and language-related disciplines using data analysis knowledge and skills. Data literacy is an invaluable skillset for academics and professionals but also is an indispensable aptitude for in the 21st century citizens to navigate and actively participate in the ‘Information Age’ in which we live (Carmi et al. 2020).\n\n\n\nResearch skills: Design, implement, and communicate quantitative research\n\n\nThis aim does not differ significantly, in spirit, from common learning outcomes in a research methods course. However, working with text will incur a series of key steps in the selection, collection, and preparation of the data that are unique to text analysis projects. In addition, I will stress the importance of research documentation and creating reproducible research as an integral part of modern scientific inquiry (Buckheit and Donoho 1995).\n\n\n\nProgramming skills: Apply programmatic strategies to develop and collaborate on reproducible research projects\n\n\nModern data analysis, and by extension, text analysis is conducted using programming. There are various key reasons for this: a programming approach (1) affords researchers unlimited research freedom –if you can envision it, you can program it, (2) underlies well-documented and reproducible research (Gandrud 2015), and (3) invites researchers to engage more intimately with the data and the methods for analysis.\n\nThese aims are important for linguistics students because they provide a foundation for concepts and in the skills required to succeed in the rapidly evolving landscape of 21st-century research. These abilities enable researchers to evaluate and conduct high-quality empirical investigation across linguistic fields on a wide variety of topics. Moreover, these skills go beyond linguistics research; they are widely applicable across many disciplines where quantitative data analysis and programming are becoming increasingly important. Thus, this textbook provides students with a comprehensive introduction to quantitative text analysis that is relevant to linguistics research and that equips them with valuable skills for their future careers."
  },
  {
    "objectID": "preface.html#sec-p-approach",
    "href": "preface.html#sec-p-approach",
    "title": "Preface",
    "section": "Approach",
    "text": "Approach\nThe approach taken in this textbook is designed to accomodate linguistics students and researchers with little to no prior experience with programming or quantitative methods. With this in mind the objective is connect conceptual understanding with practical application. Real-world data and research tasks relevant to linguistics are used thoughtout the book to provide context and to motivate the learning process1. Furthermore, as an introduction to the field, the textbook focuses on the most common and fundamental methods and techniques for quantitative text analysis and prioritizes breadth over depth and intuitive understanding over technical explanations. On the programming side, the Tidyverse approach to programming in R will be adopted. This approach provides a consistent syntax across different packages and is known for its legibility, making it easier for readers to understand and write code. Together, these strategies form an approach that is intended to provide readers with an accessible resource to gain a foothold in the field and to equip them with the knowledge and skills to apply quantitative text analysis in their own research."
  },
  {
    "objectID": "preface.html#sec-p-structure",
    "href": "preface.html#sec-p-structure",
    "title": "Preface",
    "section": "Structure",
    "text": "Structure\nThe aims and approach described above is reflected in the overall structure of the book and each chapter.\nBook level\nAt the book level, there are five interdependent parts:\nPart I “Orientation” provides the necessary background knowledge to situate quantitative text analysis in the wider context of data analysis and linguistic research and to provide a clearer picture of what text analysis entails and its range of applications.\nThe subsequent parts are directly aligned with the data analysis process. The building blocks of this process are reflected in ‘Data to Insight Hierarchy (DIKI)’ visualized in Figure 12.\n\n\n\n\nFigure 1: Data to Insight Hierarchy (DIKI)\n\n\n\nThe DIKI Hierarchy highlights the stages and intermediate steps required to derive insight from data. Part II “Foundations” provides a conceptual introduction to the DIKI Hierarchy and establishes foundational knowledge about data, information, knowledge, and insight which is fundamental to developing a viable research plan.\nParts III “Preparation” and IV “Analysis” focus on the implementation process. Part III covers the steps involved in preparing data for analysis, including data acquisition, curation, and transformation. Part IV covers the steps involved in conducting analysis, including exploratory, predictive, and inferential data analysis.\nThe final part, Part V “Communication”, covers the final stage of the data analysis process, which is to communicate the results of the analysis. This includes the structure and content of research reports as well as the process of publishing, sharing, and collaborating on research.\nChapter level\nAt the chapter level, both conceptual and programming skills are developed in stages3. The chapter-level structure is consistent across chapters and can be seen in Table 1.\n\n\nTable 1: The general structure of a chapter including: the component, its purpose, where to find the resource, and the target learning stage.\n\n\n\n\n\n\n\nComponent\nPurpose\nResource\nStage\n\n\n\nOutcomes\nIdentify the learning objectives for the chapter\nTextbook\nIntroduction\n\n\nOverview\nProvide a brief introduction to the chapter topic\nTextbook\nIntroduction\n\n\nCoding Lessons\nTeach programming techniques with hands-on interactive exercises\nGitHub\nSkills\n\n\nContent\nCombine conceptual discussions and programming skills, incorporating thought-provoking questions, relevant studies, and advanced topic references\nTextbook\nKnowledge\n\n\nRecipes\nOffer step-by-step programming examples related to the chapter\nResources website\nComprehension\n\n\nLabs\nAllow readers to apply chapter-specific concepts and techniques\nGitHub\nApplication\n\n\nSummary\nReview the key concepts and skills covered in the chapter\nTextbook\nReview\n\n\nQuestions\nAssess and expand the reader’s knowledge and abilities\nTextbook\nAssessment\n\n\n\n\nEach chapter will begin with a list of key learning outcomes followed by a brief introduction to the chapter’s content. The goal is to orient the reader to the chapter. Next there will be a prompt to complete the interactive coding lesson(s) to introduce reader’s to key programming concepts related to the chapter though hands-on experience and then the main content of the chapter will follow. The content will be a combination of conceptual discussions and programming skills, incorporating thought-provoking questions (‘Consider this’), relevant studies (‘Case study’), and advanced topic references (‘Dive deeper’). Together these components form the skills and knowledge phase. The next phase is the application phase. This phase will include step-by-step programming demonstrations related to the chapter (Recipes) and lab exercises that allow readers to apply their knowledge and skills chapter-related tasks. Finally the chapters conclude with a summary of the key concepts and skills covered in the chapter and a set of questions to assess and expand the reader’s knowledge and abilities."
  },
  {
    "objectID": "preface.html#sec-p-resources",
    "href": "preface.html#sec-p-resources",
    "title": "Preface",
    "section": "Resources",
    "text": "Resources\nThere are three main resources available to support the aims and approach of this textbook. Firstly, the textbook itself provides prose discussion, figures/ tables, R code, case studies, and thought and practical exercises. Secondly, there is a companion R package called qtalrkit (Francom 2023), which includes functions for accessing data and datasets, as well as various useful functions developed specifically for this textbook. In addition, there is a comprehensive website Quantitative Text Analysis for Linguistics Resources(qtalr website) that includes programming tutorials and demonstrations to enhance the reader’s recognition of how programming strategies are implemented. Finally, a GitHub repository is provided which contains both a set of interactive R programming lessons (Swirl) and lab exercises designed to guide the reader through practical hands-on programming applications. The companion qtalrkit package and the GitHub repository are both under active development and will be updated regularly to ensure that supplementary materials remain relevant to the content of the text4."
  },
  {
    "objectID": "preface.html#sec-p-getting-started",
    "href": "preface.html#sec-p-getting-started",
    "title": "Preface",
    "section": "Getting started",
    "text": "Getting started\nBefore jumping in to this and subsequent chapter’s textbook activities, it is important to prepare your computing environment and understand how to take advantage of the resources available, both those directly and indirectly associated with the textbook.\nR and IDEs\nProgramming is the backbone for modern quantitative research. Among the many programming languages available, R is a popular open-source language and software environment for statistical computing. R is popular with statisticians and has been adopted as the de facto language by many other fields in natural and social sciences, including linguistics. It is freely downloadable from The R Project for Statistical Programming website and is available for macOS, Linux, and Windows operating systems.\nSuccessfully installing R is rarely the last step in setting up your R-enabled computing environment. The majority of R users also install an integrated development environment (IDE). An IDE, such as RStudio or Visual Studio Code, provide a graphical user interface (GUI) for working with R. In effect, IDEs provide a dashboard for working with R and are designed to make it easier to write and execute R code. IDEs also provide a number of other useful features such as syntax highlighting, code completion, and debugging. IDEs are not required to work with R but they are highly recommended.\nChoosing to install R and an IDE on your personal computer, which is know as your local environment, is not the only option to work with R. You can also choose to work with R in the cloud, a remote environment. There are a number of cloud-based options for working with R, including RStudio Cloud and Microsoft Azure. These options provide a pre-configured R environment that you can access from any computer with an internet connection. The advantage of working in the cloud is that you do not need to install R or an IDE on your local computer. The disadvantage is that you will need to be connected to the internet to work with R and the free tiers for these services are limited.\nIf you are new to R, you may want to consider working in the cloud to get started. If you plan to continue to work with R in the future, you will most likely want to install R and an IDE on your local computer or explore using a virtual environment. Virtual environments, such as Docker, provide a way to use a pre-configured computing environment or create your own that you can share with others. Virtual environments are a good option if you want to ensure that everyone in your research group is working with the same computing environment. Pre-configured virtual environments exist for R through the Rocker project and can be used locally or in the cloud.\nThere are trade-offs in terms of cost, convenience, and flexibility when choosing to work with R in a local, remote, or virtual environment. The choice is yours and you can always change your mind later. The important thing is to get started and begin learning R. Furthermore, any of the approaches described here will be compatible with this textbook.\nFor more information and instructions on setting up an R environment consult the following guides.\n\n\n\n\n\n\n Guides\n\nInstalling R\nChoosing and setting up an IDE\nWorking with R in remote and virtual environments\n\n\n\n\nR packages\nAs you progress in your R programming experience, you’ll find yourself leveraging code from other R users, which is typically provided as packages. Packages are sets of functions and/or datasets that are freely accessible for download, designed to perform a specific set of interrelated tasks. They enhance the capabilities of R. Official R packages can be found in repositories like CRAN (Comprehensive R Archive Network), while other packages can be obtained from code-sharing platforms such as GitHub.\n\n\n\n\n\n\n Consider this\nThe Comprehensive R Archive Network (CRAN) includes groupings of popular packages related to a given applied programming task called Task Views. Explore the available CRAN Task Views listings. Note the variety of areas (tasks) that are covered in this listing. Now explore in more detail one of the following task views which are directly related to topics covered in this textbook noting the associated packages and their descriptions: (1) Cluster, (2) MachineLearning, (3) NaturalLanguageProcessing, or (4) ReproducibleResearch.\n\n\n\nYou will download a number of packages at different stages of this textbook, but there is a set of packages that will be key to have from the get go. Once you have access to a working R/ RStudio environment, you can proceed to install the following packages.\nInstall the following packages from CRAN.\n\n\ntidyverse (Wickham 2023)\n\n\nrmarkdown (Allaire et al. 2023)\n\n\nquarto (Allaire 2023)\n\n\ntinytex (Xie 2023)\n\n\ndevtools (Wickham et al. 2022)\n\n\nusethis (Wickham et al. 2023)\n\n\nswirl (Kross et al. 2020)\n\n\nYou can do this by running the following code in an R console:\n # install key packages from CRAN\ninstall.packages(c(\"tidyverse\", \"rmarkdown\", \"quarto\", \"tinytex\", \"devtools\", \"usethis\", \"swirl\"))\nFor instructions on how to install the qtalrkit package from GitHub and download and use the interactive R programming lessons for this textbook, see the following guides.\n\n\n\n\n\n\n Guides\n\nGetting started\n\n\n\n\nGit and GitHub\nGitHub is a code sharing website. Modern computing is highly collaborative and GitHub is a very popular platform for sharing and collaborating on coding projects. The lab exercises for this textbook are shared on GitHub. To access and complete these exercises you will need to sign up for a (free) GitHub account and then set up the version control software git on your computing environment. git is the conduit to interfacing GitHub and for many git will already be installed on your computer (or cloud computing environment).\nFor more information and instructions on setting up version control consult the following guide.\n\n\n\n\n\n\n Guides\n\nSetting up Git and GitHub\n\n\n\n\nGetting help\nThe technologies employed in this approach to text analysis will include a somewhat steep learning curve. And in all honesty, the learning never stops! Both seasoned programmers and beginners alike need assistance. Fortunately there is a very large community of programmers who have developed many official support resources and who actively contribute to official and unofficial discussion forums. Together these resources provide many avenues for overcoming challenges.\nIn Table 2, I provide a list of steps for seeking help with R.\n\n\nTable 2: Recommended order for seeking help with R.\n\n\n\n\n\n\nOrder\nResource\nDescription\n\n\n\n1\nOfficial R Documentation\nAccess the official documentation by running help(package = \"package_name\") in an R console. Use the ? operator followed by the package or function name. Check out available Vignettes by running browseVignettes(\"package_name\").\n\n\n2\nWeb Search\nLook for package documentation and vignettes on the web. A popular site for this is R-Universe.\n\n\n3\nRStudio IDE Help Toolbar\nIf you’re using RStudio IDE, use the “Help” toolbar menu. It provides links to help resources, guides, and manuals.\n\n\n4\nOnline Discussion Forums\nSites like Stack Overflow and RStudio Community are great platforms where the programming community asks and answers questions related to real-world issues.\n\n\n5\nPost Questions with Reprex\nWhen posting a question, especially those involving coding issues or errors, provide enough background and include a reproducible example (reprex) - a minimal piece of code that demonstrates your issue. This helps others understand and answer your question effectively.\n\n\n\n\nThe first place to look for help with R is the official documentation of the R package you are using. You can access this documentation by running help(package = \"package_name\") in an R console or using the ? operator and then the package or function name. Many R packages often include “Vignettes” (long-form documentation and demonstrations). These can be accessed either by running browseVignettes() in an R console with the package name in quotes (e.g. browseVignettes(\"tidyverse\")).\nYou can also search the web for package documentation and vignettes. A popular site for this purpose is R-Universe.\nIf you are using the RStudio IDE, the easiest and most convenient place to get help with either R or RStudio is through the RStudio “Help” toolbar menu. There you will find links to help resources, guides, and manuals.\nThere are a number of very popular discussion forum websites where the programming community asks and answers questions to real-world issues. These sites often have subsections dedicated to particular programming languages or software. The most popular of these sites is Stack Overflow. There are also R-specific discussion forums such as RStudio Community.\nIf you post a question on one of these communities ensure that if your question involves some coding issue or error that you provide enough background such that the community will be able to help you. This is often referred to as a reproducible example or “reprex”. A reprex is a minimal piece of code that demonstrates the issue you are having. It is a very useful tool for both asking and answering questions.\nFor information on how to create a reprex consult the following guide.\n\n\n\n\n\n\n Guides\n\nCreating reproducible examples\n\n\n\n\nThe take-home message here is that you are not alone. There are many people world-wide that are learning to program and/ or contribute to the learning of others. The more you engage with these resources and communities the more successful your learning will be. As soon as you are able, pay it forward. Posting questions and offering answers helps the community and engages and refines your skills –a win-win."
  },
  {
    "objectID": "preface.html#sec-p-conventions",
    "href": "preface.html#sec-p-conventions",
    "title": "Preface",
    "section": "Conventions",
    "text": "Conventions\nTo facilitate the learning process, this textbook will employ a number of conventions. These conventions are intended to help the reader navigate the text and to signal the reader’s attention to important concepts and information.\nProse\nThe following typographic conventions are used throughout the text:\n\n\nItalics\n\nFilenames, file extensions, directory paths, and URLs.\n\n\n\nFixed-width\n\nPackage names, function names, variable names, and in-line code including expressions and operators.\n\n\n\nBold\n\nKey concepts when first introduced.\n\n\n\nLinked text\n\nLinks to internal and external resources, footnotes, and citations including references to R packages when first introduced.\n\n\nCode blocks\nMore lengthy code will be presented in code blocks, as seen in Example 1.\n\nExample 1  \n\n# A function that takes a name and returns a greeting\ngreet &lt;- function(name) { # function definition\n  paste(\"Hello\", name) # print greeting\n} # end function definition\n\ngreet(name = \"Jerid\") # apply function to a name\n\n&gt; [1] \"Hello Jerid\"\n\n\n\nThere are a couple of things to note about the code in Example 1. First, it shows the code that is run in R as well as the ouput that is returned. The code will appear in a box and the output will appear below the box. Both code and output will appear in fixed-width font. Output which is text will be prefixed with &gt;. Second, the # symbol is used to signal a code comment, a human-facing description. Everything right of a # is not run as code. In this textbook you will see code comments above code on a separate line and to the right of code on the same line. It is good practice to comment your code to enhance readability and to help others understand what your code is doing.\n\n\n\n\n\n\n Tip\nSince you are reading this textbook in a web browser there are two more features that you should be aware of. First, you can click on the code block to copy the code to your clipboard. Second, you can click on a function name to see the help documentation for that function.\n\n\n\nAll figures, tables, and images in this textbook are generated by code blocks but only code for those elements that are relevant for discussion will be shown. However, if you wish to see the code for any element in this textbook, you can visit the GitHub repository https://qtalr.github.io/book/.\nWhen a reference to a file and its contents is made, it will appear as in File 1.\n\nFile 1: Example R script\n\nexample.R\n\n# Load libraries\nlibrary(tidyverse)\n\n# Add 1 and 1\n1 + 1\n\nCallouts\nCallouts are used to signal the reader’s attention to content, activity, and other important sections. The following callouts are used in this textbook:\nContent\n\n\n\n\n\n\n Outcomes\nLearning outcomes for the chapter appear here.\n\n\n\n\n\n\n\n\n\n Consider this\nPoints for you to consider and questions to explore appear here.\n\n\n\n\n\n\n\n\n\n Case study\nCase studies for applying conceptual knowledge and coding skills covered in the chapter appear here.\n\n\n\n\n\n\n\n\n\n Dive deeper\nLinks to additional resources for diving deeper into the topic appear here.\n\n\n\nActivities\n\n\n\n\n\n\n Swirl lesson\nLinks to swirl lessons for practicing coding skills for the chapter appear here.\n\n\n\n\n\n\n\n\n\n Recipe\nLinks to demonstration programming tasks on the qtalr site for the chapter appear here.\n\n\n\n\n\n\n\n\n\n Lab\nLinks to lab exercises for applying conceptual knowledge and coding skills on the qtalr GitHub repository for the chapter appear here.\n\n\n\nOther\n\n\n\n\n\n\n Tip\nTips for using R and related tools appear here.\n\n\n\n\n\n\n\n\n\n Warning\nWarnings for using R and related tools appear here."
  },
  {
    "objectID": "preface.html#sec-p-instructor",
    "href": "preface.html#sec-p-instructor",
    "title": "Preface",
    "section": "To the instructor",
    "text": "To the instructor\nDepending on the experience level and expectations of your readers, you may want to consider adopting one of the following course designs for using this textbook.\nBasic Introduction\n\nCover chapters 1-5 in sequence to give your readers a foundational understanding of quantitative text analysis.\nCulminate the course with a research proposal assignment that requires them to identify an interesting linguistic problem, propose ways of solving it using the methods covered in class, and identify potential data sources.\nIf your readers have little to no experience with R, you may want to consider using the RStudio Cloud platform to host the course. This will provide them with a pre-installed R environment and allow them to focus on learning the material rather than troubleshooting.\nIntermediate Introduction\n\nCover chapters 1, 5-10 in sequence to give your readers a deeper understanding of quantitative text analysis methods. Explore additional case studies or dataset examples throughout the course if you wish to supplement your lectures.\nCulminate the course with a research project assignment that allows your readers to apply what they’ve learned to linguistic content of their choice.\nYou may consider using the RStudio Cloud platform to host the course, but ensure that your readers have access to R and RStudio on their own computers as well.\nAdvanced Introduction\n\nCover all 12 chapters to give your readers a thorough understanding of quantitative text analysis concepts and techniques. Devote more time chapters 5-10 providing demonstrations of how to approach different problems and evaluating alternative approaches.\nCulminate the course with a collaborative research project that requires your readers to work in groups to conduct a comprehensive analysis of a given dataset.\nEnsure that your readers install R and RStudio on their own computers as they will need full control over their coding environment.\n\nFor all course designs, it is strongly recommend that you evaluate the readers’ success in understanding the material by providing a combination of quizzes, lab assignments, programming exercises, and written reports. Additionally, encourage your readers to ask questions5, collaborate with peers, and seek help from the ample resources available online when they encounter scope-limited programming problems.\nFor more information on how to use this textbook in your course, visit the Instructor Guide on the compansion website."
  },
  {
    "objectID": "preface.html#sec-p-activities",
    "href": "preface.html#sec-p-activities",
    "title": "Preface",
    "section": "Activities",
    "text": "Activities\n\nAt this point you should have a working R environment with the core packages including qtalrkit installed. You should also have verified that you have a working Git environment and that you have a GitHub account. If you have not completed these tasks, return to the guides listed above in “Getting started” of this Preface and complete them before proceeding.\nThe following activities are designed to help you become familiar with the tools and resources that you will be using throughout this textbook. These and subsequent activities are designed to be completed in the order that they are presented in this textbook.\n\n\n\n\n\n\n Swirl lesson\n\nWhat: Intro to SwirlHow: In the R console load swirl, run swirl(), and follow prompts to select the lesson.Why: To familiarize you with navigating, selecting, and completing swirl lessons.\n\n\n\n\n\n\n\n\n\n Recipe\n\nWhat: Literate Programming: writing with codeHow: Read Recipe 0 and participate in collaborative discussion with peers.Why: To introduce the concept of Literate Programming and how to create literate documents using R and Quarto.\n\n\n\n\n\n\n\n\n\n Lab\n\n\nWhat: Literate programming IHow: Clone, fork, and complete the steps in Lab 0.Why: To put literate programming techniques covered in Recipe 0 into practice. Specifically, you will create and edit a Quarto document and render a report in PDF format."
  },
  {
    "objectID": "preface.html#sec-p-summary",
    "href": "preface.html#sec-p-summary",
    "title": "Preface",
    "section": "Summary",
    "text": "Summary\nIn the Preface, we lay the groundwork by introducing the textbook’s underlying principles, learning goals, teaching methods, and target audience. The chapter also offers advice on how to navigate the book’s layout, comprehend its subject matter, and make use of supplementary materials. Crucial insights from this section involve grasping the book’s objectives and aims, which center around instructing readers on quantitative text analysis for linguistics using R while emphasizing reproducible research. This chapter assists readers in setting up a working R development environment ensuring they can effectively engage with the material. Moreover, the Preface provides guidance on how to get help with R and other related software tools and deciphering conventions in the text. With this foundation, you’re now prepared to delve into the captivating realm of text analysis in the subsequent chapter, titled “Text Analysis in Context.”"
  },
  {
    "objectID": "preface.html#sec-p-questions",
    "href": "preface.html#sec-p-questions",
    "title": "Preface",
    "section": "Questions",
    "text": "Questions\n\n\n\n Revise/ add questions.\n\n\n\n\n\n\n\nConceptual questions\n\nHow is the textbook designed to be accessible for both novice and seasoned practitioners in the area of quantitative text analysis?\nWhat is the purpose of the textbook and what are the three areas it aims to scaffold?\nWhat are the main components of each chapter, and how are they structured to support learning outcomes?\nHow does the structure of the textbook and associated resources work to support learning and proficiency in areas?\nWhat is the role of programmatic approaches in quantitative text analysis?\nWhat is the relationship between R and an IDE (e.g. RStudio, VS Code)?\nWhat is the relationship between R and a version control system (e.g. Git)?\n\n\n\n\n\n\n\n\n\n\nTechnical exercises\n\nInstall the latest version of R by following the instructions for your operating system. https://cran.r-project.org/\n\nInstall RStudio Desktop https://www.rstudio.com/products/rstudio/download/\n\nVerify a Git installation or install Git (Windows: https://git-scm.com/downloads). Git a version control system that allows you to track changes to files and collaborate with others through GitHub.\nCreate a free GitHub account at https://github.com/join.\nInstall the tidyverse package in R by running install.packages(\"tidyverse\") in the R Console pane.\nInstall the swirl package by running install.packages(\"swirl\") in the R Console pane.\nOpen RStudio and create a new project for this textbook. This will help you keep your code and files organized.\n\n\n\n\n\n\n\n\nAckoff, Russell L. 1989. “From Data to Wisdom.” Journal of Applied Systems Analysis 16 (1): 3–9.\n\n\nAllaire, JJ. 2023. Quarto: R Interface to Quarto Markdown Publishing System. https://github.com/quarto-dev/quarto-r.\n\n\nAllaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, et al. 2023. Rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown.\n\n\nBuckheit, Jonathan B., and David L. Donoho. 1995. “Wavelab and Reproducible Research.” In Wavelets and Statistics, 55–81. Springer.\n\n\nCarmi, Elinor, Simeon J. Yates, Eleanor Lockley, and Alicja Pawluczuk. 2020. “Data Citizenship: Rethinking Data Literacy in the Age of Disinformation, Misinformation, and Malinformation.” Internet Policy Review 9 (2).\n\n\nFrancom, Jerid. 2023. Qtalrkit: Quantitative Text Analysis for Linguists Resource Kit. https://github.com/qtalr/qtalrkit.\n\n\nGandrud, Christopher. 2015. Reproducible Research with r and r Studio. Second edition. CRC Press.\n\n\nKrathwohl, David R. 2002. “A Revision of Bloom’s Taxonomy: An Overview.” Theory into Practice 41 (4): 212–18.\n\n\nKross, Sean, Nick Carchedi, Bill Bauer, and Gina Grdina. 2020. Swirl: Learn r, in r. http://swirlstats.com.\n\n\nRowley, Jennifer. 2007. “The Wisdom Hierarchy: Representations of the DIKW Hierarchy.” Journal of Information Science 33 (2): 163–80. https://doi.org/10.1177/0165551506070706.\n\n\nWickham, Hadley. 2023. Tidyverse: Easily Install and Load the Tidyverse. https://tidyverse.tidyverse.org.\n\n\nWickham, Hadley, Jennifer Bryan, Malcolm Barrett, and Andy Teucher. 2023. Usethis: Automate Package and Project Setup. https://usethis.r-lib.org.\n\n\nWickham, Hadley, Jim Hester, Winston Chang, and Jennifer Bryan. 2022. Devtools: Tools to Make Developing r Packages Easier. https://devtools.r-lib.org/.\n\n\nXie, Yihui. 2023. Tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents. https://github.com/rstudio/tinytex."
  },
  {
    "objectID": "preface.html#footnotes",
    "href": "preface.html#footnotes",
    "title": "Preface",
    "section": "",
    "text": "Research data and questions are primarily based on English for wide accessibility as it is the de facto language of academics and research. However, the methods and techniques presented in this textbook are applicable to many other languages.↩︎\nAdapted from Ackoff (1989) and Rowley (2007).↩︎\nThese stages attempt to capture the general progression of learning reflected in Bloom’s Taxonomy (see Krathwohl (2002) for a description and revision).↩︎\nErrata for the textbook is found on the qtalr website.↩︎\nIf you are using this textbook in a course, consider using a CMS (e.g. Canvas, Blackboard, etc.) or the web-based social annotation tool Hypothes.is to facilitate reader questions and discussion.↩︎"
  },
  {
    "objectID": "orientation.html",
    "href": "orientation.html",
    "title": "Orientation",
    "section": "",
    "text": "In this section the aims are to: 1) provide an overview of quantitative research and their applications, by both highlighting visible applications and notable research in various fields, 2) consider how quantitative research contributes to language research, and 3) layout the main types of research and situate quantitative text analysis inside these.\n\n Update the overview of Part I “Orientation” to reflect the new structure of the chapter."
  },
  {
    "objectID": "text-analysis.html#text-making-sense-of-a-complex-world",
    "href": "text-analysis.html#text-making-sense-of-a-complex-world",
    "title": "1  Text analysis in context",
    "section": "\n1.1 Making sense of a complex world",
    "text": "1.1 Making sense of a complex world\n\n\n1.1.1 Heuristic Understanding\n\nThe world around us is full of actions and interactions so numerous that it is difficult to really comprehend. As each individual sees and experiences this world, we gain knowledge and build up heuristic understanding about how it works and how we can interact with it. This happens regardless of your educational background. As humans we are built for this. Our minds process countless sensory inputs. They underlie skills and abilities that we take for granted like being able to predict what will happen if you see someone about to knock a wine glass off a table and onto a concrete floor. You’ve never seen this object before and this is the first time you’ve been to this winery, but somehow and from somewhere you ‘instinctively’ make an effort to warn the would-be-glass-breaker before it is too late. You most likely have not stopped to consider where this predictive knowledge comes from, or if you have, you may have just chalked it up to ‘common sense’. As common as it may be, it is an incredible display of the brain’s capacity to monitor your environment, relate the events and observations that take place, and store that information all the time not making a big fuss to tell your conscious mind what it’s up to.\nSo wait, this is a textbook on text analysis, right? So what does all this have to do with that? Well, there are two points to make that are relevant for framing our journey: (1) the world is constantly churning out data in real-time at a scale that is daunting and (2) for all the power of the brain that works so efficiently behind the scene making sense of the world, we are one individual living one life that has a limited view of the world at large. Let me expand on these two points a little more.\nFirst let’s be clear. There is no way for anyone to experience all things at all times. But even extremely reduced slices of reality are still vastly outside of our experiential capacity, at least in real-time. One can make the point that since the inception of the internet an individual’s ability to experience larger slices of the world has increased. But could you imagine reading, watching, and listening to every file that is currently accessible on the web? Or has been? (See the Wayback Machine.) Scale this down even further; let’s take Wikipedia, the world’s largest encyclopedia. Can you imagine reading every wiki entry? As large as a resource such as Wikipedia is 1, it is still a small fragment of the written language that is produced on the web, just the web 2. Consider that for a moment.\nTo my second framing point, which is actually two points in one. I underscored the efficiency of our brain’s capacity to make sense of the world. That efficiency comes from some clever evolutionary twists that lead our brain to take in the world but it makes some shortcuts that compress the raw experience into heuristic understanding. What that means is that the brain is not a supercomputer. It does not store every experience in raw form, we do not have access to the records of our experience like we would imagine a computer would have access to the records logged in a database. Where our brains do excel is in making associations and predictions that help us (most of the time) navigate the complex world we inhabit. This point is key –our brains are doing some amazing work, but that work can give us the impression that we understand the world in more detail that we actually do. Let’s do a little thought experiment. Close your eyes and think about the last time you saw your best friend. What were they wearing? Can you remember the colors? If your like me, or any other human, you probably will have a pretty confident feeling that you know the answers to these questions and there is a chance you a right. But it has been demonstrated in numerous experiments on human memory that our confidence does not correlate with accuracy (Talarico and Rubin 2003; Roediger and McDermott 2000). You’ve experienced an event, but there is no real reason that we should bet our lives on what we experienced. It’s a little bit scary, for sure, but the magic is that it works ‘good enough’ for practical purposes.\nSo here’s the deal: as humans we are (1) clearly unable to experience large swaths of experience by the simple fact that we are individuals living individual lives and (2) the experiences we do live are not recorded in memory with perfect precision and therefore we cannot ‘trust’ our intuitions, at least not in an absolute sense.\n\n\n\n\n\n\n Consider this\nHow might your own experiences and biases influence your understanding of the world? What are some ways that you can mitigate these biases? Is ever possible to be completely objective? How might biases influence the way you approach text analysis?\n\n\n\n\n1.1.2 Science to advance understanding\n\nWhat does that mean for our human curiosity about the world around us and our ability to reliably make sense of it? In short it means that we need to approach understanding our world with the tools of science. Science starts with a question, identifies and collects data, careful selected slices of the complex world, submits this data to analysis through clearly defined and reproducible procedures, and reports the results for others to evaluate. This process is repeated, modifying, and manipulating the procedures, asking new questions and positing new explanations, all in an effort to make inroads to bring the complex into tangible view.\nIn essence what science does is attempt to subvert our inherent limitations in understanding by drawing on carefully and purposefully collected slices of observable experience and letting the analysis of these observations speak, even if it goes against our intuitions (those powerful but sometime spurious heuristics that our brains use to make sense of the world)."
  },
  {
    "objectID": "text-analysis.html#data-analysis",
    "href": "text-analysis.html#data-analysis",
    "title": "1  Text analysis in context",
    "section": "\n1.2 Data analysis",
    "text": "1.2 Data analysis\n\n\n1.2.1 Emergence of data science\n\nAt this point I’ve sketched an outline strengths and limitations of humans’ ability to make sense of the world and why science is used to address these limitations. This science I’ve described is the one you are familiar with and it has been an indispensable tool to make sense of the world. If you are like me, this description of science may be associated with visions of white coats, labs, and petri dishes. While science’s foundation still stands strong in the 21st century, a series of intellectual and technological events mid-20th century set in motion changes that have changed aspects about how science is done, not why it is done. We could call this Science 2.0, but let’s use the more popularized term data science. The recognized beginnings of data science are attributed to work in the “Statistics and Data Analysis Research” department at Bell Labs during the 1960s. Although primarily conceptual and theoretic at the time, a framework for quantitative data analysis took shape that would anticipate what would come: sizable datasets which would “[…] require advanced statistical and computational techniques […] and the software to implement them.” (Chambers 2020) This framework emphasized both the inference-based research of traditional science, but also embraced exploratory research and recognized the need to address practical considerations that would arise when working with and deriving insight from an abundance of machine-readable data.\nFast-forward to the 21st century a world in which machine-readable data is truly in abundance. With increased computing power, the emergence of the world wide web, and wide adoption of mobile devices electronic communication skyrocketed around the globe. To put this in perspective, in 2019 it was estimated that every minute 511 thousand tweets were posted, 18.1 million text messages were sent, and 188 million emails were sent (“Data Never Sleeps 7.0 Infographic” 2019). The data flood has not been limited to language, there are more sensors and recording devices than ever before which capture evermore swaths of the world we live in (Desjardins 2019). Where increased computing power gave rise to the influx of data, it is also one of the primary methods for gathering, preparing, transforming, analyzing, and communicating insight derived from this data (Donoho 2017). The vision laid out in the 1960s at Bell Labs had come to fruition.\n\n1.2.2 Computing skills, statistical knowledge, and domain knowledge\n\nData science is not predicated on data alone. Turning data into insight takes computing skills (i.e. programming), statistical knowledge, and domain expertise. This triad has been popularly represented as a Venn diagram such as in Figure 1.1.\n\n\n\n\nFigure 1.1: Data Science Venn Diagram adapted from Drew Conway.\n\n\n\nThe computing skills component of data science is the ability to write code to perform the data analysis process. This is the primary approach for working with data at scale. The statistical knowledge component of data science is the ability to apply statistical methods to data to derive insight. Domain expertise provides researchers insight at key junctures in the development of a research project and aid researchers in evaluating results.\nThis triad of skills in combination with reproducible research practices is the foundational toolbelt of data science (Hicks and Peng 2019). Reproducible research entails the use of computational tools to automate the process of data analysis. This automation is achieved by writing code that can be executed to replicate the data analysis. This code can then be shared through code sharing repositories, such as GitHub, where it can be viewed, downloaded, and executed by others. This adds transparency to the process and allows others to build on previous work. This is in contrast to traditional approaches where data analysis is performed (semi-)manually, results are reported in a static document such as a report or journal article, and the data analysis process is not shared. This approach is not reproducible because the data analysis process is not transparent and cannot be replicated. This is problematic because it is difficult to evaluate the results and build on previous work. Reproducible research practices are a key component of data science and are emphasized throughout this book.\n\n1.2.3 Applications of data science\nEquipped with the data science toolbelt, the interest in deriving insight from the available data is now almost ubiquitous. The science of data has now reached deep into all aspects of life where making sense of the world is sought. Predicting whether a loan applicant will get a loan (Bao, Lianju, and Yue 2019), whether a lump is cancerous (Saxena and Gyanchandani 2020), what films to recommend based on your previous viewing history (Gomez-Uribe and Hunt 2015), what players a sports team should sign (Lewis 2004) all now incorporate a common set of data analysis tools.\nThe data science toolbelt also underlies well-known public-facing language applications. From the language-capable chat applications, plagiarism detection software, machine translation algorithms, and search engines, tangible results of quantitative approaches to language are becoming standard fixtures in our lives, as seen in Figure 1.2.\n\n\n\n\n\nFigure 1.2: Well-known public-facing language applications\n\n\n\nThe spread of quantitative data analysis too has taken root in academia. Even in areas that on first blush don’t appear readily approachable in a quantitative manner, such as fields in the social sciences and humanities, data science is making important and sometimes disciplinary changes to the way that academic research is conducted. This textbook focuses in on a domain that cuts across many of these fields; namely language. At this point let’s turn to quantitative approaches to language analysis as we work closer to contextualizing text analysis."
  },
  {
    "objectID": "text-analysis.html#language-analysis",
    "href": "text-analysis.html#language-analysis",
    "title": "1  Text analysis in context",
    "section": "\n1.3 Language analysis",
    "text": "1.3 Language analysis\nLanguage is a defining characteristic of our species. Since antiquity, language has attracted interest across disciplines and schools of thought. In the early 20th century, the development of the rigorous approach to study of language as a field in its own right took root (Campbell 2001), yet a plurality of theoretical views and methodological approaches remained. Contemporary linguistics bares this complex history and is far from theoretically and methodologically unified.\nEither based on the tenets of theoretical frameworks and/or the objects of study of particular fields, approaches to language research vary. On the one hand some language research commonly applies qualitative assessment of language structure and/ or use. Qualitative approaches describe and account for characteristics, or “qualities”, that can be observed, but not measured (e.g. introspective methods, ethnographic methods, etc.)\nOn the other hand other language research programs employ quantitative research methods either out of necessity given the object of study (phonetics, psycholinguistics, etc.) or based on theoretical principles (Cognitive Linguistics, Connectionism, etc.). Quantitative approaches involve measurements of properties of language that can be observed and measured (e.g. frequency of use, reaction time, etc.).\nThese latter research areas and theoretical paradigms employ methods that share much of the common data analysis toolbox described in the previous section. In effect, this establishes a common methodological language between other language research fields but also with research outside of linguistics.\nHowever, there is never a one-size-fits all approach to anything –much less data analysis. And even in quantitative language analysis there is a key methodological distinction that has downstream effects in terms of procedure but also in terms of interpretation. The key distinction that we need to make at this point, which will provide context for our introduction to quantitative text analysis, comes down to the approach to collecting language data and the nature of that data. This distinction is between experimental data and observational data.\nExperimental approaches start with a intentionally designed hypothesis and lay out a research methodology with appropriate instruments and a plan to collect data that shows promise for shedding light on the validity of the hypothesis. Experimental approaches are conducted under controlled contexts, usually a lab environment, in which participants are recruited to perform a language related task with stimuli that have been carefully curated by researchers to elicit some aspect of language behavior of interest. Experimental approaches to language research are heavily influenced by procedures adapted from psychology. This link is logical as language is a central area of study in cognitive psychology. This approach looks much like the white-coat science that we made reference to earlier but, as in most quantitative research, has now taken advantage of the data analysis toolbelt to collect and organize much larger quantities of data and conduct statistically more robust analysis procedures and communicate findings more efficiently.\nObservational approaches are a bit more of a mixed bag in terms of the rationale for the study; they may either start with a testable hypothesis or in other cases may start with a more open-ended research question to explore. But a more fundamental distinction between the two is drawn in the amount of control the researcher has on contexts and conditions in which the language behavior data to be collected is produced. Observational approaches seek out records of language behavior that is produced by language speakers for communicative purposes in natural(istic) contexts. This may take place in labs (language development, language disorders, etc.), but more often than not, language is collected from sources where speakers are performing language as part of their daily lives –whether that be posting on social media, speaking on the telephone, making political speeches, writing class essays, reporting the latest news for a newspaper, or crafting the next novel destined to be a New York Times best-seller. What is more, data collected from the ‘wild’ varies more in structure relative to data collected in experimental approaches and requires a number of steps to prepare the data to sync up with the data analysis toolbelt.\nI liken this distinction between experimental and observational data collection to the difference between farming and foraging. Experimental approaches are like farming; the groundwork for a research plan is designed, much as a field is prepared for seeding, then the researcher performs as series of tasks to produce data, just as a farmer waters and cares for the crops, the results of the process bear fruit, data in our case, and this data is harvested. Observational approaches are like foraging; the researcher scans the available environmental landscape for viable sources of data from all the naturally existing sources, these sources are assessed as to their usefulness and value to address the research question, the most viable is selected, and then the data is collected.\nThe data acquired from both of these approaches have their trade-offs, just as farming and foraging. Experimental approaches directly elicit language behavior in highly controlled conditions. This directness and level of control has the benefit of allowing researchers to precisely track how particular experimental conditions effect language behavior. As these conditions are an explicit part of the design and therefore the resulting language behavior can be more precisely attributed to the experimental manipulation. The primary shortcoming of experimental approaches is that there is a level of artificialness to this directness and control. Whether it is the language materials used in the task, the task itself, or the fact that the procedure takes place under supervision the language behavior elicited can diverge quite significantly from language behavior performed in natural communicative settings. Observational approaches show complementary strengths and shortcomings.\nWhereas experimental approaches may diverge from natural language use, observational approaches strive to identify and collected language behavior data in natural, uncontrolled, and unmonitored contexts, as seen in Figure 1.3. In this way observational approaches do not have to question to what extent the language behavior data is or is not performed as a natural communicative act. On the flipside, the contexts in which natural language communication take place are complex relative to experimental contexts. Language collected from natural contexts are nested within the complex workings of a complex world and as such inevitably include a host of factors and conditions which can prove challenging to disentangle from the language phenomenon of interest but must be addressed in order to draw reliable associations and conclusions.\n\n\n\n\nFigure 1.3: Trade-offs between experimental and observational data collection methods.\n\n\n\nThe upshot, then, is twofold: (1) data collection methods matter for research design and interpretation and (2) there is no single best approach to data collection, each have their strengths and shortcomings. In the ideal, a robust science of language will include insight from both experimental and observational approaches (Gilquin and Gries 2009). And evermore there is greater appreciation for the complementary nature of experimental and observational approaches and a growing body of research which highlights this recognition.\n\n\n\n\n\n\n Case study \nManning (2003) discusses the use of probabilistic models in syntax to account for the variability in language usage and the presence of both hard and soft constraints in grammar. The paper touches on the statistical methods in text analysis, the importance of distinguishing between external and internal language, and the limitations of Generative Grammar. Overall, the paper suggests that usage-based and formal syntax can learn from each other to better understand language variation and change.\n\n\n\nGiven their particular trade-offs observational data is often used as an exploratory starting point to help build insight and form predictions that can then be submitted to experimental conditions. In this way, studies based on observational data serve as an exploratory tool to gather a better and more externally valid view of language use which can then serve to make prediction that can be explored with more precision in an experimental paradigm. However, this is not always the case; observational data is also often used in hypothesis-testing contexts as well. And furthermore, some in some language-related fields, a hypothesis-testing is not the approach for deriving knowledge and insight."
  },
  {
    "objectID": "text-analysis.html#text-analysis",
    "href": "text-analysis.html#text-analysis",
    "title": "1  Text analysis in context",
    "section": "\n1.4 Text analysis",
    "text": "1.4 Text analysis\nIn a nutshell, text analysis is the process of leveraging the data science toolbelt to derive insight from textual data collected through observational methods. In the next subsections, I will unpack this definition and discuss the primary components that make up text analysis including research appoaches and technical implementation, as well as practical applications.\n\n1.4.1 Approaches\nText analysis is a multifacited research methodology. It can be used use facilitate the qualitative exploration of smaller, human-digestable textual information, but is more often employed quantitatively to bring to the surface patterns and relationships in large samples of textual data that would be otherwise difficult, if not impossible, to identify manually.\nText being text, there are a series of data prepration steps that must be taken to ready the data for analysis. In addition to collecting the data, the data must be organized, cleaned, and transformed into a format that is amenable to statistical analysis.\nThe statistical and evaluative approach employed in the analysis is dependent on the aim of the research. For research aimed at exploring and uncovering patterns and relationships in a data-driven manner, Exploratory Data Analysis (EDA) is employed. EDA combines descriptive statistics, visualizations, and statistical learning methods in an iterative and interactive way to provide the researcher the ability to identify patterns and relationships and to evaluate whether and why they are meaningful.\nPredictive Data Analysis (PDA), applied in research for outcome prediction, is a supervised machine learning task. It uses feature sets to predict an outcome variable. Its primary evaluation metric is the prediction accuracy on new data. However, for many text analysis tasks, human interpretation is crucial to provide context and assess the significance of the results.\nResearch aimed at explaining relationships between variables and the population from which the sample was drawn will adopt an Inferential Data Analysis (IDA) approach. IDA is a theory-driven process that employs statistical models for hypothesis testing. The extent to which the results can be confidently generalized to the population is the primary evaluation metric.\nAs we see, text analysis can be used for a variety of purposes; from data-driven exploration and discovery to hypothesis testing and generalization.\n\n1.4.2 Implementation\nTo ensure that the results of text analysis projects are replicable and transparent, programming strategies play an integral role at each stage of the implementation of a research project. While there are a number of programming languages that can be used for text analysis, R widely adopted in linguistics research. R is a free and open-source programming language that is specifically designed for statistical computing and graphics. It has a large and active community of users and developers, and a robust ecosystem of packages which make it a powerful and flexible language that is well-suited for core text analysis tasks: data collection, organization, transformation, analysis, and visualization. When combined with Quarto for literate programming and GitHub for version control and collaboration, R provides a robust and reproducible workflow for text analysis.\n\n1.4.3 Applications\nSo what are some applications of text analysis? Most public facing applications stem from Computational Linguistic research, often known as Natural Language Processing (NLP) by practitioners. Whether it be using search engines, online translators, submitting your paper to plagiarism detection software, etc. many of the text analysis methods we will cover are at play.\n\n\n\n\n\n\n\n Consider this\nWhat are some other public facing applications of text analysis that you are aware of?\n\n\n\nIn academia the use of quantitative text analysis is even more widespread, despite the lack of public fanfare. In linguistics, text analysis is applied to a wide range of topics and research questions in both theoretical and applied subfields, as seen in Example 1.1 and Example 1.2.\n\nExample 1.1 Theoretical linguistics\n\n\nHay (2002) use a corpus study to investigate the role of frequency and phonotatics in affix ordering in English.\n\nRiehemann (2001) explores the extent to which idiomatic expressions (e.g. ‘raise hell’) are lexical or syntactic units.\n\nBresnan (2007) investigate the claim that possessed deverbal nouns in English (e.g. ‘the city’s destruction’) are subject to a syntactic constraint that requires the possessor to be affected by the action denoted by the deverbal noun.\n\n\n\nExample 1.2 Applied linguistics\n\n\n\nWulff, Stefanowitsch, and Gries (2007) explore differences between British and American English at the lexico-syntactic level in the into-causative construction (e.g. ‘He tricked me into employing him.’). \n\n\nEisenstein et al. (2012) track the geographic spread of neologisms (e.g. ‘bruh’, ‘af’, ’-__-’) from city to city in the United States using Twitter data collected between 6/2009 and 5/2011. \n\n\nBychkovska and Lee (2017) investigates possible differences between L1-English and L1-Chinese undergraduate students’ use of lexical bundles, multiword sequences which are extended collocations (e.g. ‘as the result of’), in argumentative essays. \n\n\nJaeger and Snider (2007) use a corpus study to investigate the phenomenon of syntactic persistence, the increased tendency for speakers to use a particular syntactic form over an alternate when the syntactic form has been recently processed. \n\n\nVoigt et al. (2017) explore potential racial disparities in officer respect in police body camera footage. \n\n\nOlohan (2008) investigate the extent to which translated texts differ from native texts do to ‘explicitation’.\n\n\nSo too, text analysis is used in a variety of other fields where insight from language is sought, as seen in Example 1.3.\n\nExample 1.3 Language-related fields\n\n\n\nKloumann et al. (2012) explore the extent to which languages are positively, neutrally, or negatively biased. \n\n\nMosteller and Wallace (1963) provide a method for solving the authorship debate surrounding The Federalist papers. \n\n\nConway et al. (2012) investigate whether the established drop in language complexity of rhetoric in election seasons is associated with election outcomes.\n\n\n\n\n\n\n\n\n Consider this\nLanguage is a key component of human communication and interaction. What are some other areas of research in and outside linguistics that you think could be explored using text analysis methods?\n\n\n\nThese studies in Examples 1.1, 1.2, and 1.3 are just a few illustrations of the contributions of text analysis as the primary method to gain a deeper understanding of language structure, function, variation, and acquisition. As a method, however, text analysis can also be used to support other research methods. For example, text analysis can be used collect data, generate authentic materials, provide linguistic annotation, to generate hypotheses, for either qualitative and/ or quantitative approaches. Together these efforts contribute to a more robust language science by incorporating externally valid data and providing methodological triangulation (Francom 2022).\nIn sum, the applications highlighted in this section underscore the versatility of text analysis as a research method. Whether it be in the public sphere or in academia, text analysis methods furnish a set of powerful tools for gaining insight from language data."
  },
  {
    "objectID": "text-analysis.html#summary",
    "href": "text-analysis.html#summary",
    "title": "1  Text analysis in context",
    "section": "Summary",
    "text": "Summary\nIn this chapter I started with some general observations about the difficulty of making sense of a complex world. The standard approach to overcoming inherent human limitations in sense making is science. In the 21st century the toolbelt for doing scientific research and exploration has grown in terms of the amount of data available, the statistical methods for analyzing the data, and the computational power to manage, store, and share the data, methods, and results from quantitative research. The methods and tools for deriving insight from data have made significant inroads in and outside academia, and increasingly figure in the quantitative investigation of language. Text analysis is a particular branch of this enterprise based on observational data from real-world language and is used in a wide variety of fields.\nIn the end I hope that you enjoy this exploration into text analysis. Although the learning curve at times may seem steep –the experience you will gain will not only improve your data literacy, research skills, and programmings skills but also enhance your appreciation for the richness of human language and its important role in our everyday lives."
  },
  {
    "objectID": "text-analysis.html#actitivies",
    "href": "text-analysis.html#actitivies",
    "title": "1  Text analysis in context",
    "section": "Actitivies",
    "text": "Actitivies\nThe following activities build on your introduction to R and Quarto in the previous chapter. In these activities you will uncover more features offered by Quarto which will enhance your ability to produce comprehensive reproducible research documents. You will apply the capabilities of Quarto in a practical context conveying the objectives and key discoveries from a primary research article.\n\n\n\n\n\n\n Recipe\n\nWhat: Academic writing with QuartoHow: Read Recipe 1 and participate in the Hypothes.is online social annotation.Why: To explore additional functionality in Quarto: numbered sections, table of contents, in-line citations and a document-final references list, and cross-referenced tables and figures.\n\n\n\n\n\n\n\n\n\n Lab\n\n\nWhat: Literate programming IIHow: Clone, fork, and complete the steps in Lab 1.Why: To put into practice Quarto functionality to communicate the aim(s) and main finding(s) from a primary research article and to interpret a related plot."
  },
  {
    "objectID": "text-analysis.html#questions",
    "href": "text-analysis.html#questions",
    "title": "1  Text analysis in context",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\n\nConceptual questions\n\nHow has scientific research and exploration changed in the 21st century?\nWhat are the three basic skill sets that make up the data science toolbelt?\nWhat are the benefits of reproducible research in data science?\nExplain the trade-offs between experimental and observational data collection methods.\nWhat is text analysis and how is it used in various fields?\nIdentify research in an area of interest in linguistics that has taken a quantitative approach to text analysis.\nIn your own words, define literate programming?\nWhat are the benefits of literate programming?\nWhat are the benefits of using R and Quarto for literate programming?\n\n\n\n\n\n\n\n\n\n\nTechnical exercises\n\nCreate a literate programming document in Quarto. Edit the yaml header to reflect details of the work and add your work with the data types in R to code chunks. Add, commit, and push the project to GitHub.\nIn the Quarto document, explore using R to create vectors and explore their properties.\nExplore the following resources and with the goal to identify a quantitative text analysis project. Rpubs, GitHub, DataCamp, Kaggle, R-bloggers.\n\n … more to come …\n\n\n\n\n\n\n\n\n\n\nBao, Wang, Ning Lianju, and Kong Yue. 2019. “Integration of Unsupervised and Supervised Machine Learning Algorithms for Credit Risk Assessment.” Expert Systems with Applications 128 (August): 301–15. https://doi.org/10.1016/j.eswa.2019.02.033.\n\n\nBresnan, Joan. 2007. “A Few Lessons from Typology.” Linguistic Typology 11 (1): 297–306.\n\n\nBychkovska, Tetyana, and Joseph J. Lee. 2017. “At the Same Time: Lexical Bundles in L1 and L2 University Student Argumentative Writing.” Journal of English for Academic Purposes 30 (November): 38–52. https://doi.org/10.1016/j.jeap.2017.10.008.\n\n\nCampbell, Lyle. 2001. “The History of Linguistics.” In The Handbook of Linguistics, edited by Mark Aronoff and Janie Rees-Miller, 81–104. Blackwell Handbooks in Linguistics. Blackwell Publishers.\n\n\nChambers, John M. 2020. “S, r, and Data Science.” Proceedings of the ACM on Programming Languages 4 (HOPL): 1–17. https://doi.org/10.1145/3386334.\n\n\nConway, Lucian Gideon, Laura Janelle Gornick, Chelsea Burfeind, Paul Mandella, Andrea Kuenzli, Shannon C. Houck, and Deven Theresa Fullerton. 2012. “Does Complex or Simple Rhetoric Win Elections? An Integrative Complexity Analysis of u.s. Presidential Campaigns.” Political Psychology 33 (5): 599–618. https://doi.org/10.1111/j.1467-9221.2012.00910.x.\n\n\n“Data Never Sleeps 7.0 Infographic.” 2019. https://www.domo.com/learn/infographic/data-never-sleeps-7.\n\n\nDesjardins, Jeff. 2019. “How Much Data Is Generated Each Day?” Visual Capitalist.\n\n\nDonoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734.\n\n\nEisenstein, Jacob, Brendan O’Connor, Noah A Smith, and Eric P Xing. 2012. “Mapping the Geographical Diffusion of New Words.” Computation and Language, 1–13. https://doi.org/10.1371/journal.pone.0113114.\n\n\nFrancom, Jerid. 2022. “Corpus Studies of Syntax.” In The Cambridge Handbook of Experimental Syntax, edited by Grant Goodall, 687–713. Cambridge Handbooks in Language and Linguistics. Cambridge University Press.\n\n\nGilquin, Gaëtanelle, and Stefan Th Gries. 2009. “Corpora and Experimental Methods: A State-of-the-Art Review.” Corpus Linguistics and Linguistic Theory 5 (1): 1–26. https://doi.org/10.1515/CLLT.2009.001.\n\n\nGomez-Uribe, Carlos A., and Neil Hunt. 2015. “The Netflix Recommender System: Algorithms, Business Value, and Innovation.” ACM Transactions on Management Information Systems (TMIS) 6 (4): 1–19.\n\n\nHay, Jennifer. 2002. “From Speech Perception to Morphology: Affix Ordering Revisited.” Language 78 (3): 527–55.\n\n\nHicks, Stephanie C., and Roger D. Peng. 2019. “Elements and Principles for Characterizing Variation Between Data Analyses.” arXiv. https://doi.org/10.48550/arXiv.1903.07639.\n\n\nJaeger, T Florian, and Neal Snider. 2007. “Implicit Learning and Syntactic Persistence: Surprisal and Cumulativity.” University of Rochester Working Papers in the Language Sciences 3 (1).\n\n\nKloumann, IM, CM Danforth, KD Harris, and CA Bliss. 2012. “Positivity of the English Language.” PloS One.\n\n\nLewis, Michael. 2004. Moneyball: The Art of Winning an Unfair Game. WW Norton & Company.\n\n\nManning, Christopher. 2003. “Probabilistic Syntax.” In Probabilistic Linguistics, edited by Bod, Jennifer Hay, and Jannedy, 289–341. Cambridge, MA: MIT Press.\n\n\nMosteller, Frederick, and David L Wallace. 1963. “Inference in an Authorship Problem.” Journal of the American Statistical Association 58 (302): 275–309. https://www.jstor.org/stable/2283270.\n\n\nOlohan, Maeve. 2008. “Leave It Out! Using a Comparable Corpus to Investigate Aspects of Explicitation in Translation.” Cadernos de Tradução, 153–69.\n\n\nRiehemann, Susanne Z. 2001. “A Constructional Approach to Idioms and Word Formation.” PhD thesis, Stanford.\n\n\nRoediger, H. L. L, and K. B. B McDermott. 2000. “Distortions of Memory.” The Oxford Handbook of Memory, 149–62.\n\n\nSaxena, Shweta, and Manasi Gyanchandani. 2020. “Machine Learning Methods for Computer-Aided Breast Cancer Diagnosis Using Histopathology: A Narrative Review.” Journal of Medical Imaging and Radiation Sciences 51 (1): 182–93.\n\n\nTalarico, Jennifer M., and David C. Rubin. 2003. “Confidence, Not Consistency, Characterizes Flashbulb Memories.” Psychological Science 14 (5): 455–61. https://doi.org/10.1111/1467-9280.02453.\n\n\nVoigt, Rob, Nicholas P. Camp, Vinodkumar Prabhakaran, William L. Hamilton, Rebecca C. Hetey, Camilla M. Griffiths, David Jurgens, Dan Jurafsky, and Jennifer L. Eberhardt. 2017. “Language from Police Body Camera Footage Shows Racial Disparities in Officer Respect.” Proceedings of the National Academy of Sciences 114 (25): 6521–26.\n\n\nWulff, S, A Stefanowitsch, and Stefan Th. Gries. 2007. “Brutal Brits and Persuasive Americans.” Aspects of Meaning."
  },
  {
    "objectID": "text-analysis.html#footnotes",
    "href": "text-analysis.html#footnotes",
    "title": "1  Text analysis in context",
    "section": "",
    "text": "As of 22 July 2021, there are 6,341,359 articles in the English Wikipedia containing over 3.9 billion words occupying around 19 gigabytes of information.↩︎\nFor reference, Common Crawl has millions of gigabytes collected since 2008.↩︎"
  },
  {
    "objectID": "foundations.html",
    "href": "foundations.html",
    "title": "Foundations",
    "section": "",
    "text": "Before working on the specifics of a data project, it is important to establish a fundamental understanding of the characteristics of each of the levels in the “Data, Information, Knowledge, and Insight Hierarchy (DIKI)” (see Figure 1) and the roles each of these levels have in deriving insight from data. In Chapter 2 we will explore the Data and Information levels drawing a distinction between two main types of data (populations and samples) and then cover how data is structured and transformed to generate information (datasets) that is fit for statistical analysis. In Chapter 3 I will outline the importance and distinct types of statistical procedures (descriptive and analytic) that are commonly used in text analysis. Chapter 4 aims to tie these concepts together and cover the required steps for preparing a research blueprint to conduct an original text analysis project."
  },
  {
    "objectID": "understanding-data.html#sec-ud-data",
    "href": "understanding-data.html#sec-ud-data",
    "title": "2  Understanding data",
    "section": "\n2.1 Data",
    "text": "2.1 Data\nData is data, right? The term ‘data’ is so common in popular vernacular it is easy to assume we know what we mean when we say ‘data’. But as in most things, where there are common assumptions there are important details that require more careful consideration. Let’s turn to the first key distinction that we need to make to start to break down the term ‘data’: the difference between populations and samples.\n\n\n2.1.1 Populations\n\nThe first thing that comes to many people’s mind when the term population is used is human populations (derived from Latin ‘populus’). Say for example we pose the question –What’s the population of Milwuakee? When we speak of a population in these terms we are talking about the total sum of individuals living within the geographical boundaries of Milwaukee. In concrete terms, a population an idealized set of objects or events in reality which share a common characteristic or belong to a specific category. The term to highlight here is idealized. Although we can look up the US Census report for Milwaukee and retrieve a figure for the population, this cannot truly be the population. Why is that? Well, whatever method that was used to derive this numerical figure was surely incomplete. If not incomplete, by the time someone recorded the figure some number of residents of Milwaukee moved out, moved in, were born, or passed away. In either case, this example serves to point out that populations are not fixed and are subject to change over time.\nLikewise when we talk about populations in terms of language we dealing with an idealized aspect of linguistic reality. Let’s take the words of the English language as an analog to our previous example population. In this case the words are the people and English is the grouping characteristic. Just as people, words move out, move in, are born, and pass away. Any compendium of the words of English at any moment is almost instananeously incomplete. This is true for all populations, save those relatively rare cases in which the grouping characteristics select a narrow slice of reality which is objectively measurable and whose membership is fixed (the complete works of Shakespeare, for example).\nIn sum, (most) populations are amorphous moving targets. We subjectively hold them to exist, but in practical terms we often cannot nail down the specifics of populations. So how do researchers go about studying populations if they are theoretically impossible to access directly? The strategy employed is called sampling.\n\n2.1.2 Samples\n\nA sample is the product of a subjective process of selecting a finite set of observations from an idealized population with the goal of capturing the relevant characteristics of the target population. The degree of representativeness of a sample is the extent to which the sample reflects the characteristics of the population. The degree of representativeness is crucial for research as it directly impacts of any findings based on the sample.\n\nTo maximize the representativeness of a sample, researchers employ a variety of strategies. One of the first and sometimes the easiest strategy is to increase the sample size. A larger sample will always be more representative than a smaller sample. Sample size, however, is often not enough. It is not hard to imagine a large sample which by chance captures only a subset of the features of the population. Another step to enhance sample representativeness is to apply random sampling. Together a large random sample has an even better chance of reflecting the main characteristics of the population better than a large or random sample. But, random as random is, we still run the risk of acquiring a skewed sample (i.e. a sample with low representativeness).\nTo help mitigate these issues, there are two more strategies that can be applied to improve sample representativeness. Note, however, that while size and random samples can be applied to any sample with few assumptions about internal characteristics of the population, these next two strategies require decisions depend on the presumed internal characteristics of the population.\nThe first of these more informed sampling strategies is called stratified sampling. Stratified samples make (educated) assumptions about sub-components within the population of interest. With these sub-populations in mind, large random samples are acquired for each sub-population, or strata. At a minimum, stratified samples can be no less representative than random sampling alone, but the chances that the sample is better increases. Can there be problems in the approach? Yes, and on two fronts. First knowledge of the internal components of a population are often based on a limited or incomplete knowledge of the population (remember populations are idealized). In other words, strata are selected subjectively by researchers using various heuristics some of which are based on some sense of ‘common knowledge’.\nThe second front on which stratified sampling can err concerns the relative sizes of the sub-components relative to the whole population, which is known as balance. Even if the relevant sub-components are identified, their relative size adds another challenge which researchers must address in order to maximize the representativeness of a sample.\nTogether, large randomly selected and balanced stratified samples set the benchmark for sampling. However, hitting this ideal is not always feasible. There are situations where sizeable samples are not accessible. Alternatively, there may be instances where the population or its strata are not well understood. In such scenarios, researchers have to work with the most suitable sample they can obtain given the limitations of their research project.\n\n2.1.3 Corpora\nA key feature of a sample is that it is purposely selected to model a target population. In text analysis, a purposely sampled collection of texts, of the type defined here, is known as a corpus (pl. corpora). A set of texts or documents which have not been selected purposely lack a sampling frame, and therefore is not a corpus. The sampling frame, hence the populations modeled, in any given corpus will vary. It is key to vet corpora to ensure that the resource’s sampling frame and the research project’s target populations align as closely as possible to safeguard the integrity of research findings later in the research process.\n\n\n\n\n\n\n Consider this\n\n\n\nThe ‘Standard Sample of Present-Day American English’ (known commonly as the Brown Corpus) is widely recognized as one of the first large, machine-readable corpora. Compiled by Kucera and Francis (1967), the corpus is comprised of 1,014,312 words from edited English prose published in the United States in 1961.\nGiven the sampling frame for this corpus visualized in Figure 2.1, can you determine what language population this corpus aims to represent? What types of research might this corpus support or not support?\n\n\n \n\n\n\n\n\n\nFigure 2.1: Overview of the sampling frame of the Brown Corpus.\n\n\n\n\n\n\n\n\n\nTypes\nLet’s take a look at some key characteristics, attributes, and features that distinguish corpora.\nReference\nThe least common and most ambitious corpus resources are those which aim to model the characteristics of a language population. These are known as reference corpora. These are projects designed with wide sampling frames, and require significant investments of time in corpus design and implementation (and continued development) that are usually undertaken by research teams (Ädel 2020).\nThe American National Corpus (ANC) or the British National Corpus (BNC) are corpora which aim to model the general characteristics of a variety of the English language, the former of American English and the later British English. Reference corpora exist for other languages as well: Spanish Reference Corpus of Present-Day Spanish (CREA), German The German Reference Corpus (DeReKo), Turkish Turkish National Corpus (TNC), and many others.\n\n\n\n\n\n\n Consider this\nOf note is the fact that, at present, most of the world’s languages lack reference corpus resources, or any corpus resources whatsoever. “Low-resourced” languages are often less studied, resource scarce, less available in born-digital formats, etc. (Magueresse, Carles, and Heetderks 2020).\nVisit the Clarin overview on reference corpora and then visit LRE Map. Can you find a reference corpus for a language you speak or are interested in studying? If not, consider what can be done to address this gap in the research community.\n\n\n\nSpecialized\n\nSpecialized corpora aim to represent more specific populations. The population may be defined either by modality, genre, time, location, or speaker-oriented characteristics, or some combination thereof. What specialized corpora lack in breadth of coverage, they make up for in depth of coverage by providing a more targeted representation of specific language populations.\nThe Santa Barbara Corpus of Spoken American English (SBCSAE), as you can imagine from the name of the resource, aims to model spoken American English. No claim to written English is included. There are even more specific types of corpora which attempt to model other types of sub-populations such as academic writing, computer-mediated communication (CMC), language use in specific regions of the world, a country, a region of a country, etc.\n\n\n\n\n\n\n\n Consider this\n\n\n\nGrieve, Nini, and Guo (2018) compiled a 8.9 billion-word corpus of geotagged posts from Twitter between 2013-2014 in the United States. The authors provide a search interface to explore relationship between lexical usage and geographic location. Explore this corpus searching for terms related to slang (“hella”, “wicked”), geographical (“mountain”, “river”), meteorological (“snow”, “rain”), and/ or any other term types. What types of patterns do you find? What are the benefits and/ or limitations of this type of data and/ or interface?\n\n\n \n\n\n\n\n\n\nFigure 2.2: Example distribution of the term ‘Ya’ll’ the Word Mapper project.\n\n\n\n\n\n\n\n\n\nAnother set of specialized corpora are resources which aim to compile texts from different languages or different language varieties for direct or indirect comparison. Corpora that are directly comparable, that is they include source and translated texts, are called parallel corpora. Parallel corpora include different languages or language varieties that are indexed and aligned at some linguistic level (i.e. word, phrase, sentence, paragraph, or document), see OPUS. Corpora that are compiled with different languages or language varieties but are not directly aligned are called comparable corpora. The comparable language or language varieties are sampled with the same or similar sampling frame, for example Brown and LOB corpora.\nThe aim of the quantitative text researcher is to select the corpus, or corpora, which best align with the purpose of the research. For example, a general corpus such as the American National Corpus may be better suited to address a question dealing with the way American English works, but this general resource may lack detail in certain areas, such as medical language, that may be vital for a research project aimed at understanding changes in medical terminology. Furthermore, a researcher studying spoken language might collect a corpus of transcribed conversations from a particular community or region, such as the SBCSAE. While this would not include every possible spoken utterance produced by members of that group, it could be considered a representative sample of the population of speech in that context.\nSources\n\nPublished\nThe most common source of data used in contemporary quantitative research is the internet. On the web an investigator can access corpora published for research purposes. Many organizations exist around the globe that provide access to published corpora in browsable catalogs, or repositories. There are repositories dedicated to language research, in general, such as the Language Data Consortium or that specialize in specific domains, such as the spoken language repository TalkBank. It is always advisable to start looking for the available language data in a repository. The advantage of beginning your data search in repositories is that a repository, especially those geared towards the linguistic community, will make identifying language corpora faster than through a general web search. Furthermore, repositories often require certain standards for corpus format and documentation for publication.\nRepositories are by no means the only source of published corpora on the web. Researchers from around the world provide access to corpora and datasets on their own sites or through data sharing platforms. Corpora of various sizes and scopes will often be accessible on a dedicated homepage or appear on the homepage of a sponsoring institution. These resources may be available for download or via search inferaces. Finding these resources is often a matter of doing a web search with the word ‘corpus’ and a list of desired attributes, including language, modality, register, etc.\nAs part of a general movement towards reproducibility, more corpora are available on data sharing platforms such as GitHub, Zenodo, Re3data, OSF, etc. These platforms enable researchers to securely store, manage, and share data with others. Support is provided for various types of data, including documents and code, and as such they are a good place to look as they often include reproducible research projects as well.\nDevelop\nLanguage corpora prepared by researchers and research groups listed on repositories or hosted by the researchers themselves is often the first place to look for data. The web, however, contains a wealth of language and language-related data that can be accessed by researcher to compile their own corpus. There are two primary ways to attain language data from the web. The first is through an Application Programming Interface (API). APIs are, as the title suggests, programming interfaces which allow access, under certain conditions, to information that a website or database accessible via the web contains.\nThe second, more involved, way to acquire data from the web is is through the process of web scraping. Web scraping is the process of harvesting data from the public-facing web. Language texts may be found on sites as uploaded files, such as pdf or doc (Word) documents, or found displayed as the primary text of a site. Given the wide variety of documents uploaded and language behavior recorded daily on news sites, blogs and the like, compiling a corpus has never been easier. Having said that, how the data is structured and how much data needs to be retrieved can pose practical obstacles to collecting data from the web, particularly if the approach is to acquire the data by manually instead of automating the task.\n\n\n\n\n\n\n Dive deeper\nThe process of corpus development is a topic in and of itself. For a more in-depth discussion of the process, see Ädel (2020).\n\n\n\n\n\n\n\n\n\n Consider this\nExplore some of the resources listed on the qtalrkit compansion site and consider their sampling frames. Can you think of a research question or questions that this resource may be well-suited to support research into? What types of questions would be less-than-adequate for a given resource?\n\n\n\nEthical considerations\nJust because data is available on the web does not mean it is free to use. Repositories, APIs, and individual data resources often have licensing agreements and terms of use, ranging from public domain to proprietary licenses. Public domain licenses, such as those found in Project Gutenberg, allow anyone to use the data for any purpose. Creative Commons licenses, like those used by the American National Corpus, Wikipedia, and TalkBank, span from public domain to more restrictive uses, including requirements for attribution or prohibiting commercial use. Even more restrictive licenses, such as those for the Corpus of Contemporary American English and the British National Corpus, may require a fee to access and use the data, even for research purposes.\nRespecting intellectual property rights is crucial when working with corpus data. Violating these rights can lead to legal and ethical issues, including lawsuits, fines, and damage to one’s professional reputation. To avoid these problems, researchers must ensure they have the necessary permissions to use copyrighted works in their corpora. Obtaining permissions involves contacting the author or publisher and requesting consent to use their work for research purposes. Documenting all obtained permissions and providing attribution and/ or citation is essential respecting the intellectual property rights of others.\nFormats\nWhether you are using a published corpus or developing your own, it is important to understand how the data you want to work with is formatted. When referring to the format of a corpus, this includes the folder and file structure, the file types, the internal structure of the files themselves, and how file content is encoded electronically.\nFolder and file structure\nSome corpus resources are contained in a single file, such as a spreadsheet or a text file, but more often than not a corpus will be comprised of multiple files and folders. The folder and file structure will reflect the organization of the corpus and may include sub-folders for different types or groupings of data. In addition to the corpus data itself, metadata and documentation will often be included in the corpus folder structure. The corpus data may be grouped by language, modality, register, or other attributes such as types of linguistic annotation.\nTo illustrate, in Example 2.1 we have the file and folder structure of a toy corpus.\n\nExample 2.1 Toy corpus structure\ncorpus/\n├── documentation/\n│   ├── README.md\n│   ├── LICENSE\n├── metadata/\n│   ├── speakers.csv\n├── data/\n│   ├── spoken/\n│   │   ├── inter-09-a.xml\n│   │   ├── inter-09-b.xml\n│   │   ├── convo-09-a.xml\n│   │   ├── ...\n│   ├── written/\n│   │   ├── essay-09-a.xml\n│   │   ├── essay-09-b.xml\n│   │   ├── respo-09-a.xml\n│   │   ├── ...\n\nIn this example, we have a corpus folder with three sub-folders: documentation/, metadata/, and data/. The data/ folder contains two sub-folders: spoken/ and written/. Each folder contains the relevant data files.\nWhere a single file is easy to download from the web, a corpus with a more complex folder structure can be more difficult to access. For that reason, many corpus resources are packaged into and made into a single compressed file. File compression has two benefits: it preserves the folder structure in a format which is contained in a single file and it also reduces the overall storage size. Common file compression formats are .zip and .tar.gz. So a compressed corpus file for the example above may be named something like corpus.zip or corpus.tar.gz. To access the original data within a compressed file, one must use a decompression tool or software to extract the contents after downloading it.\nFile types\nIn our toy corpus example, you may have noticed that each of the filenames appear with either .md, .csv, .xml, or nothing appended. These are examples of file extensions. File extensions a short sequence of characters, usually preceded by a period (.) which are used to indicate the type or format of file. File extensions help both users and software programs to identify the content and purpose of a file.\n\n\n\n\n\n\n Warning\nIf you are working on your own desktop computer, you may not see the file extensions. This is because the file explorer is configured to hide them by default. To see the file extensions, you will need to change the settings in your file explorer. Use a search engine to find instructions for your operating system.\n\n\n\nIn addition to those listed above, other file extensions often encountered when working with data for text analysis include .txt, .pdf, .docx, .xlsx, .json, and .html. Common file extensions will often be associated with specific software programs on your computer, especially those which are directly associated with proprietary software such as .docx for Microsoft Word or .xlsx for Microsoft Excel. However, many file extensions are not directly associated with any specific software program and can be opened and edited with any text editor.\nIt is important to note that file extensions are helpful conventions, but they are not a guarantee of the file type or structure of the file content. Furthermore, corpus developers may create their own file extensions to signal the unique structure of their data. For example, the .utt file extension used in the Switchboard Dialogue Act Corpus (SWDA) or the .cha extension used for TalkBank resource transcripts signal project-specific structuring. In either case, it is recommended to open the file in a text editor to inspect the structure of the file content to confirm the file structure before processing the data contained therein.\nFile content\nThe internal structure of the content of corpus data files is an important aspect of any corpus both in terms of what data is included and how to approach accessing and processing the data. A corpus may include various types of linguistic (e.g. part of speech, syntactic structure, named entities, etc.) or non-linguistic (e.g. source, dates, speaker information, etc.) attributes. These attributes are known as metadata, or data about data. As a general rule, files which include more metadata tend to be more internally structured. Internal file structure refers to the degree to which the content is easy to query and analyze by a computer. Let’s review characteristics of the three main types of file structure types and associate common file extensions that files in each have.\nUnstructured data is data which does not have a machine-readable internal structure. This is the case for plain text files (.txt), which are simply a sequence of characters. For example, in Example 2.2 we see a snippet of a plain text file from the the Manually Annotated Sub-Corpus of American English (MASC) (Ide et al. 2008):\n\nExample 2.2 MASC plain text\n&gt;Hotel California\n\nFact: Sound is a vibration. Sound travels as a mechanical wave through a medium, and in space, there is no\nmedium. So when my shuttle malfunctioned and the airlocks didn't keep the air in, I heard nothing. After the\nfirst whoosh of the air being sucked away, there was lightning, but no thunder. Eyes bulging in\npanic, but no screams. Quiet and peaceful, right? Such a relief to never again hear my crewmate Jesse natter\nabout his girl back on Earth and that all-expenses-paid vacation-for-two she won last time he was on leave. I\nswore, if I ever had to see a photo of him in a skimpy bathing suit again, giving the camera a cheesy thumbs-up\nfrom a lounge chair on one of those white sandy beaches, I'd kiss a monkey. Metaphorically, of course.\n\nOther examples of files which often contain unstructured data include .pdf and .docx files. While these file types may contain data which appears structured to the human eye, the structure is not designed to be machine-readable. As such the data would typically be read into R as a vector of character strings. It is possible to perform only the most rudimentary queries on this type of data, such as string matches. For anything more informative, it is necessary to further process this data.\nOn the other end of the spectrum, structured data is data which conforms to a tabular format in which elements in tables and relationships between tables are defined. This makes querying and analyzing easy and efficient. Relational databases (e.g. MySQL, PostgreSQL, etc.) are designed to store and query structured data. The data frame object in R is also a structured data format. In each case, the data is stored in a tabular format in which each row represents a single observation and each column represents a single attribute whose values are of the same type.\nIn Example 2.3 we see an example of an R data frame object which overlaps with the data in the plain text file above in Example 2.2:\n\nExample 2.3 MASC data frame\n   title             date modality domain          ref_num word  lemma pos  \n   &lt;chr&gt;            &lt;dbl&gt; &lt;fct&gt;    &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 Hotel California  2008 Writing  General Fiction       0 &gt;     &gt;     NN   \n 2 Hotel California  2008 Writing  General Fiction       1 Hotel hotel NNP  \n 3 Hotel California  2008 Writing  General Fiction       2 Cali… cali… NNP  \n 4 Hotel California  2008 Writing  General Fiction       3 Fact  fact  NNP  \n 5 Hotel California  2008 Writing  General Fiction       4 :     :     :    \n 6 Hotel California  2008 Writing  General Fiction       5 Sound sound NNP  \n 7 Hotel California  2008 Writing  General Fiction       6 is    be    VBZ  \n 8 Hotel California  2008 Writing  General Fiction       7 a     a     DT   \n 9 Hotel California  2008 Writing  General Fiction       8 vibr… vibr… NN   \n10 Hotel California  2008 Writing  General Fiction       9 .     .     .    \n11 Hotel California  2008 Writing  General Fiction      10 Sound sound NNP  \n\nHere we see that the data is stored in a tabular format with each row representing a single observation (word) and each column representing a single attribute. Internally, R applies a schema to ensure the values in each column are of the same type (e.g. &lt;chr&gt;, &lt;dbl&gt;, &lt;fct&gt;, etc.). This structured format is designed to be easy to query and analyze and as such is the primary format for data analysis in R.\n\n\n\n\n\n\n Tip\nIt is conventional to work with column names for datasets in R using the same conventions that are used for naming objects. It is a matter of taste which convention is used, but I have adopted snake case as my personal preference (e.g ref_num). There are also alternatives. Regardless of the convention you choose, it is good practice to be consistent.\nIt is also of note that the column names should be balanced for meaningfulness and brevity. This brevity is of practical concern but can be somewhat opaque. For questions into the meaning of the column and is values consult the resource’s dataset documentation, consult Section 2.3.\n\n\n\nSemi-structured data falls between unstructured and structured data. This covers a wide range of file structuring approaches. For example, a otherwise plain text file with part-of-speech tags appended to each word is minimally structured (Example 2.4).\n\nExample 2.4 MASC plain text with part-of-speech tags\n&gt;/NN Hotel/NNP California/NNP Fact/NNP :/: Sound/NNP is/VBZ a/DT vibration/NN ./. Sound/NNP travels/VBZ as/IN a/DT mechanical/JJ wave/NN through/IN a/DT medium/NN ,/, and/CC in/IN space/NN ,/, there/EX is/VBZ no/DT medium/NN ./. So/RB when/WRB my/PRP$ shuttle/NN malfunctioned/JJ and/CC the/DT airlocks/NNS did/VBD n't/RB keep/VB the/DT air/NN in/IN ,/, I/PRP heard/VBD nothing/NN ./. After/IN the/DT\n\nTowards the more structured end of semi-structured data, many file formats including .xml and .json contain highly structured, hierarchical data. For example, in Example 2.5 shows a snippet from a .xml file from the MASC corpus.\n\nExample 2.5 MASC XML\n&lt;a xml:id=\"penn-N65571\" label=\"tok\" ref=\"penn-n0\" as=\"anc\"&gt;                                                                                                                                                        \n  &lt;fs&gt;\n    &lt;f name=\"base\" value=\"&gt;\"/&gt;\n    &lt;f name=\"msd\" value=\"NN\"/&gt;\n    &lt;f name=\"string\" value=\"&gt;\"/&gt;\n  &lt;/fs&gt;\n&lt;/a&gt;\n&lt;node xml:id=\"penn-n1\"&gt;\n  &lt;link targets=\"seg-r1\"/&gt;\n&lt;/node&gt;\n&lt;a xml:id=\"penn-N65599\" label=\"tok\" ref=\"penn-n1\" as=\"anc\"&gt;\n  &lt;fs&gt;\n    &lt;f name=\"base\" value=\"hotel\"/&gt;\n    &lt;f name=\"msd\" value=\"NNP\"/&gt;\n    &lt;f name=\"string\" value=\"Hotel\"/&gt;\n  &lt;/fs&gt;\n&lt;/a&gt;\n\nThe format of semi-structured data is often influenced by characteristics of the data or reflect an author’s individual preferences. It is sometimes the case that data will be semi-structured in a less-standard format. For example, the SWDA corpus includes a .utt file extension for files which contain utterances annotated with dialogue act tags.\n\nExample 2.6 SWDA .utt file\no          A.1 utt1: Okay.  /\nqw          A.1 utt2: {D So, }\n\nqy^d          B.2 utt1: [ [ I guess, +\n\n+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\n\n+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\n\nqy          A.5 utt1: Does it say something? /\n\nWhether standard or not, semi-structured data is often designed to be machine-readable. As with unstructured data, the ultimate goal is to convert the data into a structured format and augment the data where necessary to prepare it for a particular research analysis.\nFile encoding\nThe last aspect to consider about corpus formats is file encoding. For a computer to display and process text characters, it must be encoded in a way that the computer can understand (i.e. 1’s and 0’s). Historically, character encoding schemes were developed to represent characters from specific character script sets (e.g. ASCII only includes characters from the English alphabet). However, as the need for a consistent and more inclusive way to encode characters from multiple languages and scripts became apparent, the Unicode standard, Unicode Transformation Format (UTF), was developed in the early 1990s. UTF encodings (UTF-8, UTF-16, and UTF-32) are now the most common way to encode text data and modern computers typically use them by default. Although other more script-specific encoding schemes can still be found in older data (e.g. ISO-8859, Windows-1252, Shift JIS).\nWhen working with corpus data, it is important to know if the encoding scheme used for the data is compatible with your computing environment’s default (most likely UTF). If it is not, you will need to convert the data to a compatible encoding scheme. Rest assured, there is support in R for converting between different encoding schemes if the need arises."
  },
  {
    "objectID": "understanding-data.html#information",
    "href": "understanding-data.html#information",
    "title": "2  Understanding data",
    "section": "\n2.2 Information",
    "text": "2.2 Information\nIdentifying an adequate corpus resource, in terms of content, licensing, and formatting, for the target research question is the first step in moving a quantitative text research project forward. The next step is to select the components or characteristics of this resource that are relevant for the research and then move to organize the attributes of this data into a more informative format. This is the process of converting corpus data into a dataset –a tabular representation of particular attributes of the data as the basis for generating information. Once the data represented as dataset, it is often manipulated and transformed adjusting and augmenting the data such that it better aligns with the research question and the analytical approach.\n\n2.2.1 Organization\nData alone is not informative. Only through explicit organization of the data in a way that makes relationships and meaning explicit does data become information. In this form, our data is called a dataset. This is a particularly salient hurdle in text analysis research. Many textual sources are unstructured or semi-structured, that is relationships that will be used in the analysis have yet to be purposefully drawn and organized from the data.\nTidy Data\nThe selection of the attributes from a corpus and the juxtaposition of these attributes in a relational format, or dataset, that converts data into information is known as data curation. The process of data curation minimally involves creating a base dataset, or curated dataset, which establishes the main informational associations according to philosophical approach outlined by Wickham (2014).\nIn this work, a tidy dataset refers both to the structural (physical) and informational (semantic) organization of the dataset. Physically, a tidy dataset is a tabular data structure, illustrated in Figure 2.3, where each row is an observation and each column is a variable that contains measures of a feature or attribute of each observation. Each cell where a given row-column intersect contains a value which is a particular attribute of a particular observation for the particular observation-feature pair also known as a data point.\n\n\n\n\nFigure 2.3: Visual summary of the tidy format.\n\n\n\nIn terms of semantics, columns and rows both contribute to the informational value of the dataset. Let’s start with columns. In a tidy dataset, each column is a variable, an attribute that can take on a number of values. Although variables vary in terms of values, they do not in type. A variable is of one and only one informational type. Statistically speaking, informational types are defined as levels of measurement, a classification system used to semantically distiguish between types of variables. There are four levels (or types) in this system: nominal, ordinal, interval, and ratio.\nIn practice, however, text analysis researchers often group these levels into three main informational types: categorical, ordinal, and numeric (Gries 2021). What do these informational types represent? Categorical data is for labeled data or classes that answer the question “what?” Ordinal data is categorical data with rank order that answers the question “what order?” Numeric data is ordinal data with equal intervals between values that answers the question “how much or how many?”\nLet’s look at an example of a tidy dataset. Using the criteria just described, let’s see if we can identify the informational values (categorical, ordinal, or numeric) of the variables that appear in a snippet from the MASC corpus in dataset form in Table 2.1.\n\n\n\n\nTable 2.1: MASC dataset variables.\n\ntitle\nmodality\ndate\nref_num\nword\npos\nnum_letters\n\n\n\nHotel California\nWriting\n2008\n0\n&gt;\nNN\n1\n\n\nHotel California\nWriting\n2008\n1\nHotel\nNNP\n5\n\n\nHotel California\nWriting\n2008\n2\nCalifornia\nNNP\n10\n\n\nHotel California\nWriting\n2008\n3\nFact\nNNP\n4\n\n\nHotel California\nWriting\n2008\n4\n:\n:\n1\n\n\nHotel California\nWriting\n2008\n5\nSound\nNNP\n5\n\n\nHotel California\nWriting\n2008\n6\nis\nVBZ\n2\n\n\nHotel California\nWriting\n2008\n7\na\nDT\n1\n\n\nHotel California\nWriting\n2008\n8\nvibration\nNN\n9\n\n\nHotel California\nWriting\n2008\n9\n.\n.\n1\n\n\n\n\n\n\n\n\nWe have seven variables listed as headers for each of the columns. We could go one-by-one left-to-right but let’s take another tack. Instead, let’s identify all those variables that cannot be numeric –these are all the non-numeral variables: title, modality, word, and pos. The question to ask of these variables is whether they represent an order or rank. Since titles, modalities, words, and parts-of-speech are not ordered values, they are all categorical.\nNow in relation to date, ref_num, and num_letters. All three are numerals, so they could be numeric. But they could also be numeral representations of ordinal data. Before we can move forward, we need to make sure we understand what each variable means and how it is measured, or operationalized. The variable name and the values can be helpful in this respect. date is what it sounds like, a date, and is operationalized as a year in the Gregorian calendar. And num_letters seems quite descriptive as well, number of letters, appearing as a letter count. But in some cases it may be opaque as to what is being measured by the variable name alone, for example ref_num, and one will have to refer to the dataset documentation. In this case ref_num is a reference number operationalized as a unique identifier for each word per document in the corpus.\nWith this in mind, let’s return to the question of whether date, ref_num, and num_letters are numeric or ordinal. Starting with the trickiest one, date, we can ask the question to identify numeric data: “how much or how many?”. In the case of date, the answer is neither. A date is a point in time, not a quantity. So date is not numeric. But it does provide information about order. Hence, date is ordinal. ref_num is also ordinal because the question “what order?” can be asked of it. Finally, num_letters is numeric because it answers the question “how many?”.\nLet’s turn to the second semantic value of a tidy dataset. In a tidy dataset, each row is an observation. But an observation of what? This depends on what the unit of observation is. That sounds circular, but its not. The unit of observation is simply the primary entity that is being observed or measured (Sedgwick 2015). Even without context, it can often be identified in a dataset by looking at the level of specificity of the variable values and asking what each variable describes. When one variable appears to be the most individualized and other variables appear to describe that variable, then the most individualized variable is likely the unit of observation of the dataset, i.e. the meaning of each observation.\nApplying these strategies to the Table in 2.1, we can see that each observation at its core is a word. We see that the values of each observation are the attributes of each word. word is the most individualized variable and the pos (part-of-speech), num_letters, and ref_num all describe the word.\nThe other variables title, modality, and date are not direct attributes of the word. Instead, they are attributes of the document in which the word appears. Together, however, they all provide information about the word.\n\n\n\n\n\n\n Consider this\nData can be organized in many ways. It is important to make clear that data in tabular format in itself does not constitute a dataset, in the tidy sense we will be using. Can you think of examples of tabular information that would not be in a tidy format? What would be the implications of this for data analysis?\n\n\n\nAs we round out this section on data organization, it is important to stress that the purpose of curation is to represent the corpus data in an informative, tidy format. A curated dataset serves as a reference point making relationships explicit, enabling more efficient querying, and paving the way for further processing before analysis. In the subsequent section, we will highlight common approaches to modifying the curated dataset, either row-wise or column-wise, to make it more amenable to the particular aims of a given analysis.\n\n2.2.2 Transformation\nAt this point have introduced the first step creating a dataset ready for analysis, data curation. However, a curated dataset is rarely the final organizational step before proceeding to statistical analysis. Many times, if not always, the curated dataset requires data transformation to derive or generate new data for the dataset. This process may incur row-wise (observation) or column-wise (variable) level changes, as illustrated in Figure 2.4.\n\n\n\n\nFigure 2.4: Visualization of row-wise and column-wise transformation operations on a dataset.\n\n\n\nThe results build on and manipulate the curated dataset to produce a derived dataset. While there is typically one curated dataset that serves as the base organizational dataset, there may be multiple derived datasets, each aligning with the informational needs of specific analyses in the research project.\nIn what follows, we will discuss the most common types of data transformation: text normalization, variable recoding, text tokenization, variable generation, and observation/ variable merging. Note, however, that the order in which these transformations are applied in a given research project is not fixed and will vary depending on the dataset and the research question(s) to be addressed.\nText normalization\nThe process of text normalization aims to prepare and standardize text. It is often a preliminary step in data transformation processes which include variables with text. The aim is to convert the text into a uniform format to reduce unwanted variation and noise.\nLet’s take a toy dataset, in Table 2.2, as an example starting point. In this dataset, we have two variables, text_id and text. It only has one observation.\n\n\n\n\nTable 2.2: A toy dataset with two variables, text_id and text.\n\n\n\n\n\ntext_id\ntext\n\n\n1\nIt’s a beautiful day in the US, and our group decided to visit the famous Grand Canyon. As we reached the destination, Jane said, “I can’t believe we’re finally here!” The breathtaking view left us speechless; indeed, it was a sight to behold. During our trip, we encountered tourists from different countries, sharing stories and laughter. For all of us, this experience will be cherished forever.\n\n\n\n\n\nThe types of transformations we apply will depend on the specific needs of the project, but can include those found in Table 2.3.\n\n\nTable 2.3: Common text normalization tasks\n\n\n\n\n\n\nTask name\nRelevant example\nTypical purpose\n\n\n\nLowercasing\n\n\"Text\" to \"text\"\n\nMinimizing case sensitivity in subsequent analysis\n\n\nRemoval of Punctuation and Special Characters\n\n\"Hello, World!\" to \"Hello World\"\n\nRemoving non-alphanumeric characters that may not carry semantic value\n\n\nAdjustment of Forms\n\n\"colour\" to \"color\", \"it's\" to \"it is\", \"1\" to \"one\"\n\nStandardizing variations in spelling, contractions, and numeric forms to a common format\n\n\n\n\nThese transformations are column-wise operations, meaning they preserve the number of rows in the dataset. They also preserve the number of columns, but do change the values of the variables. These tasks should be applied with an understanding of how the changes will impact the analysis. For example, lowercasing can be useful for reducing differences between words that are otherwise identical, yet differ in case due to word position in a sentence (“The” versus “the”). However, lowercasing can also be problematic if the case of the word carries semantic value, such as in the case of “US” (United States) and “us” (first person plural pronoun).\nLet’s be conservative and only apply lowercasing to our toy dataset as seen in Table 2.4.\n\n\n\n\nTable 2.4: A toy dataset with two variables, text_id and text, where the text has been lowercased.\n\n\n\n\n\ntext_id\ntext\n\n\n1\nit’s a beautiful day in the us, and our group decided to visit the famous grand canyon. as we reached the destination, jane said, “i can’t believe we’re finally here!” the breathtaking view left us speechless; indeed, it was a sight to behold. during our trip, we encountered tourists from different countries, sharing stories and laughter. for all of us, this experience will be cherished forever.\n\n\n\n\n\nWhen text normalization steps are motivated and applied with foresight they serve to enhance the quality of the data and improves the reliability of subsequent transformation steps.\nVariable recoding\nRecoding is the process of transforming the values of one or more variables into new values which are more amenable to analysis. The aim is to simplify complex variables, making it easier to identify patterns and trends relevant for the research question. This is a column-wise operation which can be applied to categorical or numeric variables.\nLet’s return to the MASC dataset and demonstrate recoding of categorical and numeric variables. In Table 2.1 the pos variable whose values represent the part-of-speech (POS) of each token in the text. The measure is a POS tag from the Penn Treebank tagset (Marcus, Santorini, and Marcinkiewicz 1993). This tagset makes twelve major and 45 minor grammatical class distinctions. In an analysis that aims to explore only major class distinctions, it would be useful to recode the pos variable into major classes only (i.e. noun, pronoun, adjective, verb, adverb, etc.) to facilitate queries, summaries, and visualizations.\n\n\n\n\nTable 2.5: A toy dataset with three variables, text_id, pos, major_pos, where the pos variable has been recoded into major grammatical classes major_pos.\n\n\n\n\n\n\n\n\n\n\n\ntitle\nmodality\ndate\nref_num\nword\npos\nmajor_pos\nnum_letters\n\n\n\nHotel California\nWriting\n2008\n0\n&gt;\nNN\nnoun\n1\n\n\nHotel California\nWriting\n2008\n1\nHotel\nNNP\nnoun\n5\n\n\nHotel California\nWriting\n2008\n2\nCalifornia\nNNP\nnoun\n10\n\n\nHotel California\nWriting\n2008\n3\nFact\nNNP\nnoun\n4\n\n\nHotel California\nWriting\n2008\n4\n:\n:\npunctuation\n1\n\n\nHotel California\nWriting\n2008\n5\nSound\nNNP\nnoun\n5\n\n\nHotel California\nWriting\n2008\n6\nis\nVBZ\nverb\n2\n\n\nHotel California\nWriting\n2008\n7\na\nDT\ndeterminer\n1\n\n\nHotel California\nWriting\n2008\n8\nvibration\nNN\nnoun\n9\n\n\nHotel California\nWriting\n2008\n9\n.\n.\npunctuation\n1\n\n\n\n\n\n\nIn Table 2.5, the pos variable has been recoded into major grammatical classes. The major_pos variable is a categorical variable with 12 levels, one for each major grammatical class in the Penn Treebank tagset. While the demonstration here demonstrates the simplification of a categorical variable, recoding can also be used to transliterate categorical variables. Continuing with the theme of POS tags, the pos variable could be recoded into a different tagset, such as the Universal Dependencies tagset (Nivre et al. 2016).\nNow, let’s look at recoding the numeric variable num_letters. This variable represents the number of letters in each token. In the MASC dataset, the num_letters variable is a numeric variable with a range of values from 1 to 21. In some analyses, it may be useful to recode this variable into discrete categories, or bins, such as short, medium, and long words.\n\n\n\n\nTable 2.6: The MASC dataset with the num_letters variable recoded into three categories: short, medium, and long words in word_length.\n\n\n\n\n\n\n\n\n\n\n\n\ntitle\nmodality\ndate\nref_num\nword\npos\nmajor_pos\nnum_letters\nword_length\n\n\n\nHotel California\nWriting\n2008\n0\n&gt;\nNN\nnoun\n1\nshort\n\n\nHotel California\nWriting\n2008\n1\nHotel\nNNP\nnoun\n5\nmedium\n\n\nHotel California\nWriting\n2008\n2\nCalifornia\nNNP\nnoun\n10\nlong\n\n\nHotel California\nWriting\n2008\n3\nFact\nNNP\nnoun\n4\nmedium\n\n\nHotel California\nWriting\n2008\n4\n:\n:\npunctuation\n1\nshort\n\n\nHotel California\nWriting\n2008\n5\nSound\nNNP\nnoun\n5\nmedium\n\n\nHotel California\nWriting\n2008\n6\nis\nVBZ\nverb\n2\nshort\n\n\nHotel California\nWriting\n2008\n7\na\nDT\ndeterminer\n1\nshort\n\n\nHotel California\nWriting\n2008\n8\nvibration\nNN\nnoun\n9\nlong\n\n\nHotel California\nWriting\n2008\n9\n.\n.\npunctuation\n1\nshort\n\n\n\n\n\n\nIn Table 2.6 the variable word_length appears with the values short, medium, and long. This is now a categorical variable of type ordinal. Of note, is that the operational definition of used to create these word length bins should be made explicit in the documentation of the dataset.\nIn sum, recoding is a useful data transformation technique that can be used to simplify complex variables, making it easier to identify patterns and trends relevant for the research question.\nText tokenization\nA text-oriented transformation step is text tokenization. This process involves adapting the text such that it reflects the target linguistic unit that will be used in the analysis. This is a row-wise operation expanding the number of rows, if the linguistic unit is smaller than the original variable, or reducing the number of rows, if the linguistic unit is larger than the original variable. At its core, tokenization is the process which enables the quantitative analysis of text.\nText variables can be tokenized at any linguistic level. To illustrate, consider our toy dataset from Table 2.4. We can tokenize the text at the sentence level, in Table 2.7, by splitting the text at the period followed by a space. This results in a dataset with four observations, one for each sentence in the original text.\n\n\n\n\nTable 2.7: A toy dataset with two variables, text_id and sentence, where the text has been tokenized at the sentence level.\n\n\n\n\n\ntext_id\nsentence\n\n\n\n1\nit’s a beautiful day in the us, and our group decided to visit the famous grand canyon\n\n\n1\nas we reached the destination, jane said, “i can’t believe we’re finally here!” the breathtaking view left us speechless; indeed, it was a sight to behold\n\n\n1\nduring our trip, we encountered tourists from different countries, sharing stories and laughter\n\n\n1\nfor all of us, this experience will be cherished forever.\n\n\n\n\n\n\nIt is important to make explicit what the operationalization of our linguistic unit is as common terms such as sentence, word, etc. can be defined in different ways. For example, the sentence tokenization above is based on the assumption that sentences are separated by a period followed by a space. This is a suitable definition for this text, but likely will not be for other English text or for other languages/ writing scripts. For words, a very simple operationalization is to use whitespace separation (e.g. “I cannot believe it.” – [“I”, “cannot”, “believe”, “it.”]). However, this approach does not handle puntuation marks (e.g. [“it.”]) or contractions (e.g. [“can’t”]). A more sophisticated operationalization will be necessary for these, and possibly other, cases.\nAnother important token unit is the \\(n\\)-gram. Words or characters can be grouped into contiguous sequences with a moving window of a certain size \\(n\\). Single unit windows are referred to as unigrams, two units as bigrams, three units as trigrams, and so on. Let’s tokenize our toy dataset at the bigram level for words using a simple whitespace separation for words, as seen in Table 2.8.\n\n\n\n\nTable 2.8: A toy dataset with two variables, text_id and bigram, where the text has been tokenized at the bigram word level.\n\ntext_id\nword\n\n\n\n1\nit’s a\n\n\n1\na beautiful\n\n\n1\nbeautiful day\n\n\n1\nday in\n\n\n1\nin the\n\n\n1\nthe us\n\n\n1\nus and\n\n\n1\nand our\n\n\n1\nour group\n\n\n1\ngroup decided\n\n\n\n\n\n\nIn Table 2.8 we see that the first bigram is “it’s a” –the first two words (based on whitespace separation) in the text. The second bigram is “a toy” –the second and third words in the text. This continues to the end of the text. \\(N\\)-gram tokenization can be useful to capture context that would otherwise would be lost from tokenizing words or characters at the unigram level.\nUp to this point our tokens have been surface forms. That is, they are the actual words or characters as they appear in the text. However, we may want to reduce the tokens to their base form, removing their inflectional forms. This is known as lemmatization. For example, the word “run” is the lemma of the words “running”, “runs”, and “ran”. Let’s lemmatize the third sentence in our toy dataset. For comparison, word and lemma are shown side-by-side in Table 2.9.\n\n\n\n\nTable 2.9: A toy dataset with two variables, text_id and word, where the text has been tokenized at the unigram word level and lemmatized.\n\ntext_id\nword\nlemma\n\n\n\n1\nduring\nduring\n\n\n1\nour\nour\n\n\n1\ntrip\ntrip\n\n\n1\nwe\nwe\n\n\n1\nencountered\nencounter\n\n\n1\ntourists\ntourist\n\n\n1\nfrom\nfrom\n\n\n1\ndifferent\ndifferent\n\n\n1\ncountries\ncountry\n\n\n1\nsharing\nshare\n\n\n1\nstories\nstory\n\n\n1\nand\nand\n\n\n1\nlaughter\nlaughter\n\n\n\n\n\n\n\n\n\n\n\n\n Case study\nInflectional family size is the number of inflectional forms for a given word and can be calculated from a corpus by counting the number of surface forms for each lemma in the corpus (Kostić, Marković, and Baucal 2003). Baayen, Feldman, and Schreuder (2006) found that words with larger inflectional family size are associated with faster word recognition times in lexical processing tasks.\n\n\n\nTogether tokenization and lemmatization are powerful tools for transforming text. If our dataset contains more robust linguistic annotation or that annotation can be generated (see Section 2.2.2.4), this information can also be leveraged to tokenize language into a format that is easier to explore and quantify in an analysis.\nVariable generation\nThe process of variable generation aims to augment existing variables or create new ones, and as such it is a column-wise operation. Generation can include applying calculations or extracting relevant information from existing variables or enhancing text variables with linguistic annotation. Simplifying a bit, generation helps make implicit attributes explicit. The results of this process enables direct access during analysis to features that were otherwise hidden or difficult to access.\nLet’s highlight a some common calculation and extraction examples that generate variables. First, let’s look at the calculation of measures. In text analysis, measures are often used to describe the properties of a document or linguistic unit. For example, the number of words in a corpus document, the lengths of sentences, the number of clauses in a sentence, etc.. In turn, these measures can be used to calculate other measures, such as lexical diversity or syntactic complexity measures.\nIn terms of extraction, the goal is to distill relevant information from existing variables. For example, extracting the year from a date variable, or extracting the first name from a full name variable. In text analysis, extraction is often used to extract information from text variables. Say we have a dataset with a variable containing conversation utterances. We may want to extract some characteristic from those utterances and capture their occurrence in a new variable.\nBut what if we want to extract linguistic information from a text variable that is not explicitly present in the text? This is where linguistic annotation comes in. Linguistic annotation is the process of enriching text with linguistic information, such as morphological features, part-of-speech tags, syntactic structure, etc.. This can be done manually by linguist coders and/ or done using natural language processing (NLP) tools, many of which are available in R (see Chapter 7).\nTo illustrate the process of generating linguistic annotation with existing tools, I will use the plain text version of the MASC. In Table 2.10, the text has been organized into a dataset and tokenized into sentences. The text_id variable is a unique identifier for each document, and the sentence_id variable is a unique identifier for each sentence.\n\n\n\n\nTable 2.10: A MASC sample document in dataset tokenized into sentences.\n\ntext_id\nsentence_id\nsentence\n\n\n\n1\n1\n&gt;Hotel California Fact: Sound is a vibration.\n\n\n1\n2\nSound travels as a mechanical wave through a medium, and in space, there is no medium.\n\n\n1\n3\nSo when my shuttle malfunctioned and the airlocks didn't keep the air in, I heard nothing.\n\n\n1\n4\nAfter the first whoosh of the air being sucked away, there was lightning, but no thunder.\n\n\n1\n5\nEyes bulging in panic, but no screams.\n\n\n\n\n\n\n\n\nApplying a pre-trained model from the Universal Dependencies (UD)1 project, we can generate linguistic annotation for each token in the MASC.\n\n\n\n\nTable 2.11: Automatic linguistic annotation for grammatical category and syntactic structure for an example English sentence from the MASC.\n\n\n\n\n\n\n\n\n\n\ndoc_id\nsentence_id\ntoken_id\ntoken\nxpos\nfeatures\nsyntactic_relation\n\n\n\n1\n4\n1\nAfter\nIN\nNA\nmark\n\n\n1\n4\n2\nthe\nDT\nDefinite=Def|PronType=Art\ndet\n\n\n1\n4\n3\nfirst\nJJ\nDegree=Pos|NumType=Ord\namod\n\n\n1\n4\n4\nwhoosh\nNN\nNumber=Sing\nnsubj:pass\n\n\n1\n4\n5\nof\nIN\nNA\ncase\n\n\n1\n4\n6\nthe\nDT\nDefinite=Def|PronType=Art\ndet\n\n\n1\n4\n7\nair\nNN\nNumber=Sing\nnmod\n\n\n1\n4\n8\nbeing\nVBG\nVerbForm=Ger\naux:pass\n\n\n1\n4\n9\nsucked\nVBN\nTense=Past|VerbForm=Part|Voice=Pass\nadvcl\n\n\n1\n4\n10\naway\nRB\nNA\nadvmod\n\n\n1\n4\n11\n,\n,\nNA\npunct\n\n\n1\n4\n12\nthere\nEX\nNA\nexpl\n\n\n1\n4\n13\nwas\nVBD\nMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\nroot\n\n\n1\n4\n14\nlightning\nNN\nNumber=Sing\nnsubj\n\n\n1\n4\n15\n,\n,\nNA\npunct\n\n\n1\n4\n16\nbut\nCC\nNA\ncc\n\n\n1\n4\n17\nno\nDT\nNA\ndet\n\n\n1\n4\n18\nthunder\nNN\nNumber=Sing\nconj\n\n\n1\n4\n19\n.\n.\nNA\npunct\n\n\n\n\n\n\nThe annotated dataset now includes the key variables xpos (Penn treebank tags), features (morphological features), and syntactic_relation. The results of this process can then be further transformed as need be to fit the needs of the analysis.\nA word of caution: automated linguistic annotation offers rapid access to abundant and highly dependable linguistic data for numerous languages. However, linguistic annotation tools are not infallible. They are tools developed by training computational algorithms to identify patterns in previously annotated and verified datasets, resulting in a language model. This model is then employed to predict linguistic annotations for new language data (as seen in Table 2.11). The accuracy of the linguistic annotation heavily relies on the congruence between the language sampling framework of the trained data and the language data set to be automatically annotated.\nObservation/ variable merging\n\n\n\nThe processing of merging datasets is a transformation step which can be row-wise or column-wise. Row-wise merging is the process of combining datasets by appending observations from one dataset to another. Column-wise merging is the process of combining datasets by appending variables from one dataset to another. In either case, merging provides a way to enrich a dataset by incorporating additional information.\nTo merge in row-wise manner the datasets involved in the process must have the same variables and variable types. This process is often referred to as concatenating datasets. It can be thought of as stacking datasets on top of each other to create a larger dataset. Remember, having the same variables and variable types is not the same has having the same values.\nTake, for example, a case when a corpus resource contains data for two populations. In the course of curating and transforming the datasets it may make more sense to work with the datasets separately. However, when it comes time to analyze the data, it may be more convenient to work with the datasets as a single dataset. In this case, the datasets can be concatenated to create a single dataset.\nTo illustate, consider the toy datasets in Table 2.12 and Table 2.13.\n\n\n\n\nTable 2.12: Toy dataset of written text data.\n\n\n\n\n\n\n\nparticipant_id\ntext_id\nmodality\ntext\n\n\n\nP1\nT1\nWritten\nTechnology has revolutionized our lives in many ways. It has made communication easier, faster, and more efficient.\n\n\nP3\nT3\nWritten\nClimate change is a pressing issue that affects everyone on Earth. We must take immediate action to reduce our carbon footprint.\n\n\nP5\nT5\nWritten\nEducation is the key to personal and societal growth. Investing in quality education will lead to a brighter future for all.\n\n\n\n\n\n\n\n\n\n\nTable 2.13: Toy dataset of spoken text data.\n\n\n\n\n\n\n\nparticipant_id\ntext_id\nmodality\ntext\n\n\n\nP2\nT2\nSpoken\nHello, my name is X. I am a software engineer working at XYZ company.\n\n\nP4\nT4\nSpoken\nHi, I’m X, and I work as a project manager. My main responsibility is to ensure that projects are completed on time and within budget.\n\n\nP6\nT6\nSpoken\nHi, my name is X, and I’m a teacher. I teach English at a local high school.\n\n\n\n\n\n\nThese datasets, in Table 2.12 and Table 2.13, contain the same variables and variable types, but different observations –one in which the sample contains written language and the other spoken. Conveniently, they can be concatenated to create a single dataset that contains all of the observations, as seen in Table 2.14.\n\n\n\n\nTable 2.14: Toy dataset of written and spoken text data concatenated.\n\n\n\n\n\n\n\nparticipant_id\ntext_id\nmodality\ntext\n\n\n\nP1\nT1\nWritten\nTechnology has revolutionized our lives in many ways. It has made communication easier, faster, and more efficient.\n\n\nP3\nT3\nWritten\nClimate change is a pressing issue that affects everyone on Earth. We must take immediate action to reduce our carbon footprint.\n\n\nP5\nT5\nWritten\nEducation is the key to personal and societal growth. Investing in quality education will lead to a brighter future for all.\n\n\nP2\nT2\nSpoken\nHello, my name is X. I am a software engineer working at XYZ company.\n\n\nP4\nT4\nSpoken\nHi, I’m X, and I work as a project manager. My main responsibility is to ensure that projects are completed on time and within budget.\n\n\nP6\nT6\nSpoken\nHi, my name is X, and I’m a teacher. I teach English at a local high school.\n\n\n\n\n\n\nMerging datasets can be performed in a column-wise manner as well. In this process, the datasets need not have the exact same variables and variable types, rather it is required that the datasets share a common variable of the same informational type that can be used to index the datasets. This process is often referred to as joining datasets.\nCorpus resources often include metadata in stand-off annotation format. That is, the metadata is not embedded in the corpus files, but rather is stored in a separate file. The metatdata and corpus files will share a common variable which is used to join the metadata with the corpus files.\nTo exemplify, here’s another toy dataset that shares the participant_id index with the previous dataset in Table 2.14 and includes the variables native_speaker_eng, age, and gender:\n\n\n\n\nTable 2.15: Toy dataset of participant data with a shared variable participant_id to index the datasets.\n\nparticipant_id\nnative_speaker_eng\nage\ngender\n\n\n\nP1\nYes\n28\nM\n\n\nP2\nNo\n35\nM\n\n\nP3\nYes\n42\nF\n\n\nP4\nNo\n26\nF\n\n\nP5\nYes\n31\nM\n\n\nP6\nNo\n39\nF\n\n\n\n\n\n\nThis dataset provides additional information about each participant, such as their English native speaker status, age, and gender.\nSince the two datasets share the participant_id variable, we can merge them to create a new dataset that combines the information from both datasets, as we see in Table 2.16.\n\n\n\n\nTable 2.16: Joining variables from two datasets based on a shared index variable.\n\n\n\n\n\n\n\n\n\n\nparticipant_id\ntext_id\nmodality\ntext\nnative_speaker_eng\nage\ngender\n\n\n\nP1\nT1\nWritten\nTechnology has revolutionized our lives in many ways. It has made communication easier, faster, and more efficient.\nYes\n28\nM\n\n\nP3\nT3\nWritten\nClimate change is a pressing issue that affects everyone on Earth. We must take immediate action to reduce our carbon footprint.\nYes\n42\nF\n\n\nP5\nT5\nWritten\nEducation is the key to personal and societal growth. Investing in quality education will lead to a brighter future for all.\nYes\n31\nM\n\n\nP2\nT2\nSpoken\nHello, my name is X. I am a software engineer working at XYZ company.\nNo\n35\nM\n\n\nP4\nT4\nSpoken\nHi, I’m X, and I work as a project manager. My main responsibility is to ensure that projects are completed on time and within budget.\nNo\n26\nF\n\n\nP6\nT6\nSpoken\nHi, my name is X, and I’m a teacher. I teach English at a local high school.\nNo\n39\nF\n\n\n\n\n\n\nJoining datasets is a powerful tool for enriching a dataset with additional column-wise information. It is important to note that merging datasets can also remove information in a row-wise manner. For example, when merging two datasets with a shared variable, it is possible to remove observations that do not have a match one of the two datasets. This process effectively filters out observations not shared between the two datasets. On the other hand, an anti-join explicitly removes observations that are shared between the two datasets.\n\n\n\n\n\n\n Dive deeper\nIn some analyses, it may be useful to remove words with little semantic value, such as articles, prepositions, and conjunctions or words that are very common in the language. These are known as stopwords. There are various predefined lists of stopwords for different languages available on the web and through R in the stopwords package (Benoit, Muhr, and Watanabe 2021). Anti-joining a stopword list with a dataset of word tokens is often used to remove stopwords from the dataset.\nHowever, it is important to note the criteria used to determine which words are considered stopwords in a particular resource may not fit a researcher’s needs or the characteristics of the data. Learn more about approaches to identifying stopwords in Kaur and Buttar (2018).\n\n\n\n\nIn sum, the transformation steps described here collectively aim to produce higher quality datasets that are relevant in content and structure to submit to analysis. The process may include one or more of the previous transformations but is rarely linear and is most often iterative. It is typical to do some normalization then generation, then recoding, and then return to normalizing, and so forth. This process is highly idiosyncratic given the characteristics of the curated dataset and the ultimate goal for the derived dataset(s)."
  },
  {
    "objectID": "understanding-data.html#sec-ud-documentation",
    "href": "understanding-data.html#sec-ud-documentation",
    "title": "2  Understanding data",
    "section": "\n2.3 Documentation",
    "text": "2.3 Documentation\nAs we have seen in this chapter, acquiring corpus data and converting that data into information involves a number of conscious decisions and implementation steps. As a favor to ourselves, as researchers, and to the research community, it is crucial to document these decisions and steps. This makes it both possible to retrace our own steps and also provides a guide for future researchers that want to reproduce and/ or build on your research. A programmatic approach to quantitative research helps ensure that the implementation steps are documented and reproducible but it is also vital that the decisions that are made are documented as well. This includes data origin information for the acquired corpus data and data dictionaries for the curated and derived datasets.\n\n2.3.1 Data origin\nData acquired from corpus resources should be accompanied by information about the data origin. Table 2.17 provides a list of the types of information that should be included in the data origin information.\n\n\n\n\nTable 2.17: Data origin information.\n\n\n\n\n\nInformation\nDescription\n\n\n\nResource name\nName of the corpus resource.\n\n\nData source\nURL, DOI, etc.\n\n\n\nData sampling frame\nLanguage, language variety, modality, genre, etc.\n\n\n\nData collection date(s)\nThe date or date range of the data collection.\n\n\nData format\nPlain text, XML, HTML, etc.\n\n\n\nData schema\nRelationships between data elements: files, folders, etc.\n\n\n\nLicense\nCC BY, CC BY-NC, etc.\n\n\n\nAttribution\nCitation information for the data source.\n\n\n\n\n\n\nFor many corpus resources, the corpus documentation will include all or most of this information as part of the resource download or documented online. If this information is not present in the corpus resource or you compile your own, it is important to document this information yourself. This information can be documented in file, such as a plain text file or spreadsheet, that is included with the corpus resource.\n\n2.3.2 Data dictionaries\nThe process of organizing the data into a dataset, curation, and modifications to the dataset in preparation for analysis, transformation, each include a number of project-specific decisions. These decisions should be documented.\nOn the one hand each dataset that is created should have a data dictionary file. A data dictionary is a document, usually in a spreadsheet format, that describes the variables in a dataset. The key information that should be included in a data dictionary is provided in Table 2.18.\n\n\n\n\nTable 2.18: Data dictionary information.\n\n\n\n\n\nInformation\nDescription\n\n\n\nVariable name\nThe name of the variable as it appears in the dataset, e.g. participant_id, modality, etc.\n\n\n\nReadable variable name\nA human-readable name for the variable, e.g. ‘Participant ID’, ‘Language modality’, etc.\n\n\n\nVariable type\nThe type of information that the variable contains, e.g. ‘categorical’, ‘ordinal’, etc.\n\n\n\nVariable description\nA prose description expanding on the readable name and can include measurement units, allowed values, etc.\n\n\n\n\n\n\n\nOrganizing this information in a tabular format, such as a spreadsheet, can make it easy for others to read and understand your data dictionary.\nOn the other hand, the data curation and transformation steps should be documented in the code that is used to create the dataset. This is one of the valuable features of a programmatic approach to quantitative research. The transparency of this documentation is enhanced by using literate programming strategies to intermingling prose descriptions and code the steps in the same, reproducible document.\nBy providing a comprehensive data dictionary and using a programmatic approach to data curation and transformation, you ensure that others can easily understand and work with your dataset, facilitating collaboration and reproducibility."
  },
  {
    "objectID": "understanding-data.html#summary",
    "href": "understanding-data.html#summary",
    "title": "2  Understanding data",
    "section": "Summary",
    "text": "Summary\nIn this chapter we have focused on data and information –the first two components of DIKI Hierarchy. This process is visualized in Figure 2.5.\n\n\n\n\nFigure 2.5: Understanding data: visual summary\n\n\n\nFirst a distinction is made between populations and samples, the latter being a intentional and subjective selection of observations from the world which attempt to represent the population of interest. The result of this process is known as a corpus. Whether developing a corpus or selecting an existing a corpus it is important to vet the sampling frame for its applicability and viability as a resource for a given research project.\nOnce a viable corpus is identified, then that corpus is converted into a curated dataset which adopts the tidy dataset format where each column is a variable, each row is an observation, and the intersection of columns and rows contain values. This curated dataset serves to establish the base informational relationships from which your research will stem.\nThe curated dataset will most likely require transformations which may include normalization, tokenization, recoding, generation, and/ or merging to enhance the usefulness of the information to analysis. A derived dataset or set of datasets will the result from this process.\nFinally, documentation should be implemented at the acquisition, curation, and transformation stages of the analysis project process. The combination of data origin, data dictionary, and literate programming files establishes documentation of the data and implementation steps to ensure transparent and reproducible research."
  },
  {
    "objectID": "understanding-data.html#activities",
    "href": "understanding-data.html#activities",
    "title": "2  Understanding data",
    "section": "Activities",
    "text": "Activities\nIn the following activities you will learn how to read, inspect, and write data and datasets in R using reproducible strategies.\n\n\n\n\n\n\n Recipe\n\nWhat: Reading, inspecting, and writing dataHow: Read Recipe 2 and participate in the Hypothes.is online social annotation.Why: To use literate programming in Quarto to work with R coding strategies for reading, inspecting, and writing datasets.\n\n\n\n\n\n\n\n\n\n Lab\n\n\nWhat: Reading, inspecting, and writing dataHow: Clone, fork, and complete the steps in Lab 2.Why: To read datasets from packages and from plain-text files, inspect and report characteristics of datasets, and write datasets to plain-text files."
  },
  {
    "objectID": "understanding-data.html#questions",
    "href": "understanding-data.html#questions",
    "title": "2  Understanding data",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\n\n\nConceptual questions\n\nWhat is the difference between a population and a sample?\nWhy is it important to vet a corpus before using it in a research project?\nWhat is a curated dataset in the context of linguistic research?\nWhat is the difference between a variable, an observation, and a value?\nWhy is it important to identify the informationasl types of variables in a dataset?\nWhat kinds of transformations may be performed on a curated dataset to enhance its usefulness for analysis?\nWhat is an transformed dataset and why is it important in linguistic research?\nWhy is documentation important in the process of conducting linguistic analysis?\nHow does a programmatic approach enhance documentation in linguistic research?\nHow does documenting the corpus data and the curated and derived datasets contribute to transparent and reproducible research in linguistics?\n\n\n\n\n\n\n\n\n\n\n\n Technical questions\n\n\nCreating a sample corpus.\nWriting a corpus documentation.\nConverting a corpus to a derived dataset.\nWriting a data dictionary.\nTransforming a derived dataset.\nMerging datasets.\nWriting a dataset to disk.\nConsider (an example dataset) and its data dictionary, write a script to read the dataset, inspect it, and write it to disk.\nConsider a dataset and its data dictionary what appears to be the unit of analysis and the unit of observation?\n\n\n\n\n\n\n\n\n\n\nÄdel, Annelie. 2020. “Corpus Compilation.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Th. Gries, 3–24. Switzerland: Springer.\n\n\nBaayen, R. Harald, L. B. Feldman, and R. Schreuder. 2006. “Morphological Influences on the Recognition of Monosyllabic Monomorphemic Words.” Journal of Memory and Language 55: 290–313. https://doi.org/10.1016/j.jml.2006.03.008.\n\n\nBenoit, Kenneth, David Muhr, and Kohei Watanabe. 2021. Stopwords: Multilingual Stopword Lists. https://github.com/quanteda/stopwords.\n\n\nGries, Stefan Th. 2021. Statistics for Linguistics with r. De Gruyter Mouton.\n\n\nGrieve, Jack, Andrea Nini, and Diansheng Guo. 2018. “Mapping Lexical Innovation on American Social Media.” Journal of English Linguistics 46 (4): 293–319.\n\n\nIde, Nancy, Collin Baker, Christiane Fellbaum, Charles Fillmore, and Rebecca Passonneau. 2008. “MASC: The Manually Annotated Sub-Corpus of American English.” In 6th International Conference on Language Resources and Evaluation, LREC 2008, 2455–60. European Language Resources Association (ELRA).\n\n\nKaur, Jashanjot, and P. Kaur Buttar. 2018. “A Systematic Review on Stopword Removal Algorithms.” International Journal on Future Revolution in Computer Science & Communication Engineering 4 (4): 207–10.\n\n\nKostić, Aleksandar, Tanja Marković, and Aleksandar Baucal. 2003. “Inflectional Morphology and Word Meaning: Orthogonal or Co-Implicative Cognitive Domains?” In Morphological Structure in Language Processing, edited by R. Harald Baayen and Robert Schreuder, 1–44. De Gruyter Mouton. https://doi.org/10.1515/9783110910186.1.\n\n\nKucera, H, and W N Francis. 1967. Computational Analysis of Present Day American English. Brown University Press Providence.\n\n\nMagueresse, Alexandre, Vincent Carles, and Evan Heetderks. 2020. “Low-Resource Languages: A Review of Past Work and Future Challenges.” arXiv. https://arxiv.org/abs/2006.07264.\n\n\nMarcus, Mitchell P, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. “Building a Large Annotated Corpus of English: The Penn Treebank.” Computational Linguistics 19 (2): 313–30.\n\n\nNivre, Joakim, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajič, Christopher D Manning, Ryan McDonald, et al. 2016. “Universal Dependencies V1: A Multilingual Treebank Collection.” Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), 1659–66. https://doi.org/?\n\n\nSedgwick, Philip. 2015. “Units of Sampling, Observation, and Analysis.” BMJ (Online) 351 (October): h5396. https://doi.org/10.1136/bmj.h5396.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10."
  },
  {
    "objectID": "understanding-data.html#footnotes",
    "href": "understanding-data.html#footnotes",
    "title": "2  Understanding data",
    "section": "",
    "text": "The Universal Dependency project is an effort to develop cross-linguistically consistent treebank annotation for many languages. The project has developed a set of annotation guidelines and a set of tools for generating linguistic annotation. The project has also developed a set of pre-trained models for many languages.↩︎"
  },
  {
    "objectID": "approaching-analysis.html#sec-aa-diagnose",
    "href": "approaching-analysis.html#sec-aa-diagnose",
    "title": "3  Approaching analysis",
    "section": "\n3.1 Diagnose",
    "text": "3.1 Diagnose\n\nThe purpose of diagnostic measures is to inspect your data to ensure its quality and understand its characteristics. There are two primary types of diagnostic measures: verfication and description. Verification methods are applied to catch missing or erroneous data while descriptive methods are used to gain a better understanding of the data. Although treated in two separate sections, in practice these methods are complementary and are often addressed in tandem.\nTo ground this discussion I will introduce a new dataset. This dataset is drawn from the Barcelona English Language Corpus (BELC) (Muñoz 2006), which is found in the TalkBank repository. I’ve selected the “Written composition” task from this corpus which contains 80 writing samples from 36 second language learners of English at different ages. Participants were given the task of writing for 15 minutes on the topic of “Me: my past, present and future”. Data was collected for participants from one to three times over the course of seven years (at 10, 12, 16, and 17 years of age).\nIn Table 3.1 we see the data dictionary for the BELC dataset which reflects structural and transformational steps I’ve done so we start with a tidy dataset with word as the unit of observation.\n\n\n\n\nTable 3.1: Data dictionary for the BELC dataset.\n\nvariable\nname\ndescription\nvariable_type\n\n\n\npart_id\nParticipant ID\nUnique identifier for each participant\ncategorical\n\n\nsex\nParticipant's sex\nSex of the participant\ncategorical\n\n\ngroup\nTime group\nLongitudinal group to which the participant belongs\nordinal\n\n\nmonth_age\nParticipant's age in months\nAge of the participant in months\nnumeric\n\n\nutt_id\nUtterance ID\nUnique identifier for each utterance\nnumeric\n\n\nword_id\nWord ID\nUnique identifier for each word within an utterance\nnumeric\n\n\nword\nWord\nThe word spoken by the participant\ncategorical\n\n\nlemma\nWord lemma\nBase form of the word\ncategorical\n\n\npos\nPart of speech\nGrammatical category of the word\ncategorical\n\n\n\n\n\n\n\n\nThe data dictionary provides a easily accessible overview of the dataset. This includes a human-readable mapping from variable names to variable descriptions. Further, it provides information about the type of variable (e.g., categorical, ordinal, numeric). As we will see the informational type of variables is key to diagnostic measures, as well as all other components of analysis.\n\n3.1.1 Verify\n\nAlthough a dataset has undergone curation and transformation, it is still important to verify the data. This is a process of checking the data to ensure that it is accurate and complete. In the case that it is not, consideration should be given to how to address the issues.\n\nThe most basic and usually the first step is to check for missing data to ensure that all necessary data points are present. In Table 3.2, there are missing values for the lemma and pos variables in the BELC dataset.\n\n\n\n\nTable 3.2: Summary output for missing values in the BELC dataset.\n\nvariable\ntype\nn_missing\ncomplete_rate\n\n\n\npart_id\ncharacter\n0\n1.000\n\n\nsex\ncharacter\n0\n1.000\n\n\ngroup\ncharacter\n0\n1.000\n\n\nword\ncharacter\n0\n1.000\n\n\nlemma\ncharacter\n79\n0.985\n\n\npos\ncharacter\n23\n0.996\n\n\nmonth_age\nnumeric\n0\n1.000\n\n\nutt_id\nnumeric\n0\n1.000\n\n\nword_id\nnumeric\n0\n1.000\n\n\n\n\n\n\n\n\nThere are two primary approaches to dealing with missing data: deletion and recoding. Since these missing values account for only 1.5% and 0.4% of the data respectively, we might be safe to remove these observations. Another approach is to recode the missing values by either applying a unique value for missing values (e.g., NULL) or by imputing values. Imputing values is usually done by replacing missing values with some middle-of-the-road value (e.g., mean, median, mode), but other, more nuanced approaches are possible.\n\n\n\n\n\n\n Dive deeper\nFor more information on missing data, see the  in this book.\n\n\n\nIn either case, it is important to consider the implications of missing data for the analysis. For example, if the missing data is not at random or include a sizeable portion of the values of interest, then the analysis may be biased.\n\nValue coding schemes, annotation errors, or other issues may result in anomalies in the data. These are values that are unusual, unexpected, or inconsistent with the rest of the data or effect the treatment of the data for the particular analysis to be performed.\nFor categorical variables, this may include values that are not expected or are not in the set of values that are expected. A summary of the values for a given variable can be used as a first step to identify anomalies. In Table 3.3, we see the minimum and maximum number of characters and the number of unique values for each categorical variable in the BELC dataset.\n\n\n\n\nTable 3.3: Summary output for categorical variables in the BELC dataset.\n\nvariable\nmin_chars\nmax_chars\nnum_unique\n\n\n\npart_id\n3\n3\n36\n\n\nsex\n4\n6\n2\n\n\ngroup\n2\n2\n4\n\n\nword\n1\n20\n913\n\n\nlemma\n1\n20\n774\n\n\npos\n1\n9\n38\n\n\n\n\n\n\n\n\nFrom our knowledge of the data, we can gauge whether these values are expected. For example, sex has two values; likely corresponding to some coding of ‘male’ and ‘female’. The variable part_id has 36 distinct values, which is expected since there are 36 participants and group has four, corresponding to the longitudinal time groups. It is also possible to gauge the expected values for lemma as we know that these are the base words and should be less than the number of words in the dataset.\nFurther verfication of the categorical variables is need, of course. This may include aggregating the data to see the distribution of values and/ or checking the values against the documentation.\n\nLet’s now consider numeric variables. Numeric variables, by their very nature, do not lend themselves to the same type of summary used for categorical variables (i.e. character lengths, number of unique values, or aggregation) to detect anomalies. For numeric variables there are two types of anomalies that we will consider: outliers and errors in coding. Outliers are anomalies that are extreme values that are not representative of the great majority of the data points. To determine what is extreme, we need to consider the distribution of the data, that is, the range of values and the frequency of values. It is rarely the case that we can eyeball the distribution of the data based on raw values. Instead, a combination of summary statistics and visualizations are used to determine the distribution of the data. For this reason, the detection of outliers is often carried out as part of the descriptive assessment of the data, as we will see in Section 3.1.2.\nOn the other hand, coding anomalies are values that are not expected or are not in the set of values that are expected. These can sometimes be detected by visual inspection of the data. For example, in Table 3.4, we see the first 10 observations for each variable in the BELC dataset.\n\n\n\n\nTable 3.4: First 10 observations for variables in the BELC dataset.\n\npart_id\nsex\ngroup\nmonth_age\nutt_id\nword_id\nword\nlemma\npos\n\n\n\nL01\nfemale\nT2\n153\n0\n0\nI\nI\npro:sub\n\n\nL01\nfemale\nT2\n153\n0\n1\nwas\nbe\ncop\n\n\nL01\nfemale\nT2\n153\n0\n2\nborn\nborn\nadj\n\n\nL01\nfemale\nT2\n153\n0\n3\nin\nin\nprep\n\n\nL01\nfemale\nT2\n153\n0\n4\nBarcelona\nBarcelona\nn:prop\n\n\nL01\nfemale\nT2\n153\n0\n5\nand\nand\ncoord\n\n\nL01\nfemale\nT2\n153\n0\n6\nI\nI\npro:sub\n\n\nL01\nfemale\nT2\n153\n0\n7\nlive\nlive\nv\n\n\nL01\nfemale\nT2\n153\n0\n8\nin\nin\nprep\n\n\nL01\nfemale\nT2\n153\n0\n9\nBarcelona\nBarcelona\nn:prop\n\n\n\n\n\n\n\n\nLeaving month_age aside, we see that the other two numeric variables utt_id and word_id index utterances and words respectively. However, in contrast to part_id which is a categorical variable as it serves as a unique identifier for each participant, these variables are numeric as they serve to not only index utterances and words but also to provide a measure of how many utterances or words have been produced. Seen in this light, 0 for the first value of utt_id and word_id is unexpected. To adjust for this, we can add 1 to each value of these variables.\n\n3.1.2 Describe\n\nThe goal of descriptive statistics is to summarize the data in order to understand and prepare the data for the analysis approach to be performed. This is accomplished through a combination of statistic measures and/ or tabular or graphic summaries. The choice of descriptive statistics is guided by the type of data, as well as the question(s) being asked of the data.\nTo that end, let’s consider a reconfiguration of the BELC dataset, in Table 3.5, which will provide a more illustrative dataset.\n\n\n\n\nTable 3.5: First 10 observations of the reconfigured BELC dataset.\n\nessay_id\npart_id\nsex\ngroup\ntokens\ntypes\nttr\nprop_l2\n\n\n\nE1\nL01\nfemale\nT2\n79\n46\n0.582\n0.987\n\n\nE2\nL02\nfemale\nT1\n18\n18\n1.000\n0.667\n\n\nE3\nL02\nfemale\nT3\n101\n53\n0.525\n1.000\n\n\nE4\nL05\nfemale\nT1\n20\n17\n0.850\n0.900\n\n\nE5\nL05\nfemale\nT3\n158\n80\n0.506\n0.987\n\n\nE6\nL05\nfemale\nT4\n184\n94\n0.511\n0.995\n\n\nE7\nL07\nmale\nT3\n98\n60\n0.612\n1.000\n\n\nE8\nL07\nmale\nT4\n134\n84\n0.627\n0.978\n\n\nE9\nL10\nfemale\nT1\n38\n28\n0.737\n0.974\n\n\nE10\nL10\nfemale\nT3\n118\n74\n0.627\n1.000\n\n\n\n\n\n\n\n\nIn this new configuration, the unit of observation is now essay_id. Each of the following variable are attributes or measures of this variable. The new variables in this dataset are aggregates of the previous BELC dataset: tokens is the number of total words, types is the number of unique words, ttr is the ratio of unique words to total words. This is known as the Type-Token Ratio and it is a standard metric for measuring lexical diversity. Finally, the proportion of L2 words (English) to the total words (tokens) is provided in prop_l2.\nIn descriptive statistics, there are four basic questions that are asked of each of the variables in the dataset. Each correspond to a different type of descriptive measure.\n\nCentral Tendency: Where do the data points tend to be located?\nDispersion: How spread out are the data points?\nDistribution: What is the overall shape of of the data points?\nInterdependence: How are these data points related to other data?\n\nCentral tendency\n\n\nThe central tendency is measure which aims to summarize the data points in a variable as the most representative, middle or most typical value. There are three common measures of central tendency: the mode, mean and median. Each differ in how they summarize the data points.\nThe mode is the value, or values, that appears most frequently in a set of values. If there are multiple values with the highest frequency, then the variable is said to be multimodal. The most versatile of the central tendency measures as it can be applied to all levels of measurement, although the mode is not often used for numeric variables as it is not as informative as other measures.\nThe more common measures for numeric variables are the mean and the median. The mean is a summary statistic calculated by summing all the values and dividing by the number of values. The median is calculated by sorting all the values in the variable and then selecting the middle value. Given that the mean and median are calculated differently, they will not always yield the same result. Differences that appear between the mean and median will be of interest to us later in this chapter.\nDispersion\n\nThe mean, median, and mode provide summary information where data points tend to be located. However, they do not provide us with any understanding as to how representative this value is. To provide this context, the spread of the values around the central tendency, or dispersion, is calculated.\nFor categorical variables, the spread is framed in terms of how balanced the values are across the levels. One way to do this is to calculate the (normalized) entropy. Entropy is a measure of uncertainty. The more balanced the values are across the levels, the higher the entropy. The less balanced the values are across the levels, the lower the entropy. Normalized entropy scores range from 0 to 1, with 0 indicating that all the values are the same and 1 indicating that all the values are different.\nThe most common measure of dispersion for numeric variables is the standard deviation. The standard deviation is calculated by taking the square root of the variance. The variance is the average of the squared differences from the mean. So, more succinctly, the standard deviation is a measure of the spread of the values around the mean. Where the standard deviation is anchored to the mean, the interquartile range (IQR) is tied to the median. The median represents the sorted middle of the values, in other words the 50th percentile. The IQR is the difference between the 75th percentile and the 25th percentile. Again, just as the mean and the median, the standard deviation and the IQR are calculated in different ways, they are not always the same.\nLet’s now consider the relevant central tendency and dispersion of the variables in the BELC dataset in Table 3.6.\n\n\nTable 3.6: Central tendency and dispersion of the variables in the BELC dataset\n\n\n\n\n(a) Categorical variables\n\nvariable\ntop_counts\nnorm_entropy\n\n\n\nessay_id\nE1: 1, E10: 1, E11: 1, E12: 1\n1.000\n\n\npart_id\nL05: 3, L10: 3, L11: 3, L12: 3\n0.983\n\n\nsex\nfem: 48, mal: 32\n0.971\n\n\ngroup\nT1: 25, T3: 24, T2: 16, T4: 15\n0.981\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Numeric variables\n\nvariable\nmean\nmedian\nsd\niqr\n\n\n\ntokens\n67.62\n56.50\n44.20\n61.25\n\n\ntypes\n41.85\n38.50\n23.03\n31.50\n\n\nttr\n0.68\n0.66\n0.13\n0.15\n\n\nprop_l2\n0.96\n0.99\n0.10\n0.03\n\n\n\n\n\n\n\n\n\n\nIn Table 3.6 (a) we see the measures for categorical variables. The top_counts variable gives us a short list of the most frequent levels of the variable. From top_count we can gather whether the variable has one mode or is multimodel. Both essay_id and part_id have the same most frequent value for the levels listed. On the other hand, sex and group have a single mode. We can also appreciate the dispersion of these variables based on the norm_entropy of each variable. essay_id is completely balanced across the levels, so it has a normalized entropy of 1. the other variables are not as balanced, but still quite balanced as the normalized entropy is close to 1.\nIn Table 3.6 (b) the numeric variables have a column for the mean, median, standard deviation, and IQR for each. The variable tokens has a larger difference between the mean and median than the other variables and the standard deviation is relatively large suggesting that the values are more spread out around the mean. In the case of ttr the mean and median are quite close and the standard deviation is relatively small suggesting that the values are more tightly clustered around the mean.\nWhen interpreting these summary values, it is important to only directly compare column-wise. That is, focusing only on a single variable, not across variables. Each variable, as is, is measured on a different scale and only relative to itself can we make sense of the values.\nHowever, we can transform the central tendency and dispersion scores for numeric variables to make them more comparable by standardizing the scale of the values. Standardization is a scale-based transformation that changes the scale of the values to a common scale, or z-scores. It involves two separate transformations: centering and scaling. Centering is a transformation that subtracts the mean or median from each value. The result is a mean and median of zero. Scaling is a transformation that divides each value by the standard deviation or IQR.\nIn Table 3.7, we see the same summary statistics as in Table 3.6 (b), but the values have been standardized for the mean and standard deviation. The mean is now zero and the standard deviation is one. This allows us to compare the median and IQR of the variables more directly.\n\n\n\n\nTable 3.7: Standardized central tendency and dispersion of numeric variables\n\nvariable\nmean\nmedian\nsd\niqr\n\n\n\ntokens\n0\n-0.25\n1\n1.39\n\n\ntypes\n0\n-0.15\n1\n1.37\n\n\nttr\n0\n-0.19\n1\n1.14\n\n\nprop_l2\n0\n0.25\n1\n0.27\n\n\n\n\n\n\n\n\nOne more caveat to keep in mind is that we need to be mindful of the nature of the data being standardized and what the standardized values mean. For example, the variables tokens and types were originally counts. But the standardized values are not interpretable as counts, they are now on a different scale –specifically a z-score scale. In the same way since the ttr and prop_l2 variables were originally proportions, the standardized values are also not interpretable as proportions. One additional twist, however, is that the original scales for these pairs of variables were not the same: tokens and types were counts, but ttr and prop_l2 were proportions. So, even though the standardized values are on the same scale, they are not directly comparable.\nBeyond comparing central tendency and dispersion across variables, standarization is useful for analytic statistics to mitigate the influence of variables with large values. In some cases, the statistical method will require standardization of variables before analysis.\nDistributions\n\nSummary statistics of the central tendency and dispersion of a variable provide a sense of the most representative value and how spread out the data is around this value. However, to gain a more comprehensive understanding of the variable, it is key to consider the frequencies of all the data points. The distribution of a variable is the pattern or shape of the data that emerges when the frequencies of all data points are considered. This can reveal patterns that might not be immediately apparent from summary statistics alone. Understanding the frequency and distribution of data points is vital as it informs subsequent choices of statistical analysis and evaluative methods, ensuring they are appropriate for the specific characteristics of the data.\nWhen assessing the distribution of categorical variables, we can use a frequency table or bar plot. A frequency table is a useful method to display the frequency and proportion of each level in a categorical variable in a clear and concise manner. In Table 3.8 we see the frequency table for the variable sex.\n\n\n\n\n\n\nTable 3.8: Frequency table for the variable sex.\n\nsex\nfrequency\nproportion\n\n\n\nfemale\n48\n0.6\n\n\nmale\n32\n0.4\n\n\n\n\n\n\n\n\nA bar plot is a type of plot where the x-axis is a categorical variable and the y-axis is the frequency of the values. The frequency is represented by the height of the bar. The variables can be ordered by frequency, alphabetically, or some other order. Figure 3.1 is a bar chart for the variables sex, group, and part_id, ordered alphabetically.\n\n\n\n\n\n\n(a) Sex\n\n\n\n\n\n(b) Time group\n\n\n\n\n\n(c) Participant ID\n\n\n\nFigure 3.1: Bar plots for categorical variables sex, group, part_id in the BELC dataset.\n\n\nSo for a frequency table or barplot, we can see the frequency of each level of a categorical variable. This gives us some knowledge about the BELC dataset: there are more girls in the dataset, more essays appear in first and third time groups, and the number of essays written by each participant is scattered from one to three. If we were to see any clearly loopsided categories, this would be a sign of imbalance in the data and we would need to consider how this might impact our analysis.\n\n\n\n\n\n\n Consider this\nThe goal of descriptive statistics is to summarize the data in a way that is meaningful and interpretable. With this in mind, compare the frequency table in 3.8 and bar plot in 3.1 (a). Does one provide a more interpretable summary of the data? Why or why not? Are there any other ways you might communicate this distribution more effectively?\n\n\n\nFor numeric variables, understanding the distribution is more complex, and also more important. In essence, however, we are assessing two things: the appearance of outliers in relation to and the overall shape of the distribution.\n\nNow, a frequency table, as in Table 3.8, does not summarize the distribution of a numeric variable in a concise, readily human-consumable format. Instead, the distribution of a numeric variable is best understood visually.\nThe most common visualizations of the distribution of a numeric variable are histograms and density plots. Histograms are a type of bar plot where the x-axis is a numeric variable and the y-axis is the frequency of the values falling within a determined range of values, or bins. The frequency of values within each bin is represented by the height of the bars. Density plots are a smoothed version of histograms. The y-axis of a density plot is the probability of the values. When frequent values appear closely together, the plot line is higher. When the frequency of values is lower or more spread out, the plot line is lower. An example of these plots is show in Figure 3.2 for the variable tokens.\n\n\n\n\n\n(a) Histogram\n\n\n\n\n\n(b) Density plot\n\n\n\nFigure 3.2: Distribution plots for the variable tokens.\n\n\nBoth the histogram in Figure 3.2 (a) and the density plot in Figure 3.2 (b) show the distribution of the variable tokens in slightly different ways which translate into trade-offs in terms of interpretability.\nThe histogram shows the frequency of the values in bins. The number of bins and/ or binwidth can be changed for more or less granularity. A rough grain histogram shows the general shape of the distribution, but it is difficult to see the details of the distribution. A fine grain histogram shows the details of the distribution, but it is difficult to see the general shape of the distribution. The density plot shows the general shape of the distribution, but it hides the details of the distribution. Given this trade-off, it is often useful explore outliers with histograms and the overall shape of the distribution with density plots.\nIn Figure 3.3 we see histograms for the variables tokens, types, and ttr.\n\n\n\n\n\n(a) Number of tokens\n\n\n\n\n\n(b) Number of types\n\n\n\n\n\n(c) Type-token ratio score\n\n\n\nFigure 3.3: Histograms for numeric variables tokens, types, and ttr.\n\n\nFocusing on the details captured in the histogram we are better able to detect potential outliers. Outliers can reflect valid values that are simply extreme or they can reflect something erroneous in the data. To distinguish between these two possibilities, it is important to know the context of the data. Take, for example, Figure 3.3 (c). We see that there is a bin near the value 1.0. Given that the type-token ratio is a ratio of the number of types to the number of tokens, it is unlikely that the type-token ratio would be exactly 1.0 as this would mean that every word in an essay is unique. Another, less dramatic, example is the bin to the far right of Figure 3.3 (a). In this case, the bin represents the number of tokens in an essay. An uptick in the number of essays with a large number of tokens is not surprising and would not typically be considered an outlier. On the other hand, consider the bin near the value 0 in the same plot. It is unlikely that a true essay would have 0, or near 0, words and therefore a closer look at the data is warranted.\nIt is important to recognize that outliers contribute undue influence to overall measures of central tendency and dispersion. To appreciate this, let’s consider another helpful visualization called a boxplot. A boxplot is a visual representation which aims to represent the central tendency, dispersion, and distribution of a numeric variable in one plot.\n\n\n\n\n\nFigure 3.4: Boxplot for the variable ttr.\n\n\n\nIn Figure 3.4 we see a boxplot for ttr variable. The box in the middle of the plot represents the interquartile range (IQR) which is the range of values between the first quartile and the third quartile. The solid line in the middle of the box represents the median. The lines extending from the box are called ‘whiskers’ and provide the range of values which are within 1.5 times the IQR. Values outside of this range are plotted as individual points.\nNow let’s consider boxplots from another angle. In Figure 3.5 (b) I’ve plotted the boxplot horizontally, right below the histogram in Figure 3.5 (a). In this view, we can see that a boxplot is a simplifed histogram augmented with central tendency and dispersion statistics. While histograms focus on the frequency distribution of data points, boxplots focus on the data’s quartiles and potential outliers.\n\n\n\n\n\n(a) Histogram\n\n\n\n\n\n\n\n(b) Boxplot (horizontal)\n\n\n\nFigure 3.5: Histogram and boxplot for the variable ttr.\n\n\nI’ve added a dashed line in Figure 3.5 (a) and Figure 3.5 (b) to signal the mean in this set of plots, but it is not typically included. I include the dashed line to make a point: the mean is more sensitive to outliers than the median. As I pointed out in Section 3.1.2.1, the mean is the sum of all values divided by the number of values. If there are extreme values, the mean will be pulled in the direction of the extreme values. The median, however, is the middle value and a few extreme values have less effect. So, when central tendency is reported, if there is a sizeable difference between the mean and the median, measures of dispersion will be larger and the direction of the difference can be used to infer the presence of outliers.\nReturning to outliers, it is important to address them to safeguard the accuracy of the analysis. There are two main ways to address outliers: 1) transform the data and 2) eliminate observations with outliers (trimming). Trimming is more extreme as it removes data but can be the best approach for true outliers. Transforming the data is an approach to mitigating the influence of extreme but valid values. Transformation involves applying a mathematical function to the data which changes the scale and/ or shape of the distribution, but does not remove data nor does it change the relative order of the values.\nIn Figure 3.8, we see two boxplots. Figure 3.6 (a) is the original ttr data and Figure 3.6 (b) reflects the data trimmed to remove outliers. In this case, we have removed essays with a type-token ratio of 1.\n\n\n\n\n\n(a) Type-token ratio score\n\n\n\n\n\n(b) Type-token ratio score (trimmed)\n\n\n\nFigure 3.6: Boxplots for ttr before and after trimming.\n\n\nWe can now appreciate the relatively larger effect that the outliers had on the mean value of the ttr variable. As outliers are removed as the difference between the mean and median will become smaller.\n\nThe exploration the data points with histograms and boxplots has helped us to identify outliers. Now we turn to the question of the overall shape of the distribution. The key question is whether the observed distribution of each variable approximates the Normal Distribution, or not.\nThe Normal Distribution is a theoretical distribution where the values are symmetrically dispersed around the central tendency (mean/ median). In terms we can now understand, this means that the mean and median are the same. The Normal Distribution is important because many statistical tests assume that the data distribution is normal or near normal.\nStepping away from our BELC dataset, I’ve created simulated data that fit normal and non-normal, or skewed, distributions. I present each of these distributions as density plots with mean and median line overlays in Figure 3.7.\n\n\n\n\n\n(a) Left skewed distribution\n\n\n\n\n\n(b) Normal distribution\n\n\n\n\n\n(c) Right skewed distribution\n\n\n\nFigure 3.7: Mean and median for normal and skewed distributions.\n\n\nA Normal Distribution, illustrated in Figure 3.7 (b), is a distribution where the values are symmetrically dispersed around the central tendency (mean/ median). This means that in a theoretical distribution that the mean and median are the same. The Normal Distribution is also known as the Gaussian Distribution or the Bell Curve, for the hallmark bell shape of the distribution. In this distribution, extreme values are less likely than values near the center.\nA skewed distribution is not a specific type of distribution but rather a characteristic than many distributions can exhibit where the values are not symmetrically dispersed around the central tendency. A distribution in which values tend to disperse to the left of the central tendency is left skewed as in Figure 3.7 (a) and dispersion to the right is right skewed as in Figure 3.7 (c).\nData that are normally, or near-normally distributed are often analyzed using parametric tests while data that exhibit a skewed distributed are often analyzed using non-parametric tests. Divergence from normality is not a binary distinction. Rather, it is a matter of degree. A visual inspection is usually sufficient for experienced researchers to determine whether a distribution is normal or skewed. However, for those who are less experienced or if you want to be more precise, there are two primary measures which can help ascertain the degree to which a distribution is normal: skewness and kurtosis. Skewness is a measure of the degree to which a distribution is asymmetrical. Kurtosis is a measure of the degree to which a distribution is peaked.\nIn Table 3.10 I provide the skewness and kurtosis scores for our simulated distributions along with central tendency measures for context.\n\n\n\n\nTable 3.9: Skewness and kurtosis for normal and skewed distributions.\n\ndistribution\nmean\nmedian\nhistogram\nskewness\nkurtosis\n\n\n\nLeft skew\n0.746\n0.767\n▁▂▅▇▆\n-0.711\n3.27\n\n\nNormal\n0.016\n0.009\n▁▅▇▃▁\n0.065\n2.93\n\n\nRight skew\n0.254\n0.233\n▆▇▅▂▁\n0.711\n3.27\n\n\n\n\n\n\n\n\nAll things distribution are matters of degree, so there are no hard and fast rules for determining whether a distribution is normal or skewed. However, there are some general guidelines that can be used to determine the degree to which a distribution is normal or skewed, as shown in Table 3.10.\n\n\nTable 3.10: Rules of thumb for skewness and kurtosis scores.\n\n\n\n\n(a) Skewness scores\n\n\n\n\n\nScore Range\nEvaluation\n\n\n\n-0.5 to 0.5\nApproximately symmetric\n\n\n-1 to -0.5 or 0.5 to 1\nModerately skewed\n\n\n&lt; -1 or &gt; 1\nHighly skewed\n\n\n\n\n\n\n(b) Kurtosis scores\n\n\n\n\n\nScore Range\nEvaluation\n\n\n\n&lt; 3\nLess peaked than normal\n\n\nEqual to 3\nNormal peak\n\n\n&gt; 3\nMore peaked than normal\n\n\n\n\n\n\n\n\n\n\n\n\n Dive deeper\nAnother approach for visually summarizing a single numeric variable is the Empirical Cumulative Distribution Function, or ECDF. An ECDF plot is a summary of the cummulative proportion of each of the values of a numeric variable. In addition to providing insight into the distribution of a variable, ECDF plots can be useful in determing what proportion of the values fall above or below a certain percentage of the data.\n\n\n\nThe question is which type of distribution does each numeric variable in the BELC dataset fit? Comparing the variables ttr, types and prop_l2 in Figure 3.8 to the three distributions in Figure 3.7, we see that all three numeric variables in the BELC dataset are skewed to some degree.\n\n\n\n\n\n(a) Type-token ratio score\n\n\n\n\n\n(b) Number of types\n\n\n\n\n\n(c) Proportion of L2 words\n\n\n\nFigure 3.8: Histogram/ Density plots for numeric variables in the BELC dataset.\n\n\nFigure 3.8 (a) for ttr has some right skewing but not as much as types in Figure 3.8 (b). prop_l2 in Figure 3.8 (c) is the most skewed of the three variables. As mentioned earlier, skewed distributions can take many forms, some are more skewed than others.\nTo view statistics on our three variables in Figure 3.8, we can calculate the skewness and kurtosis.\n\n\n\n\nTable 3.11: Skewness and kurtosis for numeric variables in the BELC dataset.\n\ndistribution\nmean\nmedian\nhistogram\nskewness\nkurtosis\n\n\n\nttr\n0.655\n0.648\n▂▇▇▆▁\n0.319\n2.90\n\n\ntypes\n46.044\n46.000\n▅▇▇▃▂\n0.407\n2.45\n\n\ntokens\n75.338\n77.000\n▇▇▇▃▂\n0.669\n2.98\n\n\nprop_l2\n0.986\n0.990\n▁▁▂▃▇\n-1.273\n4.13\n\n\n\n\n\n\n\n\nGiven the characteristics of the numeric variables in the BELC dataset, although none of them are perfectly normal, but only prop_l2 is highly skewed. Therefore, if we intend to use these variables ‘as-is’ in statistical measures or tests, we now know whether to choose parametric or non-parametric alternatives.\nIn the case that a variable is highly skewed, it is often useful to attempt transform the variable to reduce the skewness. In contrast to scale-based transformations (e.g. centering and scaling), shape-based transformations change the scale and the shape of the distribution. The most common shape-based transformation is the logarithmic transformation. The logarithmic transformation (log-transformation) takes the log (typically base 10) of each value in a variable. The log-transformation is useful for reducing the skewness of a variable as it compresses large values and expands small values. If the skewness is due to these factors, the log-transformation can help.\nIt is important to note, however, that if scale-based transformations are to be applied to a variable, they should be applied after the log-transformation as the log of negative values is undefined.\nInterdependence\n\nWe have covered the first three of the four questions we are interested in asking in a descriptive analysis. The fourth, and last, question is whether there is mutual dependence between variables. If so, what is the directionality and how strong is the dependence? Knowing the answers to these questions will help frame our approach to analysis.\nTo assess interdependence, the number and information types of the variables under consideration are important. Let’s start by considering two variables. If we are working with two variables, we are dealing with a bivariate relationship. Given there are three informational types (categorical, ordinal, and numeric), there are six logical bivariate combinations: categorical-categorical, categorical-ordinal, categorical-numeric, ordinal-ordinal, ordinal-numeric, and numeric-numeric.\nThe directionality of a relationship will take the form of a tabular or graphic summary depending on the informational value of the variables involved. In Table 3.12, we see the appropriate summary types for each of the six bivariate combinations.\n\n\nTable 3.12: Appropriate summary types for different combinations of variable types.\n\n\n\n\n\n\n\n\nCategorical\nOrdinal\nNumeric\n\n\n\nCategorical\nContingency table\nContingency table/ Bar plot\nPivot table/ Boxplot\n\n\nOrdinal\n-\nContingency table/ Bar plot\nPivot table/ Boxplot\n\n\nNumeric\n-\n-\nScatterplot\n\n\n\n\n\nLet’s first start with the combinations that include a categorical or ordinal variable. Categorical and ordinal variables reflect measures of class-type information, with add meaningful ranks to ordinal variables. To assess a relationship with these variable types, a table is always a good place to start. When combined together, a contingency table is the appropriate table. A contingency table is a cross-tabulation of two class-type variables, basically a two-way frequency table. This means that three of the six bivariate combinations are assessed with a contingency table: categorical-categorical, categorical-ordinal, and ordinal-ordinal.\nIn Table 3.13 we see contingency tables for the categorical variable sex and ordinal variable group in the BELC dataset.\n\n\n\n\nTable 3.13: Contingency tables for categorical variable sex and ordinal variable group in the BELC dataset.\n\n\n\n\n(a) Counts\n\ngroup\nfemale\nmale\nTotal\n\n\n\nT1\n7\n9\n16\n\n\nT2\n11\n4\n15\n\n\nT3\n13\n10\n23\n\n\nT4\n9\n5\n14\n\n\nTotal\n40\n28\n68\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Percentages\n\ngroup\nfemale\nmale\nTotal\n\n\n\nT1\n43.75%\n56.25%\n100.00%\n\n\nT2\n73.33%\n26.67%\n100.00%\n\n\nT3\n56.52%\n43.48%\n100.00%\n\n\nT4\n64.29%\n35.71%\n100.00%\n\n\nTotal\n58.82%\n41.18%\n100.00%\n\n\n\n\n\n\n\n\n\n\nA contingency table may include only counts, as in Table 3.13 (a), or may include proportions or percentages in an effort to normalize the counts and make them more comparable, as in Table 3.13 (b).\nIt is sometimes helpful to visualize a contingency table as a bar plot when there are a larger number of levels in either or both of the variables. Again, looking at the relationship between sex and group, we see that we can plot the counts or the proportions. In Figure 3.9, we see both.\n\n\n\n\n\n\n(a) Counts\n\n\n\n\n\n(b) Proportions\n\n\n\nFigure 3.9: Bar plots for the relationship between sex and group in the BELC dataset.\n\n\nTo summarize and assess the relationship between a categorical or an ordinal variable and a numeric variable, we cannot use a contingency table. Instead, this type of relationship is best summarized in a table using a summary statistic in a pivot table. A pivot table is a table in which a class-type variable is used to group a numeric variable by some summary statistic appropriate for numeric variables, e.g. mean, median, standard deviation, etc.\nIn Table 3.14, we see a pivot table for the relationship between group and tokens in the BELC dataset. Specifically, we see the mean number of tokens by group.\n\n\n\n\n\n\nTable 3.14: Pivot table for the relationship between group and tokens in the BELC dataset.\n\ngroup\nmean_tokens\n\n\n\nT1\n35.4\n\n\nT2\n62.5\n\n\nT3\n85.0\n\n\nT4\n118.9\n\n\n\n\n\n\n\n\nWe see that the mean number of tokens increases from Group T1 to T4, which is consistent with the idea that the students in the higher groups are writing longer essays.\nAlthough a pivot table may be appropriate for targeted numeric summaries, a visualization is often more informative for assessing the dispersion and distribution of a numeric variable by a categorical or ordinal variable. There are two main types of visualizations for this type of relationship: a boxplot and a violin plot. A violin plot is a visualization that summarizes the distribution of a numeric variable by a categorical or ordinal variable, adding the overall shape of the distribution, much as a density plot does for histograms.\nIn Figure 3.10, we see both a boxplot and a violin plot for the relationship between group and tokens in the BELC dataset.\n\n\n\n\n\n\n(a) Boxplot\n\n\n\n\n\n(b) Violin plot\n\n\n\nFigure 3.10: Boxplot and violin plot for the relationship between group and tokens in the BELC dataset.\n\n\nFrom the boxplot in Figure 3.10 (a), we see that the general trend towards more tokens used by students in higher groups. But we can also appreciate the dispersion of the data within each group looking at the boxes and whiskers. On the surface it appears that the data for groups T1 and T3 are closer to each other than groups T2 and T4, in which there is more variability within these groups. Furthermore, we can see outliers in groups T1 and T3, but not in groups T2 and T4. From the violin plot in Figure 3.10 (b), we can see the same information, but we can also see the overall shape of the distribution of tokens within each group. In this plot, it is very clear that group T4 includes a wide range of token counts.\n\nThe last bivariate combination is numeric-numeric. To summarize this type of relationship a scatterplot is used. A scatterplot is a visualization that plots each data point as a point in a two-dimensional space, with one numeric variable on the x-axis and the other numeric variable on the y-axis. Depending on the type of relationship you are trying to assess, you may want to add a trend line to the scatterplot. A trend line is a line that summarizes the overall trend in the relationship between the two numeric variables. To assess the extent to which the relationship is linear, a straight line is drawn which minimizes the distance between the line and the points.\nIn Figure 3.11, we see a scatterplot and a scatterplot with a trend line for the relationship between ttr and types in the BELC dataset.\n\n\n\n\n\n\n(a) Points\n\n\n\n\n\n(b) Points with a linear trend line\n\n\n\nFigure 3.11: Scatter plot for the relationship between ttr and types in the BELC dataset.\n\n\nWe see that there is an apparent positive relationship between these two variables, which is consistent with the idea that as the number of types increases, the type-token ratio increases. In other words, as the number of unique words increases, so does the lexical diversity of the text. Since we are evaluating a linear relationship, we are assessing the extent to which there is a correlation between ttr and types. A correlation simply means that as the values of one variable change, the values of the other variable change in a consistent manner.\n\nOnce a sense of the directionality of a relationship can be established, the next step is to gauge the relative strength, or association. Association refers to any relationship in which there is a dependency between two variables. Quantitative measures of association, in combination with tabular and visual summaries, can provide a more complete picture of the relationship between two variables.\nThere are a number of measures of assocation, depending on the types of variables being assessed and, for numeric variables, whether the distribution is normal (parametric) or non-normal (non-parametric), as seen in Table 3.15.\n\n\nTable 3.15: Measures of association or correlation strength for different combinations of variable types.\n\n\n\n\n\n\n\n\n\nCategorical\nOrdinal\nNumericNon-parametric\n\nNumericParametric\n\n\n\n\nCategorical\nChi-square \\(\\chi^2\\), Cramér’s \\(V\\)\n\nGoodman and Kruskal’s \\(\\gamma\\)\n\nRank biserial Correlation\nPoint-biseral Correlation\n\n\nOrdinal\n-\nKendall’s \\(\\tau\\)\n\nKendall’s \\(\\tau\\)\n\nPearson’s \\(r\\)\n\n\n\n\nNumericNon-parametric\n\n-\n-\nKendall’s \\(\\tau\\)\n\nPearson’s \\(r\\)\n\n\n\n\nNumericParametric\n\n-\n-\n-\nPearson’s \\(r\\)\n\n\n\n\n\nAssociation measures often are expressed as a number between -1 and 1, where 0 indicates no association, -1 indicates a perfect negative association, and 1 indicates a perfect positive association. The closer the number is to 0, the weaker the association. The closer the number is to -1 or 1, the stronger the association. Association statistics are often accompanied by a confidence interval (CI), which is a range of values that is likely to contain the true value of the association in the population. The confidence interval is expressed as a percentage, such as 95%, which means that if we were to repeat the study 100 times, 95 of those studies would produce a confidence interval that contains the true value of the association in the population. If the range between the lower and higher bounds of the confidence interval contains 0, then the association is likely no different than chance.\nGiven these measures and interpretations, let’s consider the different types of bivariate relationships we have seen so far in the BELC dataset. The first interdependence we explored involved the categorical variable sex and the ordinal variable group. This relationship may not be of primary interest to a study on L2 writing, but it is a good example of how to assess the strength of an association between a categorical and ordinal variable. Furthermore, it could be the case that we want to assess whether we have widely unbalanced female/ male proportions in our time groups.\nUsing Table 3.15, we see that we can use Goodman and Kruskal’s \\(\\gamma\\) (gamma) to assess the strength of the association between these two variables. The measures of association in Table 3.16 suggest that the proportion of male participants is higher in group T1 and lower in group T2. However, these associations are moderately strong, as the gamma value is near \\(\\pm\\) 0.4.\n\n\n\n\n\nTable 3.16: Gamma for the relationship between sex and group in the BELC dataset.\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\n\n\n\nsex.female\ngroup.T1\n-0.381\n0.95\n-0.568\n-0.157\n\n\nsex.female\ngroup.T2\n0.389\n0.95\n0.167\n0.575\n\n\nsex.male\ngroup.T1\n0.381\n0.95\n0.157\n0.568\n\n\nsex.male\ngroup.T2\n-0.389\n0.95\n-0.575\n-0.167\n\n\n\n\n\n\n\n\nWhen paired with Figure 3.9 we can appreciate that groups T1 and T2 have contrasting proportions of females to males and that groups T3 and T4 are more closely proportioned. This observation should be considered when approaching statistical analyses in which categorical variables required (near) equal proportions of categories.\n\nNow let’s take a look at a more interesting relationship, the one between the ordinal variable group and the numeric variable tokens. Since we determined that tokens was near normally distributed, we can choose the parametric version of our association measure, Pearson’s \\(r\\). The measures of association in Table 3.17 suggest that there is a negative association between group T1 and a positive one betwen group T4 and tokens, which is consistent with the idea that as the group number increases, the number of tokens increases. These associations are moderate to strong, as the Pearson’s \\(r\\) values are near \\(\\pm\\) 0.5. However, the other groups (T2 and T3) have very weak assocations with tokens and the CI includes 0, which means that the association is likely no different than chance.\n\n\n\n\nTable 3.17: Pearson’s r for the relationship between group and tokens in the BELC dataset.\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\n\n\n\ngroup.T1\ntokens\n-0.520\n0.95\n-0.675\n-0.322\n\n\ngroup.T2\ntokens\n-0.160\n0.95\n-0.384\n0.082\n\n\ngroup.T3\ntokens\n0.161\n0.95\n-0.080\n0.385\n\n\ngroup.T4\ntokens\n0.521\n0.95\n0.322\n0.675\n\n\n\n\n\n\n\n\nThese association measures suggest that there is a relationship between group and tokens, but that the relationship is not the same for all groups. This may due to a number of factors, such as the number of participants in each group, the effect of outliers within particular levels, etc. or may simply underscore that the relationship between group and tokens is not linear. What we do with this information will depend on our research aims. Whatever the case, we can use these measures to inform our next steps, as we will see in the next section.\n\nFinally, let’s look at the relationship between the numeric variables ttr and types. Since we determined both ttr and types are normally distributed, we can choose the parametric version of our association measure, Pearson’s \\(r\\). The measures of association in Table 3.18 suggest that there is a negative association between ttr and types, which is consistent with the idea that as the number of types increases, the type-token ratio decreases. This association is strong, as Pearson’s \\(r\\) value is near 0.6.\n\n\n\n\nTable 3.18: Pearson’s r for the relationship between ttr and types in the BELC dataset.\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\n\n\nttr\ntypes\n-0.606\n0.95\n-0.738\n-0.43\n\n\n\n\n\n\n\nBefore moving on to the next section, it is important to remember than through the process of diagnostic measures, we gain a thorough understanding of our data’s characteristics and quality, preparing us for the next step in our analysis. However, remember that these measures do not exist in isolation. The decisions we make at this stage, from handling missing data to understanding the distribution of our variables, can have significant implications on our subsequent analysis. So, this initial step of data analysis deserves our careful attention and scrutiny."
  },
  {
    "objectID": "approaching-analysis.html#sec-aa-analyze",
    "href": "approaching-analysis.html#sec-aa-analyze",
    "title": "3  Approaching analysis",
    "section": "\n3.2 Analyze",
    "text": "3.2 Analyze\nHaving ensured that our dataset is clean, valid, and thoroughly understood, we can proceed to the next key stage of our data analysis process - employing analytic methods. The goal of analysis, generally, is to generate knowledge from information. The type of knowledge generated and the process by which it is generated, however, differ and can be broadly grouped into three analysis types: exploratory, predictive, and inferential.\nIn this section I will provide an overview of how each of these analysis types are tied to research aims and how the general purpose of each type affect: (1) how to identify the variables of interest, (2) how to interrogate these variables, and (3) how to interpret the results. I will structure the discussion of these analysis types moving from the least structured (inductive) to most structured (deductive) approach to deriving knowledge from information with the aim to provide enough information for you to identify these research approaches in the literature and to make appropriate decisions as to which approach your research should adopt.\n\n\n\n3.2.1 Explore\n\nIn Exploratory Data Analysis (EDA), we use a variety of methods to identify patterns, trends, and relations within and between variables. The goal of EDA is uncover insights in an inductive, data-driven manner. That is to say, that we do not enter into EDA with a fixed hypothesis in mind, but rather we explore intuition, probe anecdote, and follow hunches to identify patterns and relationships and to evaluate whether and why they are meaningful. We are admittedly treading new or unfamiliar terrain letting the data guide our analysis. This means that we can use and reuse the same data to explore different angles and approaches adjusting our methods and measures as we go. In this way, EDA is an iterative, meaning generating process.\n\nIn line with the investigative nature of EDA, the identification of variables of interest is a discovery process. We most likely have a intution about the variables we would like to explore, but we are able to adjust our variables as need be to suit our research aims. When the identification and selection of variables is open, the process is known as feature engineering. A process that is much an art as a science, feature engineering leverages a mixture of relevant domain knowledge, intuition, and trial and error to identify features that serve to best represent the data and to best serve the research aims. Furthermore, the roles of features in EDA are fluid –no variable has a special status, as seen in Figure 3.12. We will see that in other types of analysis, some or all the roles of the variables are fixed.\n\n\n\n\nFigure 3.12: Roles of variables in EDA.\n\n\n\nFor illustrative purposes let’s consider the State of the Union Corpus (SOTU) (Benoit 2020). The presidential addresses and a set of metadata variables are included in the corpus. I’ve subsetted this corpus to only include U.S. presidents since 1946. A tabular preview of the first 10 addresses (truncated for display) can be found in Table 3.19.\n\n\n\n\n\nTable 3.19: First ten addresses from the SOTU Corpus.\n\npresident\ndate\ndelivery\nparty\naddresses\n\n\n\nTruman\n1946-01-21\nwritten\nDemocratic\nTo the Congress of the United States: A quarter...\n\n\nTruman\n1947-01-06\nspoken\nDemocratic\nMr. President, Mr. Speaker, Members of the Cong...\n\n\nTruman\n1948-01-07\nspoken\nDemocratic\nMr. President, Mr. Speaker, and Members of the ...\n\n\nTruman\n1949-01-05\nspoken\nDemocratic\nMr. President, Mr. Speaker, Members of the Cong...\n\n\nTruman\n1950-01-04\nspoken\nDemocratic\nMr. President, Mr. Speaker, Members of the Cong...\n\n\nTruman\n1951-01-08\nspoken\nDemocratic\nMr. President, Mr. Speaker, Members of the Cong...\n\n\nTruman\n1952-01-09\nspoken\nDemocratic\nMr. President, Mr. Speaker, Members of the Cong...\n\n\nTruman\n1953-01-07\nwritten\nDemocratic\nTo the Congress of the United States: I have th...\n\n\nEisenhower\n1953-02-02\nspoken\nRepublican\nMr. President, Mr. Speaker, Members of the Eigh...\n\n\nEisenhower\n1954-01-07\nspoken\nRepublican\nMr. President, Mr. Speaker, Members of the Eigh...\n\n\n\n\n\n\n\n\nA dataset such as this one could serve as a starting point to explore many different types of research questions. In order to maintain research coherence so our efforts to not careen into a free-for-all, we need to tether our feature engineering to a unit of analysis that is relevant to the research question. A unit of analysis is the entity that we are interested in studying. Not to be confused with the unit of observation, which is the entity that we are able to observe and measure (Sedgwick 2015).\nTo demonstrate the distinction, let’s look consider different approaches to analyzing the SOTU dataset. For example, the unit of analysis could be the language of particular presidents, party ideology, or political rhetoric in general and the unit of observation could be individual words, phrases, sentences, etc. In some cases the unit of analysis and the unit of observation are the same. For example, if we were interested in potential changes use of the word “terrorist” over time in SOTU addresses, the unit of analysis and the unit of observation would be the same –individual addresses. So, depending on the perspective we are interested in investigating, the choice of how to approach engineering features to gain insight will vary.\n\nBy the same token, approaches for interrogating the dataset can differ significantly, between research projects and within the same project, but for instructive purposes, let’s draw a distinction between descriptive methods and unsupervised learning methods, as seen in Table 3.20.\n\n\n\n\nTable 3.20: Some common EDA methods\n\nDescriptive methods\nUnsupervised learning methods\n\n\n\nFrequency analysis\nCluster analysis\n\n\nKeyness analysis\nTopic Modeling\n\n\nCo-occurence analysis\nVector Space Models\n\n\n\n\n\n\n\n\nThe first group, descriptive methods can be seen as a (more robust) extenstion of the descriptive statistics covered earlier in this chapter including statistic, tabular, and visual techniques. For example, a frequency analysis of the SOTU dataset could be used to identify the most common words used by U.S. political parties in their addresses, in Figure 3.13 (a), or a co-occurence analysis could be used to identify the most common words the appear after the term “free”, in Figure 3.13 (b), in the dataset.\n\n\n\n\n\n\n(a) Frequency analysis of the 20 most frequent terms by party.\n\n\n\n\n\n(b) Co-occurence analysis of the terms that appear after the term ‘free’.\n\n\n\nFigure 3.13: Example of descriptive methods applied to the SOTU dataset.\n\n\nThe second group, unsupervised learning, is a subtype of machine learning in which an algorithm is used to find patterns within and between variables in the data without any guidance (supervision). In this way, the algorithm, or machine learner, is left to make connections and associations wherever they may appear in the input data. If we were interested in finding word-use continuities and discontinuities between presidents, we could use a clustering algorithm, seen in Figure 3.14 (a). Or if we wanted to uncover themes …  [ADD: modify plot] we could use a vector space model, as in Figure 3.14 (b).\n\n\n\n\n\n\n(a) Hierarchical clustering of the SOTU corpus.\n\n\n\n\n\n(b) Word embedding space in the SOTU corpus.\n\n\n\nFigure 3.14: Example of unsupervised learning methods applied to the SOTU dataset.\n\n\n\nEither through descriptive, unsupervised learning methods, or a combination of both, EDA employs quantitative methods to summarize, reduce, and sort complex datasets in order to provide the researcher novel perspective to be qualitatively assessed. Exploratory methods produce results that require associative thinking and pattern detection. Speculative as they are, the results from exploratory methods can be highly informative and lead to new insight and inspire further study in directions that may not have been expected.\n\n3.2.2 Predict\n\nPredictive Data Analysis (PDA) employs a variety of techniques to examine and evaluate the association strength between a variable or set of variables, with a specific focus on predicting a target variable. The aim of PDA is to construct models that can accurately forecast future outcomes, using either data-driven or theory-driven approaches. In this process, supervised learning methods, where the machine learning algorithm is guided (supervised) by a target outcome variable, are used. This means we don’t begin PDA with a completely open-ended exploration, but rather with an objective - accurate predictions. However, the path to achieving this objective can be flexible, allowing us freedom to adjust our models and methods. Unlike EDA, where the entire dataset can be reused for different approaches, PDA requires a portion of the data to be reserved for evaluation, enhancing the validity of our predictive models. Thus, PDA is an iterative process that combines the flexibility of exploratory analysis with the rigor of confirmatory analysis.\n\n\n\nThere are two types of variables in PDA: the outcome variable and the predictor variables, or features. The outcome variable is the variable that the researcher is trying to predict. It is the only variable that is necessarily fixed as part of the research question. The features are the variables that are used to predict the outcome variable. An overview of the roles of these variables in PDA is shown in Figure 3.15.\n\n\n\n\nFigure 3.15: Roles of variables in PDA.\n\n\n\nFeature selection can be either data-driven or theory-driven. Data-driven features are those that are engineered to enhance predictive power, while theory-driven features are those that are selected based on theoretical relevance.\nLet’s consider the Europarl corpus of native, non-native and translated texts (ENNTT) (Nisioi et al. 2016). This is a monolingual English corpus of translated and non-translated texts from the European Parliament.\n\n\n\n\nTable 3.21: Data dictionary of the ENNTT corpus.\n\nvariable\nname\nvariable_type\ndescription\n\n\n\nsession_id\nSession ID\ncategorical\nUnique identifier for each session\n\n\nseq_speaker_id\nSequential Speaker ID\nordinal\nUnique numeric identifier for each speaker\n\n\nstate\nState\ncategorical\nCountry of the session speaker\n\n\nlanguage\nLanguage\ncategorical\nOriginal language in which the sentence was uttered\n\n\ntype\nType\ncategorical\nCategory of the speaker: natives, nonnatives, or translations\n\n\ntext\nText\ncategorical\nText spoken in the session\n\n\n\n\n\n\n\n\nNow depending on our research question, we will have a different outcome variable. If we want to examine the potential linguistic differences between native and non-native speakers, we will select our outcome variable to be the type (natives/ nonnatives). The features selected to use to predict type depend on our research question. If our research is guided by data, we will choose features that are specifically designed to boost the ability to predict. On the other hand, if our research is steered by theory, we will opt for features that are chosen due to their theoretical significance. In either case, the original dataset will likely need to be transformed.\nThe approach to interrogating the dataset includes three main steps: feature engineering, model selection, and model evaluation. We’ve discussed feature engineering, so what is model selection and model evaluation? And how do we go about performing these steps?\nModel selection is the process of choosing a machine learning algorithm and set of features that produces the best prediction accuracy for the outcome variable. To refine our approach such that we arrive at the best combination of algorithm and features, we need to train our machine learner on a variety of combinations and evaluate the accuracy of each. We don’t want to train and evaluate on the same data, as this would be cheating, and likely would not produce a model that generalizes well to new data. Instead, we split our data into two sets: a training set and a test set. The training set is used to train the machine learner, while the test set is used to evaluate the accuracy of the model1. The larger portion of the data, from 60% to 80%, is used for training, while the remaining portion is used for testing.\n\nThe elephant in the room is, what type of machine learning algorithm do I use? Well, there are many different types of machine learning algorithms, each with their own strengths and weaknesses. The first rough cut is to decide what type of outcome variable we are predicting: categorical or numeric. If the outcome variable is categorical, we are performing a classification task, and if the outcome variable is numeric, we are performing a regression task. As we see in Table 3.22, there are various algorithms that can be used for each task.\n\n\nTable 3.22: Some common supervised learning algorithms used in PDA.\n\n\n\n\n\n\nClassification\nRegression\nLearner type\n\n\n\nLogistic Regression\nLinear Regression\nInterpretable\n\n\nDecision Tree\nRegression Tree\nInterpretable\n\n\nSupport Vector Machine\nSupport Vector Regression\nBlack box\n\n\nMultilayer Perceptron\nMultilayer Perceptron\nBlack box\n\n\n\n\nI’ve included a column in Table 3.22 that charaterizes a second consideration which is whether we want an interpretable model or a black box model. When talking about whether a model is interpretable or not, we are not referring to the evaluation of the accuracy of the model. Rather, we are referring to the inner workings of the model itself that allow us to understand how the model is making its predictions. An interpretable model is one that can be understood and explored by humans, while a black box model is one whose inner workings are not trivially unraveled. The advantage of an interpretable model is that it researchers can go beyond evaluating prediction accuracy and probe feature-outcome associations. On the other hand, if the goal is to simply boost prediction accuracy, interpretability may not be a concern.\n\n\nFinally, there are a number of algorithm-specific strengths and weaknesses to be considered in the process of model selection. These hinge on characteristics of the data, such as the size of the dataset, the number of features, the type of features, and the expected type of relationships between features or on computing resources, such as the amount of time available to train the model or the amount of memory available to store the model.\n\nModel evaluation is the process of assessing the accuracy of the model on the test set, which is a proxy for how well the model will generalize to new data. Model evaluation is performed quantitatively by calculating the accuracy of the model on the training, to develop the model, and ultimately, the test set. The accuracy of a model is calculated by comparing the predicted values to the actual values. For the results of classification tasks, this results in a contingency table, known as a confusion matrix. A confusion matrix juxtaposes predicted and actual values allowing various metrics to be calculated, for example in Table 3.23.\n\n\n\n\nTable 3.23: Confusion matrix for the utterance type classification task.\n\n\n\n\n\n\n\nPredicted: natives\nPredicted: nonnatives\n\n\n\nActual: natives\n26294 (90% of 29215)\n2921 (10% of 29215)\n\n\nActual: nonnatives\n730 (10% of 7304)\n6574 (90% of 7304)\n\n\n\n\nSince regression tasks predict numeric values, the accuracy of the model is calculated by comparing the difference between the predicted and actual values.\nIt is important to note that whether the accuracy metrics are good is to some degree qualitative judgment. For example, classification accuracy overall may be relatively high, but the model may be performing poorly on one of the classes. In this case, the model may not be useful for the task at hand, despite the overall accuracy.\nIn the end, PDA offers a versitle path to discover data-driven insights, to probe theory-driven associations, or even simply to perform tasks that are too complex or time-consuming for humans to perform.\n\n3.2.3 Infer\n\nThe most commonly recognized of the three data analysis approaches, Inferential data analysis (IDA) is the bread-and-butter of science. IDA is a deductive, theory-driven approach in which all aspects of analysis stem from a pre-determined premise, or hypothesis, about the nature of a relationship in the world and then aims to test whether this relationship is statistically supported given the evidence. Since the goal is to infer conclusions about a certain relationship in the population based on a statistical evaluation of a (corpus) sample, the representativeness of the sample is of utmost importance. Furthermore, the use of the data is limited to the scope of the hypothesis –that is, the data cannot be used for exploratory purposes.\n\n\nThe selection of variables and the roles they play in the analysis are determined by the hypothesis. In a nutshell, a hypothesis is a formal statement about the state of the world. This statement is theory-driven meaning that it is predicated on previous research. We are not exploring or examining relationships, rather we are testing a specific relationship. In practice, however, we are in fact proposing two mutally exclusive hypotheses. The first is the Alternative Hypothesis, or \\(H_1\\). This is the hypothesis I just described –the statement grounded in the previous literature outlining a predicted relationship. The second is the Null Hypothesis, or \\(H_0\\). This is the flip-side of the hypothesis testing coin and states that there is no difference or relationship. Together \\(H_1\\) and \\(H_0\\) cover all logical outcomes.\nTo connect hypotheses to variable selection and variable roles, let’s consider a study in which a researcher is investigating the claim that men and women differ in terms of the number of questions they use in spontaneous conversations. The unit of analysis is individuals (i.e. men and women) and the unit of observation is (sponteanous) conversations.\nA dataset based on the Switchboard Dialog Act Corpus (SWDA) (University of Colorado Boulder 2008), seen in Table 3.24, aligns well with this investigation. It is a large collection of transcribed telephone conversations between strangers. The dataset includes gender information for each participant and dialog act annotation for each utterance, including a range of question types.\n\n\n\n\nTable 3.24: Data dictionary of the SWDA dataset.\n\nvariable\nname\ndescription\nvariable_type\n\n\n\ndoc_id\nDocument ID\nIdentification number for each document\nnumeric\n\n\nspeaker_id\nSpeaker ID\nIdentification number for each speaker\nnumeric\n\n\nsex\nSex\nGender of the speaker\ntext\n\n\ndamsl_tag\nDAMSL Tag\nLinguistic tag indicating speech act or dialogue act\ntext\n\n\nutterance_text\nUtterance Text\nThe actual text of the speaker's utterance\ntext\n\n\n\n\n\n\n\n\nThe Alternative Hypothesis may be formulated in this way:\n\\(H_1\\): Men and women differ in the frequency of the use of questions in spontaneous conversations.\nThe Null Hypothesis, then, would be a statement describing the remaining logical outcomes. Specifically:\n\\(H_0\\): Men and women do not differ in the frequency of the use of questions in spontaneous conversations.\nNow, in standard IDA one variable is the dependent variable and one or more variables are predictor variables. The dependent variable, sometimes referred to as the outcome or response variable, is the variable which contains the information which is hypothesized to depend on the information in the predictor variable(s). It is the variable whose variation a research study seeks to explain. A predictor variable, sometimes referred to as a independent or explanatory variable, is a variable whose variation is hypothesized to explain the variation in the dependent variable.\nReturning to our hypothetical study and the hypotheses presented, we can identify the variables in our study and map them to their roles. The frequency of questions used by each speaker would be our dependent variable and the biological sex of the speakers our predictor variable. This is so because \\(H_1\\) states the proposition that a speaker’s sex will predict the frequency of questions used. The next step would be to operationalize what we mean by ‘frequency of questions’ and then transform the dataset to reflect this definition.\nIn our hypothetical study we’ve identified two variables, one dependent and one predictor. It is important keep in mind that there can be multiple predictor variables in cases where the dependent variable’s variation is predicted to be related to multiple variables. This relationship would need to be explicitly part of the original hypothesis, however. Due to the increasing difficulty for interpretation, in practice, IDA studies rarely include more than two or three predictor variables in the same analysis.\nPredictor variables add to the complexity of a study because they are part of our research focus, specifically our hypothesis. It is, however, common to include other variables which are not of central focus, but are commonly assumed to contribute to the explanation of the variation of the dependent variable. Let’s assume that the background literature suggests that the age of speakers also plays a role in the number of questions that men and women use in spontaneous conversation. Let’s also assume that the data we have collected includes information about the age of speakers. If we would like to factor out the potential influence of age on the use of questions and focus on the particular predictor variables we’ve defined in our hypothesis, we can include the age of speakers as a control variable. A control variable will be added to the statistical analysis and documented in our report but it will not be included in the hypothesis nor interpreted in our results.\nWe can now see in Figure 3.16 the variables roles assigned to variables in a hypothesis-driven study.\n\n\n\n\nFigure 3.16: Roles of variables in IDA.\n\n\n\n\nAt this point let’s look at the main characteristics that need to be taken into account to statistically interrogate the variables we have chosen to test our hypothesis. The type of statistical test that one chooses is based on (1) the informational value of the dependent variable and (2) the number of predictor variables included in the analysis. Together these two characteristics go a long way in determining the appropriate class of statistical test, but other considerations about the distribution of particular variables (i.e. normality), relationships between variables (i.e. independence), and expected directionality of the predicted effect may condition the appropriate method to be applied.\nAs you can imagine, there are a host of combinations and statistical tests that apply in particular scenarios, too many to consider in given the scope of this coursebook (see Gries (2013) and Paquot and Gries (2020) for a more exhaustive description). Below I’ve summarized some common statistical scenarios and their associated tests which focus on the juxtaposition of informational values and the number of variables, leaving aside alternative tests which deal with non-normal distributions, ordinal variables, etc.\nIn Table 3.25 we see monofactorial tests, tests with only one predictor variable.\n\n\n\n\nTable 3.25: Common monofactorial tests used in IDA.\n\n\n\n\n\n\n\n\nVariable roles\n\n\n\n\nDependent\nPredictor\nTest\n\n\n\n\nCategorical\nCategorical\nPearson's Chi-squared test\n\n\nNumeric\nCategorical\nStudent's t-Test\n\n\nNumeric\nNumeric\nPearson's correlation test\n\n\n\n\n\n\n\n\nTable 3.26 includes a listing of multifactorial tests, tests with more than one predictor and/ or control variables.\n\n\n\n\nTable 3.26: Common multifactorial tests used in IDA.\n\n\n\n\n\n\n\n\n\nVariable roles\n\n\n\n\nDependent\nPredictor\nControl\nTest\n\n\n\n\nCategorical\nvaried\nvaried\nLogistic regression\n\n\nNumeric\nvaried\nvaried\nLinear regression\n\n\n\n\n\n\n\n\n\nIDA relies heavily on quantitative evaluation methods to draw conclusions that can be generalized to the target population. It is key to understand that our goal in hypothesis testing is not to find evidence in support of \\(H_1\\), but rather to assess the likelihood that we can reliably reject \\(H_0\\). The metric used to determine if there is sufficient evidence is based on the probability that given the nature of the relationship and the characteristics of the data, the likelihood of there being no difference or relationship is low. The threshold for likelihood has traditionally been summarized in the \\(p\\)-value statistic. In the Social Sciences, a \\(p\\)-value lower than .05 is considered statistically significant which when interpreted correctly means that there is more than a 95% chance that the observed relationship would not be predicted by \\(H_0\\). Note that we are working in the realm of probability, not in absolutes, therefore an analysis that produces a significant result does not prove \\(H_1\\) is correct or that \\(H_0\\) is incorrect, for that matter. A margin of error is always present. For this reason, other metrics such as effect size and confidence intervals are also used to interpret the results of statistical tests."
  },
  {
    "objectID": "approaching-analysis.html#sec-aa-communicate",
    "href": "approaching-analysis.html#sec-aa-communicate",
    "title": "3  Approaching analysis",
    "section": "\n3.3 Communicate",
    "text": "3.3 Communicate\n\nConducting research should be enjoyable and personally rewarding but the effort you have invested and knowledge you have generated should be shared with others. Whether part of a blog, presentation, journal article, or for your own purposes it is important to document your analysis results and process in a way that is informative and interpretable. This enhances the value of your work, allowing others to learn from your experience and build on your findings.\n\n3.3.1 Report\n\nThe most widely recognized form of communicating research is through a report. A report is a narrative of your analysis, including the research question, the data you used, the methods you applied, and the results you obtained. We are both reporting our findings and documenting our process to inform others of what we did and why we did it but also to invite readers to evaluate our findings for themselves. The scientific process is a collaborative one and evaluation by peers is a key component of the process.\n\nThe audience for your report will determine the level of detail and the type of information you will need to include in your report but there are some common elements to reference in any report. First, the research question and/ or hypothesis should be clearly stated and the motivation for the question should be explained. This will help the reader understand the context of the analysis and the importance of the results. Second, diagnostic procedures to verifiy or describe the data should be explained. This may include anomaly correction, missing data, data transformation, etc. and/ or descriptive summaries of the data including assessments of individual variables (central tendency, dispersion, distribution) and/ or relationships between variables (association strength). Third, a blueprint of the methods used will describe the variable selection process, how the variables are operationalized, what analysis methods are employed, and how the variables are used in the statistical analysis. Fourth, the results from the analysis are reported. Reporting details will depend on the type of analysis and the particular method(s) employed. For inferential analyses this will include the test statistic(s) and some measure of confidence. In predictive analyses, accuracy results will be reported. For exploratory analyses, the reporting of results will vary and often include visualizations and metrics that require more human interpretation than the other analysis types. Finally, the results are interpreted in light of the research question and/ or hypothesis. This will include a discussion of the limitations of the analysis and a discussion of the implications of the results for future research.\n\n3.3.2 Document\n\nWhile a good report will include the most vital information to understand the procedures, results, and findings of an analysis, there is much more information generated in the course of an analysis which does not traditionally appear in prose. If a research project is conducted programmatically, however, data, code, and documentation can be made available to others as part of the communication process. Increasingly, researchers are sharing their data and code as part of the publication process. This allows others to reproduce the analysis and verify the results contributing to the collaborative nature of the scientific process.  [CITATION]\n\nTogether, data, code, and documentation form a research compendium. As you can imagine the research process can quickly become complex and unwieldy as the number of files and folders grows. If not organized properly, it can be difficult to find the information you need. Furthermore, if not documented, decisions made in the course of the analysis can be difficult or impossible to trace. For this reason it is recommendable to follow a set of best practices for organizing and documenting your research compendium.\n\nWe will have more to say about this in the next chapter but for now it will suffice to point to some key elements in a research compendium. First, the data used in the analysis should be saved as a separate file(s). As a given research project progresses to analysis, the data may be transformed and manipulated to best fit the needs of the analysis. Preserving the data at each stage adds to the complete picture of the data from collection to analysis. Second, since you are working programmatically, you can share your precise analysis step-by-step in code form. This allows others to reproduce your analysis and verify your results. Including code comments provides additional information to communicate the steps taken and your thought process. Finally, a codebook documents any additional information that helps understand the research better. This will often include guides for installing software and running the code to reproduce the analysis and an overview of the aims of the scripts and the contents of the data and datasets."
  },
  {
    "objectID": "approaching-analysis.html#summary",
    "href": "approaching-analysis.html#summary",
    "title": "3  Approaching analysis",
    "section": "Summary",
    "text": "Summary\nIn this chapter we have focused on description and analysis –the third component of DIKI Hierarchy. This process is visually summarized in Figure 3.17.\n\n\n\n\nFigure 3.17: Approaching analysis: visual summary\n\n\n\nBuilding on the strategies covered in Chapter 2 “Understanding data” to derive a rich relational dataset, in this chapter we outlined key points in approaching analysis. The first key step in any analysis is to perform a diagnostic assessment of the individual variables and relationships between variables. To select the appropriate descriptive measures we covered the various informational values that a variable can take. In addition to providing key information for reporting purposes, descriptive measures are important to explore so the researcher can get a better feel for the dataset before conducting an analysis.\nWe outlined three data analysis types in this chapter: exploratory, predictive, and inferential. Each of these embodies distinct approaches to deriving knowledge from data. Ultimately the choice of analysis type is highly dependent on the goals of the research. Inferential analysis is centered around the goal of testing a hypothesis, and for this reason it is the most highly structured approach to analysis. This structure is aimed at providing the mechanisms to draw conclusions from the results that can be generalized to the target population. Predictive analysis has a less-ambitious but at times more relevant goal of examining the extent to which a given relationship can be established from the data to provide a model of language that can accurately predict an outcome using new data. This methodology is highly effective for applying different algorithmic approaches and examining relationships between an outcome variable and various configurations of variables. The ability to explore the data in multiple ways, is also a key strength of employing an exploratory analysis. The least structured and most variable of the analysis types, exploratory analyses are a powerful approach to generating knowledge from data in an area where clear predictions cannot be made.\nI rounded out this chapter with a short description of the importance of communicating the analysis process and results. Reporting, in its traditional form, is documented in prose in an article. This reporting aims to provide the key information that a reader will need to understand what was done, how it was done, and why it was done. This information also provides the necessary information for reader’s with a critical eye to understand the analysis in more detail. Yet even the most detailed reporting in a write-up still leaves many practical, but key, points of the analysis obscured. A programming approach provides the procedural steps taken that when shared provide the exact methods applied. Together with the write-up, a research compendium which provides the scripts to run the analysis and documentation on how to run the analysis forms an integral part of creating reproducible research."
  },
  {
    "objectID": "approaching-analysis.html#activities",
    "href": "approaching-analysis.html#activities",
    "title": "3  Approaching analysis",
    "section": "Activities",
    "text": "Activities\n\n\n Add description of outcomes …\n\n\n\n\n\n\n\n Recipe\n\nWhat: Descriptive assessment of datasetsHow: Read Recipe 3 and participate in the Hypothes.is online social annotation.Why: To explore appropriate methods for summarizing variables in datasets given the number and informational values of the variable(s).\n\n\n\n\n\n\n\n\n\n Lab\n\n\nWhat: Descriptive assessment of datasetsHow: Clone, fork, and complete the steps in Lab 3.Why: To identify and apply the appropriate descriptive methods for a vector’s informational value and to assess both single variables and multiple variables with the appropriate statistical, tabular, and/ or graphical summaries."
  },
  {
    "objectID": "approaching-analysis.html#questions",
    "href": "approaching-analysis.html#questions",
    "title": "3  Approaching analysis",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\n\nConceptual questions\n\nWhat are the key differences between assessment and analysis?\nWhat are the potential measures of central tendency and dispersion for a variable? Does it depend on the informational value of the variable?\nConsider the following variables: \\(X\\) = number of children, \\(Y\\) = number of siblings, \\(Z\\) = number of siblings who are older than the participant. Which of these variables are categorical, ordinal, numeric? What are the measures of central tendency and dispersion for each variable?\nWhat type(s) of tables or plots are appropriate for summarizing a variable? What type(s) of tables or plots are appropriate for summarizing the relationship between two variables?\nIn the following variables and informational values, identify if the plots are appropriate for summarizing the relationship. …\nWhat are the key differences between exploratory, predictive, and inferential analysis?\nHow do the goals of the research influence the choice of analysis type?\nGiven the following research questions, identify which type of analysis is most appropriate and why. …\nHow are the results of inferential, predictive, and exploratory analysis evaluated?\nResearch compendia are an important part of reproducible research. What are the key components of a research compendium? What are the benefits of sharing a research compendium?\n\n\n\n\n\n\n\n\n\n\n Technical exercises\n\nCreate a contingency table for the following variables:\nCreate a plot for the following variables:\nReport these tables and plots with a short interpretation of what they show.\n…\n\n\n\n\n\n\n\n\n\nBenoit, Kenneth. 2020. Quanteda.corpora: A Collection of Corpora for Quanteda. http://github.com/quanteda/quanteda.corpora.\n\n\nGries, Stefan Th. 2013. Statistics for Linguistics with r. A Practical Introduction. 2nd revise.\n\n\nMuñoz, Carmen, ed. 2006. Age and the Rate of Foreign Language Learning. 1st ed. Vol. 19. Second Language Acquisition Series. Clevedon: Multilingual Matters.\n\n\nNisioi, Sergiu, Ella Rabinovich, Liviu P. Dinu, and Shuly Wintner. 2016. “A Corpus of Native, Non-Native and Translated Texts.” In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016). Portoroz̆, Slovenia: European Language Resources Association (ELRA).\n\n\nPaquot, Magali, and Stefan Th. Gries, eds. 2020. A Practical Handbook of Corpus Linguistics. Switzerland: Springer.\n\n\nSedgwick, Philip. 2015. “Units of Sampling, Observation, and Analysis.” BMJ (Online) 351 (October): h5396. https://doi.org/10.1136/bmj.h5396.\n\n\nUniversity of Colorado Boulder. 2008. “Switchboard Dialog Act Corpus. Web Download.” Linguistic Data Consortium."
  },
  {
    "objectID": "approaching-analysis.html#footnotes",
    "href": "approaching-analysis.html#footnotes",
    "title": "3  Approaching analysis",
    "section": "",
    "text": "Depending on the application and the amount of available data, a third development set is sometimes created as a pseudo test set to facilitate the testing of multiple approaches on data outside the training set before the final evaluation on the test set is performed.↩︎"
  },
  {
    "objectID": "framing-research.html#sec-fr-frame",
    "href": "framing-research.html#sec-fr-frame",
    "title": "4  Framing research",
    "section": "\n4.1 Frame",
    "text": "4.1 Frame\nTogether a research area, problem, aim and question and the research blueprint that forms the conceptual and practical scaffolding of the project ensure from the outset that the project is solidly grounded in the main characteristics of good research. These characteristics, summarized by Cross (2006), are found in Table 4.1.\n\n\n\n\nTable 4.1: Characteristics of research (Cross, 2006).\n\n\n\n\n\nCharacteristic\nDescription\n\n\n\nPurposive\nBased on identification of an issue or problem worthy and capable of investigation\n\n\nInquisitive\nSeeking to acquire new knowledge\n\n\nInformed\nConducted from an awareness of previous, related research\n\n\nMethodical\nPlanned and carried out in a disciplined manner\n\n\nCommunicable\nGenerating and reporting results which are feasible and accessible by others\n\n\n\n\n\n\nWith these characteristics in mind, let’s get started with the first component to address –connecting with the literature."
  },
  {
    "objectID": "framing-research.html#sec-fr-connect",
    "href": "framing-research.html#sec-fr-connect",
    "title": "4  Framing research",
    "section": "\n4.2 Connect",
    "text": "4.2 Connect\n\n4.2.1 Research area\nThe first decision to make in the research process is to identify a research area. A research area is a general area of interest where a researcher wants to derive insight and make a contribution to understanding. For those with an established research trajectory in language, the area of research to address through text analysis will likely be an extension of their prior work. For others, which include new researchers or researcher’s that want to explore new areas of language research or approach an area through a language-based lens, the choice of area may be less obvious. In either case, the choice of a research area should be guided by a desire to contribute something relevant to a theoretical, social, and/ or practical matter of personal interest. Personal relevance goes a long way to developing and carrying out purposive and inquisitive research.\nSo how do we get started? The first step is to reflect on your own areas of interest and knowledge, be it academic, professional, or personal. Language is at the heart of the human experience and therefore found in some fashion anywhere one seeks to find it. But it is a big world and more often than not the general question about what area to explore language use is sometimes the most difficult. To get the ball rolling, it is helpful to peruse disciplinary encyclopedias or handbooks of linguistics and language-related an academic fields (e.g. Encyclopedia of Language and Linguistics (Brown 2005), A Practical Guide to Electronic Resources in the Humanities (Dubnjakovic and Tomlin 2010), Routledge encyclopedia of translation technology (Chan 2014))\nA more personal, less academic, approach is to consult online forums, blogs, etc. that one already frequents or can be accessed via an online search. For example, Reddit has a wide variety of active subreddits (r/LanguageTechnology, r/Linguistics, r/corpuslinguistics, r/DigitalHumanities, etc.). Twitter and Facebook also have interesting posts on linguistics and language-related fields worth following. Through one of these social media site you may find particular people that maintain a blog worth browsing. For example, I follow Julia Silge, Rachel Tatman, and Ted Underwood, inter alia. Perusing these resources can help spark ideas and highlight the kinds of questions that interest you.\nRegardless of whether your inquiry stems from academic, professional, or personal interest, try to connect these findings to academic areas of research. Academic research is highly structured and well-documented and making associations with this network will aid in subsequent steps in developing a research project.\n\n4.2.2 Research problem\nOnce you’ve made a rough-cut decision about the area of research, it is now time to take a deeper dive into the subject area and jump into the literature. This is where the rich structure of disciplinary research will provide aid to traverse the vast world of academic knowledge and identify a research problem. A research problem highlights a particular topic of debate or uncertainty in existing knowledge which is worthy of study.\nSurveying the relevant literature is key to ensuring that your research is informed, that is, connected to previous work. Identifying relevant research to consult can be a bit of a ‘chicken or the egg’ problem –some knowledge of the area is necessary to find relevant topics, some knowledge of the topics is necessary to narrow the area of research. Many times the only way forward is to jump into conducting searches. These can be world-accessible resources (e.g. Google Scholar) or limited-access resources that are provided through an academic institution (e.g. Linguistics and Language Behavior Abstracts), ERIC, PsycINFO, etc.). Some organizations and academic institutions provide research guides to help researcher’s access the primary literature.\nAnother avenue to explore are journals dedicated to areas in which linguistics and language-related research is published. In Table 4.2, Table 4.3, and Table 4.4, I’ve listed a number of highly visible journals in linguistics, digital humanities, and computational linguistics.\n\n\n\n\nTable 4.2: A list of some linguistics journals.\n\n\n\n\n\nResource\nDescription\n\n\n\nCorpora\nAn international, peer-reviewed journal of corpus linguistics focusing on the many and varied uses of corpora both in linguistics and beyond.\n\n\nCorpus Linguistics and Linguistic Theory\nCorpus Linguistics and Linguistic Theory (CLLT) is a peer-reviewed journal publishing high-quality original corpus-based research focusing on theoretically relevant issues in all core areas of linguistic research, or other recognized topic areas.\n\n\nInternational Journal of Corpus Linguistics\nThe International Journal of Corpus Linguistics (IJCL) publishes original research covering methodological, applied and theoretical work in any area of corpus linguistics.\n\n\nInternational Journal of Language Studies\nIt is a refereed international journal publishing articles and reports dealing with theoretical as well as practical issues focusing on language, communication, society and culture.\n\n\nJournal of Child Language\nA key publication in the field, Journal of Child Language publishes articles on all aspects of the scientific study of language behaviour in children, the principles which underlie it, and the theories which may account for it.\n\n\nJournal of Linguistic Geography\nThe Journal of Linguistic Geography focuses on dialect geography and the spatial distribution of language relative to questions of variation and change.\n\n\nJournal of Quantitative Linguistics\nPublishes research on the quantitative characteristics of language and text in mathematical form, introducing methods of advanced scientific disciplines.\n\n\n\n\n\n\n\n\n\n\nTable 4.3: A list of some humanities journals.\n\n\n\n\n\nResource\nDescription\n\n\n\nDigital Humanities Quarterly\nDigital Humanities Quarterly (DHQ), an open-access, peer-reviewed, digital journal covering all aspects of digital media in the humanities.\n\n\nDigital Scholarship in the Humanities\nDSH or Digital Scholarship in the Humanities is an international, peer reviewed journal which publishes original contributions on all aspects of digital scholarship in the Humanities including, but not limited to, the field of what is currently called the Digital Humanities.\n\n\nJournal of Cultural Analytics\nCultural Analytics is an open-access journal dedicated to the computational study of culture. Its aim is to promote high quality scholarship that applies computational and quantitative methods to the study of cultural objects (sound, image, text), cultural processes (reading, listening, searching, sorting, hierarchizing) and cultural agents (artists, editors, producers, composers).\n\n\n\n\n\n\n\n\n\n\nTable 4.4: A list of some computational linguistics journals.\n\n\n\n\n\nResource\nDescription\n\n\n\nComputational Linguistics\nComputational Linguistics is the longest-running publication devoted exclusively to the computational and mathematical properties of language and the design and analysis of natural language processing systems.\n\n\nLREC Conferences\nThe International Conference on Language Resources and Evaluation is organised by ELRA biennially with the support of institutions and organisations involved in HLT.\n\n\nTransactions of the Association for Computational Linguistics\nTransactions of the Association for Computational Linguistics (TACL) is an ACL-sponsored journal published by MIT Press that publishes papers in all areas of computational linguistics and natural language processing.\n\n\n\n\n\n\nTo explore research related to text analysis it is helpful to start with the (sub)discipline name(s) you identified in when selecting your research area, more specific terms that occur to you or key terms from the literature, and terms such as ‘corpus study’ or ‘corpus-based’. The results from first searches may not turn out to be sources that end up figuring explicitly in your research, but it is important to skim these results and the publications themselves to mine information that can be useful to formulate better and more targeted searches. Relevant information for honing your searches can be found throughout an academic publication (article or book). However, pay particular attention to the abstract, in articles, and the table of contents, in books, and the cited references. Abstracts and tables of contents often include discipline-specific jargon that is commonly used in the field. In some articles there is even a short list of key terms listed below the abstract which can be extremely useful to seed better and more precise search results. The references section will contain relevant and influential research. Scan these references for publications which appear to narrowing in on topic of interest and treat it like a search in its own right.\nOnce your searches begin to show promising results it is time to keep track and organize these references. Whether you plan to collect thousands of references over a lifetime of academic research or your aim is centered around one project, software such as Zotero1, Mendeley, or BibDesk provide powerful, flexible, and easy-to-use tools to collect, organize, annotate, search, and export references. Citation management software is indispensable for modern research –and often free!\nAs your list of relevant references grows, you will want to start the investigation process in earnest. Begin skimming (not reading) the contents of each of these publications, starting with the most relevant first2. Annotate these publications using highlighting features of the citation management software to identify: (1) the stated goal(s) of the research, (2) the data source(s) used, (3) the information drawn from the data source(s), (4) the analysis approach employed, and (5) the main finding(s) of the research as they pertain to the stated goal(s). Next, in your own words, summarize these five key areas in prose adding your summary to the notes feature of the citation management software. This process will allow you to efficiently gather and document references with the relevant information to guide the identification of a research problem, to guide the formation of your problem statement, and ultimately, to support the literature review that will figure in your project write-up.\nFrom your preliminary annotated summaries you will undoubtedly start to recognize overlapping and contrasting aspects in the research literature. These aspects may be topical, theoretical, methodological, or appear along other lines. Note these aspects and continue to conduct more refine searches, annotate new references, and monitor for any emerging patterns of uncertainty or debate (gaps) which align with your research interest(s). When a promising pattern takes shape, it is time to engage with a more detailed reading of those references which appear most relevant highlighting the potential gap(s) in the literature.\nAt this point you can focus energy on more nuanced aspects of a particular gap in the literature with the goal to formulate a problem statement. A problem statement directly acknowledges a gap in the literature and puts a finer point on the nature and relevance of this gap for understanding. This statement reflects your first deliberate attempt to establish a line of inquiry. It will be a targeted, but still somewhat general, statement framing the gap in the literature that will guide subsequent research design decisions."
  },
  {
    "objectID": "framing-research.html#sec-fr-define",
    "href": "framing-research.html#sec-fr-define",
    "title": "4  Framing research",
    "section": "\n4.3 Define",
    "text": "4.3 Define\n\n4.3.1 Research aim\nWith a problem statement in hand, it is now time to consider the goal(s) of the research. A research aim frames the type of inquiry to be conducted. Will the research aim to explore, examine, or explain? In other words, will the research seek to uncover novel relationships, assess the potential strength of a particular relationship, or test a particular relationship? As you can appreciate, the research aim is directly related to the analysis methods we touched upon in Chapter 3.\nTo gauge how to frame your research aim, reflect on the literature that led you to your problem statement and the nature of the problem statement itself. If the gap at the center of the problem statement is a lack of knowledge, your research aim may be exploratory. If the gap concerns a conjecture about a relationship, then your research may take a predictive approach. When the gap points to the validation of a relationship, then your research will likely be inferential in nature. Before selecting your research aim it is also helpful to consult the research aims of the primary literature that led you to your research statement.\nTypically, a problem statement addressing a subtle, specific issue tends to adopt research objectives similar to prior studies. In contrast, a statement focusing on a broader, more distinct issue is likely to have unique research goals. Yet, this is more of a guideline than a strict rule.\nIt’s crucial to understand both the existing literature and the nature of various types of analyses. Being clear about your research goals is important to ensure that your study is well-placed to produce results that add value to the current understanding in an informed manner.\n\n4.3.2 Research question\nThe next step in research design is to craft the research question. A research question is clearly defined statement which identifies an aspect of uncertainty and the particular relationships that this uncertainty concerns. The research question extends and narrows the line of inquiry established in the research statement and research aim. To craft a research question, we can use the research statment for the content and the research aim for the form.\nForm\nThe form of a research question will vary based on the research aim, which as I mentioned, is inimately connected to the analysis approach. For inferential-based research, the research question will actually be a statement, not a question. This statement makes a testable claim about the nature of a particular relationship –i.e. asserts a hypothesis.\nFor illustration, let’s return to the hypothesis (\\(H_1\\)) we previously sketched out in Chapter 3, leaving aside the implicit null hypothesis, seen in Example 4.1.\n\nExample 4.1 Women use more questions than men in spontaneous conversations.\n\nFor predictive- and exploratory-based research, the research question is in fact a question. A reframing of the example hypothesis for a predictive-based research question might take the form seen in Example 4.2.\n\nExample 4.2 Can the number of questions used in spontaneous conversations predict if a speaker is male or female?\n\nAnd a similar exploratory-based research question might take the form seen in Example 4.3.\n\nExample 4.3 Do men and women differ in terms of the number of questions they use in spontaneous conversations?\n\nThe central research interest behind these hypothetical research questions is, admittedly, quite basic. But from these simplified examples, we are able to appreciate the similarities and differences between the forms of research statements that correspond to distinct research aims.\nContent\nIn terms of content, the research question will make reference to two key components. First, is the unit of analysis. The unit of analysis is the entity which the research aims to investigate. For our three example research aims, the unit of analysis is the same, namely speakers. Note, however, that the current unit of analysis is somewhat vague in the example research questions. A more precise unit of analysis would include more information about the population from which the speakers are drawn (i.e. English speakers, American English speakers, American English speakers of the Southeast, etc.).\nThe second key component is the unit of observation. The unit of observation is the primary element on which the insight into the unit of analysis is derived and in this way constitutes the essential organizational unit of the dataset to be analyzed. In our examples, the unit of observation, again, is unchanged and is spontaneous conversations. Note that while the unit of observation is key to identify as it forms the organizational backbone of the research, it is very common for the research to derive variables from this unit to provide evidence to investigate the research question.\nIn examples 4.1, 4.2, and 4.3, we identified the number of conversations as part of the research question. Later in the research process it will be key to operationalize this variable. For example, will the number of conversations be the total number of conversations in the dataset or will it be the average number of conversations per speaker? These are important questions to consider as they will influence variable selection, statistical choices, and ultimately the interpretation of the results."
  },
  {
    "objectID": "framing-research.html#sec-fr-blueprint",
    "href": "framing-research.html#sec-fr-blueprint",
    "title": "4  Framing research",
    "section": "\n4.4 Blueprint",
    "text": "4.4 Blueprint\nEfforts to craft a research question are a very important aspect of developing purposive, inquisitive, and informed research (returning to Cross’s characteristics of research). Moving beyond the research question in the project means developing and laying out the research design in a way such that the research is Methodical and Communicable. In this textbook, the method to achieve these goals is through the development of a research blueprint. The blueprint includes two components: (1) laying out a conceptual plan and (2) deriving the organizational scaffolding that will support the implementation of the research.\nAs Ignatow and Mihalcea (2017) point out:\n\nResearch design is essentially concerned with the basic architecture of research projects, with designing projects as systems that allow theory, data, and research methods to interface in such a way as to maximize a project’s ability to achieve its goals […]. Research design involves a sequence of decisions that have to be taken in a project’s early stages, when one oversight or poor decision can lead to results that are ultimately trivial or untrustworthy. Thus, it is critically important to think carefully and systematically about research design before committing time and resources to acquiring texts or mastering software packages or programming languages for your text mining project.\n\nIn what follows, I will cover the main aspects of developing a research blueprint. I will start with the conceptual plan and then move on to the organizational scaffolding.\n\n4.4.1 Plan\nImportance of establishing a feasible research design from the outset and documenting the key aspects required to conduct the research cannot be understated. On the one hand this process links a conceptual plan to a tangible implementation. In doing so, a researcher is better-positioned to conduct research with a clear view of what will be entailed. On the other hand, a promising research question may present unexpected challenges once a researcher sets about to implement the research. This is not uncommon to encounter issues that require modification or reevaluation of the viability of the project. However, a well-documented research plan will help a researcher to identify and address many of these challenges at the conceptual level before expending effort on the implementation.\nLet’s now consider the main aspects of developing a research plan: identifying data source(s), key variables, and analysis methods. Before we do, however, it is important to reiterate the importance of a research question or hypothesis before moving forward in research planning. The research question or hypothesis is the central component of the research plan. It guides every step which follows from data selection to interpretation of the analysis results. Furthermore, a well-founded research question is based on a solid literature review from which can provide helpful guidance at key choice points in the research process.\n\nFirst, identify a viable data source. Viability includes the accessibility of the data, availability of the data, and the content of the data. If a purported data source is not accessible and/ or is has stringent restrictions on its use, then it is not a viable data source. If a data source is accessible and available, but does not contain the building blocks needed to address the research question, then it is not a viable data source. In the case that research is inferential in nature, the sampling frame of the corpus is of primary importance as the goal is to generalize the findings to a target population. A corpus resource should align, to the extent feasible, with this target population. For predictive and exploratory research, the goal to generalize a claim is not central and for this reason the there is some freedom in terms of how representative a corpus sample is of a target population. Ideally, a researcher will find and be able to model a language population of target interest. Since the goal, however, is not to test a hypothesis, but rather to explore particular or evaluate potential relationships, either in an exploratory or predictive fashion, the research can often continue with the stipulation that the results are interpreted in the light of the characteristics of the available corpus sample.\n\n\nThe second step is to identify the key variables need to conduct the research are and then ensure that this information can be derived from the corpus data. The research question will reference the unit of analysis and the unit of observation, but it is important at this point to then pinpoint what the key variables will be. If the unit of observation is spontaneous conversations. The question as to what aspects of these conversations will be used in the analysis. In the research questions presented in this chapter, we will want to envision what needs to be done to derive a variable which measures the number of questions in each of the conversations. In other research, their may be features that need to be extracted, recoded, and/ or generated to address the research question. Other variables of importance may be non-linguistic in nature. In cases where there the metadata is incomplete for the goals of the research, it is sometimes possible to merge metadata from other sources.\n\nThe third step is to identify a method of analysis to interrogate the dataset. The selection of the analysis approach that was part of the research aim (i.e. explore, predict, or infer) and then the research question goes a long way to narrowing the methods that a researcher must consider. But there are a number of factors which will make some methods more appropriate than others.\nExploratory research is the least restricted of the three types of analysis approaches. Although it may be the case that a research will not be able to specify from the outset of a project what the exact analysis methods will be, an attempt to consider what types of analysis methods will be most promising to provide results to address the research question goes a long way to steering a project in the right direction and grounding the research. As with the other analysis approaches, it is important to be aware of what the analysis methods available and what type of information they produce in light of the research question.\nFor predictive-based research, the informational value of the target variable is key to deciding whether the prediction will be a classification task or a regression task. This has downstream effects when it comes time to evaluate and interpret the results. Although the feature engineering process in predictive analyses means that the features do not need to be specified from the outset and can be tweaked and changed as needed during an analysis, it is a good idea to start with a basic sense of what features most likely will be helpful in developing a robust predictive model. Furthermore, while the number and informational values of the features (predictor variables) are not as important to selecting a prediction method (algorithm) as they are in inferential analysis methods, it is important to recognize that particular algorithms have strengths and shortcomings when working large numbers and/ or types of features (Lantz 2013).\nIn inferential research, the number and information values of the variables to be analyzed will be of key importance (Gries 2013). The informational value of the dependent variable will again narrow the search for the appropriate method. The number of predictor variables also plays an important role. For example, a study with a categorical dependent variable with a single categorical predictor variable will lead the researcher to the Chi-squared test. A study with a numeric dependent variable with multiple predictor variables will lead to linear regression. Another aspect of note for inference studies is the consideration of the distribution of numeric variables –a normal distribution will use a parametric test where a non-normal distribution will use a non-parametric test. These details need not be nailed down at this point, but it is helpful to have them on your radar to ensure that when the time comes to analyze the data, the appropriate steps are taken to test for normality and then apply the correct test.\n\nThe last of the main components of the research plan concerns the interpretation and evaluation of the results. This step brings the research plan full circle connecting the research question to the methods employed. It is important to establish from the outset what the criteria will be to evaluate the results. This is in large part a function of the relationship between the research question and the analysis method. For example, in exploratory research, the results will be evaluated qualitatively in terms of the associative patterns that emerge. Predictive and inferential research leans more heavily on quantitative metrics in particular the accuracy of the prediction or the strength of the relationship between the dependent and predictor variable(s), respectively. However, these quantitative metrics require qualitative interpretation to determine whether the results are meaningful in light of the research question.\nTo summarize these planning steps, I’ve created a checklist in Table 4.5.\n\n\nTable 4.5: Research Plan Checklist\n\n\n\n\n\nSteps\nDescription\n\n\n\nResearch Question or Hypothesis\nFormulate a research question or hypothesis based on a thorough review of existing literature including references. This will guide every subsequent step from data selection to interpretation of results.\n\n\nData Source(s)\nIdentify viable data source(s) and vet the sample data in light of the research question. Consider to what extent the goal is to generalize findings to a target population, and ensure that the corpus aligns as much as feasible with this target.\n\n\nKey Variables\nDetermine the key variables needed for the research, define how they will be operationalized, and ensure they can be derived from the corpus data. Additionally, identify any features that need to be extracted, recoded or generated.\n\n\nAnalysis Method\nChoose an appropriate method of analysis to interrogate the dataset. This choice should be in line with your research aim (e.g., exploratory, predictive, inferential). Be aware of what each method can offer and how it addresses your research question.\n\n\nInterpretation & Evaluation\nEstablish criteria to interpret and evaluate the results. This will be a function of the relationship between the research question and the analysis method.\n\n\n\n\n\nIn addition to addressing the steps outlined in Table 4.5, it is also important to document the strengths and shortcomings of the research plan including the data source(s), the information to be extracted from the data, and the analysis methods. If there are potential shortcomings, which there most often are, sketch out contingency plans to address these shortcomings. This will help buttress your research and ensure that your time and effort is well-spent.\nTogether the information collected from this process will serve to guide the research and provide a solid foundation for the research write-up. Furthermore, you may consider pre-registering your research project to ensure that your plans are well-documented and to provide a timestamp for your research. Pre-registration can also be a helpful way to get feedback on your research plan from colleagues and experts in the field. Popular pre-registration platforms include Open Science Framework and Center for Open Science.\n\n\n\n4.4.2 Scaffold\nThe next step in creating a research blueprint is to consider how to physically implement your project. This includes how to organize files and directories in a fashion that both provides the researcher a logical and predictable structure to work with but also ensures that the research is Communicable. On the one hand, communicable research includes a strong write-up of the research, but, on the other hand, it is also important that the research is reproducible. Reproducibility strategies are a benefit to the researcher (in the moment and in the future) as it leads to better work habits and to better teamwork and it makes changes to the project easier. Reproducibility is also of benefit to the scientific community as shared reproducible research enhances replicability and encourages cumulative knowledge development (Gandrud 2015).\nThere are a set of guiding principles to accomplish these goals (Gentleman and Temple Lang 2007; Marwick, Boettiger, and Mullen 2018), seen in Example 4.4.\n\nExample 4.4 Reproducible Research Principles\n\nAll files should be plain text which means they contain no formatting information other than whitespace.\nThere should be a clear separation between the data, method, and output of research. This should be apparent from the directory structure.\nA separation between original, derived, and analysis data should be made. Original data should be treated as ‘read-only’. Any changes to the original data should be justified, generated by the code, and documented (see point 6).\nEach project file (script) should represent a particular, well-defined step in the research process.\nEach project script should be modular –that is, each file should correspond to a specific goal in the analysis procedure with input and output only corresponding to this step.\nAll project scripts should be tied together by a ‘main’ script that is used to coordinate the execution of all the project steps.\nEverything should be documented. This includes data collection, data preprocessing, analysis steps, script code comments, data description in data dictionaries, information about the computing environment and packages used to conduct the analysis, and detailed instructions on how to reproduce the research. \n\n\n\nThese seven principles can be physically implemented in numerous ways. In recent years, there has been a growing number of efforts to create R packages and templates to quickly generate the scaffolding and tools to facilitate reproducible research. Some notable R packages include workflowr (Blischak, Carbonetto, and Stephens 2023), ProjectTemplate (White 2023), and targets (Landau 2023), but there are many other resources for R included on the CRAN Task View for Reproducible Research. There are many advantages to working with pre-existing frameworks for the savvy R programmer including the ability to quickly generate a project scaffold, to efficiently manage changes to the project, and to buy in to a common framework that is supported by a community of developers.\n\nOn the other hand, these frameworks can be a bit daunting for the novice R programmer. At the most basic level, a project can implement the seven principles outlined above by creating a directory structure and a set of files manually.\n\nExample 4.5 Minimal Project Framework\nproject/\n├── data/\n│   ├── analysis/\n│   ├── derived/\n│   └── original/\n├── output/\n│   ├── figures/\n│   ├── reports/\n│   ├── results/\n│   └── tables/\n├── code/\n│   └── ...\n├── _main.R\n└── README\n\nIn Example 4.5, I provide a minimal framework that aligns with the reproducible research principles. Let me now make the connections between the principles and this project structure.\nThe project/ directory is composed of three main sections: data/, output/, and code/ corresponding to the input, output, and code, respectively.\nThe data/ section is divided into three subsections:\n\n\nanalysis/ for storing data used to perform analysis\n\nderived/ for housing data produced in curation and transformation steps\n\noriginal/ for keeping the original ‘read-only’ data\n\nThe output/ section contains four subsections:\n\n\nfigures/ for visualizations produced as part of the project\n\nreports/ for the resulting reports (e.g. article, presentation, blog post, etc.)\n\nresults/ for statistical results from the analysis\n\ntables/ for summary tables\n\nThe code/ directory houses the code, with the _main.R file at the root of the project orchestrating the execution of the scripts that conduct the project steps (1-acquire-data.qmd, 2-curate-dataset.qmd, 3-transform-datasets.qmd, and 4-analyze-datasets.qmd).\nLastly, the README file provides a description of the project and instructions on how to reproduce the research. \nThe project structure in Example 4.5 meets the minimal structural requirements for reproducible research but can be augmented in more sophisticated ways to support more functionality, as we will see in Chapter 12. One enhancement that I highly recommend is the use of literate programming, in the form of Quarto documents, to serve as the main project scripts. This facilitates the combination of executable code and prose documentation for each project step in a single, modular file."
  },
  {
    "objectID": "framing-research.html#summary",
    "href": "framing-research.html#summary",
    "title": "4  Framing research",
    "section": "Summary",
    "text": "Summary\nThe aim of this chapter is to provide the key conceptual and practical points to guide the development of a viable research project. Good research is purposive, inquisitive, informed, methodological, and communicable. It is not, however, always a linear process. Exploring your area(s) of interest and connecting with existing work will help couch and refine your research. But practical considerations, such as the existence of viable data, technical skills, and/ or time constrains, sometimes pose challenges and require a researcher to rethink and/ or redirect the research in sometimes small and other times more significant ways. The process of formulating a research question and developing a viable research plan is key to supporting viable, successful, and insightful research. To ensure that the effort to derive insight from data is of most value to the researcher and the research community, the research should strive to be methodological and communicable adopting best practices for reproducible research.\n\n\n\n\nFigure 4.1: Framing research: visual summary\n\n\n\nThis chapter concludes the Foundations part of this textbook. At this stage our overview of fundamental characteristics of research are in place to move a project towards implementation, as seen in Figure 4.1. From this point forward we will integrate your conceptual knowledge and emerging R programming skills as we cover common scenarios encountered when conducting reproducible research with real-world data.\nThe next part, Preparation, aims to cover R coding strategies to acquire, curate, and transform data in preparation for analysis. These are the first steps in putting a research blueprint into action and by no coincidence the first components in the Data to Insight Hierarchy. Without further ado, let’s get started!"
  },
  {
    "objectID": "framing-research.html#activities",
    "href": "framing-research.html#activities",
    "title": "4  Framing research",
    "section": "Activities",
    "text": "Activities\n\n\n Add description of outcomes …\n\n\n\n\n\n\n\n Recipe\n\n\nWhat: Project managementHow: Read Recipe 4 and participate in the Hypothes.is online social annotation.Why:  To learn how to use … reproducible research projects.\n\n\n\n\n\n\n\n\n\n Lab\n\n\nWhat: Project managementHow: Clone, fork, and complete the steps in Lab 4.Why:  To … a reproducible research project."
  },
  {
    "objectID": "framing-research.html#questions",
    "href": "framing-research.html#questions",
    "title": "4  Framing research",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\n\nConceptual questions\n\nWhat are the key characteristics of good research as described in this chapter?\nWhat are some strategies researchers can use to identify potential research areas and problems to investigate?\nFor each strategy, describe how it contributes to research that is purposive, inquisitive, and informed.\nWhy is framing a clear, focused research question or hypothesis important? Briefly explain how the research question guides the overall research process.\nWhat is the difference between the unit of analysis and the unit of observation? How do these concepts relate to the research question?\nWhat does it mean to operationalize a variable? Why is this important?\nThe process of developing a research blueprint involves both conceptual planning and practical implementation steps. Explain how going through this process not only aids the individual researcher, but also the research community.\nDescribe the main aspects of developing a research plan.\nExplain why it is important for research to be methodological and reproducible. What are some challenges researchers may face in achieving this?\n\n\n\n\n\n\n\n\n\n\n Technical exercises\n\nMatching research questions with data sources\nMatching research questions with research plans\nPreregistering a research project (?)\nPropose a quantitative research topic (or question if possible). Support your topic with supporting literature. (?)\n…\n\n\n\n\n\n\n\n\n\nBlischak, John, Peter Carbonetto, and Matthew Stephens. 2023. Workflowr: A Framework for Reproducible and Collaborative Data Science. https://github.com/workflowr/workflowr.\n\n\nBrown, Keith. 2005. Encyclopedia of Language and Linguistics. Vol. 1. Elsevier.\n\n\nChan, Sin-wai. 2014. Routledge Encyclopedia of Translation Technology. Routledge.\n\n\nCross, Nigel. 2006. “Design as a Discipline.” Designerly Ways of Knowing, 95–103.\n\n\nDubnjakovic, Ana, and Patrick Tomlin. 2010. A Practical Guide to Electronic Resources in the Humanities. Elsevier.\n\n\nGandrud, Christopher. 2015. Reproducible Research with r and r Studio. Second edition. CRC Press.\n\n\nGentleman, Robert, and Duncan Temple Lang. 2007. “Statistical Analyses and Reproducible Research.” Journal of Computational and Graphical Statistics 16 (1): 1–23.\n\n\nGries, Stefan Th. 2013. Statistics for Linguistics with r. A Practical Introduction. 2nd revise.\n\n\nIgnatow, Gabe, and Rada Mihalcea. 2017. An Introduction to Text Mining: Research Design, Data Collection, and Analysis. Sage Publications.\n\n\nLandau, William Michael. 2023. Targets: Dynamic Function-Oriented Make-Like Declarative Pipelines. https://docs.ropensci.org/targets/.\n\n\nLantz, Brett. 2013. Machine Learning with r. Birmingham: Packt Publishing.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging Data Analytical Work Reproducibly Using r (and Friends).” The American Statistician 72 (1): 80–88.\n\n\nWhite, John Myles. 2023. ProjectTemplate: Automates the Creation of New Statistical Analysis Projects. http://projecttemplate.net."
  },
  {
    "objectID": "framing-research.html#footnotes",
    "href": "framing-research.html#footnotes",
    "title": "4  Framing research",
    "section": "",
    "text": "Zotero Guide↩︎\nOr what appears to be most relevant. This may change as you start to take a closer look.↩︎"
  },
  {
    "objectID": "preparation.html",
    "href": "preparation.html",
    "title": "Preparation",
    "section": "",
    "text": "At this point we begin our journey to implement the research blueprint. As such, the content will be more focused on the practical steps to bring a plan to fruition integrating our conceptual understanding of the research process from the previous chapters with our emerging programming skills developed in lessons, recipes, and labs.\nThis part, Preparation, will address data acquistion, curation, and transformation steps. The goal of data preparation is to create a dataset which is ready for analysis. In each of these three upcoming chapters, I will outline some of the main characteristics to consider in each of these research steps and provide authentic examples of working with R to implement these steps. In Chapter 5 this includes the most common strategies for acquiring data: downloads and APIs. In Chapter 6 we turn to organize data into rectangular, or ‘tidy’, format. Depending on the data or dataset acquired for the research project, the steps necessary to shape our data into a base dataset will vary, as we will see. In Chapter 7 we will work to manipulate curated datasets to create datasets which are aligned with the research aim and research question. This often includes normalizing values, recoding variables, and generating new variables as well as and sourcing and merging information from other datasets with the dataset to be submitted for analysis.\nEach of these chapters will cover the necessary documentation to trace our steps and provide a record of the data preparation process. Documentation serves to inform the analysis and interpretation of the results and also forms the cornerstone of reproducible research."
  },
  {
    "objectID": "acquire-data.html#downloads",
    "href": "acquire-data.html#downloads",
    "title": "5  Acquire data",
    "section": "\n5.1 Downloads",
    "text": "5.1 Downloads\nThe most common and straightforward method for acquiring corpus data is through direct downloads. In a nutshell, this method involves navigating to a website, locating the data, and downloading it to your computing environment. In some cases access to the data requires manual intervention and in others the process can be implemented programmatically. The data may be contained in a single file or multiple files. The files may be compressed or uncompressed. The data may be hierarchically organized or not. Each resource will have its own unique characteristics that will influence the process of acquiring the data. In this section we will work through a few examples to demonstrate the general process of acquiring data through downloads.\n\n5.1.1 Manual\nIn contrast to the other data acquisition methods we will cover in this chapter, manual downloads require human intervention. This means that manual downloads are non-reproducible in a strict sense and require that we keep track of and document our procedure. It is a very common for research projects to acquire data through manual downloads as many data resources require some legwork before they are accessible for downloading. These can be resources that require institutional or private licensing and fees (Language Data Consortium, International Corpus of English, BYU Corpora, etc.), require authorization/ registration (The Language Archive, COW Corpora, etc.), and/ or are only accessible via resource search interfaces (Corpus of Spanish in Southern Arizona, Corpus Escrito del Español como L2 (CEDEL2), etc.).\nLet’s take a look at how to acquire data from a resource that requires manual intervention. The resource we will use is the Corpus Escrito del Español como L2 (CEDEL2) (Lozano 2009), a corpus of Spanish learner writing. It includes L2 writing from students with a variety of L1 backgrounds. For comparative puposes it also includes native writing for Spanish, English, and several other languages.\nThe CEDEL2 corpus is a freely available resource, but to access the data you must first use a search interface to select the relevant characteristics of the data of interest. Following the search/ download link you can find a search interface that allows the user to select the subcorpus and filter the results by a set of attributes, seen in Figure 5.1.\n\n\n\n\nFigure 5.1: Search and download interface for the CEDEL2 Corpus\n\n\n\nFor this example let’s assume that we want to acquire data to use in a study comparing the use of the Spanish preterite and imperfect past tense aspect in written texts by English L1 learners of Spanish to native Spanish speakers. To acquire data for such a project, we will first select the subcorpus “Learners of L2 Spanish”. We will set the results to provide full texts and filter the results to “L1 English - L2 Spanish”. Additionally, we will set the medium to “Written”. This will provide us with a set of texts for the L2 learners that we can use for our study. The search parameters and results are shown in Figure 5.2.\n\n\n\n\nFigure 5.2: Search results for the CEDEL2 Corpus\n\n\n\nThe ‘Download’ link now appears for this search criteria. Following this link will provide the user a form to fill out. This particular resource allows for access to different formats to download (Texts only, Texts with metadata, CSV (Excel), CSV (Others)). I will select the ‘CSV (Others)’ option so that the data is structured for easier processing downstream when we work to curate the data in our next processing step. Then I will choose to save the CSV in the data/original/ directory of my project and create a sub-directory named cedel2/, as seen in Example 5.1.\n\nExample 5.1 Download CEDEL2 L2 Spanish Learners data\ndata/\n├── analysis/\n├── derived/\n└── original/\n    └── cedel2/\n       └── cedel2-l1-english-learners.csv\n\nNote that the file is named cedel2-l1-english-learners.csv to reflect the search criteria used to acquire the data. In combination with other data documentation, this will help us to maintain transparency.\nNow, after downloading the L2 learner and the native speaker data into the appropriate directory, we move on to the next processing step, right? Not so fast! Imagine we are working on a project with a collaborator. How will they know where the data came from? What if we need to come back to this data in the future? How will we know what characteristics we used to filter the data? The directory and filenames may not be enough. To address these questions we need to document the origin of the data, and in the case of data acquired through manual downloads, we need to document the procedures we took to acquire the data to the best of our ability.\nAs discussed in Section 2.3.1, all acquired data should be accompanied by a data origin file. The majority of this information can typically be identified on the resource’s website and/ or the resource’s documentation. In the case of the CEDEL2 corpus, the corpus homepage provides most of the information we need.\nStructurally, data documentation files should be stored close to the data they describe. So for our data origin file this means adding it to the data/original/ directory. Naming the file in a transparent way is also important. I’ve named the file cedel2_do.csv to reflect the name of the corpus, the meaning of the file as data origin with *_do, and the file extension .csv* to reflect the file format. CSV files reflect tabular content. It is not required that data origin files are tabular, but it makes it easier to read and display them in literate programming documents.\n\n\n\n\n\n\n Tip\nThere are many ways to create and edit CSV files. You can use a spreadsheet program like MS Excel or Google Sheets, a text editor like Notepad or TextEdit, or a code editor like RStudio or VS Code. The qtalrkit package provides a convenient function, create_data_origin() to create a CSV file with the data origin boilerplate structure. This CSV file then can be edited to add the relevant information in any of the above mentioned programs.\nUsing a spreadsheet program is the easiest method for editing tabular data. The key is to save the file as a CSV file, and not as an Excel file, to maintain our adherence to the principle of using open formats for reproducible research.\n\n\n\nIn Table 5.1, I’ve created a data origin file for the CEDEL2 corpus.\n\n\n\n\nTable 5.1: Data origin file for the CEDEL2 corpus\n\nattribute\ndescription\n\n\n\nResource name\nCEDEL2: Corpus Escrito del Español como L2.\n\n\nData source\nhttp://cedel2.learnercorpora.com/, https://doi.org/10.1177/02676583211050522\n\n\nData sampling frame\nCorpus that contains samples of the language produced from learners of Spanish as a second language. For comparative purposes, it also contains a native control subcorpus of the language produced by native speakers of Spanish from different varieties (peninsular Spanish and all varieties of Latin American Spanish), so it can be used as a native corpus in its own right.\n\n\nData collection date(s)\n2006-2020.\n\n\nData format\nCSV file. Each row corresponds to a writing sample. Each column is an attribute of the writing sample.\n\n\nData schema\nA CSV file for L2 learners and a CSV file for native speakers.\n\n\nLicense\nCC BY-NC-ND 3.0 ES\n\n\nAttribution\nLozano, C. (2022). CEDEL2: Design, compilation and web interface of an online corpus for L2 Spanish acquisition research. Second Language Research, 38(4), 965-983. https://doi.org/10.1177/02676583211050522.\n\n\n\n\n\n\n\n\nGiven this is a manual download we also need to document the procedure used to retrieve the data in prose. The script in the code/ directory that is typically used to acquire the data is not used to programmatically retrieve data in this case. However, to keep things predictable we will use this file to document the download procedure. I’ve created a Quarto file named 1_acquire_data.qmd in the code/ directory of my project.\nA glimpse at the directory structure of the project at this point is seen in Example 5.2.\n\nExample 5.2 Project structure for the CEDEL2 corpus data acquisition\nproject/\n├── code/\n│   ├── 1_acquire_data.qmd\n│   └── ...\n├── data/\n│   ├── analysis/\n│   ├── derived/\n│   └── original/\n│       ├── cedel2_do.csv\n│       └── cedel2/\n|           ├── cedel2-l1-english-learners.csv\n|           └── cedel2-native-spanish-speakers.csv\n├── output/\n│   ├── figures/\n│   ├── reports/\n│   ├── results/\n│   └── tables/\n├── README.md\n└── _main.R\n\nIn the 1_acquire_data.qmd file I’ve added example sections to display the data origin CSV file as a table and to document the data download procedures, as seen in File 5.1.\n\nFile 5.1: Acquire data file\n\n1-acquire-data.qmd\n\n---\ntitle: \"Acquire data\"\nformat: html\n---\n\n## Overview\nThe goal of this script is to acquire and document data for this project from the CEDEL2 corpus. The acquired data will be stored in the `data/original/cedel2/` directory.\n\n## Data origin\nTo document the origin of the data we created a file named `cedel2_do.csv` in the `data/original/` directory. This file contains the following information: \n\n```{r}\n#| label: tbl-cedel2-data-origin\n#| tbl-cap: \"Data origin file for the CEDEL2 corpus\"\n#| echo: false\n\n# Display data origin file\nreadr::read_csv(\"../data/original/cedel2_do.csv\") |&gt; knitr::kable()\n```\n## Download procedures  \nThe process to acquire data from the CEDEL2 corpus involved the following steps:\n\nL2 Spanish Learners:  \n1. Navigate to the [CEDEL2 Corpus](http://cedel2.learnercorpora.com/search) search interface\n2. Select the subcorpus \"Learners of L2 Spanish\"\n3. Set the results to provide full texts\n4. Filter the results to \"L1 English - L2 Spanish\"\n5. Set the medium to \"Written\"\n6. Download the data in CSV format\n7. Save the CSV file to the `data/original/cedel2/` directory as `cedel2-l1-english-learners.csv`\n\nSpanish natives: \n\nThe output from 1_acquire_data.qmd will contain a table displaying the data origin file and a prose section documenting the data acquisition process. This will provide a transparent record of the data acquisition process for future reference.\nManually downloading other resources will inevitably include unique processes for obtaining the data, but in the end the data should be archived in the research structure in the data/original/ directory and documented in the appropriate places. The acquired data is treated as ‘read-only’, meaning it is not modified in any way. This gives us a transparent starting point for subsequent steps in the data preparation process.\n\n5.1.2 Programmatic\nThere are many resources that provide corpus data that is directly accessible for which programmatic downloads can be applied. A programmatic download is a download in which the process can be automated through code. Thus, this is a reproducible process. The data can be acquired by anyone with access to the necessary code.\nIn this case, and subsquent data acquisition procedures in this chapter, we use the 1_acquire_data.qmd Quarto file to its full potential intermingling prose, code, and code comments to execute and document the download procedure. In File 5.2, I’ve added example sections to display example boilerplate structure for a programmatic data acquisition and documentation.\n\n\nFile 5.2: Acquire data file\n\n1-acquire-data.qmd\n\n---\ntitle: \"Acquire data\"\nformat: html\n---\n\n## Overview\n\nThe goal of this script is to ...\n\n## Data origin\n\nTo document the origin of the data we created a file named ...\n\n## Download procedures\n\n```{r}\n#| label: setup\n\n# Load libraries\nlibrary(tidyverse)\n```\n```{r}\n# .. additional code here to acquire data ...\n```\n... and so on\n\nTo illustrate how this works to conduct a programmatic download, we will work with the Switchboard Dialog Act Corpus (SWDA) (University of Colorado Boulder 2008). The version that we will use is found on the Linguistic Data Consortium under the Switchboard-1 Release 2 Corpus. The corpus and related documentation are linked on the catalog page https://catalog.ldc.upenn.edu/docs/LDC97S62/.\nFrom the documentation we learn that the corpus contains transcripts for 1155 5-minute two-way telephone conversations among English speakers for all areas of the United States. The speakers were given a topic to discuss and the conversations were recorded. The corpus metadata and annotations for sociolinguistic and discourse features.\nThe SWDA was referred to in Section 3.2.3 to support our toy hypothesis that men and women differ in the frequency of the use of questions in spontaneous conversations. This corpus, as you can image, could support a wide range of interesting reseach questions. Let’s assume we are following research conducted by Tottie (2011) to explore the use of filled pauses such as “um” and “uh” and traditional sociolinguistic variables such as sex, age, and education in spontaneous speech by American English speakers.\nWith this goal in mind, let’s get started writing the code to download and organize the data in our project directory. First we need to identify the URL (Uniform Resource Locator) for the data that we want to download. More often than not this file will be some type of compressed archive file with an extension such as .zip (Zipped file), .tar (Tarball file), or tar.gz (Gzipped tarball file), which is the case for the SWDA corpus. Compressed files make downloading multiple files easy by grouping files and directories into one file.\n\n\n\n\n\n\n Consider this\nYou may be wondering what the difference betwen .zip, .tar, and .tar.gz files are. The .zip file format is the most common. It groups file and directories into one file (archives) and compresses them to reduce the size of the file in one step when the file is created.\nThe .tar file format is used archive files and folders, it does not perform compression. Gzipping peforms the compression to the .tar file resulting in a file with the .tar.gz extension. Notably the .gz compression is highly efficient for large files. Take the swda.tar.gz file for example. It has a compressed file size of 4.6 MB, but when uncompressed it is 16.9 MB. This is a 73% reduction in file size.\n\n\n\nIn R we can use the download.file() function from base R. The download.file() function minimally requires two arguments: url and destfile. These correspond to the file to download and the location where it is to be saved to disk. To break out the process a bit, I will assign the URL and destination file path to variables and then use the download.file() function to download the file.\n\nExample 5.3  \n\n# URL to SWDA corpus compressed file\nfile_url &lt;- \n  \"https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1_dialogact_annot.tar.gz\"\n\n# Relative path to project/data/original directory\nfile_path &lt;- \"../data/original/swda.tar.gz\"\n\n# Download SWDA corpus compressed file\ndownload.file(url = file_url, destfile = file_path)\n\n\n\n\n\n\n\n\n Warning\nNote that the file_path variable in Example 5.3 is a relative path to the data/original/ directory. This is because the 1_acquire_data.qmd file that we are using for this code is located in the code/ directory and the data/ directory is a sibling directory to the code/ directory.\nIt is also possible to use an absolute path to the data/original/ directory. I will have more to say about the advantages and disadvantages of relative and absolute paths in reproducible research in Chapter 12.\n\n\n\nAs we can see looking at the directory structure, in Example 5.4, the swda.tar.zip file has been added to the data/original/ directory.\n\nExample 5.4 Downloaded SWDA corpus compressed file\ndata/\n├── analysis/\n├── derived/\n└── original/\n    └── swda.tar.zip\n\nOnce an compressed file is downloaded, however, the file needs to be ‘decompressed’ to reveal the directory structure and files. To decompress this .tar.gz file we use the untar() function with the arguments tarfile pointing to the .tar.gz file and exdir specifying the directory where we want the files to be extracted to. Again, I will assign the arguments to variables. Then we can decompress the file using the untar() function.\n\nExample 5.5  \n\n# Relative path to the compressed file\ntar_file &lt;- \"../data/original/swda.tar.gz\"\n\n# Relative path to the directory to extract to\nextract_to_dir &lt;- \"../data/original/swda/\"\n\n# Decompress .zip file and extract to our target directory\nuntar(tar_file, extract_to_dir)\n\n\nThe directory structure of data/ in Example 5.6 now shows the swda.tar.gz file and the swda directory that contains the decompressed directories and files.\n\nExample 5.6  \ndata/\n├── analysis/\n├── derived/\n└── original/\n    ├── swda/\n    │   ├── README\n    │   ├── doc/\n    │   ├── sw00utt/\n    │   ├── sw01utt/\n    │   ├── sw02utt/\n    │   ├── sw03utt/\n    │   ├── sw04utt/\n    │   ├── sw05utt/\n    │   ├── sw06utt/\n    │   ├── sw07utt/\n    │   ├── sw08utt/\n    │   ├── sw09utt/\n    │   ├── sw10utt/\n    │   ├── sw11utt/\n    │   ├── sw12utt/\n    │   └── sw13utt/\n    └── swda.tar.gz\n\nAt this point we have acquired the data programmatically and with this code as part of our workflow anyone could run this code and reproduce the same results. The code as it is, however, is not ideally efficient. Firstly the swda.tar.gz file is not strictly needed after we decompress it and it occupies disk space if we keep it. And second, each time we run this code the file will be downloaded from the remote server. This leads to unnecessary data transfer and server traffic and will overwrite the data if it already exists in our project directory which could be problematic if the data changes on the remote server. Let’s tackle each of these issues in turn.\nTo avoid writing the swda.tar.gz file to disk (long-term) we can use the tempfile() function to open a temporary holding space for the file in the computing environment. This space can then be used to store the file, decompress it, and then the temporary file will automatically be deleted. We assign the temporary space to an R object we will name temp_file with the tempfile() function. This object can now be used as the value of the argument destfile in the download.file() function.\n\nExample 5.7  \n\n# URL to SWDA corpus compressed file\nfile_url &lt;- \n  \"https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1_dialogact_annot.tar.gz\"\n\n# Create a temporary file space for our .tar.gz file\ntemp_file &lt;- tempfile()\n\n# Download SWDA corpus compressed file\ndownload.file(file_url, temp_file)\n\n\n\n\n\n\n\n\n Tip\nIn Example 5.7, I’ve used the values stored in the objects file_url and temp_file in the download.file() function without specifying the argument names –only providing the names of the objects. R will assume that values of a function map to the ordering of the arguments. If your values do not map to ordering of the arguments you are required to specify the argument name and the value. To view the ordering of objects hit tab after entering the function name or consult the function documentation by prefixing the function name with ? and hitting enter.\n\n\n\nAt this point our downloaded file is stored temporarily on disk and can be accessed and decompressed to our target directory using temp_file as the value for the argument tarfile from the untar() function. I’ve assigned our target directory path to extract_to_dir and used it as the value for the argument exdir.\n\n\nExample 5.8  \n\n# Assign our target directory to `extract_to_dir`\nextract_to_dir &lt;- \"../data/original/swda/\"\n\n# Decompress .tar.gz file and extract to our target directory\nuntar(tarfile = temp_file, exdir = target_dir)\n\n\nOur directory structure in Example 5.8 is the same as in Example 5.6, minus the swda.tar.gz file.\nThe second issue I raised concerns the fact that running this code as part of our project will repeat the download each time. Since we would like to be good citizens and avoid unnecessary traffic on the web and avoid potential issues in overwriting data, it would be nice if our code checked to see if we already have the data on disk and if it exists, then skip the download, if not then download it.\nThe desired functionality we’ve described can be implemented using the if() function. The if() function is one of a class of functions known as control statements. Control statments allow us to control the flow of our code by evaluating logical statements and processing subsequent code based on the logical value it is passed as an argument.\nSo in this case we want to evaluate whether the data directory exists on disk. If it does then skip the download, if not, proceed with the download. In combination with else which provides the ‘if not’ part of the statement, we have the following logical flow in Example 5.9.\n\nExample 5.9  \nif (DIRECTORY_EXISTS) {\n  # Do nothing\n} else {\n  # Download data\n}\n\nWe can simplify this statement by using the ! operator which negates the logical value of the statement it precedes. So if the directory exists, !DIRECTORY_EXISTS will return FALSE and if the directory does not exist, !DIRECTORY_EXISTS will return TRUE. In other words, if the directory does not exist, download the data. This is shown in Example 5.10.\n\nExample 5.10  \nif (!DIRECTORY_EXISTS) {\n  # Download data\n}\n\nNow, to determine if a directory exists in our project directory we will turn to the fs package (Hester, Wickham, and Csárdi 2023). The fs package provides a set of functions for interacting with the file system, including dir_exists(). dir_exists() takes a path to a directory as an argument and returns the logical value, TRUE, if that directory exists, and FALSE if it does not.\nWe can use this function to evaluate whether the directory exists and then use the if() function to process the subsequent code based on the logical flow we set out in Example 5.10. Applied to our project, the code will look like Example 5.11.\n\nExample 5.11  \n\n# Load the `fs` package\nlibrary(fs)\n\n# URL to SWDA corpus compressed file\nfile_url &lt;- \n  \"https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1_dialogact_annot.tar.gz\"\n\n# Create a temporary file space for our .tar.gz file\ntemp_file &lt;- tempfile()\n\n# Assign our target directory to `extract_to_dir`\nextract_to_dir &lt;- \"../data/original/swda/\"\n\n# Check if our target directory exists\n# If it does not exist, download the file and extract it\nif (!dir_exists(extract_to_dir)) {\n  # Download SWDA corpus compressed file\n  download.file(file_url, temp_file)\n  \n  # Decompress .tar.gz file and extract to our target directory\n  untar(tarfile = temp_file, exdir = extract_to_dir)\n}\n\n\nThe code in Example 5.11 is added to the 1_acquire_data.qmd file we introduced in File 5.2. When this file is run, the SWDA corpus data will be downloaded and extracted to our project directory. If the data already exists, the download will be skipped, just as we wanted.\nBefore we move on, we need to make sure to create and add the appropriate information to the data origin file. To make this easier, the qtalrkit package includes a function, create_data_origin(), to create a data origin file template in CSV format. This function takes the path for the desired file. In the SWDA Corpus case, this might be something like: ../data/original/swda_do.csv. The function only needs to be run once and does not need to be part of the reproducible workflow.\nRunning the code in Example 5.12 at the console will create the file. Open it in your preferred text or spreadsheet editor to add the appropriate information.\n\nExample 5.12  \n\n# Load the `qtalrkit` package\nlibrary(qtalrkit)\n\n# Create a data origin file template\ncreate_data_origin(\"../data/original/swda_do.csv\")\n\n\nOur complete project structure for the SWDA corpus data acquisition is shown in Example 5.13.\n\nExample 5.13 Project structure for the SWDA corpus data acquisition\nproject/\n├── code/\n│   ├── 1_acquire_data.qmd\n│   └── ...\n├── data/\n│   ├── analysis/\n│   ├── derived/\n│   └── original/\n│       ├── swda_do.csv\n│       └── swda/\n│          ├── README\n│          ├── doc/\n│          ├── sw00utt/\n│          ├── sw01utt/\n│          ├── sw02utt/\n│          ├── sw03utt/\n│          ├── sw04utt/\n│          ├── sw05utt/\n│          ├── sw06utt/\n│          ├── sw07utt/\n│          ├── sw08utt/\n│          ├── sw09utt/\n│          ├── sw10utt/\n│          ├── sw11utt/\n│          ├── sw12utt/\n│          └── sw13utt/\n├── output/\n│   ├── figures/\n│   ├── reports/\n│   ├── results/\n│   └── tables/\n├── README.md\n└── _main.R\n\nGreat, we’ve successfully acquired and documented the SWDA Corpus data. We’ve leveraged R to automate the download and extraction of the data, depending on the existence of the data in our project directory. But you may be asking yourself, “Can’t I just navigate to the corpus page and download the data manually myself?” The simple answer is, “Yes, you can.” The more nuanced answer is, “Yes, but consider the trade-offs.”\nThe following scenarios highlight the some advantages to automating the process. If you are acquiring data from multiple files, it can become tedious to document the manual process for each file such that it is reproducible. It’s possible, but it’s error prone. Now, if you are collaborating with others, you will want to share this data with them. It is very common to find data that has limited restrictions for use in academic projects, but the most common limitation is redistribution. This means that you can use the data for your own research, but you cannot share it with others. If you plan on publishing your project to a repository, like GitHub, to share the data as part of your reproducible project, you would be violating the terms of use for the data. By including the programmatic download in your project, you can ensure that your collaborators can easily and effectively acquire the data themselves and that you are not violating the terms of use."
  },
  {
    "objectID": "acquire-data.html#sec-apis",
    "href": "acquire-data.html#sec-apis",
    "title": "5  Acquire data",
    "section": "\n5.2 APIs",
    "text": "5.2 APIs\nA convenient alternative method for acquiring data in R is through package interfaces to web services. These interfaces are built using R code to make connections with resources on the web through Application Programming Interfaces (APIs). Websites such as Project Gutenberg, Twitter, Facebook, and many others provide APIs to allow access to their data under certain conditions, some more limiting for data collection than others. Programmers (like you!) in the R community take up the task of wrapping calls to an API with R code to make accessing that data from R convenient, and of course reproducible.\n\n\n\n\n\n\n Dive deeper\nMany, many web services provide API access. These APIs span all kinds of data, from text to images to video to audio. Visit the Public APIs website to explore the diversity of APIs available.\nROpenSci maintains a curated list of R packages that provide access to data from web services. Visit the ROpenSci website to explore the packages available.\n\n\n\n\nIn addition to popular public APIs, there are also APIs that provide access to repositories and databases which are of particular interest to linguists. For example, Wordbank provides access to a large collection of child language corpora through the wordbankr package (Braginsky 2022), and Glottolog, World Atlas of Language Structures (WALS), and PHOIBLE provide access to large collections of language metadata that can be accessed through the lingtypology package (Moroz 2023).\n\nLet’s work with an R package that provides access to the TalkBank database. The TalkBank project  [CITATION] contains a large collection of spoken language corpora from various contexts: conversation, child language, multilinguals, etc. Resource information, web interfaces, and links to download data in various formats can be found by perusing individual resources linked from the main page. However, the TBDBr package (Kowalski and Cavanaugh 2022) provides convenient access to data using R once a data resource is identified.\n\nThe CABNC (Albert, de Ruiter, and de Ruiter 2015) contains the demographically sampled portion of the spoken portion of the British National Corpus (BNC) (Leech 1992).\nUseful for a study aiming to research spoken British English, either in isoloation or in comparison to American English (SWDA).\nFirst, we need to install and load the TBDBr package. The install.package() and library() functions work just great for this. An alternative is to use the pacman package (Rinker and Kurkiewicz 2019) to install and load the package in one step with the function p_load(), as shown in Example 5.14.\n\nExample 5.14  \n\n# Install and load the TBDBr package\npacman::p_load(TBDBr)\n\n\n\nThe TBDBr package provides a set of common get*() functions for acquiring data from the TalkBank corpus resources. These include:\n\ngetParticipants()\ngetTranscripts()\ngetTokens()\ngetTokenTypes()\ngetUtterances()\n\nFor each of these function the first argument is corpusName, which is the name of the corpus resource as it appears in the TalkBank database. The second argument is corpora, which takes a character vector describing the path to the data. For the CABNC, these arguments are \"ca\" and c(\"ca\", \"CABNC\") respectively. To determine these values, TBDBr provides the getLegalValues() interactive function which allows you to interactively select the repository name, corpus name, and transcript name (if necessary).\nAnother important aspect of these function is that they return data frame objects. Since we are accessing data that is in a structured database, this makes sense. However, we should always check the documentation for the object type that is returned by function to be aware of how to work with the data.\nLet’s start by retrieving the utterance data for the CABNC and preview the data frame it returns using glimpse().\n\nExample 5.15  \n\n# Set corpus_name and corpus_path\ncorpus_name &lt;- \"ca\"\ncorpus_path &lt;- c(\"ca\", \"CABNC\")\n\n# Get utterance data\nutterances &lt;- \n  getUtterances(\n    corpusName = corpus_name, \n    corpora = corpus_path)\n\n&gt; [1] \"Fetching data, please wait...\"\n&gt; [1] \"Success!\"\n\n# Preview the data\nglimpse(utterances)\n\n&gt; Rows: 235,901\n&gt; Columns: 10\n&gt; $ filename  &lt;list&gt; \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", \"KB0RE000\",…\n&gt; $ path      &lt;list&gt; \"ca/CABNC/KB0/KB0RE000\", \"ca/CABNC/KB0/KB0RE000\", \"ca/CABNC…\n&gt; $ utt_num   &lt;list&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n&gt; $ who       &lt;list&gt; \"PS002\", \"PS006\", \"PS002\", \"PS006\", \"PS002\", \"PS006\", \"PS00…\n&gt; $ role      &lt;list&gt; \"Unidentified\", \"Unidentified\", \"Unidentified\", \"Unidentifi…\n&gt; $ postcodes &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NU…\n&gt; $ gems      &lt;list&gt; &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NULL&gt;, &lt;NU…\n&gt; $ utterance &lt;list&gt; \"You enjoyed yourself in America\", \"Eh\", \"did you\", \"Oh I c…\n&gt; $ startTime &lt;list&gt; \"0.208\", \"2.656\", \"2.896\", \"3.328\", \"5.088\", \"6.208\", \"8.32…\n&gt; $ endTime   &lt;list&gt; \"2.672\", \"2.896\", \"3.328\", \"5.264\", \"6.016\", \"8.496\", \"9.31…\n\n\n\n\nInspecting the output from Example 5.15, we see that the data frame contains 235901 observations and 10 variables.\nThe summary provided by glimpse() also provides other useful information. First, we see the data type of each variable. Interestingly, the data type for each variable in the data frame is list. Being that a list is two-dimensional data type, like a data frame, we have list-type data in each value. This is known as a nested structure. We will see, and create, nested structures later, but for now it will suffice to say that we would like to ‘unnest’ these lists and reveal the list-contained vector types at the data frame level.\nTo do this we will pass the utterances data frame to the unnest() function from the tidyr package (Wickham, Vaughan, and Girlich 2023). unnest() takes a data frame and a vector of variable names to unnest, cols = c(). To unnest all variables, we will use the everything() function from dplyr (Wickham et al. 2023) to select all variables. We will use the result to overwrite the utterances object with the unnested data frame.\n\nExample 5.16  \n\n# Unnest the data frame\nutterances &lt;- \n  utterances |&gt; \n  unnest(cols = everything())\n\n# Preview the data\nglimpse(utterances)\n\n&gt; Rows: 235,901\n&gt; Columns: 10\n&gt; $ filename  &lt;chr&gt; \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", …\n&gt; $ path      &lt;chr&gt; \"ca/CABNC/KB0/KB0RE000\", \"ca/CABNC/KB0/KB0RE000\", \"ca/CABNC/…\n&gt; $ utt_num   &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n&gt; $ who       &lt;chr&gt; \"PS002\", \"PS006\", \"PS002\", \"PS006\", \"PS002\", \"PS006\", \"PS002…\n&gt; $ role      &lt;chr&gt; \"Unidentified\", \"Unidentified\", \"Unidentified\", \"Unidentifie…\n&gt; $ postcodes &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n&gt; $ gems      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n&gt; $ utterance &lt;chr&gt; \"You enjoyed yourself in America\", \"Eh\", \"did you\", \"Oh I co…\n&gt; $ startTime &lt;chr&gt; \"0.208\", \"2.656\", \"2.896\", \"3.328\", \"5.088\", \"6.208\", \"8.32\"…\n&gt; $ endTime   &lt;chr&gt; \"2.672\", \"2.896\", \"3.328\", \"5.264\", \"6.016\", \"8.496\", \"9.312…\n\n\n\nThe output from Example 5.16 shows that the variables are now one-dimensional vector types.\nReturning to the information about our data frame from glimpse(), the second thing to notice is we get a short preview of the values for each variable. There are a couple things we can gleen from this. One is that we can confirm or clarify the meaning of the variable names by looking at the values. The other thing to consider is whether the values show any patterns that may be worthy of more scrutiny. For example, various variables appear to contain the same values for each observation. For a variable like filename, this is expected as the first values likely correspond to the same file. However, for the variables postcodes and gems the values are ‘NA’. This suggests that these variables may not contain any useful information and we may want to remove them later.\n\nFor now, however, we want to acquire and store the data in its original form (or as closely as possible). So now, we have acquired the utterances data and have it in our R session as a data frame. To store this data in a file, we will first need to consider the file format. Data frames are tabular, so that gives us a few options. Since we are working in R, we could store this data as an R object, in the form of an RDS file. An RDS file is a binary file that can be read back into R as an R object. This is a good option if we want to store the data for use in R, but not if we want to share the data with others or use it in other software. Another option is to store the data as a spreadsheet file, such as XSLX (MS Excel). This may make viewing and editing the contents more convenient, but it depends on the software available to you and others. A third, more viable option, is to store the data as a CSV file. CSV files are plain text files that can be read and written by most software. This makes CSV files one of the most popular for sharing tablular data. For this reason, we will store the data as a CSV file.\nThe readr package (Wickham, Hester, and Bryan 2023) provides the write_csv() function for writing data frames to CSV files. The first argument is the data frame to write, and the second argument is the path to the file to write. Note, however, that the directories in the path we specify need to exist. If they do not, we will get an error. In this case, I would like to write the file utterances.csv to the ../data/original/cabnc/ directory. The original project structure does not contain a cabnc/ directory, so I need to create one. To do this, I will use dir_create() from the fs package (Hester, Wickham, and Csárdi 2023).\n\nExample 5.17  \n\n# Create the target directory\ndir_create(\"../data/original/cabnc/\")\n\n# Write the data frame to a CSV file\nwrite_csv(utterances, \"../data/original/cabnc/utterances.csv\")\n\n\nChaining the steps covered in Examples 5.15, 5.16, and 5.17, we have a succinct and legible code to acquire, adjust, and write utterances from the CABNC in Example 5.18.\n\nExample 5.18  \n\n# Set corpus_name and corpus_path\ncorpus_name &lt;- \"ca\"\ncorpus_path &lt;- c(\"ca\", \"CABNC\")\n\n# Create the target directory\ndir_create(\"../data/original/cabnc/\")\n\n# Get utterance data\ngetUtterances(\n  corpusName = corpus_name,\n  corpora = corpus_path\n) |&gt;\n  unnest(cols = everything()) |&gt; \n  write_csv(\"../data/original/cabnc/utterances.csv\")\n\n\n\nIf our goal is just to acquire utterances, then we are done acquiring data and we move on to the next step. However, if we want to acquire other datasets from the CABNC, say participants, tokens, etc., then we can either repeat the steps in Example 5.18 for each data type, or we can write a function to do this for us. A function serves us to make our code more legible and reusable for the CABNC, and since the TalkBank data is structured similarly across corpora, we can also use the function to acquire data from other corpora, if need be.\nTo write a function, we need to consider the following:\n\nWhat is the name of the function?\nWhat arguments does the function take?\nWhat functionality does the function provide?\nDoes the function have optional arguments?\nHow does the function return the results?\n\nTaking each in turn, the name of the function should be descriptive of what the function does. In this case, we are acquiring and writing data from Talkbank corpora. A possible name is get_talkbank_data(). The required arguments of the the get*() functions will definitely figure in our function. In addition, we will need to specify the path to the directory to write the data.\n\nExample 5.19  \n\nget_talkbank_data &lt;- function(corpus_name, corpus_path, target_dir) {\n  # ...\n}\n\n\nThe next thing to consider is what functionality the function provides. In this case, we want to acquire and write data from Talkbank corpora. We can start by leveraging the code steps in Example 5.18, making some adjustments to the code replacing the hard-coded values with the function arguments and adding code to create the target file name based on the target_dir argument.\n\nExample 5.20  \n\nget_talkbank_data &lt;- function(corpus_name, corpus_path, target_dir) {\n  \n  # Create the target directory\n  dir_create(target_dir)\n\n  # Set up file path name\n  utterances_file  &lt;- path(target_dir, \"utterances.csv\")\n  \n  # Acquire data and write to file\n  getUtterances(corpusName = corpus_name, corpora = corpus_path) |&gt; \n    unnest(cols = everything()) |&gt; \n    write_csv(utterances_file)\n}\n\n\nBefore we address the obvious feature missing, which is the fact that this function in Example 5.20 only acquires and writes data for utterances, let’s consider some functionality which would make this function more user-friendly.\nWhat if the data is already acquired? Do we want to overwrite it, or should the function skip the process for files that already exist? By skipping the process, we can save time and computing resources. If the files are periodically updated, then we might want to overwrite existing files. To achieve this functionality we will use an if() statement to check if the file exists. If it does, then we will skip the process. If it does not, then we will acquire and write the data.\n\nExample 5.21  \n\nget_talkbank_data &lt;- function(corpus_name, corpus_path, target_dir) {\n  \n  # Create the target directory\n  dir_create(target_dir)\n\n  # Set up file path name\n  utterances_file  &lt;- path(target_dir, \"utterances.csv\")\n  \n  # If the file does not exist, then...\n  # Acquire data and write to file\n  if(!file_exists(utterances_file)) {\n    getUtterances(corpusName = corpus_name, corpora = corpus_path) |&gt; \n      unnest(cols = everything()) |&gt; \n      write_csv(utterances_file)\n  }\n}\n\n\nWe can also add functionality to Example 5.21 to force overwrite existing files, if need be. To do this, we will add an optional argument to the function, force, which will be a logical value. We will set the default to force = FALSE to preserve the existing functionality. If force = TRUE, then we will overwrite existing files. Then we add another condition to the if() statement to check if force = TRUE. If it is, then we will overwrite existing files.\n\nExample 5.22  \n\nget_talkbank_data &lt;- function(corpus_name, corpus_path, target_dir, force = FALSE) {\n  \n  # Create the target directory\n  dir_create(target_dir)\n\n  # Set up file path name\n  utterances_file  &lt;- path(target_dir, \"utterances.csv\")\n  \n  # If the file does not exist, then...\n  # Acquire data and write to file\n  if(!file_exists(utterances_file) | force) {\n    getUtterances(corpusName = corpus_name, corpora = corpus_path) |&gt; \n      unnest(cols = everything()) |&gt; \n      write_csv(utterances_file)\n  }\n}\n\n\nFrom this point, we add the functionality to acquire and write the other data available from Talkbank corpora, such as participants, tokens, etc. This involves adding additional file path names and if() statements to check if the files exist surrounding the processing steps to Example 5.22. It may be helpful to perform other input checks, print messages, etc. for functions that we plan to share with others. I will leave these enhancements as an exercise for the reader.\n\nBefore we leave the topic of functions, let’s consider where to put functions after we write them. Here are a few options:\n\nIn the same script as the code that uses the function.\nIn a separate script, such as functions.R.\nIn a package, which is loaded by the script that uses the function.\n\nThe general heuristic for choosing where to put functions is to put them in the same script as the code that uses them if the function is only used in that script. If the function is used in multiple scripts or the function or number of functions clutters the readability of the code, then put it in a separate script. If the function is used in multiple projects, then put it in an R package.\n\n\n\n\n\n\n Dive deeper\nIf you are interested in learning more about writing functions, check out the Writing Functions chapter in the R for Data Science book.\nIf you find yourself writing functions that are useful for multiple projects, you may want to consider creating an R package. R packages are a great way to share your code with others. If you are interested in learning more about creating R packages, check out the R Packages book by Hadley Wickham and Jenny Bryan.\n\n\n\nIn this case, we will put the function in a separate file, functions.R, in the same directory as the other code files as in Example 5.25.\n\nExample 5.23  \ncode/\n│   ├── 1_acquire_data.qmd\n│   ├── ...\n│   └── functions.R\n\n\n\n\n\n\n\n Tip\nNote that that the functions.R file is an R script, not a Quarto document. Therefore code blocks that are used in .qmd files are not used, only the R code and code comments.\n\n\n\nTo include this, or other functions in in the R session of the code file that uses them, use the source() function, as seen in Example 5.24.\n\nExample 5.24  \n\n# Source functions\nsource(\"functions.R\")\n\n\nGiven the utility of this function to my projects and potentially others’, I’ve included the get_talkbank_data() function in the qtalrkit package. You can view the source code by calling the function without parentheses (), or on the qtalrkit GitHub repository.\nAfter running the get_talkbank_data() function, we can see that the data has been acquired and written to the data/original/cabnc/ directory.\n\nExample 5.25  \ndata/\n├── analysis\n├── derived\n└── original\n    └── cabnc\n        ├── participants.csv\n        ├── token_types.csv\n        ├── tokens.csv\n        ├── transcripts.csv\n        └── utterances.csv\n\n\nAdd comments to your code in 1-acquire-data.Rmd and create and complete the data origin documentation file for this resource, and the acquisition is complete."
  },
  {
    "objectID": "acquire-data.html#summary",
    "href": "acquire-data.html#summary",
    "title": "5  Acquire data",
    "section": "Summary",
    "text": "Summary\nIn this chapter we have covered a lot of ground. On the surface we have discussed a few methods for acquiring corpus data for use in text analysis. In the process we have delved into various aspects of the R programming language. Some key concepts include writing custom functions, control statements, and applying functions iteratively. We have also considered topics that are more general in nature and concern interacting with data found on the internet.\nEach of these methods should be approached in a way that is transparent to the researcher and to would-be collaborators and the general research community. For this reason the documentation of the steps taken to acquire data are key both in the code and in human-facing documentation.\nAt this point you have both a bird’s eye view of the data available on the web and strategies on how to access a great majority of it. It is now time to turn to the next step in our data analysis project: data curation. In the next chapter, I will cover how to wrangle your raw data into a tidy dataset."
  },
  {
    "objectID": "acquire-data.html#activities",
    "href": "acquire-data.html#activities",
    "title": "5  Acquire data",
    "section": "Activities",
    "text": "Activities\n\n\n Add description of outcomes\n\n\n\n\n\n\n\n Recipe\n\n\n\n update\n\nWhat: Collecting and documenting dataHow: Read Recipe 5 and participate in the Hypothes.is online social annotation.Why: To understand common strategies to collect, organize, and document data using effective, concise, and reproducible code. You will see how control statements, writing custom functions, and leveraging iteration works to these goals. These programming strategies are often useful for acquiring data but, as we will see, they are powerful concepts that can be used throughout a reproducible research project.\n\n\n\n\n\n\n\n\n\n Lab\n\n\n\n update\n\nWhat: Control statements, custom functions, and iterationHow: Clone, fork, and complete the steps in Lab 5.Why: To gain experience working with coding strategies such as control statements, custom functions, and iteration, practice working with direct downloads and API interfaces to acquire data, and implement organizational strategies for organizing data in reproducible fashion."
  },
  {
    "objectID": "acquire-data.html#questions",
    "href": "acquire-data.html#questions",
    "title": "5  Acquire data",
    "section": "Questions",
    "text": "Questions\n\n\n create conceptual and technical questions\n\n\n\n\n\n\n\nConceptual questions\n\n…\nFor many resources, information to describe the data origin is found on the resource’s website. Visit the XXX resource and complete the data origin information.\n\n\n\n\n\n\n\n\n\n\nTechnical exercises\n\n…\n…\n\n\n\n\n\n\n\n\nAlbert, Saul, Laura E. de Ruiter, and J. P. de Ruiter. 2015. “CABNC: The Jeffersonian Transcription of the Spoken British National Corpus.” TalkBank.\n\n\nBraginsky, Mika. 2022. Wordbankr: Accessing the Wordbank Database. https://langcog.github.io/wordbankr/.\n\n\nHester, Jim, Hadley Wickham, and Gábor Csárdi. 2023. Fs: Cross-Platform File System Operations Based on Libuv. https://fs.r-lib.org.\n\n\nKowalski, John, and Rob Cavanaugh. 2022. TBDBr: Easy Access to TalkBankDB via r API. https://github.com/TalkBank/TalkBankDB-R.\n\n\nLeech, Geoffrey. 1992. “100 Million Words of English: The British National Corpus (BNC),” no. 1991: 1–13.\n\n\nLozano, Cristóbal. 2009. “CEDEL2: Corpus Escrito Del Español L2.” Applied Linguistics Now: Understanding Language and Mind/La Lingüística Aplicada Hoy: Comprendiendo El Lenguaje y La Mente. Almería: Universidad de Almería, 197–212.\n\n\nMoroz, George. 2023. Lingtypology: Linguistic Typology and Mapping. https://CRAN.R-project.org/package=lingtypology.\n\n\nRinker, Tyler, and Dason Kurkiewicz. 2019. Pacman: Package Management Tool. https://github.com/trinker/pacman.\n\n\nTottie, Gunnel. 2011. “Uh and Um as Sociolinguistic Markers in British English.” International Journal of Corpus Linguistics 16 (2): 173–97.\n\n\nUniversity of Colorado Boulder. 2008. “Switchboard Dialog Act Corpus. Web Download.” Linguistic Data Consortium.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2023. Readr: Read Rectangular Text Data. https://readr.tidyverse.org.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. Tidyr: Tidy Messy Data. https://tidyr.tidyverse.org."
  },
  {
    "objectID": "curate-datasets.html#unstructured",
    "href": "curate-datasets.html#unstructured",
    "title": "6  Curate datasets",
    "section": "\n6.1 Unstructured",
    "text": "6.1 Unstructured\nThe bulk of text ever created is of the unstructured variety. Unstructured data is data that has not been organized to make the information contained within machine-readable. Remember that text in itself is not information. Only when given explicit context does text become informative. The explicit contextual information that is included with data is called metadata. Metadata can be linguistic or non-linguistic in nature. So for unstructured data there is little to no metadata directly associated with the data. This information needs to be added or derived for the purposes of the research, either through manual inspection or (semi-)automatic processes. For now, however, our job is just to get the unstructured data into a structured format with a minimal set of metadata that we can derive from the resource.\n\n6.1.1 Reading data\nLet’s consider some of the common file formats which contain unstructured data, such as TXT, PDF, and DOCX, and how to read these formats into an R session.\nIt is very common for unstructured data resources to include data that is in TXT files. These are the simplest of file formats which contain no formatting or explicit metadata. Many times TXT files have the .txt extension, but it is not required. There are many ways to read TXT files into R and many packages that can be used to do so. For example, using the readr package, we can choose to read the entire file into a single vector of character strings with read_file() or read the file by lines with read_lines() in which each line is a character string in a vector.\nLess commonly used in prepared data resources, PDF and DOCX files are also formats for unstructured data. These formats are often more complex than TXT files as they contain formatting and embedded metadata. However, these attributes are primarily for visual presentation and not for machine-readability. Therefore, we need to use packages and functions that can extract the text content from these files and potentially some of the metadata. For example, using the readtext package (Benoit and Obeng 2023), we can read the text content from PDF and DOCX files into a single vector of character strings with readtext().\nWhether in TXT, PDF, or DOCX format, the resulting data structure will require further processing to convert the data into a tidy dataset. For example, we may need to split the data into multiple columns or rows, or extract metadata from the text content.\n\n6.1.2 Orientation\nAs an example of curating an unstructured source of corpus data, let’s take a look at the Europarl Parallel Corpus (Koehn 2005). This corpus contains parallel texts (source and translated documents) from the European Parliamentary proceedings between 1996-2011 for some 21 European languages.\nLet’s assuming we selected this corpus data because we are interested in researching Spanish to English translations. After consulting the corpus website, downloading the compressed file, and inspecting the decompressed structure, we have the the file structure seen in Example 6.1.\n\nExample 6.1  \nproject/\n├── code/\n│   ├── 1-acquire-data.qmd\n│   ├── 2-curate-data.qmd\n│   └── ...\n├── data/\n│   ├── analysis/\n│   ├── derived/\n│   └── original/\n│       │── europarl_do.csv\n│       └── europarl/\n│           ├── europarl-v7.es-en.en\n│           └── europarl-v7.es-en.es\n├── output/\n│   ├── figures/\n│   ├── reports/\n│   ├── results/\n│   └── tables/\n├── README.md\n└── _main.R\n\nThe europarl_do.csv file contains the data origin information documented as part of the acquisition process. The contents are seen in Table 6.1.\n\n\n\n\nTable 6.1: Data origin: Europarl Corpus\n\nattribute\ndescription\n\n\n\nResource name\nEuroparl Parallel Corpus\n\n\nData source\n&lt;https://www.statmt.org/europarl/&gt;\n\n\nData sampling frame\nSpanish transcripts from the European Parliament proceedings\n\n\nData collection date(s)\n1996-2011\n\n\nData format\nTXT files with '.es' for source (Spanish) and '.en' for target (English) files.\n\n\nData schema\nLine-by-line unannotated parallel text\n\n\nLicense\nSee: &lt;https://www.europarl.europa.eu/legal-notice/en/&gt;\n\n\nAttribution\nPlease cite the paper: Koehn, P. 2005. 'Europarl: A Parallel Corpus for Statistical Machine Translation.' MT Summit X, 12-16.\n\n\n\n\n\n\n\n\n\nNow let’s get familiar with the corpus directory structure and the files. In Example 6.1, we see that there are two corpus files, europarl-v7.es-en.es and europarl-v7.es-en.en, that contain the source and target language texts, respectively. The file names indicate that the files contain Spanish-English parallel texts. The .es and .en extensions indicate the language of the text.\nLooking at the beginning of the .es and .en files, in File 6.1 and File 6.2, we see that the files contain a series of lines in either the source or target language.\n\nFile 6.1: Spanish source text\n\ndata/original/europarl/europarl-v7.es-en.es\n\nReanudación del período de sesiones\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\nComo todos han podido comprobar, el gran \"efecto del año 2000\" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\nSus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\nA la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\n\n\nFile 6.2: English target text\n\ndata/original/europarl/europarl-v7.es-en.en\n\nResumption of the session\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\nAlthough, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\nYou have requested a debate on this subject in the course of the next few days, during this part-session.\nIn the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n\nWe can clearly appreciate that the data is unstructured. That is, there is no explicit metadata associated with the data. The data is just a series of lines. The only information that we can surmise from structure of the data is that the texts are line-aligned and that the data in each file corresponds to source and target languages.\nNow, before embarking on a data curation process, it is recommendable to define the structure of the data that we want to create. I call this the “idealized structure” of the data. For a curated dataset there are two considerations. First, we want to maintain the original structure of the data as much as possible. This provides a link to the original data and gives us a default dataset structure from which we can derive other structures that fit our analysis needs. Second, we want to add structure to the data that makes it easier to work with. This is the ‘tidy’ part of the data curation process. The aim is to imbue the dataset with as much of the metadata the data resource makes available.\nGiven what we know about the data, we can define the idealized structure of the data as seen in Table 6.2.\n\n\n\n\nTable 6.2: Idealized structure for the europarl Corpus dataset.\n\ndoc_id\ntype\nline_id\nline\n\n\n\n1\nSource\n1\n...line from source language\n\n\n2\nSource\n2\n...\n\n\n3\nSource\n3\n...\n\n\n4\nTarget\n1\n...line from target language\n\n\n5\nTarget\n2\n...\n\n\n6\nTarget\n3\n...\n\n\n\n\n\n\n\n\nThe dataset structure in Table 6.2 has four columns. The first column, doc_id is a unique identifier for each line in the corpus. type, indicates whether the line is from the source or target language. The third column, line_id, is a unique identifier for each line for each type. The last column, line, contains the text of the line, and maintains the structure of the original data. The observations are lines.\nOur task now is to develop code that will read the original data and render the idealized structure as a curated dataset we will write to the data/derived/ directory. The code we develop will be added to the 2-curate-data.qmd file. And finally, the dataset will be documented with a data dictionary file.\n\n6.1.3 Tidy the data\n\n\nTo create the idealized dataset structure in Table 6.2, lets’s start by reading the files into R. We will use the readtext() function from the readtext package. readtext() is a versatile function that can read many different types of text files (e.g. .txt, .csv, xml, etc.). It can also read multiple files at once using wildcard matching. We will use it to read the .es and .en files by passing the path to the directory where the files are located, and then use the * wildcard to read all the files in the directory.\n\n# Load package\nlibrary(readtext)\n\n# Read Europarl files .es and .en\neuroparl_docs_tbl &lt;-\n  readtext(\"../data/original/europarl/*\") |&gt; # read in files\n  tibble() # convert to tibble\n\n\nExample 6.2  \n\nIn Example 6.2, the readtext() function reads all the files in the ../data/original/europarl/ directory and returns a tibble.\n\n\n\n\n\n\n Warning\nThe readtext() function can read many different types of file formats, from structured to unstructured. However, it depends in large part on the extension of the file to recognize what algorithm to use when reading a file. In this particular case, the Europarl files do not have a typical extension (they have .en and .es). The readtext() function will treat them as plain text (.txt), but it will throw a warning message. To suppress the warning message you can add the verbosity = 0 argument or set readtext_options(verbosity = 0).\n\n\n\nLet’s inspect the europarl_docs_tbl object with the str() function, in Example 6.3.\n\nExample 6.3  \n\n# Preview data\nstr(europarl_docs_tbl) \n\n&gt; tibble [2 × 2] (S3: tbl_df/tbl/data.frame)\n&gt;  $ doc_id: chr [1:2] \"europarl-v7.es-en.en\" \"europarl-v7.es-en.es\"\n&gt;  $ text  : chr [1:2] \"Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Friday 17 Dece\"| __truncated__ \"Reanudación del período de sesiones\\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrump\"| __truncated__\n\n\n\nWe see that the output from Example 6.3 is a tibble with two columns, doc_id and text. The doc_id column contains the name of the file from which the text was read. The text column contains the text of the file. There are two observations, one for each file.\n\n\n\n\n\n\n Tip\nNote that the str() function from base R is similar to glimpse(). However, glimpse() will attempt to show you as much data as possible. In this case since our column text is a very long character vector it will take a long time to render. I’ve chosen the str() function as it will automatically truncate the data.\n\n\n\n\nThe fact that we only have one row for each file means that all the text in each file is contained in one cell! We want to break these cells up into rows for each line, as they appear in the original data. You may be wondering why the readtext() function did not do this for us, after all the original data was already separated into lines. The reason is that the readtext() function chose to read the files as plain text, and plain text does not have any structure. The line breaks, however, are still there, they are just represented as a special character, \\n. Comparing Example 6.4 and Example 6.5, we can see that the text is a single long character vector, and that the line breaks are represented as \\n.\n\nExample 6.4  \n\n# Preview first 50 characters\neuroparl_docs_tbl |&gt; \n  pull(text) |&gt; \n  str_trunc(50) |&gt; \n  str_view() \n\n&gt; [1] │ Resumption of the session\n&gt;     │ I declare resumed the...\n&gt; [2] │ Reanudación del período de sesiones\n&gt;     │ Declaro rea...\n\n\n\n\nExample 6.5  \n\n# Preview first 50 raw characters\neuroparl_docs_tbl |&gt; \n  pull(text) |&gt; \n  str_trunc(50) |&gt; \n  str_view(use_escapes = TRUE)\n\n&gt; [1] │ Resumption of the session\\nI declare resumed the...\n&gt; [2] │ Reanudaci\\u00f3n del per\\u00edodo de sesiones\\nDeclaro rea...\n\n\n\nIn Example 6.4, we can see a truncated version of the text as it is in a printed format. The str_trunc() function from the stringr package (Wickham 2022) truncates the text to the first 50 characters. The str_view() function from the same package allows us to see the text in a viewer pane.\nIn Example 6.5, we can see the raw text, that is, the text as it is stored in the computer. The use_escapes = TRUE argument tells the str_view() function to show the special characters as they are stored in the computer. This includes the \\n line-feed character, which represents a line break.\n\n\n\n\n\n\n Tip\nThe str_view() with the argument use_escapes = TRUE also allows you to see other special characters, such as \\t (tab) and \\r (carriage return) as well as Unicode characters, such as \\u00f3n (ó), \\u00ed (í), etc.. These characters are not visible in the printed version of the text, but they are there in the raw text.\n\n\n\nWe can see that the text is a single long character vector, and that the line breaks are represented as \\n. So our goal is to split the text for each file into lines creating a new row for each line created.\nTo do this we will use another function from the stringr package, str_split(), whose function is to split a character vector into smaller character vectors based on some splitting criteria. In Example 6.6, we use the str_split() function to split the text into lines based on the \\n character and assign it to the new column lines using mutate(). Since we will not need the text column anymore, we can use select() to drop it.\n\nExample 6.6  \n\n# Split text into lines\neuroparl_lines_tbl &lt;- \n  europarl_docs_tbl |&gt; \n  mutate(lines = str_split(text, \"\\n\")) |&gt; \n  select(-text)\n\n# Preview\neuroparl_lines_tbl\n\n&gt; # A tibble: 2 × 2\n&gt;   doc_id               lines            \n&gt;   &lt;chr&gt;                &lt;list&gt;           \n&gt; 1 europarl-v7.es-en.en &lt;chr [1,965,734]&gt;\n&gt; 2 europarl-v7.es-en.es &lt;chr [1,965,734]&gt;\n\n\n\nPreviewing the output in Example 6.6, we can see that the lines column contains a list of character vectors. Why is this so? str_split() takes a character vector and splits it, as we know. It is a vectorized function, meaning that it can take a vector of character vectors and split each one into subsegments. Since any given input character vector can have a different number of subsegments, the number of subsegments in each file could be different. The ideal object for such data is a list. So str_split() returns a list of character vectors.\nThe list of character vectors, lines in our case, is still associated with the respective doc_id value. However, we want to create a new row for each line in the list. To do this, we will use the unnest() function from the tidyr package (Wickham, Vaughan, and Girlich 2023). The unnest() function takes a list column and creates a new row for each element in the list. We use the unnest() function to create a new row for each line in the lines column. We can see the output from Example 6.7.\n\nExample 6.7  \n\n# Create a new row for each line\neuroparl_lines_tbl &lt;- \n  europarl_lines_tbl |&gt; \n  unnest(lines)\n\n# Preview\neuroparl_lines_tbl |&gt; \n  slice_head(n = 5)\n\n&gt; # A tibble: 5 × 2\n&gt;   doc_id               lines                                                    \n&gt;   &lt;chr&gt;                &lt;chr&gt;                                                    \n&gt; 1 europarl-v7.es-en.en Resumption of the session                                \n&gt; 2 europarl-v7.es-en.en I declare resumed the session of the European Parliament…\n&gt; 3 europarl-v7.es-en.en Although, as you will have seen, the dreaded 'millennium…\n&gt; 4 europarl-v7.es-en.en You have requested a debate on this subject in the cours…\n&gt; 5 europarl-v7.es-en.en In the meantime, I should like to observe a minute' s si…\n\n\n\nRemember that the data in the Europarl corpus is aligned, meaning that each line in the source file is aligned with a line in the target file. So we need to make sure that the number of lines in each file is the same. We can do this by grouping the data by doc_id and then counting the number of lines in each file. We can do this using the group_by() and count() functions from the dplyr package (Wickham et al. 2023). We can see the output in Example 6.8.\n\nExample 6.8  \n\n# Count the number of lines in each file\neuroparl_lines_tbl |&gt; \n  group_by(doc_id) |&gt; \n  count()\n\n&gt; # A tibble: 2 × 2\n&gt; # Groups:   doc_id [2]\n&gt;   doc_id                     n\n&gt;   &lt;chr&gt;                  &lt;int&gt;\n&gt; 1 europarl-v7.es-en.en 1965734\n&gt; 2 europarl-v7.es-en.es 1965734\n\n\n\nThe output of Example 6.8 shows that the number of lines in each file is the same. This is good. If the number of lines in each file was different, we would need to figure out why and fix it.\nWe now have our lines column and the associated observations for our idealized dataset, in Table 6.2. Let’s now leverage the existing doc_id to create the type column. The goal is to assign the value of type according to the doc_id value. Specifically, when doc_id is ‘europarl-v7.es-en.es’ type should be ‘Source’ and when doc_id is ‘europarl-v7.es-en.en’ type should be ‘Target’.\nWe can do this using the case_when() function from the dplyr package (Wickham et al. 2023). The case_when() function takes a series of conditions and assigns a value based on the first condition that is met. In this case, we will use mutate() to create the type column and then use the doc_id column to create the conditions and the type column to assign the values from case_when(). We will store the output in a new object called europarl_lines_type_tbl, as seen in Example 6.9.\n\nExample 6.9  \n\n# Create `type` column\neuroparl_lines_type_tbl &lt;- \n  europarl_lines_tbl |&gt; \n  mutate(type = case_when(\n    doc_id == \"europarl-v7.es-en.es\" ~ \"Source\",\n    doc_id == \"europarl-v7.es-en.en\" ~ \"Target\"\n  ))\n\n# Preview dataset\nglimpse(europarl_lines_type_tbl)\n\n&gt; Rows: 3,931,468\n&gt; Columns: 3\n&gt; $ doc_id &lt;chr&gt; \"europarl-v7.es-en.en\", \"europarl-v7.es-en.en\", \"europarl-v7.es…\n&gt; $ lines  &lt;chr&gt; \"Resumption of the session\", \"I declare resumed the session of …\n&gt; $ type   &lt;chr&gt; \"Target\", \"Target\", \"Target\", \"Target\", \"Target\", \"Target\", \"Ta…\n\n\n\nThe preview output from Example 6.9 shows us that we now have three columns, doc_id, lines, and type. The type column has been created and the values have been assigned according to the doc_id values.\nWe can now overwrite the doc_id column with a unique identifier for each line. A numeric identifier makes sense. We can do this by using the mutate() function to assign doc_id a sequential number with row_number(), which increments by 1 for each row. We will store the output in a new object called europarl_lines_type_id_tbl, as seen in Example 6.10.\n\nExample 6.10  \n\n# Create new `doc_id` column\neuroparl_lines_type_id_tbl &lt;- \n  europarl_lines_type_tbl |&gt; \n  mutate(doc_id = row_number())\n\n# Preview dataset\nglimpse(europarl_lines_type_id_tbl)\n\n&gt; Rows: 3,931,468\n&gt; Columns: 3\n&gt; $ doc_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, …\n&gt; $ lines  &lt;chr&gt; \"Resumption of the session\", \"I declare resumed the session of …\n&gt; $ type   &lt;chr&gt; \"Target\", \"Target\", \"Target\", \"Target\", \"Target\", \"Target\", \"Ta…\n\n\n\nThe preview output from Example 6.10 shows us that we now have the same three columns, doc_id, lines, and type. However, the values in the doc_id column now reflect a unique identifier for each line.\nThe last step to get to our envisioned dataset structure is to add the line_id column which will be calculated by grouping the data by type and then assigning a row number to each of the lines in each group. We use the group_by() function to perform the grouping as seen in Example 6.11.\n\nExample 6.11  \n\n# Create `line_id` column\neuroparl_lines_type_id_line_id_tbl &lt;- \n  europarl_lines_type_id_tbl |&gt; \n  group_by(type) |&gt; \n  mutate(line_id = row_number()) |&gt; \n  ungroup()\n\n# Preview dataset\nglimpse(europarl_lines_type_id_line_id_tbl)\n\n&gt; Rows: 3,931,468\n&gt; Columns: 4\n&gt; $ doc_id  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n&gt; $ lines   &lt;chr&gt; \"Resumption of the session\", \"I declare resumed the session of…\n&gt; $ type    &lt;chr&gt; \"Target\", \"Target\", \"Target\", \"Target\", \"Target\", \"Target\", \"T…\n&gt; $ line_id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n\n\n\nThe preview output from Example 6.11 shows us that we now have the desired four columns, doc_id, lines, type, and line_id.\nBefore we get to writing the dataset to disk, let’s organized it in a way that the columns are in the order we want and the rows are sorted in a way that makes sense.\nReordering the columns involves using the select() function and list the order of the columns. To sort by columns, we will use arrange() and specify the columns to order by. We store the results in a more legible object, europarl_curated_tbl, as seen in Example 6.12.\n\nExample 6.12  \n\n# Reorder columns and sort rows\neuroparl_curated_tbl &lt;- \n  europarl_lines_type_id_line_id_tbl |&gt; \n  select(doc_id, type, line_id, lines) |&gt; \n  arrange(line_id, type, doc_id)\n\n# Preview dataset\nglimpse(europarl_curated_tbl)\n\n&gt; Rows: 3,931,468\n&gt; Columns: 4\n&gt; $ doc_id  &lt;int&gt; 1965735, 1, 1965736, 2, 1965737, 3, 1965738, 4, 1965739, 5, 19…\n&gt; $ type    &lt;chr&gt; \"Source\", \"Target\", \"Source\", \"Target\", \"Source\", \"Target\", \"S…\n&gt; $ line_id &lt;int&gt; 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, …\n&gt; $ lines   &lt;chr&gt; \"Reanudación del período de sesiones\", \"Resumption of the sess…\n\n\n\nThe preview output from Example 6.12 shows us that we now have the desired four columns, doc_id, type, line_id, and lines. The rows are sorted by line_id, type, and doc_id. This gives us a dataset that reads left to right from document to line oriented attributes and top to bottom by source-target line pairs.\n\n6.1.4 Write dataset\nAt this point we have the curated dataset (europarl_curated_tbl) in a tidy format. This dataset, however, is only in the current R session. We will want to write this dataset to disk so that in the next step of the text analysis workflow (transformation) we will be able to start work on this dataset and make changes as needed to fit our analysis needs.\nWe will leverage the project directory structure which has distinct directories for original/ and derived/ data(sets), seen in Example 6.13.\n\nExample 6.13  \ndata/\n│── analysis/\n├── derived/\n└── original/\n    │── europarl_do.csv\n    └── europarl/\n        ├── europarl-v7.es-en.es\n        └── europarl-v7.es-en.en\n\nSince this is a tabular, tidy dataset we have various options for the file type to write. Many of these formats are software-specific, such as *.xlsx for Microsoft Excel, *.sav for SPSS, *.dta for Stata, and *.rds for R. We will use the *.csv format since it is a common format that can be read by many software packages. We will use the write_csv() function from the readr package to write the dataset to disk.\nNow the question is where to save our CSV file. Since our europarl_curated_tbl dataset is derived by our work, we will added it to the derived/ directory. I’ll create a europarl/ directory with dir_create() just to keep things organized.\n\nExample 6.14  \n\n# Create the europarl/ directory\ndir_create(path = \"../data/derived/europarl/\")\n\n# Write the curated dataset to disk\nwrite_csv(\n  x = europarl_curated_tbl,\n  file = \"../data/derived/europarl/europarl_curated.csv\"\n) \n\n\nAfter running the code in Example 6.14, the directory structure under the derived/ directory should look like Example 6.15.\n\nExample 6.15  \ndata/\n│── analysis/\n├── derived/\n│   └── europarl/\n│       └── europarl_curated.csv\n└── original/\n    └──europarl\n        ├── europarl-v7.es-en.en\n        └── europarl-v7.es-en.es\n\nThe final step, as always, is to document the dataset. For datasets the documentation is a data dictionary, as discussed in Section 2.3.2. As with data origin files, you can use spreadsheet software to create and/ or edit the data dictionary.\nIn the qtalrkit package we have a function, create_data_dictionary() that will generate the scaffolding for a data dictionary. The function takes two arguments, data and file_path. It reads the dataset columns and provides a template for the data dictionary.\n\n\n\n\n\n\n Dive deeper\nThe create_data_dictionary() function provides a rudimentary data dictionary template by default. However, you can take advantage of OpenAI’s text generation models to generate a more detailed data dictionary for you to edit. To do this create an OpenAI account and an API key and add this key to your R environment (Sys.setenv(OPENAI_API_KEY = \"sk...\"). Then you can specify the model you would like to use in the function with the model = argument. For example, model = \"gpt-3.5-turbo\" will use the GPT-3.5 Turbo model.\n\n\n\n\nExample 6.16  \n\n# Create the data dictionary\ncreate_data_dictionary(\n  data = europarl_curated_tbl,\n  file_path &lt;- \"../data/derived/europarl/europarl_curated_dd.csv\"\n)\n\n\nAn example of the data dictionary for the europarl_curated_tbl dataset is shown in Table 6.3.\n\n\n\n\nTable 6.3: Data dictionary for the europarl_curated_tbl dataset.\n\nvariable\nname\nvariable_type\ndescription\n\n\n\ndoc_id\nDocument ID\nnumeric\nUnique identification number for each document\n\n\ntype\nDocument Type\ncategorical\nType of document; either 'Source' (Spanish) or 'Target' (English)\n\n\nline_id\nLine ID\nnumeric\nUnique identification number for each line in each document type\n\n\nlines\nLines\ncategorical\nContent of the lines in the document"
  },
  {
    "objectID": "curate-datasets.html#structured",
    "href": "curate-datasets.html#structured",
    "title": "6  Curate datasets",
    "section": "\n6.2 Structured",
    "text": "6.2 Structured\nStructured data already reflects the physical and semantic structure of a tidy dataset. This means that the data is already in a tabular format and the relationships between columns and rows are already well-defined. Therefore the heavy lifting of curating the data is already done. There are two remaining questions, however, that need to be taken into account. One, logistical question, is what file format the dataset is in and how to read it into R. And the second, more research-based, is whether the data may benefit from some additional curation and documentation to make it more amenable to analysis and more understandable to others.\n\n6.2.1 Reading data\nLet’s consider some common formats for structured data and how to read them into R. First, we will consider R-native formats, such as package datasets and RDS files. Then will consider non-native formats, such as relational databases and datasets produced by other software. Finally, we will consider software agnostic formats, such as CSV.\nR and some R packages provide structured datasets that are available for use directly within R. For example, the languageR package (R-languageR?) provides the dative dataset, which is a dataset containing the realization of the dative as NP or PP in the Switchboard corpus and the Treebank Wall Street Journal collection. The janeaustenr package (Silge 2022) provides the austen_books dataset, which is a dataset of Jane Austen’s novels. Package datasets are loaded into an R session using either the data() function, if the package is loaded, or the :: operator, if the package is not loaded. For example, data(dative) or languageR::dative.\n\n\n\n\n\n\n Dive deeper\nTo explore the available datasets in a package, you can use the data(package = \"package_name\") function. For example, data(package = \"languageR\") will list the datasets available in the languageR package. You can also explore all the datasets available in the loaded packages with the data() function using no arguments. For example, data().\n\n\n\nR also provides a native file format for storing R objects, the RDS file. Any R object, including data frames, can be written from an R session to disk by using the write_rds() function from readr. The .rds files will be written to disk in a binary format that is not human-readable, which is not ideal for transparent data sharing. However, the files and the R objects can be read back into an R session using the read_rds() function with all the attributes intact, such as vector types, factor levels, etc..\nR provides a suite of tools for importing data from non-native structured sources such as databases and datasets from software such as SPSS, SAS, and Stata. For instance, if you are working with data stored in a relational database such as MySQL, PostgreSQL, or SQLite, you can use the DBI package (R Special Interest Group on Databases (R-SIG-DB), Wickham, and Müller 2022) to connect to the database and the dbplyr package (Wickham, Girlich, and Ruiz 2023) to query the database using the SQL language. Files from SPSS (.sav), SAS (.sas7bdat), and Stata (.dta) can be read into R using the haven package (Wickham, Miller, and Smith 2023).\nSoftware agnostic file formats include delimited files, such as CSV, TSV, etc.. These file formats lack the robust structural attributes of the other formats, but balance this shortcoming by storing structured data in more accessible, human-readable format. Delimited files are plain text files which use a delimiter, such as a comma (,), tab (\\t), or pipe (|), to separate the columns and rows. For example, a CSV file is a delimited file where the columns and rows are separated by commas, as seen in Example 6.17.\n\nExample 6.17  \ncolumn_1,column_2,column_3\nrow 1 value 1,row 1 value 2,row 1 value 3\nrow 2 value 1,row 2 value 2,row 2 value 3\n\nGiven the accessibility of delimited files, they are a common format for sharing structured data in reproducible research. It is not surprising, then, that this is the format which we have chosen for the derived datasets in this book.\n\n6.2.2 Orientation\n\nWith an understanding of the various structured formats, we can now turn to considerations about how the original dataset is structured and how that structure is to be used for a given research project. As an example, we will work with the CABNC datasets acquired in Chapter 5. The structure of the original dataset is shown in Example 6.18.\n\nExample 6.18  \ndata/\n├── analysis/\n├── derived/\n└── original/\n    ├── cabnc_do.csv\n    └── cabnc/\n        ├── participants.csv\n        ├── token_types.csv\n        ├── tokens.csv\n        ├── transcripts.csv\n        └── utterances.csv\n\nIn addition to other important information, the data origin file cabnc_do.csv shown in Table 6.4 informs us the the datasets are related by a common variable.\n\n\n\n\nTable 6.4: Data origin: CABNC datasets\n\nattribute\ndescription\n\n\n\nResource name\nCABNC.\n\n\nData source\n&lt;https://ca.talkbank.org/access/CABNC.html&gt;, &lt;doi:10.21415/T55Q5R&gt;\n\n\nData sampling frame\nOver 400 British English Speakers from across the UK stratified age, gender, social group, and region, and recording their language output over a set period of time.\n\n\nData collection date(s)\n1992.\n\n\nData format\nCSV Files\n\n\nData schema\nThe recordings are linked by `filename` and the participants are linked by `who`.\n\n\nLicense\nCC BY NC SA 3.0\n\n\nAttribution\nSaul Albert, Laura E. de Ruiter, and J.P. de Ruiter (2015) CABNC: the Jeffersonian transcription of the Spoken British National Corpus. https://saulalbert.github.io/CABNC/.\n\n\n\n\n\n\n\n\nThe CABNC datasets are structured in a relational format, which means that the data is stored in multiple tables that are related to each other. The tables are related by a common column or set of columns, which are called a keys. A key is used to join the tables together to create a single dataset. There are two keys in the CABNC datasets, filename and who. Each variable corresponds to recording- and/ or participant-oriented datasets.\nNow, let’s envision a scenario in which we want to organize a dataset that can be used in a study that aims to investigate the relationship between speaker demographics and utterances. An ideal dataset would contain information about speakers and their utterances. In their original format, the CABNC datasets separate information about utterances and speakers in separate tables, cabnc_utterances and cabnc_participants, respectively. The idealized dataset, then, will combine the variables from each of these tables into a single dataset.\n\n6.2.3 Tidy the dataset\nWith our idealized dataset in mind, let’s start the process of curation by reading the relevant datasets into an R session. Since we are working with CSV files will will use the read_csv() function, as seen in Example 6.19.\n\nExample 6.19  \n\n# Read the relevant datasets\ncabnc_utterances &lt;- \n  read_csv(\"data/cabnc/original/utterances.csv\")\ncabnc_participants &lt;- \n  read_csv(\"data/cabnc/original/participants.csv\")\n\n\nThe next step is to inspect the structure of the datasets. We can use the glimpse() function for this task.\n\nExample 6.20  \n\n# Preview the structure of the datasets\nglimpse(cabnc_utterances)\n\n&gt; Rows: 235,901\n&gt; Columns: 10\n&gt; $ filename  &lt;chr&gt; \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", …\n&gt; $ path      &lt;chr&gt; \"ca/CABNC/KB0/KB0RE000\", \"ca/CABNC/KB0/KB0RE000\", \"ca/CABNC/…\n&gt; $ utt_num   &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n&gt; $ who       &lt;chr&gt; \"PS002\", \"PS006\", \"PS002\", \"PS006\", \"PS002\", \"PS006\", \"PS002…\n&gt; $ role      &lt;chr&gt; \"Unidentified\", \"Unidentified\", \"Unidentified\", \"Unidentifie…\n&gt; $ postcodes &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n&gt; $ gems      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n&gt; $ utterance &lt;chr&gt; \"You enjoyed yourself in America\", \"Eh\", \"did you\", \"Oh I co…\n&gt; $ startTime &lt;dbl&gt; 0.208, 2.656, 2.896, 3.328, 5.088, 6.208, 8.320, 8.480, 10.2…\n&gt; $ endTime   &lt;dbl&gt; 2.67, 2.90, 3.33, 5.26, 6.02, 8.50, 9.31, 11.23, 14.34, 15.9…\n\nglimpse(cabnc_participants)\n\n&gt; Rows: 6,190\n&gt; Columns: 13\n&gt; $ filename  &lt;chr&gt; \"KB0RE004\", \"KB0RE004\", \"KB0RE004\", \"KB0RE006\", \"KB0RE006\", …\n&gt; $ path      &lt;chr&gt; \"ca/CABNC/0missing/KB0RE004\", \"ca/CABNC/0missing/KB0RE004\", …\n&gt; $ who       &lt;chr&gt; \"PS008\", \"PS009\", \"KB0PSUN\", \"PS007\", \"PS008\", \"PS009\", \"KB0…\n&gt; $ name      &lt;chr&gt; \"John\", \"Gethyn\", \"Unknown_speaker\", \"Alan\", \"John\", \"Gethyn…\n&gt; $ role      &lt;chr&gt; \"Unidentified\", \"Unidentified\", \"Unidentified\", \"Unidentifie…\n&gt; $ language  &lt;chr&gt; \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng…\n&gt; $ monthage  &lt;dbl&gt; 481, 481, 13, 949, 481, 481, 13, 637, 565, 13, 637, 565, 13,…\n&gt; $ age       &lt;chr&gt; \"40;01.01\", \"40;01.01\", \"1;01.01\", \"79;01.01\", \"40;01.01\", \"…\n&gt; $ sex       &lt;chr&gt; \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"mal…\n&gt; $ numwords  &lt;dbl&gt; 28, 360, 156, 1610, 791, 184, 294, 93, 3, 0, 128, 24, 0, 150…\n&gt; $ numutts   &lt;dbl&gt; 1, 9, 27, 7, 5, 7, 6, 5, 1, 0, 11, 6, 0, 110, 74, 96, 12, 1,…\n&gt; $ avgutt    &lt;dbl&gt; 28.00, 40.00, 5.78, 230.00, 158.20, 26.29, 49.00, 18.60, 3.0…\n&gt; $ medianutt &lt;dbl&gt; 28, 39, 5, 84, 64, 9, 3, 15, 3, 0, 9, 3, 0, 7, 6, 4, 3, 12, …\n\n\n\nFrom visual inspection of the output of Example 6.20 we can see that there are common variables in both datasets. It is also possible to intersect the datasets to see which variables are common using the intersect() function to intersect the column names of each data frame with the names() function, as seen in Example 6.21.\n\nExample 6.21  \n\n# Find the common variables\ncommon_vars &lt;- \n  intersect(\n    names(cabnc_utterances), \n    names(cabnc_participants)\n  )\n\n# Print the common variables\ncommon_vars\n\n&gt; [1] \"filename\" \"path\"     \"who\"      \"role\"\n\n\n\nUsing both visual inspection and column name intersection, we can make sure that the variable names and the values are consistent across the datasets. For example, if the variable filename in one dataset is called file_name in another dataset, then we will need to rename the variable in one of the datasets so that the variable names are consistent. Furthermore, if the values in the filename variable are not consistent across the datasets, then we will need to make the values consistent, if possible, before joining the datasets.\nIn this case, the variable names and values are consistent across the datasets. Therefore, we can join the datasets together using the left_join() function from the dplyr package, as seen in Example 6.22. This function will take the dataset on the left (x =) and join it to the dataset on the right (y =). The by argument specifies the variables to join on. The choice of which dataset to put on the left usually depends on which dataset has the most detailed information. In this case, the cabnc_utterances dataset has more detailed information about the utterances so we will put this dataset on the left.\n\nExample 6.22  \n\n# Join the datasets\ncabnc_tbl &lt;- \n  left_join(\n    x = cabnc_utterances, \n    y = cabnc_participants, \n    by = common_vars\n  )\n\n# Preview the dimensions of the joined dataset\ndim(cabnc_tbl)\n\n&gt; [1] 235901     19\n\n\n\nThe result of Example 6.22 should produce a dataset with the same number of observations as the cabnc_utterances dataset, seen in Example 6.20, and a combined number of unique variables from both datasets, that is 23 minus the four common variables, seen in Example 6.21. The output of dim() confirms this.\nNow we will consider the variables that will be useful for future analysis. Since we are creating a curated dataset, the goal will be to retain as much information as possible from the original datasets. There are cases, however, in which there may be variables that are not informative and thus, will not prove useful for any analysis. These removable variables tend to be of one of two types: variables which show no variation across observations and variables where the information is redundant.\nLet’s get a high-level summary of the variables in the dataset. We can use the skim() function from the skimr package (Waring et al. 2022) to get a summary of the variables in the dataset1.\n\nExample 6.23  \n\n# Load package\nlibrary(skimr)\n\n# Summarize the variables in the dataset\nskim(cabnc_tbl)\n\n\n\nTable 6.5: Summary of variables in the CABNC dataset\n\n\n\n\n\n(a) Categorical variables\n\nvariable\ncomplete_rate\nn_unique\n\n\n\nfilename\n1\n2020\n\n\npath\n1\n2020\n\n\nwho\n1\n568\n\n\nrole\n1\n1\n\n\nutterance\n1\n174414\n\n\nname\n1\n269\n\n\nlanguage\n1\n1\n\n\nage\n1\n83\n\n\nsex\n1\n2\n\n\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\n\n(b) Logical variables\n\nvariable\ncomplete_rate\nmean\n\n\n\npostcodes\n0\nNA\n\n\ngems\n0\nNA\n\n\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\n\n(c) Numeric variables\n\nvariable\ncomplete_rate\nmean\n\n\n\nutt_num\n1.000\n231.67\n\n\nstartTime\n0.841\nNA\n\n\nendTime\n0.841\nNA\n\n\nmonthage\n1.000\n448.61\n\n\nnumwords\n1.000\n1387.47\n\n\nnumutts\n1.000\n156.04\n\n\navgutt\n1.000\n9.13\n\n\nmedianutt\n1.000\n6.06\n\n\n\n\n?(caption)\n\n\n\n\n\n\n\n\nWe see from the skim() output in Table 6.5 (b), that the variables postcodes and gems have no values. Therefore we can remove these variables from the dataset. On the other hand, in Table 6.5 (a), the variables role and language have the same value for every observation as the output shows n_unique is 1. We can also remove these variables from the dataset.\nAnother set of variables of potential interest are the startTime and endTime variables, in Table 6.5 (c). These each have a completion rate of less than 100%, specifically 84%. On first blush, it would seem that we would go ahead and remove them. However, we should pause and consider that these variables may still be of some use. At the curation stage, however, it is best to err on the side of caution and leave them in the dataset. We will decide later whether to keep or remove them as we explore the dataset further in the research.\nAnother consideration is whether we have variables that are clearly redundant. For example, the variables age and monthage are both measures of age, stated in different measures, looking back at Example 6.20. As such, we can remove one of these variables from the dataset. In this case, we will remove the age variable as it is the least straightforward to interpret.\nAnother potentially redundant set of variables are who and name –both of which are speaker identifiers. The who variable is a unique identifier, but there may be some redundancy with the name variable, that is there may be two speakers with the same name. We can check this by looking at the number of unique values in the who and name variables from the skim() output in Table 6.5 (a). who has 568 unique values and name has 269 unique values. This suggests that there are multiple speakers with the same name.\nAnother way to explore this is to look at the number of unique values in the who variable for each unique value in the name variable. We can do this using the group_by() and summarize() functions from the dplyr package. For each value of name, we will count the number of unique values in who and then sort the results in descending order.\n\nExample 6.24  \n\ncabnc_tbl |&gt;\n  group_by(name) |&gt;\n  summarize(n = unique(who) |&gt; length()) |&gt;\n  arrange(desc(n))\n\n&gt; # A tibble: 269 × 2\n&gt;    name                          n\n&gt;    &lt;chr&gt;                     &lt;int&gt;\n&gt;  1 None                         59\n&gt;  2 Unknown_speaker              57\n&gt;  3 Group_of_unknown_speakers    10\n&gt;  4 Chris                         9\n&gt;  5 David                         9\n&gt;  6 Margaret                      8\n&gt;  7 Ann                           7\n&gt;  8 John                          7\n&gt;  9 Alan                          6\n&gt; 10 Jackie                        5\n&gt; # ℹ 259 more rows\n\n\n\nIt is good that we performed the check in Example 6.24 beforehand. In addition to speakers with the same name, such as ‘Chris’ and ‘David’, we also have multiple speakers with generic codes, such as ‘None’ and ‘Unknown_speaker’. It is clear that name is redundant and we can safely remove it from the dataset.\nAnother redundant variable is the path variable. This variable is not more informative than the filename, for our purposes. We will remove the path variable from the dataset too.\nIn all, we will remove the following variables from the dataset: postcodes, gems, role, language, age, name, and path. To drop variables from a data frame we can use the select() function in combination with the - operator. The - operator tells the select() function to drop the variables that follow it.\n\nExample 6.25  \n\n# Drop variables\ncabnc_tbl &lt;- \n  cabnc_tbl |&gt; \n  select(-postcodes, -gems, -role, -language, -age, -name, -path)\n\n# Preview the dataset\nglimpse(cabnc_tbl)\n\n&gt; Rows: 235,901\n&gt; Columns: 12\n&gt; $ filename  &lt;chr&gt; \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", …\n&gt; $ utt_num   &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n&gt; $ who       &lt;chr&gt; \"PS002\", \"PS006\", \"PS002\", \"PS006\", \"PS002\", \"PS006\", \"PS002…\n&gt; $ utterance &lt;chr&gt; \"You enjoyed yourself in America\", \"Eh\", \"did you\", \"Oh I co…\n&gt; $ startTime &lt;dbl&gt; 0.208, 2.656, 2.896, 3.328, 5.088, 6.208, 8.320, 8.480, 10.2…\n&gt; $ endTime   &lt;dbl&gt; 2.67, 2.90, 3.33, 5.26, 6.02, 8.50, 9.31, 11.23, 14.34, 15.9…\n&gt; $ monthage  &lt;dbl&gt; 721, 601, 721, 601, 721, 601, 721, 601, 721, 601, 721, 601, …\n&gt; $ sex       &lt;chr&gt; \"female\", \"male\", \"female\", \"male\", \"female\", \"male\", \"femal…\n&gt; $ numwords  &lt;dbl&gt; 759, 399, 759, 399, 759, 399, 759, 399, 759, 399, 759, 399, …\n&gt; $ numutts   &lt;dbl&gt; 74, 64, 74, 64, 74, 64, 74, 64, 74, 64, 74, 64, 74, 2, 74, 6…\n&gt; $ avgutt    &lt;dbl&gt; 10.26, 6.23, 10.26, 6.23, 10.26, 6.23, 10.26, 6.23, 10.26, 6…\n&gt; $ medianutt &lt;dbl&gt; 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 1, 7, 5, 7, 5, 7, 5, …\n\n\n\nNow we have a dataset with 12 informative variables which describe the utterances for each recording. There are variables about the recordings, such as the filename, about the speakers, such as the sex and who, and about the utterances, such as the utterance, utt_num, etc.. Let’s organize the columns to read left to right from most general to most specific. Again we turn to the select() function, this time including the variables in the order we want them to appear in the dataset. We will take this opportunity to rename some of the variable names so that they are more informative.\n\nExample 6.26  \n\n# Rename variables\ncabnc_tbl &lt;- \n  cabnc_tbl |&gt; \n  select(\n    doc_id = filename,\n    utt_num,\n    utt_start = startTime,\n    utt_end = endTime,\n    utterance,\n    part_id = who,\n    part_age = monthage,\n    part_sex = sex,\n    num_words = numwords,\n    num_utts = numutts,\n    avg_utt_len = avgutt,\n    median_utt_len = medianutt\n  )\n\n# Preview the dataset\nglimpse(cabnc_tbl)\n\n&gt; Rows: 235,901\n&gt; Columns: 12\n&gt; $ doc_id         &lt;chr&gt; \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", \"KB0RE0…\n&gt; $ utt_num        &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n&gt; $ utt_start      &lt;dbl&gt; 0.208, 2.656, 2.896, 3.328, 5.088, 6.208, 8.320, 8.480,…\n&gt; $ utt_end        &lt;dbl&gt; 2.67, 2.90, 3.33, 5.26, 6.02, 8.50, 9.31, 11.23, 14.34,…\n&gt; $ utterance      &lt;chr&gt; \"You enjoyed yourself in America\", \"Eh\", \"did you\", \"Oh…\n&gt; $ part_id        &lt;chr&gt; \"PS002\", \"PS006\", \"PS002\", \"PS006\", \"PS002\", \"PS006\", \"…\n&gt; $ part_age       &lt;dbl&gt; 721, 601, 721, 601, 721, 601, 721, 601, 721, 601, 721, …\n&gt; $ part_sex       &lt;chr&gt; \"female\", \"male\", \"female\", \"male\", \"female\", \"male\", \"…\n&gt; $ num_words      &lt;dbl&gt; 759, 399, 759, 399, 759, 399, 759, 399, 759, 399, 759, …\n&gt; $ num_utts       &lt;dbl&gt; 74, 64, 74, 64, 74, 64, 74, 64, 74, 64, 74, 64, 74, 2, …\n&gt; $ avg_utt_len    &lt;dbl&gt; 10.26, 6.23, 10.26, 6.23, 10.26, 6.23, 10.26, 6.23, 10.…\n&gt; $ median_utt_len &lt;dbl&gt; 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 1, 7, 5, 7, 5, 7…\n\n\n\nThe variable order is organized after running Example 6.26. Now let’s sort the rows by doc_id and utt_num so that the utterances are in order. The arrange() function takes a data frame and a list of variables to sort by, in the order they are listed.\n\nExample 6.27  \n\n# Sort rows\ncabnc_tbl &lt;-\n  cabnc_tbl |&gt;\n  arrange(doc_id, utt_num)\n\n# Preview the dataset\ncabnc_tbl |&gt; \n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 12\n&gt;    doc_id   utt_num utt_start utt_end utterance        part_id part_age part_sex\n&gt;    &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   \n&gt;  1 KB0RE000       0     0.208    2.67 You enjoyed you… PS002        721 female  \n&gt;  2 KB0RE000       1     2.66     2.90 Eh               PS006        601 male    \n&gt;  3 KB0RE000       2     2.90     3.33 did you          PS002        721 female  \n&gt;  4 KB0RE000       3     3.33     5.26 Oh I covered a … PS006        601 male    \n&gt;  5 KB0RE000       4     5.09     6.02 Oh very good ye… PS002        721 female  \n&gt;  6 KB0RE000       5     6.21     8.50 Er saw Mary and… PS006        601 male    \n&gt;  7 KB0RE000       6     8.32     9.31 Yes you did      PS002        721 female  \n&gt;  8 KB0RE000       7     8.48    11.2  in fact the who… PS006        601 male    \n&gt;  9 KB0RE000       8    10.3     14.3  Oh very nice ve… PS002        721 female  \n&gt; 10 KB0RE000       9    14.3     16.0  It is horrible … PS006        601 male    \n&gt; # ℹ 4 more variables: num_words &lt;dbl&gt;, num_utts &lt;dbl&gt;, avg_utt_len &lt;dbl&gt;,\n&gt; #   median_utt_len &lt;dbl&gt;\n\n\n\nApplying the sorting in Example 6.27, we can see that the utterances are now our desired order. We have now completed the curation of the structured dataset aiming to provide a base for further analysis into the recorded utterances of speakers from the CABNC corpus.\n\n6.2.4 Write dataset\nLet’s now write this dataset to disk. Again, since this is a dataset we have created, we will store the dataset in the data/derived/ directory. To keep the CABNC separate from any other datasets that we may need for our research project, I will create a subdirectory called cabnc/ to store the dataset. Into this directory we can write a CSV file with our cabnc_tbl data frame with the write_csv() function, as seen in Example 6.28.\n\nExample 6.28  \n\n# Create cabnc directory\ndir_create(path = \"../data/derived/cabnc\")\n\n# Write dataset to disk\ncabnc_tbl |&gt; \n  write_csv(path = \"../data/derived/cabnc/cabnc_curated.csv\")\n\n\nThe final step is to create a data dictionary file for this dataset. We can do this using the create_data_dictionary() function, passing the data frame object name and a path to the directory where we want to store the data dictionary. The create_data_dictionary() function will create a CSV file with a data dictionary template in the specified directory.\n\nExample 6.29  \n\n# Create data dictionary\ncreate_data_dictionary(\n  data = cabnc_tbl,\n  file_path = \"../data/derived/cabnc_curated_dd.csv\"\n)\n\n\nAfter running the code in Example 6.29, we can open the cabnc_curated_dd.csv file in the data/derived/cabnc/ directory and fill in the data dictionary template. An example of an edited data dictionary is shown in Table 6.6.\n\n\n\n\nTable 6.6: Data dictionary example for the curated CABNC dataset.\n\nvariable\nname\nvariable_type\ndescription\n\n\n\ndoc_id\nDocument ID\ncategorical\nUnique identifier for each document\n\n\nutt_num\nUtterance Number\nordinal\nSequential number assigned to each utterance in a document\n\n\nutt_start\nUtterance Start Time\nnumeric\nTimestamp indicating the start time of an utterance, measured in seconds\n\n\nutt_end\nUtterance End Time\nnumeric\nTimestamp indicating the end time of an utterance, measured in seconds\n\n\nutterance\nUtterance Text\ncategorical\nTextual content of an utterance\n\n\npart_id\nParticipant ID\ncategorical\nUnique identifier for each participant\n\n\npart_age\nParticipant Age\nnumeric\nAge of the participant, measured in months\n\n\npart_sex\nParticipant Sex\ncategorical\nSex of the participant. (male/ female)\n\n\nnum_words\nNumber of Words\nnumeric\nTotal number of words in utterances in the document\n\n\nnum_utts\nNumber of Utterances\nnumeric\nTotal number of utterances in the document\n\n\navg_utt_len\nAverage Utterance Length\nnumeric\nAverage length of an utterance in the document, measured in words\n\n\nmedian_utt_len\nMedian Utterance Length\nnumeric\nMedian length of an utterance in the document, measured in words\n\n\n\n\n\n\n\n\nAnd the final directory structure for the data/ directory is shown in Example 6.30.\n\nExample 6.30  \ndata/\n├── analysis/\n├── derived/\n│   ├── cabnc_curated_dd.csv\n│   └── cabnc/\n│       └── cabnc_curated.csv\n└── original/\n    ├── cabnc_do.csv\n    └── cabnc/\n        ├── participants.csv\n        ├── token_types.csv\n        ├── tokens.csv\n        ├── transcripts.csv\n        └── utterances.csv"
  },
  {
    "objectID": "curate-datasets.html#semi-structured",
    "href": "curate-datasets.html#semi-structured",
    "title": "6  Curate datasets",
    "section": "\n6.3 Semi-structured",
    "text": "6.3 Semi-structured\nAt this point we have discussed curating unstructured data and structured datasets. Between these two extremes falls semi-structured data. And as the name suggests, it is a hybrid between unstructured and structured data. This means that there will be important structured metadata included with unstructured elements. The file formats and approaches to encoding the structured aspects of the data vary widely from resource to resource and therefore often requires more detailed attention to the structure of the data and often includes more sophisticated programming strategies to curate the data to produce a tidy dataset.\n\n6.3.1 Reading data\nThe file formats associated with semi-structured data include a wide range. These include file formats conducive to more structured-leaning data, such as XML, HTML, and JSON, and file formats with more unstructured-leaning data, such as annotated TXT files. Annotated TXT files may in fact appear with the .txt extension, but may also appear with other, sometimes resource-specific, extensions, such as .utt for the Switchboard Dialogue Act Corpus or .cha for the CHILDES corpus annotation files, for example.\nThe more structured file formats use standard conventions and therefore can be read into an R session with format-specific functions. Say, for example, we are working with data in a JSON file format, as in File 6.3. We can read the data into an R session with the read_json() function from the jsonlite package (Ooms 2023). For XML and HTML files, the rvest package provides the read_xml() and read_html() functions.\n\nFile 6.3: Example JSON file\n\ndata.json\n\n{\n    \"data\": [\n        {\n            \"speaker\": \"Alice\",\n            \"age\": 27,\n            \"sex\": \"Female\",\n            \"word_form\": \"running\",\n            \"lemma_form\": \"run\"\n        },\n        {\n            \"speaker\": \"Bob\",\n            \"age\": 35,\n            \"sex\": \"Male\",\n            \"word_form\": \"jumped\",\n            \"lemma_form\": \"jump\"\n        },\n        {\n            \"speaker\": \"Charlie\",\n            \"age\": 42,\n            \"sex\": \"Male\",\n            \"word_form\": \"singing\",\n            \"lemma_form\": \"sing\"\n        },\n        {\n            \"speaker\": \"Diane\",\n            \"age\": 32,\n            \"sex\": \"Female\",\n            \"word_form\": \"walked\",\n            \"lemma_form\": \"walk\"\n        }\n    ]\n}\n\nSemi-structured data in TXT files can be read either as an entire file with read_file() or line-by-line with read_lines(). The choice of which approach to take depends on the structure of the data. If the data structure is line-based, then read_lines() often makes more sense than read_file(). However, in some cases, the data may be structured in a way that requires the entire file to be read into an R session and then subsequently parsed.\n\n6.3.2 Orientation\nTo provide an example of the curation process using semi-structured data, we will work with the ENNTT corpus, introduced in Section 3.2.2. Let’s look at the directory structure for the ENNTT corpus in Example 6.31.\n\nExample 6.31  \ndata/\n├── analysis/\n├── derived/\n└── original/\n    ├── enntt_do.csv\n    └── enntt/\n        ├── natives.dat\n        ├── natives.tok\n        ├── nonnatives.dat\n        ├── nonnatives.tok\n        ├── translations.dat\n        └── translations.tok\n\nWe now inspect the data origin file for the ENNTT corpus, enntt_do.csv, in Table 6.7.\n\n\n\n\nTable 6.7: Data origin file for the ENNTT corpus.\n\nattribute\ndescription\n\n\n\nResource name\nEuroparl corpus of Native, Non-native and Translated Texts - ENNTT\n\n\nData source\nhttps://github.com/senisioi/enntt-release\n\n\nData sampling frame\nEnglish, European Parliament texts, transcribed discourse, political genre\n\n\nData collection date(s)\nNot specified in the repository\n\n\nData format\n.tok, .dat\n\n\nData schema\n*.tok files contain the actual text; *.dat files contain the annotations corresponding to each line in the *.tok files.\n\n\nLicense\nNot specified. Contact the authors for more information.\n\n\nAttribution\nNisioi, S., Rabinovich, E., Dinu, L. P., & Wintner, S. (2016). A corpus of native, non-native and translated texts. Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016).\n\n\n\n\n\n\n\n\nAccording to the data origin file, there are two important file types, .dat and .tok. The .dat files contain annotations and the .tok files contain the actual text. Let’s inspect the first couple of lines in the .dat file for the native speakers, nonnatives.dat, in File 6.4.\n\nFile 6.4: Example .dat file for the non-native speakers.\n\n../data/original/enntt/nonnatives.dat\n\n&lt;LINE STATE=\"Poland\" MEPID=\"96779\" LANGUAGE=\"EN\" NAME=\"Danuta Hübner,\" SEQ_SPEAKER_ID=\"184\" SESSION_ID=\"ep-05-11-17\"/&gt;\n&lt;LINE STATE=\"Poland\" MEPID=\"96779\" LANGUAGE=\"EN\" NAME=\"Danuta Hübner,\" SEQ_SPEAKER_ID=\"184\" SESSION_ID=\"ep-05-11-17\"/&gt;\n\nWe see that the .dat file contains annotations for various session and speaker attributes. The format of the annotations is XML-like. XML is a form of markup language, such as YAML, JSON, etc. Markup languages are used to annotate text with additional information about the structure, meaning, and/ or presentation of text. In XML, structure is built up by nesting of nodes. The nodes are named with tags, which are enclosed in angle brackets, &lt; and &gt;. Nodes are opened with &lt;TAG&gt; and closed with &lt;/TAG&gt;. In Example 6.32 we see an example of a simple XML file structure.\n\nExample 6.32  \n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;book category=\"fiction\"&gt;\n  &lt;title lang=\"en\"&gt;The Catcher in the Rye&lt;/title&gt;\n  &lt;author&gt;J.D. Salinger&lt;/author&gt;\n  &lt;year&gt;1951&lt;/year&gt;\n&lt;/book&gt;\n\nIn Example 6.32 there are four nodes, three of which are nested inside of the &lt;book&gt; node. The &lt;book&gt; node in this example is the root node. XML files require a root node. Nodes can also have attributes, such as the category attribute in the &lt;book&gt; node, but they are not required. Furthermore, XML files also require a declaration, which is the first line in Example 6.32. The declaration specifies the version of XML used and the encoding.\nSo the .dat file is not strict XML, but is similar in that it contains nodes and attributes. An XML variant you a likely familiar with, HTML, has more relaxed rules than XML. HTML is a markup language used to annotate text with information about the organization and presentation of text on the web that does not require a root node or a declaration –much like our .dat file. So suffice it to say that the .dat file can safely be treated as HTML.\nAnd the .tok file for the native speakers, nonnatives.tok, in File 6.5, shows the actual text for each line in the corpus.\n\nFile 6.5: Example .tok file for the non-native speakers.\n\n../data/original/enntt/nonnatives.tok\n\nThe Commission is following with interest the planned construction of a nuclear power plant in Akkuyu , Turkey and recognises the importance of ensuring that the construction of the new plant follows the highest internationally accepted nuclear safety standards . \nAccording to our information , the decision on the selection of a bidder has not been taken yet .\n\nIn a study in which we are interested in contrasting the language of natives and non-natives, we will want to combine the .dat and .tok files for these groups of speakers and then join these datasets into one.\nThe question is what attributes we want to include in the curated dataset. Given the research focus, we will not need the LANGUAGE or NAME attributes. We may want to modify the attribute names so they are a bit more descriptive.\nAn idealized version of the dataset based on this criteria is shown in Table 6.8.\n\n\nTable 6.8: Idealized version of the ENNTT corpus.\n\n\n\n\n\n\n\n\n\nsession_id\nspeaker_id\nstate\ntype\nsession_seq\ntext\n\n\n\nep-05-11-17\n96779\nPoland\nnon-native\n184\nThe Commission is following with interest the planned construction of a nuclear power plant in Akkuyu , Turkey and recognises the importance of ensuring that the construction of the new plant follows the highest internationally accepted nuclear safety standards .\n\n\n…\n…\n…\n…\n…\n…\n\n\n\n\n\n6.3.3 Tidy the data\nNow that we have a better understanding of the corpus data and our target curated dataset structure, let’s work to extract and organize the data from the native and non-native files into one dataset.\nThe general approach we will take is, for native and then non-natives, to read in the .dat file as an HTML file and then extract the line nodes and their attributes combining them into a data frame. Then we’ll read in the .tok file as a text file and then combine the two into a single data frame. After producing a native and non-native data frame, we will combine them into one data frame to write and document as our curated dataset.\nStarting with the natives, we read in the .dat file as an XML file with the read_html() function and then extract the line nodes with the html_elements() function as in Example 6.33.\n\nExample 6.33  \n\n# Load packages\nlibrary(rvest)\n\n# Read in *.dat* file as HTML\nns_dat_lines &lt;-\n  read_html(\"../data/original/enntt/natives.dat\") |&gt;\n  html_elements(\"line\")\n\n# Inspect\nclass(ns_dat_lines)\ntypeof(ns_dat_lines)\nlength(ns_dat_lines)\n\n\n\n&gt; [1] \"xml_nodeset\"\n&gt; [1] \"list\"\n&gt; [1] 116341\n\n\n\nWhen can see that the ns_dat_lines object is a special type of list, xml_nodeset which contains 116,341 line nodes. Let’s now jump out of sequence and read in the .tok file as a text file, in Example 6.34, again by lines using read_lines(), and compare the two to make sure that our approach will work.\n\nExample 6.34  \n\n# Read in *.tok* file by lines\nns_tok_lines &lt;- \n  read_lines(\"data/enntt/original/natives.tok\")\n\n# Inspect\nclass(ns_tok_lines)\ntypeof(ns_tok_lines)\nlength(ns_tok_lines)\n\n&gt; [1] \"character\"\n&gt; [1] \"character\"\n&gt; [1] 116341\n\n\n\nWe do, in fact, have the same number of lines in the .dat and .tok files. So we can proceed with extracting the attributes from the line nodes and combining them with the text from the .tok file.\nLet’s start by listing the attributes of the first line node in the ns_dat_lines object. We use the html_attrs() function to get the attribute names and the values, as in Example 6.35.\n\nExample 6.35  \n\n# List attributes of first line node\nns_dat_lines[[1]] |&gt; \n  html_attrs()\n\n&gt;             state             mepid          language              name \n&gt;  \"United Kingdom\"            \"2099\"              \"EN\" \"Evans, Robert J\" \n&gt;    seq_speaker_id        session_id \n&gt;               \"2\"     \"ep-00-01-17\"\n\n\n\nNo surprise here, these are the same attributes we saw in the .dat file preview in File 6.4. At this point, it’s good to make a plan on how to associate the attribute names with the column names in our curated dataset.\n\n\nsession_id = session_id\n\n\nspeaker_id = MEPID\n\n\nstate = state\n\n\nsession_seq = seq_speaker_id\n\n\nWe can do this one attribute at a time using the html_attr() function and then combine them into a data frame with the tibble() function as in Example 6.36.\n\nExample 6.36  \n\n# Extract attributes from first line node\nsession_id &lt;- ns_dat_lines[[1]] |&gt; html_attr(\"session_id\")\nspeaker_id &lt;- ns_dat_lines[[1]] |&gt; html_attr(\"mepid\")\nstate &lt;- ns_dat_lines[[1]] |&gt; html_attr(\"state\")\nsession_seq &lt;- ns_dat_lines[[1]] |&gt; html_attr(\"seq_speaker_id\")\n\n# Combine into data frame\ntibble(session_id, speaker_id, state, session_seq)\n\n&gt; # A tibble: 1 × 4\n&gt;   session_id  speaker_id state          session_seq\n&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;      \n&gt; 1 ep-00-01-17 2099       United Kingdom 2\n\n\n\nThe results from Example 6.36 show that the attributes have been extracted and mapped to our idealized column names, but this would be tedious to do for each line node. A function to extract attributes and values from a line and add them to a data frame would help simplify this process. The function in Example 6.37 does just that.\n\nExample 6.37  \n\n# Function to extract attributes from line node\nextract_dat_attrs &lt;- function(line_node) {\n  session_id &lt;- line_node |&gt; html_attr(\"session_id\")\n  speaker_id &lt;- line_node |&gt; html_attr(\"mepid\")\n  state &lt;- line_node |&gt; html_attr(\"state\")\n  session_seq &lt;- line_node |&gt; html_attr(\"seq_speaker_id\")\n\n  tibble(session_id, speaker_id, state, session_seq)\n}\n\n\nIt’s a good idea to test out the function to verify that it works as expected. We can do this by passing the various indices to the ns_dat_lines object to the function as in Example 6.38.\n\nExample 6.38  \n\n# Test function\nextract_dat_attrs(ns_dat_lines[[1]])\n\n&gt; # A tibble: 1 × 4\n&gt;   session_id  speaker_id state          session_seq\n&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;      \n&gt; 1 ep-00-01-17 2099       United Kingdom 2\n\nextract_dat_attrs(ns_dat_lines[[20]])\n\n&gt; # A tibble: 1 × 4\n&gt;   session_id  speaker_id state          session_seq\n&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;      \n&gt; 1 ep-00-01-17 1309       United Kingdom 40\n\nextract_dat_attrs(ns_dat_lines[[100]])\n\n&gt; # A tibble: 1 × 4\n&gt;   session_id  speaker_id state          session_seq\n&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;      \n&gt; 1 ep-00-01-18 4549       United Kingdom 28\n\n\n\nLooks like the extract_dat_attrs() function is ready for prime-time. Let’s now apply it to all of the line nodes in the ns_dat_lines object using the map_dfr() function from the purrr package as in Example 6.39.\n\nExample 6.39  \n\n# Extract attributes from all line nodes\nns_dat_attrs &lt;- \n  ns_dat_lines |&gt; \n  map_dfr(extract_dat_attrs)\n\n# Inspect\nglimpse(ns_dat_attrs)\n\n&gt; Rows: 116,341\n&gt; Columns: 4\n&gt; $ session_id  &lt;chr&gt; \"ep-00-01-17\", \"ep-00-01-17\", \"ep-00-01-17\", \"ep-00-01-17\"…\n&gt; $ speaker_id  &lt;chr&gt; \"2099\", \"2099\", \"2099\", \"4548\", \"4548\", \"4541\", \"4541\", \"4…\n&gt; $ state       &lt;chr&gt; \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"Uni…\n&gt; $ session_seq &lt;chr&gt; \"2\", \"2\", \"2\", \"4\", \"4\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12…\n\n\n\n\n\n\n\n\n\n Dive deeper\nThe map*() functions from the purrr package are a family of functions that apply a function to each element of a vector, list, or data frame. The map_dfr() function is a variant of the map() function that returns a data frame that is the result of row-binding the results, hence _dfr.\n\n\n\nWe can see that the ns_dat_attrs object is a data frame with 116,341 rows and 4 columns, just has we expected. We can now combine the ns_dat_attrs data frame with the ns_tok_lines vector to create a single data frame with the attributes and the text. This is done with the mutate() function assigning the ns_tok_lines vector to a new column named text as in Example 6.40.\n\nExample 6.40  \n\n# Combine attributes and text\nns_dat &lt;- \n  ns_dat_attrs |&gt; \n  mutate(text = ns_tok_lines)\n\n# Inspect\nglimpse(ns_dat)\n\n&gt; Rows: 116,341\n&gt; Columns: 5\n&gt; $ session_id  &lt;chr&gt; \"ep-00-01-17\", \"ep-00-01-17\", \"ep-00-01-17\", \"ep-00-01-17\"…\n&gt; $ speaker_id  &lt;chr&gt; \"2099\", \"2099\", \"2099\", \"4548\", \"4548\", \"4541\", \"4541\", \"4…\n&gt; $ state       &lt;chr&gt; \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"Uni…\n&gt; $ session_seq &lt;chr&gt; \"2\", \"2\", \"2\", \"4\", \"4\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12…\n&gt; $ text        &lt;chr&gt; \"You will be aware from the press and television that ther…\n\n\n\nThis is the data for the native speakers. We can now repeat this process for the non-native speakers, or we can create a function to do it for us. Let’s explore the later option.\nThis is the data for the native speakers. We can now repeat this process for the non-native speakers, or we can create a function to do it for us. Let’s explore the later option.\nI will name this function combine_dat_tok() and it will take two arguments: dat_file and tok_file. The dat_file argument will be the path to the .dat file and the tok_file argument will be the path to the .tok file. The function first reads both the files into R as lines. Then it extracts the attributes from the .dat file using the extract_dat_attrs() function we created earlier. Finally, it combines the attributes with the text from the .tok file and returns a data frame.\nNote that there is at least one tweak we need to make to our existing code base if we plan on combining the native and non-native data. As our function stands, it will return a data frame with the same column names for both the native and non-native data. We will need to add a column to which we will indicate what type of data it is (native or non-native). This is where the type column we planned earlier comes in. There are multiple ways to do this. We could add a column to the data frame before we return it, or we could add a column to the data frame after we return it. I will opt for the former and embed it inside of our function combine_dat_tok().\n\nExample 6.41  \n\n# Load packages\nlibrary(rvest) # for reading/parsing HTML\nlibrary(purrr) # for mapping functions\nlibrary(fs) # for working with files\n\n# Function to combine *.dat* and *.tok* files\ncombine_dat_tok &lt;- function(dat_file, tok_file) {\n  # Read in files by lines\n  dat &lt;-\n    read_html(dat_file) |&gt;\n    html_nodes(\"line\")\n  tok &lt;- read_lines(tok_file)\n\n  # Create function to extract attributes\n  extract_dat_attrs &lt;- function(line_node) {\n    session_id &lt;- line_node |&gt; html_attr(\"session_id\")\n    speaker_id &lt;- line_node |&gt; html_attr(\"mepid\")\n    state &lt;- line_node |&gt; html_attr(\"state\")\n    session_seq &lt;- line_node |&gt; html_attr(\"seq_speaker_id\")\n\n    tibble(session_id, speaker_id, state, session_seq)\n  }\n\n  # Apply function to all line nodes\n  dat_attrs &lt;-\n    dat |&gt;\n    map_dfr(extract_dat_attrs)\n\n  # Combine attrs and text into data frame\n  dataset &lt;-\n    dat_attrs |&gt;\n    bind_cols(text = tok)\n\n  # Add type column with value of file name (w/o extension)\n  dataset &lt;- \n    dataset |&gt;\n    mutate(type = path_file(dat_file) |&gt; path_ext_remove())\n\n  # Return data frame\n  return(dataset)\n}\n\n\nApply the combine_dat_tok() function to read in the data for the native speakers and the non-native speakers in a few lines of code, as in Example 6.42.\n\nExample 6.42  \n\n# Native speakers\nns_dat &lt;-\n  combine_dat_tok(\n    dat_file = \"../data/original/enntt/natives.dat\",\n    tok_file = \"../data/original/enntt/natives.tok\"\n  )\n# Preview\nglimpse(ns_dat)\n\n# Non-native speakers\nnns_dat &lt;-\n  combine_dat_tok(\n    dat_file = \"../data/original/enntt/nonnatives.dat\",\n    tok_file = \"../data/original/enntt/nonnatives.tok\"\n  )\n# Preview\nglimpse(nns_dat)\n\n\n\n&gt; Rows: 116,341\n&gt; Columns: 6\n&gt; $ session_id  &lt;chr&gt; \"ep-00-01-17\", \"ep-00-01-17\", \"ep-00-01-17\", \"ep-00-01-17\"…\n&gt; $ speaker_id  &lt;chr&gt; \"2099\", \"2099\", \"2099\", \"4548\", \"4548\", \"4541\", \"4541\", \"4…\n&gt; $ state       &lt;chr&gt; \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"Uni…\n&gt; $ session_seq &lt;chr&gt; \"2\", \"2\", \"2\", \"4\", \"4\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12…\n&gt; $ text        &lt;chr&gt; \"You will be aware from the press and television that ther…\n&gt; $ type        &lt;chr&gt; \"natives\", \"natives\", \"natives\", \"natives\", \"natives\", \"na…\n\n\n&gt; Rows: 29,734\n&gt; Columns: 6\n&gt; $ session_id  &lt;chr&gt; \"ep-00-01-18\", \"ep-00-01-18\", \"ep-00-01-18\", \"ep-00-01-18\"…\n&gt; $ speaker_id  &lt;chr&gt; \"653\", \"653\", \"653\", \"653\", \"653\", \"653\", \"653\", \"653\", \"6…\n&gt; $ state       &lt;chr&gt; \"Belgium\", \"Belgium\", \"Belgium\", \"Belgium\", \"Belgium\", \"Be…\n&gt; $ session_seq &lt;chr&gt; \"157\", \"157\", \"157\", \"157\", \"157\", \"157\", \"157\", \"157\", \"1…\n&gt; $ text        &lt;chr&gt; \"The Commission is following with interest the planned con…\n&gt; $ type        &lt;chr&gt; \"nonnatives\", \"nonnatives\", \"nonnatives\", \"nonnatives\", \"n…\n\n\n\nThe last step to curate the dataset is to combine the native and non-native data into a single data frame. We can do this using the bind_rows() function from the dplyr package, as essentially we are just stacking the rows from each data frame on top of each other.\n\n# Combine native and non-native data\nenntt_tbl &lt;-\n  bind_rows(ns_dat, nns_dat)\n\nThe ennnt_tbl data frame is the curated dataset. We can perform some data checks to make everything looks good and then proceed to writing and documenting the dataset.\n\n\n\n\n\n\n Consider this\nWhat data checks could we perform to ensure that the data is in the format we expect? What are some of the things we should be looking for?\n\n\n\n\n6.3.4 Write dataset\nAs we have done for the other curated datasets, we will write our curated dataset to a .csv file. We will use the write_csv() function and name the file appropriately, as in Example 6.43.\n\nExample 6.43  \n\n# Create directory\ndir_create(\"../data/derived/enntt\")\n\n# Write dataset to *.csv* file\nwrite_csv(enntt_tbl, \"../data/derived/enntt/enntt_curated.csv\")\n\n\nDocument the dataset in a data dictionary using the create_data_dictionary() function from the qtalrkit package, as in Example 6.44.\n\nExample 6.44  \n\n# Load package\nlibrary(qtalrkit)\n\n# Create data dictionary\ncreate_data_dictionary(\n  data = enntt_tbl,\n  file_path = \"../data/derived/enntt_curated_dd.csv\"\n)\n\n\nAfter editing the data dictionary file to include the appropriate information, we will have something similar to Table 6.9.\n\n\n\n\nTable 6.9: Data dictionary for the curated ENNTT dataset\n\nvariable\nname\nvariable_type\ndescription\n\n\n\nsession_id\nSession ID\ncategorical\nUnique identifier for each session\n\n\nspeaker_id\nSpeaker ID\ncategorical\nUnique identifier for each speaker\n\n\nstate\nState\ncategorical\nName of the state or country the session is linked to\n\n\nsession_seq\nSession Sequence\nordinal\nSequence number in the session\n\n\ntext\nText\ncategorical\nText transcript of the session\n\n\ntype\nType\ncategorical\nThe type of the speaker, whether native or nonnative\n\n\n\n\n\n\n\n\nThe final step is to clean up and comment the code added to the 2-curate-datasets.qmd file. The final directory structure is seen in Example 6.45.\n\nExample 6.45  \nproject/\n├── code/\n│   ├── 1-acquire-data.qmd\n│   ├── 2-curate-data.qmd\n│   └── ...\n├── data/\n│   ├── analysis/\n│   ├── derived/\n│       │── enntt_curated_dd.csv\n│   │   └── enntt/\n│   │       └── enntt_curated.csv\n│   └── original/\n│       │── enntt_do.csv\n│       └── enntt/\n│           ├── natives.dat\n│           ├── natives.tok\n│           ├── nonnatives.dat\n│           ├── nonnatives.tok\n│           ├── translations.dat\n│           └── translations.tok\n├── output/\n│   ├── figures/\n│   ├── reports/\n│   ├── results/\n│   └── tables/\n├── README.md\n└── _main.R"
  },
  {
    "objectID": "curate-datasets.html#summary",
    "href": "curate-datasets.html#summary",
    "title": "6  Curate datasets",
    "section": "Summary",
    "text": "Summary\nIn this chapter we looked at the process of structuring data into a dataset. This included a discussion on three main types of data –unstructured, structured, and semi-structured. The level of structure of the original data(set) will vary from resource to resource and by the same token so will the file format used to support the level of metadata included. The results from data curation results in a dataset that is saved separate from the original data to maintain modularity between what the data(set) looked like before we intervene and afterwards. Since there can be multiple analysis approaches applied the original data in a research project, this curated dataset serves as the point of departure for each of the subsequent datasets derived from the transformational steps. In addition to the code we use to derived the curated dataset’s structure, we also include a data dictionary which documents the curated dataset."
  },
  {
    "objectID": "curate-datasets.html#activities",
    "href": "curate-datasets.html#activities",
    "title": "6  Curate datasets",
    "section": "Activities",
    "text": "Activities\n\n\n Add description of outcomes\n\n\n\n\n\n\n\n Recipe\n\n\nWhat: Organizing and documenting datasetsHow: Read Recipe 6 and participate in the Hypothes.is online social annotation.Why: To rehearse methods for deriving tidying datasets to use a the base for further project-specific purposes. We will explore how regular expressions are helpful in developing strategies for matching, extracting, and/ or replacing patterns in character sequences and how to change the dimensions of a dataset to either expand or collapse columns or rows.\n\n\n\n\n\n\n\n\n\n Lab\n\n\n\nWhat: Pattern Matching and Manipulate DatasetsHow: Clone, fork, and complete the steps in Lab 6.Why: To gain experience working with coding strategies reshaping data using tidyverse functions and regular expressions, to practice reading/ writing data from/ to disk, and to implement organizational strategies for organizing and documenting a dataset in reproducible fashion."
  },
  {
    "objectID": "curate-datasets.html#questions",
    "href": "curate-datasets.html#questions",
    "title": "6  Curate datasets",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\n Conceptual questions\n\n…\n…\n\n\n\n\n\n\n\n\n\n\n Technical questions\n\n…\n…\n\n\n\n\n\n\n\n\nBenoit, Kenneth, and Adam Obeng. 2023. Readtext: Import and Handling for Plain and Formatted Text Files. https://github.com/quanteda/readtext.\n\n\nKoehn, P. 2005. “Europarl: A Parallel Corpus for Statistical Machine Translation.” MT Summit X, 12–16.\n\n\nOoms, Jeroen. 2023. Jsonlite: A Simple and Robust JSON Parser and Generator for r. https://jeroen.r-universe.dev/jsonlite https://arxiv.org/abs/1403.2805.\n\n\nR Special Interest Group on Databases (R-SIG-DB), Hadley Wickham, and Kirill Müller. 2022. DBI: R Database Interface. https://dbi.r-dbi.org.\n\n\nSilge, Julia. 2022. Janeaustenr: Jane Austen’s Complete Novels. https://github.com/juliasilge/janeaustenr.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2022. Skimr: Compact and Flexible Summaries of Data. https://docs.ropensci.org/skimr/.\n\n\nWickham, Hadley. 2022. Stringr: Simple, Consistent Wrappers for Common String Operations. https://stringr.tidyverse.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nWickham, Hadley, Maximilian Girlich, and Edgar Ruiz. 2023. Dbplyr: A Dplyr Back End for Databases. https://dbplyr.tidyverse.org/.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. Haven: Import and Export SPSS, Stata and SAS Files. https://haven.tidyverse.org.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. Tidyr: Tidy Messy Data. https://tidyr.tidyverse.org."
  },
  {
    "objectID": "curate-datasets.html#footnotes",
    "href": "curate-datasets.html#footnotes",
    "title": "6  Curate datasets",
    "section": "",
    "text": "Note I’ve modified the output of skim() for display purposes.↩︎"
  },
  {
    "objectID": "transform-datasets.html#sec-td-normalization",
    "href": "transform-datasets.html#sec-td-normalization",
    "title": "7  Transform datasets",
    "section": "\n7.1 Normalization",
    "text": "7.1 Normalization\nThe process of normalizing datasets in essence is to sanitize the values of variable or set of variables such that there are no artifacts that will contaminate subsequent processing. It may be the case that non-linguistic metadata may require normalization but more often than not linguistic information is the most common target for normalization as text often includes artifacts from the acquisition process which will not be desired in the analysis.\n\n7.1.1 Orientation\nTo explore some of the strategies of normalization, we will look at the Europarl Corpus. As we are working working towards transforming a curated dataset, we will start by getting oriented to the dataset. Following the reproducible research principles, I will assume that the curated dataset and its associated data dictionary are in the data/derived/ directory, as seen in Example 7.1.\n\nExample 7.1  \ndata/\n├── analysis/\n├── derived/\n│   ├── europarl_curated_dd.csv\n│   └── europarl/\n│       └── europarl_curated.csv\n└── original/\n\nThe contents of the data dictionary for this dataset appears in Table 7.1.\n\n\n\n\nTable 7.1: Data dictionary for the Europarl Corpus.\n\nvariable\nname\nvariable_type\ndescription\n\n\n\ndoc_id\nDocument ID\nnumeric\nUnique identification number for each document\n\n\ntype\nDocument Type\ncategorical\nType of document; either 'Source' (Spanish) or 'Target' (English)\n\n\nline_id\nLine ID\nnumeric\nUnique identification number for each line in each document type\n\n\nlines\nLines\ncategorical\nContent of the lines in the document\n\n\n\n\n\n\n\n\nThis dataset contains transcribed source language (Spanish) and translated target language (English) from the proceedings of the European Parliament. The unit of observation is the lines variable whose values are are lines of dialog.\nLet’s read in the dataset CSV file with read_csv() and inspect the first lines of the dataset with slice_head() in Example 7.2.\n\nExample 7.2  \n\n# Read in the dataset\neuroparl_curated_tbl &lt;-\n  read_csv(file = \"../data/derived/europarl_curated.csv\")\n\n# Preview the first 10 lines\neuroparl_curated_tbl |&gt;\n  slice_head(n = 10)\n\n\n\n&gt; # A tibble: 10 × 4\n&gt;     doc_id type   line_id lines                                                 \n&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                                                 \n&gt;  1 1965735 Source       1 \"Reanudación del período de sesiones\"                 \n&gt;  2       1 Target       1 \"Resumption of the session\"                           \n&gt;  3 1965736 Source       2 \"Declaro reanudado el período de sesiones del Parlame…\n&gt;  4       2 Target       2 \"I declare resumed the session of the European Parlia…\n&gt;  5 1965737 Source       3 \"Como todos han podido comprobar, el gran \\\"efecto de…\n&gt;  6       3 Target       3 \"Although, as you will have seen, the dreaded 'millen…\n&gt;  7 1965738 Source       4 \"Sus Señorías han solicitado un debate sobre el tema …\n&gt;  8       4 Target       4 \"You have requested a debate on this subject in the c…\n&gt;  9 1965739 Source       5 \"A la espera de que se produzca, de acuerdo con mucho…\n&gt; 10       5 Target       5 \"In the meantime, I should like to observe a minute' …\n\n\n\nSimply looking at the first 10 lines of this dataset gives us a clearer sense of the dataset structure, but, in terms of normalization procedures we might apply, it is likely not sufficient. We want to get a sense of any potential inconsistencies in the dataset, in particular in the lines variable. Since this is a large dataset with 3931468 observations, we will need to explore the dataset in manageable chunks. The slice_sample() function will allow us to randomly sample a subset of the dataset of a certain number of observations specified by the n = argument, as seen in Example 7.3.\n\nExample 7.3  \n\n# Randomly sample 5 observations\neuroparl_curated_tbl |&gt;\n  slice_sample(n = 5)\n\n&gt; # A tibble: 5 × 4\n&gt;    doc_id type   line_id lines                                                  \n&gt;     &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                                                  \n&gt; 1 1979130 Source   13396 Nosotros no apostamos aquí, en este Parlamento, por lo…\n&gt; 2 3322228 Source 1356494 (SK) Señor Presidente, me he abstenido de votar porque…\n&gt; 3  140652 Target  140652 It is these last two, however, together with Finland, …\n&gt; 4 2145661 Source  179927 Son las mujeres de esos mismos países: Egipto, Somalia…\n&gt; 5 2269815 Source  304081 Permítanme añadir que, en nuestro último llamamiento p…\n\n\n\nWe should run the code in Example 7.3 multiple times to get a sense of the variation in the dataset.\n\n\n\n\n\n\n Tip\nR functions which return samples (e.g. slice_sample()) are generated using pseudo-random number generators. These generators are initialized with a seed value. You can control the seed value R uses to generate the random numbers by using the set.seed() function and setting the seed value to a number of your choice. It is important to note that setting a seed only affects the subsequent random number generation.\nUsing set.seed() is useful when you want to ensure that the same random numbers are generated each time you run the code. This is particularly helpful when you want to reproduce results in a report or other document.\n\n\n\nIn the case of the Europarl corpus dataset, it may be useful to see the source and target lines in the same sample. Do do this, we can first sample from the line_id variable and then filter the europarl_curated_tbl wit the filter() function and the %in% operator to select the lines that match the sampled line_id values, as seen in Example 7.4.\n\nExample 7.4  \n\n# Randomly sample 5 line_id values\nline_id_sample_vec &lt;- \n  europarl_curated_tbl |&gt;\n  distinct(line_id) |&gt; \n  slice_sample(n = 5) |&gt; \n  pull(line_id)\n\n# Select the lines that match the sampled line_id values\neuroparl_curated_tbl |&gt;\n  filter(line_id %in% line_id_sample_vec)\n\n&gt; # A tibble: 10 × 4\n&gt;     doc_id type   line_id lines                                                 \n&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                                                 \n&gt;  1 1992525 Source   26791 La sociedad del conocimiento es algo más, y también p…\n&gt;  2   26791 Target   26791 The knowledge society is more than that, and it may a…\n&gt;  3 2247038 Source  281304 Especialmente en la industria de alta tecnología pued…\n&gt;  4  281304 Target  281304 Particularly in the technical and hi-tech industry, t…\n&gt;  5 2325587 Source  359853 - (IT) Señora Presidenta, Comisario, Señorías, quiero…\n&gt;  6  359853 Target  359853 . (IT) Madam President, Commissioner, ladies and gent…\n&gt;  7 2573895 Source  608161 En consecuencia, nuestro Grupo ha votado en contra.   \n&gt;  8  608161 Target  608161 Consequently, our group voted against.                \n&gt;  9 2581569 Source  615835 Al margen de ello, es todo el club de los seis, esos …\n&gt; 10  615835 Target  615835 Beyond that, it is the whole club of six, these count…\n\n\n\nAfter running the code in Example 7.3 and Example 7.4 multiple times, I identified a number of artifacts that we will want to consider addressing. These are included in Table 7.2.\n\n\nTable 7.2: Characteristics of the Europarl Corpus dataset that may require normalization.\n\n\n\n\n\n\nDescription\nExamples\nConcern\n\n\n\nNon-speech annotations\n\n(Abucheos), (A4-0247/98), (The sitting was opened at 09:00)\n\nNot of interest for our analysis\n\n\nInconsistent whitespace\n\n5 % ,,      , Palacio' s\n\nMay be problematic for tokenization\n\n\nNon-sentence punctuation\n-\nMay be problematic for tokenization\n\n\nAbbreviations\n\nMr., Sr., Mme., Mr, Sr, Mme, Mister, Señor, Madam\n\nMay be problematic for tokenization\n\n\nText case\n\nThe, the, White, white\n\nMay be problematic for tokenization\n\n\n\n\nThe first three items in Table 7.2 are relatively straightforward. Non-speech annotations most likely are not relevant for our research. Inconsistent whitespace and non-sentence punctuation may be problematic for tokenization that depends on whitespace and punctuation regularities.\nThe other two considerations are more contingent on our research aims. The existence of various forms for the same word, abbreviated and unabreviated, introduces variability may not be of interest for our analysis. Secondly, common conventions for capitalization in prose can introduce unwanted variability. If we leave the text as is, the tokens The and the will be treated as distinct. If we convert the text to lowercase, the tokens White and white will be treated as the same, even if White corresponds to a proper noun (e.g. White House).\nThese observations provide us a roadmap for the normalization process. For demonstration, let’s focus only on a couple of these cases: removing parlimentary session descriptions and extra whitespace.\n\n7.1.2 Application\nIdentifying our normalization goals is an important first step. The next step is to identify the procedures that will accomplish these goals. The majority of text normalization procedures can be accomplished with the stringr package (Wickham 2022). This package provides a number of functions for manipulating text. The workhorse functions we will use for our tasks are the str_remove() and str_replace() functions. As the these functions give us the ability to remove or replace text. Our task is to identify the patterns we want to remove or replace.\nBefore we modify any lines, let’s try craft a search pattern to identify the text of interest. This is done to avoid over- or under-generalizing the search pattern. If we are too general, we may end up removing or replacing text that we want to keep. If we are too specific, we may not remove or replace all the text we want to remove or replace.\n\nLet’s start by identifying non-parlimentary speech. Two functions from the stringr package come in handy here: str_detect() and str_extract(). str_detect() detects a pattern in a character vector and returns a logical vector, TRUE if the pattern is detected and FALSE if it is not. str_extract() extracts the text in a character vector that matches a pattern.\nstr_detect() pairs well with the filter() function to return observations that match a pattern in a character vector. str_extract() pairs well with the mutate() function to create a new variable which contains character vector that match a pattern.\nLet’s start with the str_detect() function. We will use this function to identify the lines that contain the parliamentary session descriptions. From the examples above, we can see that these instances are wrapped with parentheses ( and ). The text within the parentheses can vary, so we need a Regular Expression to do the heavy lifting. To start out we can match any one or multiple characters with .+. But it is important to recognize the + (and also the *) operators are ‘greedy’, meaning that if there are multiple matches, the longest match will be returned. In this case, we want to match the shortest match. To do this we can use the ? operator to make the + operator ‘lazy’. This will match the shortest match.\nOur test code appears in Example 7.5.\n\nExample 7.5  \n\n# Identify non-speech lines\neuroparl_curated_tbl |&gt;\n  filter(str_detect(lines, \"\\\\(.+?\\\\)\")) |&gt;\n  slice_sample(n = 10) \n\n&gt; # A tibble: 10 × 4\n&gt;     doc_id type   line_id lines                                                 \n&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                                                 \n&gt;  1 3225772 Source 1260038 (PT) Señor Presidente, quisiera plantear dos pregunta…\n&gt;  2 3715842 Source 1750108 (El Parlamento decide la devolución a la Comisión)    \n&gt;  3 1961715 Target 1961715 (Parliament adopted the resolution)                   \n&gt;  4 1429470 Target 1429470 27, originally Greens/EFA amendment in FEMM); binding…\n&gt;  5   51632 Target   51632 Question No 8 by (H-0376/00):                         \n&gt;  6 2482671 Source  516937 La Comisión propone proporcionar a las Agencias nacio…\n&gt;  7 1059628 Target 1059628 (The President cut off the speaker)                   \n&gt;  8 1507254 Target 1507254 in writing. - (LT) I welcomed this document, because …\n&gt;  9 2765325 Source  799591 (Aplausos)                                            \n&gt; 10 2668536 Source  702802    Las preguntas que, por falta de tiempo, no han rec…\n\n\n\nThe results from Example 7.5 show that we have identified the lines that contain at least one of the parliamentary session description annotations. A more targeted search to identify specific instances of the parliamentary session descriptions can be accomplished adding the str_extract() function as seen in Example 7.6.\n\nExample 7.6  \n\n# Extract non-speech fragments\neuroparl_curated_tbl |&gt;\n  filter(str_detect(lines, \"\\\\(.+?\\\\)\")) |&gt;\n  mutate(non_speech = str_extract(lines, \"\\\\(.+?\\\\)\")) |&gt;\n  slice_sample(n = 10) \n\n&gt; # A tibble: 10 × 5\n&gt;     doc_id type   line_id lines                                       non_speech\n&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                                       &lt;chr&gt;     \n&gt;  1 3225772 Source 1260038 (PT) Señor Presidente, quisiera plantear d… (PT)      \n&gt;  2 3715842 Source 1750108 (El Parlamento decide la devolución a la C… (El Parla…\n&gt;  3 1961715 Target 1961715 (Parliament adopted the resolution)         (Parliame…\n&gt;  4 1429470 Target 1429470 27, originally Greens/EFA amendment in FEM… (para. 53)\n&gt;  5   51632 Target   51632 Question No 8 by (H-0376/00):               (H-0376/0…\n&gt;  6 2482671 Source  516937 La Comisión propone proporcionar a las Age… (correspo…\n&gt;  7 1059628 Target 1059628 (The President cut off the speaker)         (The Pres…\n&gt;  8 1507254 Target 1507254 in writing. - (LT) I welcomed this documen… (LT)      \n&gt;  9 2765325 Source  799591 (Aplausos)                                  (Aplausos)\n&gt; 10 2668536 Source  702802    Las preguntas que, por falta de tiempo,… (Véase el…\n\n\n\nThe results from Example 7.6 show that we have identified the lines that contain parliamentary session description annotations and extracted this text –or have we? What if a given line contains more than one parliamentary session description annotation? It turns out that str_extract() only returns the first match. To return all matches we can use the str_extract_all() function. Let’s try again, in Example 7.7.\n\nExample 7.7  \n\n# Extract non-speech fragments\neuroparl_curated_tbl |&gt;\n  filter(str_detect(lines, \"\\\\(.+?\\\\)\")) |&gt;\n  mutate(non_speech = str_extract_all(lines, \"\\\\(.+?\\\\)\")) |&gt; \n  slice_sample(n = 10)\n\n&gt; # A tibble: 10 × 5\n&gt;     doc_id type   line_id lines                                       non_speech\n&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;                                       &lt;list&gt;    \n&gt;  1 3225772 Source 1260038 (PT) Señor Presidente, quisiera plantear d… &lt;chr [1]&gt; \n&gt;  2 3715842 Source 1750108 (El Parlamento decide la devolución a la C… &lt;chr [1]&gt; \n&gt;  3 1961715 Target 1961715 (Parliament adopted the resolution)         &lt;chr [1]&gt; \n&gt;  4 1429470 Target 1429470 27, originally Greens/EFA amendment in FEM… &lt;chr [1]&gt; \n&gt;  5   51632 Target   51632 Question No 8 by (H-0376/00):               &lt;chr [1]&gt; \n&gt;  6 2482671 Source  516937 La Comisión propone proporcionar a las Age… &lt;chr [2]&gt; \n&gt;  7 1059628 Target 1059628 (The President cut off the speaker)         &lt;chr [1]&gt; \n&gt;  8 1507254 Target 1507254 in writing. - (LT) I welcomed this documen… &lt;chr [1]&gt; \n&gt;  9 2765325 Source  799591 (Aplausos)                                  &lt;chr [1]&gt; \n&gt; 10 2668536 Source  702802    Las preguntas que, por falta de tiempo,… &lt;chr [1]&gt;\n\n\n\nOK, that might not be what you expected. The str_extract_all() function returns a list of character vectors. This is because for any given line in lines there may be a different number of matches. To maintain the data frame as rectangular, a list is returned for each value of non_speech. We could expand the list into a data frame with the unnest() function from the tidyr package if our goal were to work with these matches. But that is not our aim. Rather, we want to know if we have multiple matches per line. Note that the information provided for the non_speech column by the tibble object tells use that we have some lines with muliple matches, as we can see in line 6 of our small sample. So good thing we checked!\nLet’s now remove these parliamentary session description annotations from each line in the lines column. We turn to str_remove_all(), a variant of str_remove(), that, as you expect, will remove multiple matches in a single line. We will use the mutate() function to overwrite the lines column with the modified text. The code is seen in Example 7.8.\n\nExample 7.8  \n\n# Remove non-speech fragments\neuroparl_curated_tbl &lt;- \n  europarl_curated_tbl |&gt;\n  mutate(lines = str_remove_all(lines, \"\\\\(.+?\\\\)\"))\n\n\nI recommend spot checking the results of this normalization step by running the code in Example 7.5 again, if nothing appears we’ve done our job.\nWhen you are content with the results, drop the observations that have no text in the lines column given the entire line was non-speech. This can be done with the is.na() function and the filter() function as seen in Example 7.9.\n\nExample 7.9  \n\n# Drop empty lines\neuroparl_curated_tbl &lt;- \n  europarl_curated_tbl |&gt;\n  filter(!is.na(lines))\n\n\n\nThe second item of business to address is the extra whitespace we observed in Table 7.2. If we consider the extra whitespace cases, we can categorize them into two types: multiple spaces that should be a single space (e.g.      ) and single spaces that occur within a word (e.g. 5 % , or Palacio' s).\nTo deal with muliple spaces, we can turn to the str_replace_all() function. This function will replace a pattern with a replacement string for every pattern match. In this case, we want to replace multiple spaces with a single space. We can use the \\\\s+ pattern to match one or more spaces and then replace it with \\\\s or a single whitespace character \" \".\nBefore we apply this normalization step, let’s assess how many instances of multiple spaces we have in the dataset. We can use the str_count() function to count the number of matches for a pattern. The pattern we want needs to be a bit more precise than \\\\s+, because this matches one or more. We want to match two or more. Using the regular expression operator {,} we can specify our pattern to be \\\\s{2,}, i.e. two or more continguous whitespaces. Let’s count and sum all these matches. The code is seen in Example 7.10.\n\nExample 7.10  \n\n# Count multiple spaces\neuroparl_curated_tbl |&gt;\n  mutate(multiple_spaces = str_count(lines, \"\\\\s{2,}\")) |&gt;\n  summarize(total_multiple_spaces = sum(multiple_spaces, na.rm = TRUE))\n\n&gt; # A tibble: 1 × 1\n&gt;   total_multiple_spaces\n&gt;                   &lt;int&gt;\n&gt; 1                130628\n\n\n\n\n\n\n\n\n\n Warning\nYou may be wondering what the extra parameter na.rm = TRUE is doing in the sum() function. This parameter tells R to ignore values that are NA (not available). This is important because if we don’t ignore NA values, the sum() function will return NA if there are any NA values in the vector. The code in Example 7.10 will return NA when the \\\\s{2,} doesn’t match for a given line. This is because the str_count() function returns NA when there are no matches. If we don’t ignore these NA values, the sum() function will return NA for the entire dataset.\n\n\n\nThe results from Example 7.10 show that we have over 130k instances of multiple spaces. Let’s replace these with a single space. The code is seen in Example 7.11.\n\nExample 7.11  \n\n# Remove multiple spaces\neuroparl_curated_tbl &lt;- \n  europarl_curated_tbl |&gt;\n  mutate(lines = str_replace_all(lines, \"\\\\s{2,}\", \"\\\\s\"))\n\n\nTo check our work, we can run the code in Example 7.10 again. We should see that there are no more instances of multiple spaces.\nNow let’s turn to the single spaces that occur within a word. We can use the str_replace_all() function again to replace these single spaces with no space –but is that what we want? Probably not, we want most of the single spaces to remain, otherwise we would have one very, very long string on each line.\nInstead, we want to narrow our scope and focus in on whitespace that occurs in particular contexts. One context we can focus on is the single quote ', as in Palacio' s. In this use, the single quote is an apostrophe in a possessive form, but we might want to see if other forms, such as contractions, that also may have this extra whitespace. Let’s check using a regular expression that matches a character string \\\\w+ with single quote ' at the end followed by whitespace \\\\s. To give some more context I will add a character string \\\\w+ after the whitespace. Using the str_extract_all() function we can extract all the matches for this pattern. It returns a list, so we unnest() the list. Then we can pull the vector of matches and list the unique matches to get a sense of the context we are working with. The code is seen in Example 7.12.\n\nExample 7.12  \n\n# Match apostrophe followed by whitespace\neuroparl_curated_tbl |&gt;\n  slice_sample(n = 1000) |&gt; \n  mutate(quote_whitespace = str_extract_all(lines, \"\\\\w+'\\\\s\\\\w+\")) |&gt; \n  unnest(quote_whitespace) |&gt;\n  pull(quote_whitespace) |&gt;\n  unique()\n\n&gt;  [1] \"Frontiers' regulation\" \"capital' s\"            \"People' s\"            \n&gt;  [4] \"communitarization' of\" \"years' standing\"       \"citizens' initiative\" \n&gt;  [7] \"EU' s\"                 \"expulsion' of\"         \"Members' assistants\"  \n&gt; [10] \"Europe' s\"             \"Commission' s\"         \"Parliament' s\"        \n&gt; [13] \"Mugabe' s\"\n\n\n\nIn Example 7.12 we can see that we have many singular possessive forms we want to ammend and other forms that we may want to keep –in particular when the single quote is part of a quote or a plural or irregular singular possessive. From various runs of this code, it looks safe to remove whitespace between the single quote and the s in the possessive form We need to be careful not to remove whitespace in other contexts where the single quote is followed by s. To do this we can use the word boundary pattern \\\\b after the s in our pattern to ensure that the s is not part of a following word. The code is seen in Example 7.13.\n\nExample 7.13  \n\n# Remove whitespace after apostrophe\neuroparl_transformed_tbl &lt;- \n  europarl_curated_tbl |&gt;\n  mutate(lines = str_replace_all(lines, \"'\\\\ss\\\\b\", \"'s\"))\n\n\nTo check our work, we can run the code in Example 7.12 again. We should see that there are no more instances of whitespace after the single quote in possessive forms. Once we are satisfied with our work, we can continue with subsequent transformation procedures using the europarl_transformed_tbl data, or save it to a CSV file, create a data dictionary, and move on to the next transformation procedure.\nNormalization goals will vary from dataset to dataset but the procedures often follow a similar line of attack to those outlined in this section. There are cases, however, in which normalization procedures are more easily accomplished after subsequent transformation steps or need to be post-poned to further the goals of other transformation steps. For example, standardizing abbreviated forms may be more easily accomplished after tokenization when each token is a word. Another example is the case of case conversion. Even if we are not directly interested in the case differences between words, certain generation procedures, Named Entity Recognition (NER) for example, may use case information to identify the names of people, locations, organizations, etc.. In these cases, it may be better to leave the case as is until after the generation step."
  },
  {
    "objectID": "transform-datasets.html#sec-td-recoding",
    "href": "transform-datasets.html#sec-td-recoding",
    "title": "7  Transform datasets",
    "section": "\n7.2 Recoding",
    "text": "7.2 Recoding\nNormalizing text can be seen as an extension of dataset curation to some extent in that the structure of the dataset is maintained. In the Europarl case, we saw this to be true. In the case of recoding, and other transformational steps, the aim will be to modify the dataset structure either by rows, columns, or both. Recoding processes can be characterized by the creation of structural changes which are derived from values in variables effectively recasting values as new variables to enable more direct access in our analyses.\n\n7.2.1 Orientation\nThe Switchboard dialog Act Corpus corpus contains a number of variables describing conversations between speakers of American English. A subset of this dataset provides a good example of the recoding process. The data dictionary for this dataset appears in Table 7.3.\n\n\n\n\nTable 7.3: Data dictionary for the Switchboard dialog Act Corpus.\n\nvariable\nname\ndescription\nvariable_type\n\n\n\ndoc_id\nDocument ID\nUnique identifier for each document\nnumeric\n\n\nspeaker_id\nSpeaker ID\nUnique identifier for each speaker\nnumeric\n\n\nsex\nSex\nGender of the speaker\ncharacter\n\n\neducation\nEducation\nLevel of education of the speaker (1 = no high school, 2 = high school degree, 3 = college degree)\nnumeric\n\n\nbirth_year\nBirth Year\nYear of birth of the speaker\nnumeric\n\n\nutt_id\nUtterance ID\nUnique identifier for each utterance\nnumeric\n\n\nutt_text\nUtterance Text\nText of the utterance\ncharacter\n\n\ndamsl_tag\nDAMSL Tag\nTag indicating the communicative function of the utterance\ncharacter\n\n\n\n\n\n\n\n\nThe data dictionary gives us a sense of the variables in the dataset. Let’s read in the dataset and preview the first 10 lines to get a sense of the values in the dataset, as in Example 7.14.\n\nExample 7.14  \n\n# Read in the dataset\nswda_curated_tbl &lt;-\n  read_csv(\"data/derived/swda/swda_curated.csv\")\n\n# Preview the first 10 lines\nswda_curated_tbl |&gt;\n  slice_head(n = 10)\n\n\n\n&gt; # A tibble: 10 × 8\n&gt;    doc_id speaker_id sex    education birth_year utt_id utt_text       damsl_tag\n&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;    \n&gt;  1   4325       1632 Female         2       1962      1 Okay.  /       o        \n&gt;  2   4325       1632 Female         2       1962      2 {D So, }       qw       \n&gt;  3   4325       1519 Female         1       1971      3 [ [ I guess, + qy^d     \n&gt;  4   4325       1632 Female         2       1962      4 What kind of … +        \n&gt;  5   4325       1519 Female         1       1971      5 I think, ] + … +        \n&gt;  6   4325       1632 Female         2       1962      6 Does it say s… qy       \n&gt;  7   4325       1519 Female         1       1971      7 I think it us… sd       \n&gt;  8   4325       1519 Female         1       1971      8 You might try… ad       \n&gt;  9   4325       1519 Female         1       1971      9 I don't know,… h        \n&gt; 10   4325       1519 Female         1       1971     10 hold it down … ad\n\n\n\nConsidering the data dictionary and the preview of the swda_curated_tbl dataset, we observe a number of metadata variables, such as doc_id, speaker_id, sex, education, birth_year, utt_id, utt_text, and damsl_tag.\nMost of these variables and their values are readily interpretable. However, the damsl_tag variable and the annotation scheme that appears interleaved with the dialog in utt_text may require a bit more explanation. If we consult the data origin file and/ or the corpus website, we seee that the damsl_tag is a utterance-level annotation which indicates the dialog act type (e.g. statement, question, backchannel, etc.). The annotation interleaved with the dialog in utt_text is a disfluency annotation scheme. This scheme includes annotation for non-sentence elements such as filled pauses (e.g. {F uh}), discourse markers (e.g. {D well}), repetitions/ restarts (e.g. [I think + I believe]), among others.\nLet’s assume that we are interested in understanding the use of filled pauses in the Switchboard dialog. Tottie (2011) investigates the relationship between speakers’ use of filled pauses uh and um and their socio-demographic background (sex, socio-economic status, and age) in British English. An American English comparison would be insightful. To do this, however, we will need to recode some of the variables in the dataset.\nIn this case, we will use education as a proxy for socio-economic status. As is, the values are numeric and are not maximally transparent. We can recode these values to be more interpretable. In the corpus documentation, the values for education are described in Table 7.4.\n\n\nTable 7.4: Values for the education variable in the Switchboard dialog Act Corpus.\n\nValue\nDescription\n\n\n\n0\nLess Than High School\n\n\n1\nLess Than College\n\n\n2\nCollege\n\n\n3\nMore Than College\n\n\n9\nUnknown\n\n\n\n\nTo derive a variable which reflects the age of each speaker, we will use the birth_year variable. This variable is a numeric value which indicates the year of birth for each speaker. We can derive a new variable age by subtracting the birth_year from the year of recordings, 1992.\nTogether sex, education, and age will provide us with the socio-demographic information we need to investigate the relationship between speakers’ use of filled pauses and their socio-demographic background. The last component we need to derive is the use of filled pauses. To do this we will need to extract the filled pauses from the utt_text variable. We also need to consider what we mean by ‘use’. In this case, we will operationalize the use of filled pauses as the number of times a filled pause is used per utterance.\n\n7.2.2 Application\nThe plan to transform the swda_curated_tbl dataset is established. Now we need to implement the plan. We will start by recoding the education variable. Specifically, we want to map the numeric values to the descriptions in Table 7.4.\nTo do this we will use the case_when() function from the dplyr package. This function allows us to specify a series of conditions and the values to return if the condition is met. case_when() evaluates the conditions and mutate() writes the variable, in this case overwrites it, as seen in Example 7.15.\n\nExample 7.15  \n\n# Recode education\nswda_curated_tbl &lt;- \n  swda_curated_tbl |&gt;\n  mutate(\n    education = case_when(\n      education == 0 ~ \"Less Than High School\",\n      education == 1 ~ \"Less Than College\",\n      education == 2 ~ \"College\",\n      education == 3 ~ \"More Than College\",\n      education == 9 ~ \"Unknown\"\n    )\n  )\n# Preview the first 10 lines\nswda_curated_tbl |&gt;\n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 8\n&gt;    doc_id speaker_id sex    education       birth_year utt_id utt_text damsl_tag\n&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    \n&gt;  1   4325       1632 Female College               1962      1 Okay.  / o        \n&gt;  2   4325       1632 Female College               1962      2 {D So, } qw       \n&gt;  3   4325       1519 Female Less Than Coll…       1971      3 [ [ I g… qy^d     \n&gt;  4   4325       1632 Female College               1962      4 What ki… +        \n&gt;  5   4325       1519 Female Less Than Coll…       1971      5 I think… +        \n&gt;  6   4325       1632 Female College               1962      6 Does it… qy       \n&gt;  7   4325       1519 Female Less Than Coll…       1971      7 I think… sd       \n&gt;  8   4325       1519 Female Less Than Coll…       1971      8 You mig… ad       \n&gt;  9   4325       1519 Female Less Than Coll…       1971      9 I don't… h        \n&gt; 10   4325       1519 Female Less Than Coll…       1971     10 hold it… ad\n\n\n\nTo create the age variable, all we need to do is subtract the birth_year from the year of recording, 1992. Again we will use mutate() to create the age variable. The values are created by a subtraction operation. Since we will not need the birth_year variable afterwards, we will drop it from the dataset. The code is seen in Example 7.16.\n\nExample 7.16  \n\n# Recode age\nswda_curated_tbl &lt;- \n  swda_curated_tbl |&gt;\n  mutate(age = 1992 - birth_year) |&gt;\n  select(-birth_year)\n\n# Preview the first 10 lines\nswda_curated_tbl |&gt;\n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 8\n&gt;    doc_id speaker_id sex    education         utt_id utt_text    damsl_tag   age\n&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;\n&gt;  1   4325       1632 Female College                1 Okay.  /    o            30\n&gt;  2   4325       1632 Female College                2 {D So, }    qw           30\n&gt;  3   4325       1519 Female Less Than College      3 [ [ I gues… qy^d         21\n&gt;  4   4325       1632 Female College                4 What kind … +            30\n&gt;  5   4325       1519 Female Less Than College      5 I think, ]… +            21\n&gt;  6   4325       1632 Female College                6 Does it sa… qy           30\n&gt;  7   4325       1519 Female Less Than College      7 I think it… sd           21\n&gt;  8   4325       1519 Female Less Than College      8 You might … ad           21\n&gt;  9   4325       1519 Female Less Than College      9 I don't kn… h            21\n&gt; 10   4325       1519 Female Less Than College     10 hold it do… ad           21\n\n\n\nOur final recoding step is to derive the frequency of filled pauses per utterance. In other words, we want to match the ‘uh’ and ‘um’ and return the number of matches for each utterance. There are a number of ways to do this. We could use an approach which applies the str_extract_all() function, which returns a list of matches, and then unnest() the list and count the number of matches. An alternative approach is to use the str_count() function to count the number of matches for a pattern. The later approach is more efficient for our purposes.\nIn either case, a pattern to match these annotations is needed. As always with pattern matching, we need to craft an expression that is as specific as possible to avoid over- or under-matching. We know from our observation and the corpus documentation that all filled pauses are wrapped by the {F ...} annotation. We can use this to our advantage. Before we jump into counting the filled pauses, let’s test a regular expression that matches the entire {F ...} annotation. An expression like {F.*} might be tempting, but this will be problematic for two reasons. First, since the { and } are regular expression operators we will need to escape them with the \\\\ convention. Second, the * operator is greedy, meaning that it will match the longest possible string. So if in a given utterance there are multiple filled pauses, the * operator will match all of them at once, not individually. To avoid this, we can use the ? operator to make the * operator lazy. With these considerations in mind, we can move forward with \\\\{F.*?\\\\} as our expression.\nWe will send each utterance to the str_extract_all() function to match the filled pauses. This function returns a list of matches, so we will need to unnest() the list to get a vector of matches. Afterwards we will apply the count() function to summarize the number of matches for each match variation. The code is seen in Example 7.17.\n\nExample 7.17  \n\n# Test filled pause pattern\nswda_curated_tbl |&gt;\n  mutate(\n    matches = str_extract_all(\n      utt_text, \n      \"\\\\{F.*?\\\\}\")\n  ) |&gt;\n  unnest(matches) |&gt; \n  count(matches)\n\n&gt; # A tibble: 120 × 2\n&gt;    matches            n\n&gt;    &lt;chr&gt;          &lt;int&gt;\n&gt;  1 \"{F  Oh, }\"        1\n&gt;  2 \"{F  uh, }\"        1\n&gt;  3 \"{F \\\"Oh, }\"       1\n&gt;  4 \"{F ((Oh)) }\"      1\n&gt;  5 \"{F ((Oh, }\"       1\n&gt;  6 \"{F ((Uh }\"        1\n&gt;  7 \"{F ((Uh)), }\"     1\n&gt;  8 \"{F ((Uh, }\"       1\n&gt;  9 \"{F + ] Um, }\"     1\n&gt; 10 \"{F + ] um, }\"     1\n&gt; # ℹ 110 more rows\n\n\n\nThe result from our test indicates that the \\\\{F .*?\\\\} pattern matches a wide variety of filled pause annotations, 120 to be exact! We can see that there are a number of filled pause annotations that we are not be interested in, e.g. {F Oh}. Furthermore, the ‘uh’ and ‘um’ we are interested in sometimes include more annotation structure, e.g. {F + ] Um, }.\nA more sophisticated pattern is needed. When faced with a pattern matching task such as this, I find it helpful to start with a simple pattern and then add complexity as needed. This is an iterative process. To speed up the process, I often extract the matches and use an online tool for developing regular expressions, such as regex101.com.\nWith a (monster) regular expression that matches each of the filled pauses we are interested in, and only those filled pauses, we can move forward with counting the number of matches for each utterance.\nTo do this we will use the str_count() function from the stringr package. This function counts the number of matches for a pattern in a character vector. We will use mutate() to create new variables uh and um which will contain the counts for the filled pauses uh and um, respectively. The code is seen in Example 7.18.\n\nExample 7.18  \n\n# Recode filled pauses \nswda_curated_tbl &lt;- \n  swda_curated_tbl |&gt;\n  mutate(\n    uh = str_count(\n      utt_text, \n      \"\\\\{F\\\\s+[\\\\(+\\\\s\\\\]]*(u|U)h[\\\\)\\\\s,\\\\.\\\\?-]*\\\\}\"\n    ),\n    um = str_count(\n      utt_text, \n      \"\\\\{F\\\\s+[\\\\(+\\\\s\\\\]]*(u|U)m[\\\\)\\\\s,\\\\.\\\\?-]*\\\\}\"\n    )\n  )\n# Preview the first 10 lines \nswda_curated_tbl |&gt;\n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 10\n&gt;    doc_id speaker_id sex   education utt_id utt_text damsl_tag   age    uh    um\n&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n&gt;  1   4325       1632 Fema… College        1 Okay.  / o            30     0     0\n&gt;  2   4325       1632 Fema… College        2 {D So, } qw           30     0     0\n&gt;  3   4325       1519 Fema… Less Tha…      3 [ [ I g… qy^d         21     0     0\n&gt;  4   4325       1632 Fema… College        4 What ki… +            30     0     0\n&gt;  5   4325       1519 Fema… Less Tha…      5 I think… +            21     1     0\n&gt;  6   4325       1632 Fema… College        6 Does it… qy           30     0     0\n&gt;  7   4325       1519 Fema… Less Tha…      7 I think… sd           21     0     0\n&gt;  8   4325       1519 Fema… Less Tha…      8 You mig… ad           21     1     0\n&gt;  9   4325       1519 Fema… Less Tha…      9 I don't… h            21     0     0\n&gt; 10   4325       1519 Fema… Less Tha…     10 hold it… ad           21     0     0\n\n\n\nNow we have two new columns, uh and um which indicate how many times the relevant pattern was matched for a given utterance. By choosing to focus on disfluencies, however, we have made a decision to change the unit of observation from the utterance to the use of filled pauses (uh and um). This means that as the dataset stands, it is not in tidy format –where each observation corresponds to the observational unit. When datasets are misaligned in this particular way, there are in what is known as ‘wide’ format. What we want to do, then, is to restructure our dataset such that each row corresponds to the unit of observation –in this case each filled pause type.\n\nTo convert our current (wide) dataset to one where each filler type is listed and the counts are measured for each utterance we turn to the pivot_longer() function. This function creates two new columns, one in which the column names are listed and one for the values for each of the column names. The code is seen in Example 7.19.\n\nExample 7.19  \n\n# Tidy filled pauses\nswda_curated_tbl &lt;- \n  swda_curated_tbl |&gt;\n  pivot_longer(\n    cols = c(\"uh\", \"um\"), \n    names_to = \"filler\", \n    values_to = \"count\"\n  )\n# Preview the first 10 lines\nswda_curated_tbl |&gt;\n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 10\n&gt;    doc_id speaker_id sex    education     utt_id utt_text damsl_tag   age filler\n&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; \n&gt;  1   4325       1632 Female College            1 Okay.  / o            30 uh    \n&gt;  2   4325       1632 Female College            1 Okay.  / o            30 um    \n&gt;  3   4325       1632 Female College            2 {D So, } qw           30 uh    \n&gt;  4   4325       1632 Female College            2 {D So, } qw           30 um    \n&gt;  5   4325       1519 Female Less Than Co…      3 [ [ I g… qy^d         21 uh    \n&gt;  6   4325       1519 Female Less Than Co…      3 [ [ I g… qy^d         21 um    \n&gt;  7   4325       1632 Female College            4 What ki… +            30 uh    \n&gt;  8   4325       1632 Female College            4 What ki… +            30 um    \n&gt;  9   4325       1519 Female Less Than Co…      5 I think… +            21 uh    \n&gt; 10   4325       1519 Female Less Than Co…      5 I think… +            21 um    \n&gt; # ℹ 1 more variable: count &lt;int&gt;\n\n\n\nNow we have a transformed dataset that is in tidy format. Each row corresponds to a filled pause type and the number of times it was used in a given utterance. It also includes the key socio-demographic variables we are interested in.\n\n\n\n\n\n\n Consider this\nAs confident as we may be in our recoding process, it is always a good idea to perform some data checks to ensure that the recoding process was successful. In the process, we can also gain some insight into the data. Considering the structure in the transformed Switchboard dialog Act Corpus dataset, what are some data checks we might want to perform? What are some insights we might gain from these data checks?\n\n\n\nFinalize the transformation process by writing the dataset to a CSV file, create a data dictionary, and review and comment your code in the transformation script."
  },
  {
    "objectID": "transform-datasets.html#sec-td-tokenization",
    "href": "transform-datasets.html#sec-td-tokenization",
    "title": "7  Transform datasets",
    "section": "\n7.3 Tokenization",
    "text": "7.3 Tokenization\nAnother common transformation process that is particularly relevant for text analysis is tokenization. Tokenization is the process of segmenting units of language into components relevant for the research question. This includes breaking text in curated datasets into smaller units, such as words, \\(n\\)-grams, sentences, etc. or combining text into larger units relative to the original text.\n\nThe process of tokenization is fundamentally row-wise. By scaling the text units up or down, we change the unit of observation. It is important both for the research and the text processing to operationalize our language units. For example, while it may appear obvious to you what ‘word’ or ‘sentence’ means, a computer, and your reproducible research, needs a definition. This can prove tricker than it seems. For example, in English, we can segment text into words by splitting on whitespace. This works fairly well but there are some cases where this is not ideal. For example, in the case of contractions, such as don't, won't, can't, etc. the apostrophe is not a whitespace character. If we want to consider these contractions as separate words, then we need to consider a different tokenization strategy.\n\n\n\n\n\n\n Consider this\nConsider the following paragraph:\n\n“As the sun dipped below the horizon, the sky was set ablaze with shades of orange-red, illuminating the landscape. It’s a sight Mr. Johnson, a long-time observer, never tired of. On the lakeside, he’d watch with friends, enjoying the ever-changing hues—especially those around 6:30 p.m.—and reflecting on nature’s grand display. Even in the half-light, the water’s glimmer, coupled with the echo of distant laughter, created a timeless scene. The so-called ‘magic hour’ was indeed magical, yet fleeting, like a well-crafted poem; it was the essence of life itself.”\n\nWhat text conventions would pose issues for word tokenization based on a whitespace critieron?\n\n\n\nFurthermore, tokenization strategies can vary between languages. For German words are often compounded together, meaning many ‘words’ will not be captured by the whitespace convention. Whitespace may not even be relevant for word tokenization in written languages, such as Chinese. The take home message is there is no one-size-fits-all tokenization strategy.\n\n7.3.1 Orientation\nLet’s look at a curated dataset from the CABNC Corpus to explore tokenization. The data dictionary for this dataset appears in Table 7.5.\n\n\n\n\nTable 7.5: Data dictionary for the CABNC Corpus.\n\nvariable\nname\ndescription\nvariable_type\n\n\n\ndoc_id\nDocument ID\nUnique identifier for each document\nstring\n\n\npart_id\nPart ID\nIdentifier for the part within a document\nstring\n\n\nsex\nSex\nGender of the person\nstring\n\n\nage\nAge\nAge of the person\nnumeric\n\n\nutt_id\nUtterance ID\nIdentifier for each utterance\nnumeric\n\n\nutt_text\nUtterance Text\nText of the utterance\nstring\n\n\n\n\n\n\n\n\nThe CABNC dataset contains a number of variables describing conversations between speakers of British English. Now let’s look at the dataset and preview the first 10 lines to get a sense of the values in the dataset, as in Example 7.20.\n\nExample 7.20  \n\n# Read in the dataset\ncabnc_curated_tbl &lt;-\n  read_csv(\"data/derived/cabnc/cabnc_curated.csv\")\n\n# Preview the first 10 lines\ncabnc_curated_tbl |&gt;\n  slice_head(n = 10)\n\n\n\n&gt; # A tibble: 10 × 6\n&gt;    doc_id   part_id sex      age utt_id utt_text                                \n&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                                   \n&gt;  1 KB0RE000 PS002   female   721      0 You enjoyed yourself in America         \n&gt;  2 KB0RE000 PS006   male     601      1 Eh                                      \n&gt;  3 KB0RE000 PS002   female   721      2 did you                                 \n&gt;  4 KB0RE000 PS006   male     601      3 Oh I covered a nice trip yes            \n&gt;  5 KB0RE000 PS002   female   721      4 Oh very good yeah                       \n&gt;  6 KB0RE000 PS006   male     601      5 Er saw Mary and Andrew and              \n&gt;  7 KB0RE000 PS002   female   721      6 Yes you did                             \n&gt;  8 KB0RE000 PS006   male     601      7 in fact the whole family was together f…\n&gt;  9 KB0RE000 PS002   female   721      8 Oh very nice very nice yes It 's horrib…\n&gt; 10 KB0RE000 PS006   male     601      9 It is horrible is n't\n\n\n\nConsidering the data dictionary and the preview of the cabnc_curated_tbl dataset, we observe a number of metadata variables, such as doc_id, part_id, sex, age, and utt_id and the utterances in utt_text.\nLet’s assume that we are performing an exploratory analysis with this dataset and would like to consider the use of words and word sequences (\\(n\\)-grams). In this case we will be deriving multiple datasets with different units of observation.\n\n7.3.2 Application\nIn the cabnc_curated_tbl data frame the utt_text variable contains the text we want to tokenize. We will start by tokenizing the text into words. Abstracting away from some of the metadata variables, if we envision what this should look like we might imagine something like Table 7.6.\n\n\nTable 7.6: Example of tokenizing the utt_text variable into words.\n\ndoc_id\nutt_id\nutt_word\n\n\n\nKB0RE000\n0\nYou\n\n\nKB0RE000\n0\nenjoyed\n\n\nKB0RE000\n0\nyourself\n\n\nKB0RE000\n0\nin\n\n\nKB0RE000\n0\nAmerica\n\n\n\n\nComparing Table 7.6 to the first line of the output of Example 7.14, we can see that we want to segment the words in the utt_text and then have each segment appear as a separate observation, retaining the relevant metadata variables.\nBefore we work with tokenizing text in a data frame, let’s start with a character vector to get a sense of how tokenization works and what we will need to do to achieve the output in Table 7.6. Let’s start with a character vector which contains the first three utterances from cabnc_curated_tbl. I will use slice_head(n = 3) and then pull() to extract the utt_text character vector in Example 7.21.\n\nExample 7.21  \n\n# Pull a character vector\ncabnc_utts_chr &lt;- \n  cabnc_curated_tbl |&gt;\n  slice_head(n = 3) |&gt;\n  pull(utt_text)\n\n# Preview the character vector\ncabnc_utts_chr\n\n&gt; [1] \"You enjoyed yourself in America\" \"Eh\"                             \n&gt; [3] \"did you\"\n\n\n\nWe have the first three utterances in cbanc_utts_chr. Now we can tokenize the utterances into words using the tokenize_words() function from the tokenizers package (Mullen 2022). It’s only required argument is a character vector, as seen in Example 7.22.\n\nExample 7.22  \n\n# Load package\nlibrary(tokenizers)\n\n# Tokenize the utterances into words\ncabnc_utts_chr |&gt; \n  tokenize_words()\n\n&gt; [[1]]\n&gt; [1] \"you\"      \"enjoyed\"  \"yourself\" \"in\"       \"america\" \n&gt; \n&gt; [[2]]\n&gt; [1] \"eh\"\n&gt; \n&gt; [[3]]\n&gt; [1] \"did\" \"you\"\n\n\n\nThe output shows that we get a list of length 3, one for each utterance. Each list element contains a character vector with different lengths based on the number of tokens created from the original utterance.\nNow if we add the tokenize_words() function to a mutate() call, we can create a new variable with the tokenized utterances. However, the value for each observation will be a list. To expand the character vectors within each list into separate observations, we can use the unnest() function on the new variable. I will assign the result to cabnc_unigrams_tbl as we will have one-word tokens, as seen in Example 7.23.\n\nExample 7.23  \n\n# Tokenize the utterances into words\ncabnc_unigrams_tbl &lt;- \n  cabnc_curated_tbl |&gt;\n  mutate(utt_words = tokenize_words(utt_text)) |&gt;\n  unnest(cols = utt_words)\n\n# Preview the first 10 lines\ncabnc_unigrams_tbl |&gt;\n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 7\n&gt;    doc_id   part_id sex      age utt_id utt_text                       utt_words\n&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                          &lt;chr&gt;    \n&gt;  1 KB0RE000 PS002   female   721      0 You enjoyed yourself in Ameri… you      \n&gt;  2 KB0RE000 PS002   female   721      0 You enjoyed yourself in Ameri… enjoyed  \n&gt;  3 KB0RE000 PS002   female   721      0 You enjoyed yourself in Ameri… yourself \n&gt;  4 KB0RE000 PS002   female   721      0 You enjoyed yourself in Ameri… in       \n&gt;  5 KB0RE000 PS002   female   721      0 You enjoyed yourself in Ameri… america  \n&gt;  6 KB0RE000 PS006   male     601      1 Eh                             eh       \n&gt;  7 KB0RE000 PS002   female   721      2 did you                        did      \n&gt;  8 KB0RE000 PS002   female   721      2 did you                        you      \n&gt;  9 KB0RE000 PS006   male     601      3 Oh I covered a nice trip yes   oh       \n&gt; 10 KB0RE000 PS006   male     601      3 Oh I covered a nice trip yes   i\n\n\n\nThe cabnc_unigrams_tbl dataset is in the format we want, each row corresponds to a word token and each column contains the relevant metadata. The tokenizers package includes a variety of tokenization functions. For example, we can use the tokenize_ngrams() function to create \\(n\\)-grams. The tokenize_ngrams() function takes a character vector and a value for \\(n\\) and returns a list of \\(n\\)-grams. Other functions can tokenize character \\(n\\)-grams, sentences, paragraphs, lines, or even allow you to specify a custom tokenization function with a regular expression.\nAs we have seen the tokenize_*() set of functions take a character vector and return a list of tokens. And if we are working with a data frame, we can then work to expand the list into separate observations. This is a common pattern in text analysis. So common, in fact, that the tidytext package (Robinson and Silge 2023) includes a function, unnest_tokens() that wraps the tokenize_* functions and expand the list of tokens into separate observations in one step. The tokenization types available are character, word, ngram, sentence, regex, and skip_ngram. We will use the word tokenization type to recreate the cabnc_unigrams_tbl dataset, as seen in Example 7.24.\n\nExample 7.24  \n\n# Load package\nlibrary(tidytext)\n\n# Tokenize the utterances into words\ncabnc_unigrams_tbl &lt;- \n  cabnc_curated_tbl |&gt;\n  unnest_tokens(\n    output = utt_word, \n    input = utt_text, \n    token = \"words\"\n  )\n# Preview the first 10 lines\ncabnc_unigrams_tbl |&gt;\n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 6\n&gt;    doc_id   part_id sex      age utt_id utt_word\n&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n&gt;  1 KB0RE000 PS002   female   721      0 you     \n&gt;  2 KB0RE000 PS002   female   721      0 enjoyed \n&gt;  3 KB0RE000 PS002   female   721      0 yourself\n&gt;  4 KB0RE000 PS002   female   721      0 in      \n&gt;  5 KB0RE000 PS002   female   721      0 america \n&gt;  6 KB0RE000 PS006   male     601      1 eh      \n&gt;  7 KB0RE000 PS002   female   721      2 did     \n&gt;  8 KB0RE000 PS002   female   721      2 you     \n&gt;  9 KB0RE000 PS006   male     601      3 oh      \n&gt; 10 KB0RE000 PS006   male     601      3 i\n\n\n\nThe utt_word column in the outputs from both Example 7.23 and Example 7.24 are identical. One thing to note, however, is that the original utt_text variable is dropped in the unnest_tokens() approach. This one of a few default values for parameters which inlcude drop = TRUE (dropping the original text variable) and to_lower = TRUE. In fact the token = \"words\" parameter is not needed as it is the default. We can change these defaults as we see fit.\n\n\n\n\n\n\n Dive deeper\nThe approach using either tokenize_*() or unnest_tokens() approaches tokenization from a segmentation point of view. That is, the context separating our tokens is tarted and is removed. In some cases it may be more feasible to turn the approach around and instead target the tokens to extract. The str_extract_all() can be used for this purpose. Note, however, the former approach is often more effective and effecient. The later requires a regular expression, or set of, which can be tricky to develop for some tokenization tasks.\n\n\n\nAs we create derived datasets to explore, let’s also create bigram tokens. We can do this by changing the token parameter to \"ngrams\" and specifying the value for \\(n\\) with the n parameter. I will assign the result to cabnc_bigrams_tbl as we will have two-word tokens, as seen in Example 7.25.\n\nExample 7.25  \n\n# Tokenize the utterances into bigrams\ncabnc_bigrams_tbl &lt;- \n  cabnc_curated_tbl |&gt;\n  unnest_tokens(\n    output = utt_bigram, \n    input = utt_text, \n    token = \"ngrams\",\n    n = 2\n  )\n# Preview the first 10 lines\ncabnc_bigrams_tbl |&gt;\n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 6\n&gt;    doc_id   part_id sex      age utt_id utt_bigram      \n&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;           \n&gt;  1 KB0RE000 PS002   female   721      0 you enjoyed     \n&gt;  2 KB0RE000 PS002   female   721      0 enjoyed yourself\n&gt;  3 KB0RE000 PS002   female   721      0 yourself in     \n&gt;  4 KB0RE000 PS002   female   721      0 in america      \n&gt;  5 KB0RE000 PS006   male     601      1 &lt;NA&gt;            \n&gt;  6 KB0RE000 PS002   female   721      2 did you         \n&gt;  7 KB0RE000 PS006   male     601      3 oh i            \n&gt;  8 KB0RE000 PS006   male     601      3 i covered       \n&gt;  9 KB0RE000 PS006   male     601      3 covered a       \n&gt; 10 KB0RE000 PS006   male     601      3 a nice\n\n\n\nThe two-word token sequences for each uttterance appear as observations in the cabnc_bigrams_tbl dataset. You may notice that in row 5 the value for utt_bigram is NA. This is because the utterance only contains one word. The unnest_tokens() function will not create a token for a sequence that does not exist.\nIf we don’t want to loose this information, we can modify the original utt_text variable to include a placeholder, some symbol that we will use to denote that a single word utterance is present. Another strategy which accomplishes the first goal and may enrich the bigram dataset is to add a start and end token to each utterance. This will allow us to identify the first and last word in each utterance. In either case we can turn to the str_c() function which will allow us to concatenate strings. In Example 7.26, I will add a start and end token to each utterance and then tokenize the utterances into bigrams.\n\nExample 7.26  \n\n# Prepend and append (x) char to `utt_text`\ncabnc_derived_tbl &lt;- \n  cabnc_curated_tbl |&gt;\n  mutate(utt_text = str_c(\"x\", utt_text, \"x\", sep = \" \")\n  )\n# Tokenize the utterances into bigrams\ncabnc_bigrams_tbl &lt;- \n  cabnc_derived_tbl |&gt;\n  unnest_tokens(\n    output = utt_bigram, \n    input = utt_text, \n    token = \"ngrams\",\n    n = 2\n  )\n# Preview the first 10 lines\ncabnc_bigrams_tbl |&gt;\n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 6\n&gt;    doc_id   part_id sex      age utt_id utt_bigram      \n&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;           \n&gt;  1 KB0RE000 PS002   female   721      0 x you           \n&gt;  2 KB0RE000 PS002   female   721      0 you enjoyed     \n&gt;  3 KB0RE000 PS002   female   721      0 enjoyed yourself\n&gt;  4 KB0RE000 PS002   female   721      0 yourself in     \n&gt;  5 KB0RE000 PS002   female   721      0 in america      \n&gt;  6 KB0RE000 PS002   female   721      0 america x       \n&gt;  7 KB0RE000 PS006   male     601      1 x eh            \n&gt;  8 KB0RE000 PS006   male     601      1 eh x            \n&gt;  9 KB0RE000 PS002   female   721      2 x did           \n&gt; 10 KB0RE000 PS002   female   721      2 did you\n\n\n\n\n\n\n\n\n\n Warning\nIn Example 7.26 I used x as the start and end token. My though process was that x will be a unique token that will not be present in the utterances. This is a good strategy, but it is not foolproof. Another, more meaningful strategy may be to encode the start and end with # and $, respectively. In this case, however, this option is not ideal as the unnest_tokens() and tokenize_ngrams() functions strip punctuation by default. This means that the # and $ will be removed.\n\n\n\nThe most common tokenization strategy is to segment text into smaller units, often words. However, there are times when we may want to segment text into larger units, effectively collapsing over rows. For example, if we are working with a curated dataset which is tokenized by words, we may want to segment the text into sentences. A couple considerations are in order, however. First, we need to be clear about what we mean by ‘sentence’ and how we will segment the text into sentences. In some cases key cues for sentence boundaries, such as sentencial punctuation have been stripped from the text, making a simple defnition of a sentence difficult or impossible to be performed computationally. Second, we also need to be clear how we will handle the metadata variables. In other words, when we collapse over rows, we need to be aware and intentional about how we group these new units of observation. For example, if we collapse the cabnc_unigrams_tbl dataset into sentences, will we group the sentences by doc_id or part_id or some other combination of metadata variables?\nLet’s see how we might collapse text rows into larger units using the original curated CABNC data frame cabnc_curated_tbl. Refer back to the output in Example 7.20. The values in utt_text are utterances, which may map to sentences in some cases, but often not. Regardless, there is no sentential punctuation to help identify sentences, even across utterance observations. Furthermore, it may not even make sense to dicuss sentences in the context of spoken language.\nA unit that may make more sense is utterances per speaker per document. Note this context ‘per speaker per document’. In effect this is a grouping parameter for our approach to collapsing the text in utt_text. With a goal in mind we can turn to the group_by() function to group our dataset and then summarize() to collapse the text in utt_text with the str_c() and the parameter collapse = \" \". The code is seen in Example 7.27.\n\nExample 7.27  \n\n# Collapse utterances by speaker by document\ncabnc_utterances_tbl &lt;- \n  cabnc_curated_tbl |&gt;\n  group_by(doc_id, part_id) |&gt;\n  summarize(utt_text = str_c(utt_text, collapse = \" \")) |&gt; \n  ungroup()\n\n# Preview the first 10 lines\ncabnc_utterances_tbl |&gt;\n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 3\n&gt;    doc_id   part_id utt_text                                                    \n&gt;    &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;                                                       \n&gt;  1 KB0RE000 KB0PSUN Hmm Hello                                                   \n&gt;  2 KB0RE000 PS002   You enjoyed yourself in America did you Oh very good yeah Y…\n&gt;  3 KB0RE000 PS006   Eh Oh I covered a nice trip yes Er saw Mary and Andrew and …\n&gt;  4 KB0RE001 KB0PSUN Neck of lamb 's off Hello Morning                           \n&gt;  5 KB0RE001 PS005   You can tape me by all means but they probably wo n't like …\n&gt;  6 KB0RE001 PS007   They want to know what spoken English is like Er now what d…\n&gt;  7 KB0RE002 PS003   No Was born in Brockly I was n't born sorry about that my f…\n&gt;  8 KB0RE002 PS007   You 're not Welsh speaking at all are you Mm mm but you are…\n&gt;  9 KB0RE003 PS006   It 's all supposed to be anonymous anyway Right now do you …\n&gt; 10 KB0RE003 PS007   I do n't think I I 'm likely to say anything use use down a…\n\n\n\nNow we have a data frame where the utterances for each speaker for each document are collapsed into a single observation. Yet by collapsing the utterances in this way, we have lost the speaker metadata sex and age. We can add this information by one of two approaches. The first is to simply add sex and age to the grouping parameter in group_by(). Since these variables are descriptors of part_id they will not change the result of our collapsing the text, but will be retained in the resulting output. The second approach is to use a join operation to add the metadata back to the collapsed dataset. I won’t go into this approach here as we will cover joins head-on in Section 7.5."
  },
  {
    "objectID": "transform-datasets.html#sec-td-generation",
    "href": "transform-datasets.html#sec-td-generation",
    "title": "7  Transform datasets",
    "section": "\n7.4 Generation",
    "text": "7.4 Generation\nThe process of generation involves the addition of information to a dataset. This differs from the previous transformation procedures in that normalization, recoding, and tokenization involve manipulating, classifiying, and/ or deriving information based on characteristics explicit in a dataset. Instead, generation involves deriving new information based on characteristics implicit in a dataset.\nThe most common type of operation involved in the generation process is the addition of linguistic annotation. This process can be accomplished manually by a researcher or research team or automatically through the use of pre-trained linguistic resources and/ or software. Ideally the annotation of linguistic information can be conducted automatically.\nThere are important considerations, however, that need to be taken into account when choosing whether linguistic annotation can be conducted automatically. First and foremost has to do with the type of annotation desired. Information such as part of speech (grammatical category) and morpho-syntactic information are the the most common types of linguistic annotation that can be conducted automatically.\nSecond, the degree to which the resource that will be used to annotate the linguistic information is aligned with the language variety and/or register is also a key consideration. As noted, automatic linguistic annotation methods are contingent on pre-trained resources. The language and language variety used to develop these resources may not be available for the language under investigation, or if it does, the language variety and/ or register may not align. The degree to which a resource does not align with the linguistic information targeted for annotation is directly related to the quality of the final annotations. To be clear, no annotation method, whether manual or automatic is guaranteed to be perfectly accurate.\n\n7.4.1 Orientation\nAs an example, we’ll posit that we are conducting translation research. Specifically, we will set up an investigation into the effect of translation on the syntactic simplification of text. The basic notion is that when translators translate text from one language to another, they subconsciously simplify the text, compared to native texts (Liu and Afzaal 2021).\nTo assess this hypothesis, we will need to identify comparable translated and native texts. The ENNTT corpus contains native and translated English. The texts are drawn from European Parliament proceedings ensuring that the texts are comparable in terms of register.\nThe data dictionary for the curated native dataset appears in Table 7.7.\n\n\n\n\nTable 7.7: Data dictionary for the curated native ENNTT dataset.\n\nvariable\nname\nvariable_type\ndescription\n\n\n\nsession_id\nSession ID\ncategorical\nUnique identifier for each session\n\n\nspeaker_id\nSpeaker ID\ncategorical\nUnique identifier for each speaker\n\n\nstate\nState\ncategorical\nThe country or region the speaker is from\n\n\nsession_seq\nSession Sequence\nordinal\nThe order in which the session occurred\n\n\ntext\nText\ncategorical\nThe spoken text during the session\n\n\ntype\nType\ncategorical\nThe type of speaker. Natives in this dataset.\n\n\n\n\n\n\n\n\nBoth the natives and translations datasets have the same variables. For the purposes of this investigation, the text and type variables are the most relevant. A variable to index the text, such as doc_id, will be useful as we move forward and will be added to the dataset.\nThe next step is to operationalize what we mean by syntactic simplification. There are many measures of syntactic complexity (Szmrecsanyi 2004). For our purposes, let’s focus on two: number of T-units and sentence length (in words). Length is straightforward to calculate after word tokenization but a T-unit is a bit more involved. A T-unit is a main clause and all of its subordinate clauses. To calculate the number of T-units, we will need to identify the main clauses and their subordinate clauses.\nAn idealized transformed dataset for this investigation would look something like Table 7.8.\n\n\nTable 7.8: Idealized transformed dataset for the syntactic simplification investigation.\n\n\n\n\n\n\n\n\ndoc_id\ntype\nt_units\nword_len\ntext\n\n\n\n1\ntranslated\n1\n5\nI am happy right now.\n\n\n2\ntranslated\n3\n11\nI think that John believes that Mary is a good person.\n\n\n3\nnative\n2\n8\nShe thinks I am not happy right now.\n\n\n4\nnative\n4\n21\nAlthough she knew that the weather was bad, Mary decided to go for a walk, hoping that she would feel better.\n\n\n\n\nTo identify the main clauses and their subordinate clauses, we will need to annotate the ENNTT texts with syntactic information. Specifically, we will need to identify and count the main clauses and their subordinate clauses.\n\n7.4.2 Application\nAs fun as it would be to hand-annotate the ENNTT corpus, we will instead turn to automatic linguistic annotation. Specifically, we will use the udpipe package (Wijffels 2023) which provides an interface for annotating text using pre-trained models from the Universal Dependencies (UD) project (Nivre et al. 2020). The UD project is an effort to develop cross-linguistically consistent treebank annotation for a variety of languages.\nOur first step, then, is to peruse the available pre-trained models for the languages we are interested in and selected the most register-aligned models. The models, model names, and licensing information are documented in the udpipe package and can be accessed by running ?udpipe::udpipe_download_model() in the R console. For illustrative purposes, the english treebank model from the https://github.com/bnosac/udpipe.models.ud repository which is released under the CC-BY-SA license. This model is trained on various sources including news, Wikipedia, and web data of various genres.\nLet’s set the stage by providing an overview of the annotation process.\n\nLoad the udpipe package.\nSelect the pre-trained model to use and the directory where the model will be stored in your local environment.\nPrepare the dataset to be annotated (if necessary). This includes ensuring that the dataset has a column of text to be annotated and a grouping column. By default, the names of these columns are expected to be text and doc_id, respectively. The text column needs to be a character vector and the doc_id column needs to be a unique index for each text to be annotated.\nAnnotate the dataset. The result returns a data frame.\n\nSteps 3 and 4 are repeated for the enntt_natives_curated and the enntt_translations_curated datasets. For brevity, I will only show the code for the dataset for the natives. Additionally, I will subset the dataset to 10,000 randomly selected lines for both dataset, as in Example 7.28 for the natives. Syntactic annotation is a computationally expensive operation and the natives and translations datasets contain 116,341 and 738,597 observations, respectively.\n\nExample 7.28  \n\n# Subset the natives ENNTT dataset\nenntt_natives_tbl &lt;- \n  enntt_natives_curated_tbl |&gt; \n  slice_sample(n = 10000)\n\n\n\n\n\n\n\n\n Tip Computational Performance\nIn your own research computationally expensive cannot be avoided, but it can be managed. One strategy is to work with a subset of the data until your code is working as expected. Once you are confident that your code is working as expected, then you can scale up to the full dataset.\nIf you are using Quarto, you can use the cache: true metadata field in your code blocks to cache the results of computationally expensive code blocks. This will allow you to run your code once and then use the cached results for subsequent runs.\nParallel processing is another strategy for managing computationally expensive code. Some packages, such as udpipe, have built-in support for parallel processing. Other packages, such as tidytext, do not. In these cases, you can use the future package (Bengtsson 2023) to parallelize your code.\n\n\n\nWith the enntt_natives_tbl object, let’s execute steps 1-4, as seen in Example 7.29.\n\nExample 7.29  \n\n# Load package\nlibrary(udpipe)\n\n# Model and directory\nmodel &lt;- \"english\"\nmodel_dir &lt;- \"../data/\"\n\n# Prepare the dataset to be annotated\nenntt_natives_prepped_tbl &lt;- \n  enntt_natives_tbl |&gt;\n  mutate(doc_id = row_number()) |&gt;\n  select(doc_id, text)\n\n# Annotate the dataset\nenntt_natives_ann &lt;- \n  udpipe(\n    x = enntt_natives_prepped_tbl, \n    object = model, \n    model_dir = model_dir\n  ) |&gt; \n  tibble()\n\n# Preview \nglimpse(enntt_natives_anno)\n\n\n\n&gt; Rows: 264,124\n&gt; Columns: 17\n&gt; $ doc_id        &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n&gt; $ paragraph_id  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n&gt; $ sentence_id   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n&gt; $ sentence      &lt;chr&gt; \"It is extremely important that action is taken to ensur…\n&gt; $ start         &lt;int&gt; 1, 4, 7, 17, 27, 32, 39, 42, 48, 51, 58, 63, 66, 73, 77,…\n&gt; $ end           &lt;int&gt; 2, 5, 15, 25, 30, 37, 40, 46, 49, 56, 61, 64, 71, 75, 82…\n&gt; $ term_id       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n&gt; $ token_id      &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\",…\n&gt; $ token         &lt;chr&gt; \"It\", \"is\", \"extremely\", \"important\", \"that\", \"action\", …\n&gt; $ lemma         &lt;chr&gt; \"it\", \"be\", \"extremely\", \"important\", \"that\", \"action\", …\n&gt; $ upos          &lt;chr&gt; \"PRON\", \"AUX\", \"ADV\", \"ADJ\", \"SCONJ\", \"NOUN\", \"AUX\", \"VE…\n&gt; $ xpos          &lt;chr&gt; \"PRP\", \"VBZ\", \"RB\", \"JJ\", \"IN\", \"NN\", \"VBZ\", \"VBN\", \"TO\"…\n&gt; $ feats         &lt;chr&gt; \"Case=Nom|Gender=Neut|Number=Sing|Person=3|PronType=Prs\"…\n&gt; $ head_token_id &lt;chr&gt; \"4\", \"4\", \"4\", \"0\", \"8\", \"8\", \"8\", \"4\", \"10\", \"8\", \"13\",…\n&gt; $ dep_rel       &lt;chr&gt; \"expl\", \"cop\", \"advmod\", \"root\", \"mark\", \"nsubj:pass\", \"…\n&gt; $ deps          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n&gt; $ misc          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\n\nThere is quite a bit of information which is returned from udpipe(). Note that the input lines have been tokenized by word. Each token includes the token, lemma, part of speech (upos and xpos), morphological features (feats), and syntactic relationships (head_token_id and dep_rel). The token_id keeps track of the token’s position in the sentence and the sentence_id keeps track of the sentence’s position in the original text. In the case of the Europarl dataset, most values of lines are just one sentence, but there are some cases where the lines variable contains multiple sentences in which the sid will be incremented. Finally, the doc_id column and its values correspond to the doc_id in the enntt_natives_tbl dataset.\nThe number of variables in the udpipe() annotation output is quite overwhelming. However, these attributes come in handy for manipulating, extracting, and plotting information based on lexical and syntactic patterns. See the dependency tree in Figure 7.1 for an example of the syntactic information that can be extracted from the udpipe() annotation output.\n\n\n\n\nFigure 7.1: Plot of the syntactic tree for a sentence in the ENNTT natives dataset.\n\n\n\n\n\n\n\n\n\n Dive deeper\nThe plot in Figure 7.1 was created using the rsyntax package (R-rsyntax?). In addition to creating dependency tree plots, the rsyntax package can be used to extract syntactic patterns from the udpipe() annotation output. See the documentation for more information.\n\n\n\nIn Figure 7.1 we see the syntactic tree for a sentence in the ENNTT natives dataset. Each node is labeled with the token_id which provides the linear ordering of the sentence. Above the nodes the dep_relation, or dependency relationship label is provided. These labels are based on the UD project’s dependency relations. We can see that the ‘ROOT’ relation is at the top of the tree and corresponds to the verb ‘brought’. ‘ROOT’ relations mark predicates in the sentence. Not seen in the example tree, ‘cop’ relation is a copular, or non-verbal predicate and should be included. These are the key syntactic pattern we will use to identify main clauses for T-units. Now we need to identify the subordinate clauses. In the UD project’s listings, the relations ‘ccomp’ (clausal complement), ‘xcomp’ (open clausal complement), and ‘acl:relcl’ (relative clause), as seen in Figure 7.1) are subordinate clauses.\nTo calculate the number of T-units and words per sentence we turn to the dplyr package. We will use the group_by() function to group the dataset by doc_id and sentence_id and then use the summarize() function to calculate the number of T-units and words per sentence, where a T-unit is the combination of the sum of main clauses and sum of subordinante clauses. The code is seen in Example 7.30.\n\nExample 7.30  \n\n# Calculate the number of T-units and words per sentence\nenntt_natives_ann_tunits_words_tbl &lt;- \n  enntt_natives_ann |&gt;\n  group_by(doc_id, sentence_id) |&gt;\n  summarize(\n    main_clauses = sum(dep_rel %in% c(\"ROOT\", \"cop\")),\n    subord_clauses = sum(dep_rel %in% c(\"ccomp\", \"xcomp\", \"acl:relcl\")),\n    t_units = main_clauses + subord_clauses,\n    word_len = n()\n  ) |&gt;\n  ungroup()\n\n# Preview\nglimpse(enntt_natives_ann_tunits_words_tbl)\n\n&gt; Rows: 10,199\n&gt; Columns: 6\n&gt; $ doc_id         &lt;chr&gt; \"1\", \"10\", \"100\", \"1000\", \"10000\", \"1001\", \"1002\", \"100…\n&gt; $ sentence_id    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n&gt; $ main_clauses   &lt;int&gt; 1, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 1, 1, 0, 2, 0, 1, 1, 1…\n&gt; $ subord_clauses &lt;int&gt; 3, 2, 1, 0, 1, 2, 1, 1, 0, 2, 1, 0, 3, 2, 0, 4, 2, 1, 1…\n&gt; $ t_units        &lt;int&gt; 4, 2, 1, 0, 3, 2, 1, 2, 1, 2, 2, 1, 4, 2, 2, 4, 3, 2, 2…\n&gt; $ word_len       &lt;int&gt; 21, 25, 27, 15, 40, 43, 29, 23, 13, 30, 33, 9, 68, 35, …\n\n\n\nA quick spot check of some sentences calculations enntt_natives_ann_tunits_words_tbl dataset against the enntt_natives_ann is good to ensure that the calculation is working as expected. In Figure 7.2 we see a sentence that has a word length of 13 and a T-unit value of 5.\n\n\n\n\nFigure 7.2: Sentence with a word length of 13 and a T-unit value of 5.\n\n\n\nNow we can drop the intermediate columns we created to calculate our key syntactic complexity measures using select() to indicate those that we do want to keep. I will assign the result to enntt_natives_syn_comp as we will be working with syntactic complexity measures for the native texts, as seen in Example 7.31.\n\nExample 7.31  \n\n# Select columns\nenntt_natives_syn_comp &lt;- \n  enntt_natives_ann_tunits_words_tbl |&gt;\n  select(doc_id, sentence_id, t_units, word_len)\n\n\nNow we can repeat the process for the ENNTT translated dataset. I will assign the result to enntt_translations_syn_comp. The next step is to join the sentences from the annotated data frames into our datasets so that we have the information we set out to generate for both datasets. Then we will concatenate both the enntt_natives_syn_comp and enntt_translations_syn_comp datasets into a single dataset. Both the join and the contactenation will be covered in the next section."
  },
  {
    "objectID": "transform-datasets.html#sec-td-merging",
    "href": "transform-datasets.html#sec-td-merging",
    "title": "7  Transform datasets",
    "section": "\n7.5 Merging",
    "text": "7.5 Merging\nOne final class of transformations that can be applied to curated datasets to enhance their informativeness for a research project is the process of merging two or more datasets. There are two primary types of merging: joins and concatenation. Joins can be row- or column-wise operations that combine datasets based on a common attribute or set of attributes. Concatenation is exclusively a row-wise operation that combines datasets that share the same attributes.\nOf the two types of merges, joins are the most powerful and sometimes more difficult to understand. When two datasets are joined at least one common variable must be shared between the two datasets. The common variable(s) are referred to as keys. The keys are used to match observations in one dataset with observations in another dataset by serving as an index.\nThere are a number of join types. The most common are left, full, semi, and anti. The type of join determines which observations are retained in the resulting dataset. Let’s see this in practice. First, let’s create two datasets to join with a common variable key, as seen in Example 7.32.\n\nExample 7.32  \n\n\n\n\na_tbl &lt;- \n  tibble(\n    key = c(1, 2, 3, 5, 8),\n    a = letters[1:5]\n  )\n\na_tbl\n\n&gt; # A tibble: 5 × 2\n&gt;     key a    \n&gt;   &lt;dbl&gt; &lt;chr&gt;\n&gt; 1     1 a    \n&gt; 2     2 b    \n&gt; 3     3 c    \n&gt; 4     5 d    \n&gt; 5     8 e\n\n\n\n\n\nb_tbl &lt;-\n  tibble(\n    key = c(1, 2, 4, 6, 8),\n    b = letters[6:10]\n  )\n\nb_tbl\n\n&gt; # A tibble: 5 × 2\n&gt;     key b    \n&gt;   &lt;dbl&gt; &lt;chr&gt;\n&gt; 1     1 f    \n&gt; 2     2 g    \n&gt; 3     4 h    \n&gt; 4     6 i    \n&gt; 5     8 j\n\n\n\n\n\n\nThe a_tbl and the b_tbl datasets share the key variable, but the values in the key variable are not identical. The two datasets share values 1, 2, and 8. The a_tbl dataset has values 3 and 5 in the key variable and the b_tbl dataset has values 4 and 6 in the key variable.\nIf we apply a left join to the a_tbl and b_tbl datasets, the result will be a dataset that retains all of the observations in the a_tbl dataset and only those observations in the b_tbl dataset that have a match in the a_tbl dataset. The result is seen in Example 7.33.\n\nExample 7.33  \n\nleft_join(x = a_tbl, y = b_tbl, by = \"key\")\n\n&gt; # A tibble: 5 × 3\n&gt;     key a     b    \n&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n&gt; 1     1 a     f    \n&gt; 2     2 b     g    \n&gt; 3     3 c     &lt;NA&gt; \n&gt; 4     5 d     &lt;NA&gt; \n&gt; 5     8 e     j\n\n\n\nNow, if the key variable has the same name, R will recognize and assume that this is the variable to join on and we don’t need the by = argument, but if there are multiple potential key variables, we use by = to specify which one to use.\nA full join retains all observations in both datasets, as seen in Example 7.34.\n\nExample 7.34  \n\nfull_join(x = a_tbl, y = b_tbl)\n\n&gt; # A tibble: 7 × 3\n&gt;     key a     b    \n&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n&gt; 1     1 a     f    \n&gt; 2     2 b     g    \n&gt; 3     3 c     &lt;NA&gt; \n&gt; 4     5 d     &lt;NA&gt; \n&gt; 5     8 e     j    \n&gt; 6     4 &lt;NA&gt;  h    \n&gt; 7     6 &lt;NA&gt;  i\n\n\n\nLeft and full joins maintain or increase the number of observations. On the other hand, semi and anti joins aim to decrease the number of observations. A semi join retains only those observations in the left dataset that have a match in the right dataset, as seen in Example 7.35.\n\nExample 7.35  \n\nsemi_join(x = a_tbl, y = b_tbl)\n\n&gt; # A tibble: 3 × 2\n&gt;     key a    \n&gt;   &lt;dbl&gt; &lt;chr&gt;\n&gt; 1     1 a    \n&gt; 2     2 b    \n&gt; 3     8 e\n\n\n\nAnd an anti join retains only those observations in the left dataset that do not have a match in the right dataset, as seen in Example 7.36.\n\nExample 7.36  \n\nanti_join(x = a_tbl, y = b_tbl)\n\n&gt; # A tibble: 2 × 2\n&gt;     key a    \n&gt;   &lt;dbl&gt; &lt;chr&gt;\n&gt; 1     3 c    \n&gt; 2     5 d\n\n\n\nOf these join types, the left join and the anti join are some of the most common to encounter in research projects.\n::: {.callout}  Consider this\nIn addition to datasets that are part of an acquired resource or derived from a corpus resource, there are also a number of datasets that are included in R packages that are particularly relevant for text analysis. For example, the tidytext package includes sentiments and stop_words datasets. The lexicon package (Rinker 2019) includes large number of datasets that include sentiment lexicons, stopword lists, contractions, and more.\nConsider the lexicon::key_contractions dataset. This dataset includes a list of common contractions and their expanded forms.\nWhat needs to be done to join these two datasets? What are the keys? What type of join should be used? What is the resulting dataset?\n\n\n\n\n\n\n\nTable 7.9: Common contractions and expanded forms.\n\ncontraction\nexpanded\n\n\n\n'cause\nbecause\n\n\n'tis\nit is\n\n\n'twas\nit was\n\n\nain't\nam not\n\n\naren't\nare not\n\n\ncan't\ncan not\n\n\ncould've\ncould have\n\n\nit's\nit is\n\n\n\n\n\n\n\n\n\n\n\nTable 7.10: Example tokenized dataset with contractions.\n\ndoc_id\nsent_id\ntoken_id\ntoken\n\n\n\n1\n1\n1\nI\n\n\n1\n1\n2\ncan’t\n\n\n1\n1\n3\nbelieve\n\n\n1\n1\n4\nthat\n\n\n1\n1\n5\nit’s\n\n\n1\n1\n6\nnot\n\n\n1\n1\n7\nbutter\n\n\n1\n1\n8\n!\n\n\n\n\n\n\n\n7.5.1 Orientation\nWith this in mind, let’s return to our syntactic simplification investigation. Recall that we started with two curated ENNTT datasets: the natives and translations. We manipulated these datasets subsetting them to 10,000 randomly selected lines, prepped them for annotation by adding a doc_id column and dropping all columns except text, and then annotated them using the udpipe package. We then calculated the number of T-units and words per sentence.\nThese steps produced two datasets for both the natives and for the translations. The first dataset for each is the annotated data frame. The second is the data frame with the sytactic complexity measures we calculated. The annotated data frames are named enntt_natives_ann and enntt_translations_ann. The data frames with the syntactic complexity measures are named enntt_natives_syn_comp and enntt_translations_syn_comp.\nIn the end, we want a dataset that looks something like Table 7.11.\n\n\nTable 7.11: Idealized merged dataset for the syntactic simplification investigation.\n\n\n\n\n\n\n\n\ndoc_id\ntype\nt_units\nword_len\ntext\n\n\n\n1\nnatives\n1\n5\nI am happy right now.\n\n\n2\ntranslation\n3\n11\nI think that John believes that Mary is a good person.\n\n\n\n\nTo create this unified dataset, we will need to apply joins and concatenation. First, we will join the prepped datasets with the annotated datasets. Then we will concatenate the two resulting datasets.\n\n7.5.2 Application\nLet’s start by joining the annotated datasets (enntt_natives_ann and enntt_translations_ann) with the datasets with the syntactic complexity calculations (enntt_natives_syn_comp and enntt_translations_syn_comp). In these joins, we can see that the prepped and calculated datasets share a couple variables, doc_id and sentence_id, in Example 7.37.\n\nExample 7.37  \n\n# Preview datasets to join\nenntt_natives_ann |&gt; \n  slice_head(n = 3)\n\n&gt; # A tibble: 3 × 17\n&gt;   doc_id paragraph_id sentence_id sentence    start   end term_id token_id token\n&gt;   &lt;chr&gt;         &lt;int&gt;       &lt;int&gt; &lt;chr&gt;       &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;\n&gt; 1 1                 1           1 It is extr…     1     2       1 1        It   \n&gt; 2 1                 1           1 It is extr…     4     5       2 2        is   \n&gt; 3 1                 1           1 It is extr…     7    15       3 3        extr…\n&gt; # ℹ 8 more variables: lemma &lt;chr&gt;, upos &lt;chr&gt;, xpos &lt;chr&gt;, feats &lt;chr&gt;,\n&gt; #   head_token_id &lt;chr&gt;, dep_rel &lt;chr&gt;, deps &lt;chr&gt;, misc &lt;chr&gt;\n\nenntt_natives_syn_comp |&gt; \n  slice_head(n = 3)\n\n&gt; # A tibble: 3 × 4\n&gt;   doc_id sentence_id t_units word_len\n&gt;   &lt;chr&gt;        &lt;int&gt;   &lt;int&gt;    &lt;int&gt;\n&gt; 1 1                1       4       21\n&gt; 2 10               1       2       25\n&gt; 3 100              1       1       27\n\n\n\nThe doc_id and sentence_id variables are both keys that we will use to join the datasets. The reason being that if we only use one of the two we will not align the two datasets at the sentence level. Only the combination of doc_id and sentence_id isolates the sentences for which we have syntactic complexity measures. Beyond a having common variable (or variables in our case), we must also ensure that join key variables are of the same vector type in both data frames and that we are aware of any differences in the values. From the output in Example 7.37, we can see that the doc_id and sentence_id variables aligned in terms of vector type; doc_id is character and sentence_id is integer in both data frames. If they happened not to be, there types would need to be adjusted.\nNow, we need to check for differences in the values. We can do this by using the setequal() function. This function returns TRUE if the two vectors are equal and FALSE if they are not. If the two vectors are not equal, the function will return the values that are in one vector but not the other. So if one has 10001 and the other doesn’t we will get FALSE. Let’s see this in practice, as seen in Example 7.38.\n\nExample 7.38  \n\n# Check for differences in the values\nsetequal(\n  enntt_natives_ann$doc_id, \n  enntt_natives_syn_comp$doc_id\n)\n\n&gt; [1] TRUE\n\nsetequal(\n  enntt_natives_ann$sentence_id, \n  enntt_natives_syn_comp$sentence_id\n)\n\n&gt; [1] TRUE\n\n\n\nSo the values are the same. The final check is to see if the vectors are of the same length. We know the values are the same, but we don’t know if the values are repeated. We do this by simply comparing the length of the vectors, as seen in Example 7.39.\n\nExample 7.39  \n\n# Check for differences in the length\nlength(enntt_natives_ann$doc_id) == \n  length(enntt_natives_syn_comp$doc_id)\n\n&gt; [1] FALSE\n\nlength(enntt_natives_ann$sentence_id) == \n  length(enntt_natives_syn_comp$sentence_id)\n\n&gt; [1] FALSE\n\n\n\nSo they are not the same length. Using the nrow() function, I can see that the annotated dataset has 264124 observations and the calculated dataset has 10199 observations. The annotation data frames will have many more observations due to the fact that the unit of observations is word tokens. The calculated data frames’ unit of observation is the sentence.\nTo appreciate the difference in the number of observations, let’s look at the first 10 observations of the natives annotated frame for just the columns of interest, as seen in Example 7.40.\n\nExample 7.40  \n\n# Preview the annotated dataset\nenntt_natives_ann |&gt; \n  select(doc_id, sentence_id, sentence, token) |&gt;\n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 4\n&gt;    doc_id sentence_id sentence                                             token\n&gt;    &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;                                                &lt;chr&gt;\n&gt;  1 1                1 It is extremely important that action is taken to e… It   \n&gt;  2 1                1 It is extremely important that action is taken to e… is   \n&gt;  3 1                1 It is extremely important that action is taken to e… extr…\n&gt;  4 1                1 It is extremely important that action is taken to e… impo…\n&gt;  5 1                1 It is extremely important that action is taken to e… that \n&gt;  6 1                1 It is extremely important that action is taken to e… acti…\n&gt;  7 1                1 It is extremely important that action is taken to e… is   \n&gt;  8 1                1 It is extremely important that action is taken to e… taken\n&gt;  9 1                1 It is extremely important that action is taken to e… to   \n&gt; 10 1                1 It is extremely important that action is taken to e… ensu…\n\n\n\nThe annotated data frames have a lot of redundancy in for the join variables and the sentence variable that we want to add to the calculated data frames. We can reduce the redundancy by using the distinct() function from the dplyr package. In this case we want all observations where doc_id, sentence_id and sentence are distinct. We then select these variables with distinct(), as seen in Example 7.41.\n\nExample 7.41  \n\n# Reduce annotated data frames to unique sentences\nenntt_natives_ann_distinct &lt;- \n  enntt_natives_ann |&gt; \n  distinct(doc_id, sentence_id, sentence)\n\nenntt_translations_ann_distinct &lt;- \n  enntt_translations_ann |&gt; \n  distinct(doc_id, sentence_id, sentence)\n\n\nWe now have two datasets that are ready to be joined with the calculated datasets. The next step is to join the two. We will employ a left join where the syntactic complexity data frames are on the left and the join variables will be both the doc_id and sentence_id variables. The code is seen in Example 7.42.\n\nExample 7.42  \n\n# Join the native datasets\nenntt_natives_transformed_tbl &lt;- \n  left_join(\n    x = enntt_natives_syn_comp, \n    y = enntt_natives_ann_distinct, \n    by = c(\"doc_id\", \"sentence_id\")\n  )\n\n# Preview \nglimpse(enntt_natives_transformed_tbl)\n\n&gt; Rows: 10,199\n&gt; Columns: 5\n&gt; $ doc_id      &lt;chr&gt; \"1\", \"10\", \"100\", \"1000\", \"10000\", \"1001\", \"1002\", \"1003\",…\n&gt; $ sentence_id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n&gt; $ t_units     &lt;int&gt; 4, 2, 1, 0, 3, 2, 1, 2, 1, 2, 2, 1, 4, 2, 2, 4, 3, 2, 2, 1…\n&gt; $ word_len    &lt;int&gt; 21, 25, 27, 15, 40, 43, 29, 23, 13, 30, 33, 9, 68, 35, 16,…\n&gt; $ sentence    &lt;chr&gt; \"It is extremely important that action is taken to ensure …\n\n# Join the translations datasets \nenntt_translations_transformed_tbl &lt;- \n  left_join(\n    x = enntt_translations_syn_comp,\n    y = enntt_translations_ann_distinct,\n    by = c(\"doc_id\", \"sentence_id\")\n  )\n\n# Preview\nglimpse(enntt_translations_transformed_tbl)\n\n&gt; Rows: 10,392\n&gt; Columns: 5\n&gt; $ doc_id      &lt;chr&gt; \"1\", \"10\", \"100\", \"1000\", \"10000\", \"1001\", \"1002\", \"1003\",…\n&gt; $ sentence_id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n&gt; $ t_units     &lt;int&gt; 0, 2, 0, 1, 3, 0, 3, 2, 3, 3, 2, 0, 1, 0, 3, 1, 0, 1, 2, 2…\n&gt; $ word_len    &lt;int&gt; 24, 31, 5, 39, 44, 26, 67, 23, 46, 28, 24, 68, 19, 18, 36,…\n&gt; $ sentence    &lt;chr&gt; \"To my great surprise , on leaving the sitting , I found t…\n\n\n\n\nThe two data frames now have the same columns and we are closer to our final dataset. The next step is to move toward concatenating the two datasets. Before we do that, we need to do some preparation. First, and most important, we need to add a type column to each dataset. This column will indicate whether the sentence is a native or a translation. The second is that our doc_id does not serve as a unique identifier for the sentences. Only in combination with sentence_id can we uniquely identify a sentence.\nSo our plan will be to add a type column to each dataset specifying the values for all the observations in the respective dataset. Then we will concatenate the two datasets. Note, if we combine them before, distiguishing the type will be more difficult. After we concatenate the two datasets, we will add a doc_id column that will serve as a unique identifier for the sentences and drop the sentence_id column. OK, that’s the plan. Let’s execute it in Example 7.43.\n\nExample 7.43  \n\n# Add a type column\nenntt_natives_transformed_tbl &lt;- \n  enntt_natives_transformed_tbl |&gt; \n  mutate(type = \"natives\")\n\nenntt_translations_transformed_tbl &lt;- \n  enntt_translations_transformed_tbl |&gt; \n  mutate(type = \"translations\")\n\n# Concatenate the datasets\nenntt_transformed_tbl &lt;- \n  bind_rows(\n    enntt_natives_transformed_tbl,\n    enntt_translations_transformed_tbl\n  )\n\n# Overwrite the doc_id column with a unique identifier\nenntt_transformed_tbl &lt;- \n  enntt_transformed_tbl |&gt; \n  mutate(doc_id = row_number()) |&gt; \n  select(doc_id, type, t_units, word_len, text = sentence)\n\n# Preview\nglimpse(enntt_transformed_tbl)\n\n&gt; Rows: 20,591\n&gt; Columns: 5\n&gt; $ doc_id   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n&gt; $ type     &lt;chr&gt; \"natives\", \"natives\", \"natives\", \"natives\", \"natives\", \"nativ…\n&gt; $ t_units  &lt;int&gt; 4, 2, 1, 0, 3, 2, 1, 2, 1, 2, 2, 1, 4, 2, 2, 4, 3, 2, 2, 1, 1…\n&gt; $ word_len &lt;int&gt; 21, 25, 27, 15, 40, 43, 29, 23, 13, 30, 33, 9, 68, 35, 16, 53…\n&gt; $ text     &lt;chr&gt; \"It is extremely important that action is taken to ensure tha…\n\n\n\nThe output of Example 7.43 now looks like Table 7.11. We have a dataset that has the syntactic complexity measures for both the natives and the translations. We can now write this dataset to disk and document it in the data dictionary."
  },
  {
    "objectID": "transform-datasets.html#summary",
    "href": "transform-datasets.html#summary",
    "title": "7  Transform datasets",
    "section": "Summary",
    "text": "Summary\nIn this chapter we covered the process of transforming datasets. The goal is to manipulate the curated dataset to make it align better for analysis. We covered various types of transformation procedures from text normalization to data frame merges. In any given research project some or all of these steps will be employed –but not necessarily in the order presented in this chapter. It is not uncommon to mix procedures as well. The etiology of the transformation is as unique as the data that you are working with.\nSince you are applying techniques that have a significant factor on the shape and contents of your dataset(s) it is important to perform data checks to ensure that the transformations are working as expected. You may not catch everything, and some things may not be caught until later in the analysis process, but it is important to do as much as you can as early as you can.\nIn line with the reproducible research principles, it is important to write the transformed dataset to disk and to document it in the data dictionary. This is especially important if you are working with multiple datasets. Good naming conventions als come into play. Choosing descriptive names is so easily overlooked by your present self but so welcomed by your future self.\n\n more on concluding this part and moving to the next part\nThis chapter concludes the section on data/ dataset preparation. The next section we turn to analyzing datasets. This is the stage where we interrogate the datasets to derive knowledge and insight either through exploratory, predictive, or inferential methods."
  },
  {
    "objectID": "transform-datasets.html#activities",
    "href": "transform-datasets.html#activities",
    "title": "7  Transform datasets",
    "section": "Activities",
    "text": "Activities\n\n\n Add description of outcomes\n\n\n\n\n\n\n\n Recipe\n\n\nWhat: Transforming and documenting datasetsHow: Read Recipe 7 and participate in the Hypothes.is online social annotation.Why: To work with to primary types of transformations, tokenization and joins. Tokenization is the process of recasting textual units as smaller textual units. The process of joining datasets aims to incorporate other datasets to augment or filter the dataset of interest.\n\n\n\n\n\n\n\n\n\n Lab\n\n\nWhat: Dataset manipulation: tokenization and joining datasetsHow: Clone, fork, and complete the steps in Lab 7.Why: To gain experience working with coding strategies for transforming datasets using tidyverse functions and regular expressions, practice reading/ writing data from/ to disk, and implement organizational strategies for organizing and documenting a dataset in reproducible fashion."
  },
  {
    "objectID": "transform-datasets.html#questions",
    "href": "transform-datasets.html#questions",
    "title": "7  Transform datasets",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\n Conceptual questions\n\n…\n…\n\n\n\n\n\n\n\n\n\n\n Technical questions\n\n…\n…\n\n\n\n\n\n\n\n\nBengtsson, Henrik. 2023. Future: Unified Parallel and Distributed Processing in r for Everyone. https://future.futureverse.org.\n\n\nLiu, Kanglong, and Muhammad Afzaal. 2021. “Syntactic Complexity in Translated and Non-Translated Texts: A Corpus-Based Study of Simplification.” Edited by Diego Raphael Amancio. PLOS ONE 16 (6): e0253454. https://doi.org/10.1371/journal.pone.0253454.\n\n\nMullen, Lincoln. 2022. Tokenizers: Fast, Consistent Tokenization of Natural Language Text. https://docs.ropensci.org/tokenizers/.\n\n\nNivre, Joakim, Marie-Catherine De Marneffe, Filip Ginter, Jan Hajič, Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. 2020. “Universal Dependencies V2: An Evergrowing Multilingual Treebank Collection.” arXiv Preprint arXiv:2004.10643. https://arxiv.org/abs/2004.10643.\n\n\nRinker, Tyler. 2019. Lexicon: Lexicons for Text Analysis. https://github.com/trinker/lexicon.\n\n\nRobinson, David, and Julia Silge. 2023. Tidytext: Text Mining Using Dplyr, Ggplot2, and Other Tidy Tools. https://github.com/juliasilge/tidytext.\n\n\nSzmrecsanyi, Benedikt. 2004. “On Operationalizing Syntactic Complexity.” In Le Poids Des Mots. Proceedings of the 7th International Conference on Textual Data Statistical Analysis. Louvain-La-Neuve, 2:1032–39.\n\n\nTottie, Gunnel. 2011. “Uh and Um as Sociolinguistic Markers in British English.” International Journal of Corpus Linguistics 16 (2): 173–97.\n\n\nWickham, Hadley. 2022. Stringr: Simple, Consistent Wrappers for Common String Operations. https://stringr.tidyverse.org.\n\n\nWijffels, Jan. 2023. Udpipe: Tokenization, Parts of Speech Tagging, Lemmatization and Dependency Parsing with the UDPipe ’NLP’ Toolkit. https://bnosac.github.io/udpipe/en/index.html."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "In this section we turn to the analysis of datasets, the evaluation of results, and the interpretation of the findings. We will outline the three main types of statistical analyses: Exploratory Data Analysis (EDA), Predictive Data Analysis (PDA), and Inferential Data Analysis (IDA). Each of these analysis types have distinct, non-overlapping aims and therefore should be determined from the outset of the research project and included as part of the research blueprint. The aim of this section is to establish a clearer picture of the goals, methods, and value of each of these approaches."
  },
  {
    "objectID": "exploration.html#sec-eda-orientation",
    "href": "exploration.html#sec-eda-orientation",
    "title": "8  Exploration",
    "section": "\n8.1 Orientation",
    "text": "8.1 Orientation\nThe aim of this section is to provide an overview of exploratory data analysis (EDA). We will delve into various descriptive methods, such as frequency analysis and co-occurrence analysis, which are fundamental tools in linguistic research. However, our exploration won’t stop there. We will also integrate modern exploratory methods from unsupervised learning approaches, including clustering, dimensionality reductin, and vector space modeling. This may sound overwhelming, but I will strive to keep explanations clear and concise, ensuring their practicality and relevance to your linguistic inquiries is apparent. To this end, we will provide real-world examples to exemplify the applicability of these methodologies.\n\n8.1.1 Research goal\nAs discussed in Section 3.2.1 and Section 4.3.1, the goal of exploratory data analysis is to discover, describe, and posit new hypotheses. The researcher does not start with a preconceived hypothesis or prediction, but rather the researcher aims to uncover patterns and associations from data allowing the data to guide the trajectory of the analysis. This analysis approach is best-suited for research where the literature on a research question is limited, or where the researcher is interested in exploring a new research question.\nSince the researcher does not start with a preconceived hypothesis, the researcher is not able to test a hypothesis and generalize to a population, but rather the researcher is able to describe the data and provide a new perspective to be qualitatively assessed. This is achieved through an iterative and inductive process of data exploration, where the researcher uses quantitative methods to summarize, reduce, and sort complex datasets and statistically and visually interrogate a dataset letting the data guide the analysis.\n\n8.1.2 Approach\n\nThe approach to exploratory data analysis is iterative and inductive. To reign in the analysis, however, it is important to have a research question to guide the analysis. The research question will often be broad and exploratory in nature, but it will provide a framework for the analysis including the unit of analysis and sometimes the units of observation. Yet the units of observation can be modified as needed to address the research question. Furthermore, the methods applied to the data can evolve as the research unfolds. The researcher may start with a descriptive analysis and then move to an unsupervised learning approach, or vice versa. The researcher may also pivot the approach to explore new questions and new variables. Ultimately, the researcher is guided by the data and the research question, but the researcher is not bound by a preconceived hypothesis or prediction.\n\nWith a research question and relevant data in hand, we can look to conduct the analysis. The general workflow for exploratory data analysis is shown in Table 8.1.\n\n\nTable 8.1: Workflow for exploratory data analysis\n\n\n\n\n\n\nStep\nName\nDescription\n\n\n\n1\nIdentify\nConsider the research question and identify variables of potential interest to provide insight into our question.\n\n\n2\nInspect\nCheck for missing data, outliers, etc. and check data distributions and transform if necessary.\n\n\n3\nInterrogate\nSubmit the selected variables to descriptive (frequency, keyword, co-occurrence analysis, etc.) or unsupervised learning (clustering, dimensionality reduction, vector spacing modeling, etc.) methods to provide quantitative measures to evaluate.\n\n\n4\nInterpret\nEvaluate the results and determine if they are valid and meaningful to respond to the research question.\n\n\n5\n(Optional) Iterate\nRepeat steps 1-4 as new questions emerge from your interpretation.\n\n\n\n\nLet’s elaborate on each of these steps. First, we want to consider our research question and identify the variables of potential interest to provide insight to our question. Starting with a transformed dataset means that much of the data preparation has already been done, but we may need to further transform the data, either up front or as we explore the data. In text analysis, this often includes identifying and extracting the linguistic variables of interest, such as words, \\(n\\)-grams, sentences, etc. Depending on the annotation scheme, other linguistic variables may be of interest, such as part-of-speech tags, syntactic dependencies, semantic roles, etc.\nWe may also want to consider the operational measures of the variables derived from the text, such as frequency, dispersion, co-occurrence, keyness, etc. We may also want to consider the other variables in the dataset that may be target for grouping or filtering the dataset, such as speaker information, document information, linguistic unit information, etc.\nDuring or after extracting and operationalizing the variables of interest, we want to inspect the dataset to ensure the quality of the data and understand its characteristics. This may include checking for missing data, checking for outliers, checking for errors, checking for inconsistencies, etc. We may also want to inspect the distribution of the variables of interest to understand their characteristics. Summary statistics and visualizations, such as those covered in Section 3.1, are useful for inspecting the dataset and also provide a foundation for interrogating the dataset.\nOnce we have identified the variables of interest and inspected the dataset, we can interrogate the dataset using descriptive analysis and/ or unsupervised learning. Descriptive analysis is a set of methods that statistically and/ or visually summarizes a dataset. Descriptive analysis can be used to describe a dataset and to identify linguistic units (frequency analysis) or co-occuring (co-occurrence analysis) units that are distinctive to a particular group or sub-group in the dataset. Unsupervised learning is a machine learning approach that does not assume any particular relationship between variables in a dataset. It can be used to identify groupings (clustering) in the data including patterning of linguistic units, identifying semantically similar topics (topic modeling), and estimating word context relationships (vector space modeling).\n\nExploratory methods will produce a set of statistical and/ or visual results. The researcher must interpret these results to determine if they are meaningful and if they provide a new perspective on the research question. Many times the results from one method will lead to new questions which can be explored with other methods. In some cases, the results may not be meaningful and the researcher may need to return to the data preparation stage to modify the dataset or the variables of interest. As the aim of exploratory analysis is just that, to explore, the researcher can pivot the approach to explore new questions and new variables. Ultimately, what is meaningful is determined by the researcher in the light of the research question and the potential insight obtained from the results."
  },
  {
    "objectID": "exploration.html#sec-eda-analysis",
    "href": "exploration.html#sec-eda-analysis",
    "title": "8  Exploration",
    "section": "\n8.2 Analysis",
    "text": "8.2 Analysis\nIn this section will discuss exploratory data analysis (EDA) for linguists, with a focus on descriptive methods such as frequency analysis and co-occurence analysis, as well as unsupervised learning approaches such as clustering, topic modelling, and word embedding. To ground the discussion, we will use the the Manually Annotated Sub-Corpus (MASC) of the American National Corpus. The data dictionary for the masc_transformed dataset is shown in Table 8.2.\n\n\n\n\n\nTable 8.2: Data dictionary for the MASC dataset.\n\nvariable\nname\nvariable_type\ndescription\n\n\n\ndoc_id\nDocument ID\nnumeric\nUnique identifier for each document\n\n\ndescription\nDescription\ncategorical\nDescription of the content of the document\n\n\nmodality\nModality\ncategorical\nThe form in which the document is presented (written or spoken)\n\n\ngenre\nGenre\ncategorical\nThe category or type of the document\n\n\ndomain\nDomain\ncategorical\nThe subject or field to which the document belongs\n\n\nterm_num\nTerm Number\nnumeric\nIndex number term per document\n\n\nterm\nTerm\ncategorical\nIndividual word forms in the document\n\n\nlemma\nLemma\ncategorical\nBase or dictionary form of the term\n\n\npos\nPart of Speech\ncategorical\nGrammatical category of the term (modified PENN Treebank tagset)\n\n\n\n\n\n\n\n\n\nWe will work with the MASC as our dataset to approach a task, more than a question. The task will be to identify relevant materials for an English Language Learner (ELL) textbook. This will involve multiple research questions and allow us to illustrate some very fundamental concepts that will emerge across text analysis research.\nFirst, I’ll read in the dataset and only keep the variables that will pertain to our task, dropping the description and domain variables, and preview the dataset in Example 8.1.\n\nExample 8.1  \n\n# Read and subset the MASC dataset\nmasc_tbl &lt;- \n  read_csv(\"../data/masc/masc_transformed.csv\") |&gt; \n  select(-description, -domain)\n\n# Preview the MASC dataset\nmasc_tbl |&gt; \n  slice_head(n = 5)\n\n\n\n&gt; # A tibble: 5 × 7\n&gt;   doc_id modality genre   term_num term         lemma        pos  \n&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;\n&gt; 1      1 Written  Letters        0 December     december     NNP  \n&gt; 2      1 Written  Letters        1 1998         1998         CD   \n&gt; 3      1 Written  Letters        2 Your         your         PRP$ \n&gt; 4      1 Written  Letters        3 contribution contribution NN   \n&gt; 5      1 Written  Letters        4 to           to           TO\n\n\n\nFrom the output in Example 8.1, we should note a couple of things. First the doc_id is treated as numeric &lt;dbl&gt; and it is not a quantitative variable –we should change this vector type to &lt;chr&gt;. Second, at some point in our analysis we may need to recode some of the character variables to factor variables as analysis methods may require this.\n\nExample 8.2  \n\n# Change doc_id to character\nmasc_tbl &lt;- \n  masc_tbl |&gt; \n  mutate(doc_id = as.character(doc_id))\n\n\nTo get a better sense of distribution of the dataset, let’s use skim() from the skimr package to generate a summary of the dataset. In particular, let’s just focus on the character variables by using yank(\"character\"), as seen in Example 8.3.\n\nExample 8.3  \n\n# Load package\nlibrary(skimr)\n\n# Generate summary of the MASC dataset\nmasc_tbl_skm &lt;- \n  masc_tbl |&gt; \n  skim()\n\n# Pull character variables\nmasc_tbl_skm |&gt; \n  yank(\"character\") |&gt; \n  kable()\n\n\n\nTable 8.3: Summary of the MASC dataset.\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\ndoc_id\n0\n1\n1\n3\n0\n392\n0\n\n\nmodality\n0\n1\n6\n7\n0\n2\n0\n\n\ngenre\n0\n1\n4\n12\n0\n18\n0\n\n\nterm\n25\n1\n1\n99\n0\n39470\n0\n\n\nlemma\n4\n1\n1\n99\n0\n28008\n0\n\n\npos\n0\n1\n2\n5\n0\n39\n0\n\n\n\n\n\n\n\n\n\nLooking at Table 8.3, we see that there are 392 documents, two modalities, 18 genres, over 30k unique terms (which are words), over 28k lemmas (word base forms), and 39 distinct part-of-speech tags.\n\n\n8.2.1 Descriptive analysis\nDescriptive analysis includes common techniques such as frequency analysis to determine the most frequent words or phrases, dispersion analysis to see how terms or topics are distributed throughout a document or corpus, keyword analysis to identify distinctive terms, and/ or co-occurrence analysis to see what terms tend to appear together.\nUsing the MASC dataset, we will entertain questions such as:\n\nWhat are the most common terms a beginning ELL should learn?\nAre there term differences between spoken and written discourses that should be emphasized?\nWhat are the most common verb particle constructions?\n\nAlong the way, we will introduce some fundamental concepts in text analysis such as tokens and types and frequency, dispersion, and co-occurrence measures. In addition, we will apply various descriptive analysis techniques and visualizations to explore the dataset and identify new questions and new variables of interest.\nFrequency analysis\n\nAt its core, frequency analysis is a descriptive method that counts the number of times a linguistic unit, or term, (i.e. word, \\(n\\)-gram, sentence, etc.) occurs in a dataset. The results of frequency analysis can be used to describe the dataset and to identify terms that are linguistically distinctive or distinctive to a particular group or sub-group in the dataset.\n\nRaw frequency\nLet’s consider what the most common words in the MASC dataset are as a starting point to making inroads on our task by identifying relevant vocabulary for an ELL textbook.\nIn the masc_tbl data frame we have the linguistic unit term which corresponds to the word-level annotation of the MASC. The lemma corresponds to the base form of each term, for words with inflectional morphology, the lemma is the word sans the inflection (e.g. is - be, are - be). For other words, the term and the lemma will be the same (e.g. the - the, in - in). These two variables pose a choice point for us: do we consider words to be the actual forms or the base forms? There is an argument to be made for both. In this case I will operationalize our linguistic unit as the lemma variable, as this will allow us to group words with inflectional morphology together.\nTo perform a basic word frequency analysis, we start by using the count() function from the dplyr package to count the number of times each lemma occurs in the dataset. We’ll sort by the most frequent lemmas, as seen in Example 8.4.\n\nExample 8.4  \n\n# Lemma count, sorted\nmasc_tbl |&gt; \n  count(lemma, sort = TRUE)\n\n&gt; # A tibble: 28,009 × 2\n&gt;    lemma     n\n&gt;    &lt;chr&gt; &lt;int&gt;\n&gt;  1 ,     27113\n&gt;  2 .     26258\n&gt;  3 the   26137\n&gt;  4 be    19466\n&gt;  5 to    13548\n&gt;  6 and   12528\n&gt;  7 of    12005\n&gt;  8 a     10480\n&gt;  9 in     8374\n&gt; 10 i      7783\n&gt; # ℹ 27,999 more rows\n\n\n\nThe output of this frequency tabulation in Example 8.4 is a data frame with two columns: lemma and n. The lemma column contains the unique lemmas in the dataset, and the n column contains the frequency of each lemma. The data frame is sorted in descending order by the frequency of lemmas. Now the result includes over 28,000 rows –which corresponds to the number of unique lemmas in the dataset.\nAt this point, it is important to define a few key concepts that are fundamental to working with text. First, a term is a defined linguistic unit extracted from a corpus. In our dataset, the terms are words, such as ‘the’, ‘houses’, ‘are’. A lemma is an annotated recoding of words which represent the uninflected base form of a word. In either case, the term or lemma is an instance of a linguistic unit. These instances are called tokens. When we count the number of times a term or lemma occurs in a dataset, we are counting the number of tokens (n), such as in Example 8.4. Now, the list of unique linguistic units is a list of types (lemma). By definition, then, there will always be at least as many tokens as types, but more often than not (many) more tokens than types.\nOur first pass at calculating lemma frequency in Example 8.4 should bring something else to our attention. As we can see among the most frequent lemmas are non-words such as ,, and .. As you can imagine, given the conventions of written and transcriptional language, these types are very frequent. For a frequency analysis focusing on words, however, we should probably remove them. Thinking ahead, there may also be other non-words that we want to remove, such as symbols, numbers, etc. Let’s take a look at Figure 8.1, where I’ve counted the part-of-speech tags pos in the dataset to see what other non-words we might want to remove.\n\nExample 8.5  \n\n# Part-of-speech tags\nmasc_tbl |&gt; \n  count(pos) |&gt; \n  ggplot(\n    aes(x = reorder(pos, desc(n)), y = n)\n    ) +\n  geom_col() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + \n  labs(x = \"Part-of-speech tags\", y = \"Token frequency\")\n\n\n\nFigure 8.1: Part-of-speech tags in the MASC dataset.\n\n\n\n\nConsulting the PENN Tagset online, we can see that the pos variable includes a number of non-words or other elements to exclude including:\n\n‘CD’ - Cardinal number\n‘FW’ - Foreign word\n‘LS’ - List item marker\n‘SYM’ - Symbol\n\nThis modified tagset has grouped the punctuation tags into a single tag, ‘PUNCT’.\nWe can use this information to remove lemmas that are tagged with either of these values. We can do this by filtering the data frame to only include lemmas that are not tagged with the pos values listed above, as seen in Example 8.6.\n\nExample 8.6  \n\n# Filter out lemmas with PUNCT or SYM for pos\nmasc_tbl &lt;- \n  masc_tbl |&gt; \n  filter(!(pos %in% c(\"CD\", \"FW\", \"LS\", \"SYM\", \"PUNCT\")))\n\n# Lemma count, sorted (again)\nmasc_tbl |&gt; \n  count(lemma, sort = TRUE)\n\n&gt; # A tibble: 26,164 × 2\n&gt;    lemma     n\n&gt;    &lt;chr&gt; &lt;int&gt;\n&gt;  1 the   26137\n&gt;  2 be    19466\n&gt;  3 to    13548\n&gt;  4 and   12528\n&gt;  5 of    12005\n&gt;  6 a     10461\n&gt;  7 in     8374\n&gt;  8 i      7783\n&gt;  9 that   7082\n&gt; 10 you    5276\n&gt; # ℹ 26,154 more rows\n\n\n\nNow we are only viewing the most frequent words in the dataset, which reduces the number of observations to around 26k. Let’s now explore the frequency distribution of the tokens. In Figure 8.2, I’ve created three plots which include: 1) all the types, 2) the top 100 types, and 3) the top 10 types in the dataset.\n\n# [ ] consider how to present the 'all types' plot better, more concisely\n\n# Plot lemma count for all types\nmasc_tbl |&gt; \n  count(lemma) |&gt;\n  arrange(desc(n)) |&gt;\n  ggplot(aes(x = reorder(lemma, desc(n)), y = n)) +\n  geom_col() +\n  labs(x = \"Types\", y = \"Token frequency\") +\n  theme(axis.text.x = element_blank())\n\n# Plot lemma count for top 100 types\nmasc_tbl |&gt;\n  count(lemma) |&gt;\n  arrange(desc(n)) |&gt;\n  slice_head(n = 100) |&gt;\n  ggplot(aes(x = reorder(lemma, desc(n)), y = n)) +\n  geom_col() +\n  labs(x = \"Types\", y = \"Token frequency\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1.3))\n\n# Plot lemma count for top 10 types\nmasc_tbl |&gt; \n  count(lemma) |&gt;\n  arrange(desc(n)) |&gt;\n  slice_head(n = 10) |&gt;\n  ggplot(aes(x = reorder(lemma, desc(n)), y = n)) +\n  geom_col() +\n  labs(x = \"Types\", y = \"Token frequency\") +\n  theme(axis.text.x = element_text(angle = 65, hjust = 1.3))\n\n\n\n\n\n(a) All types\n\n\n\n\n\n(b) Top 100 types\n\n\n\n\n\n(c) Top 10 types\n\n\n\nFigure 8.2: Frequency plots of tokens in the MASC dataset\n\n\n\nThe distributions we see in Figure 8.2 are highly right-skewed (in Figure 8.2 (a) in a very extreme way!). This is typical of natural language distributions, notably documented by George Kingsley Zipf (Zipf 1949). This type of distribution approaches the theoretical Zipf distribution. A Zipf (or Zipfian) distribution is characterized by the fact that the frequency of any word is inversely proportional to its rank in the frequency table. In other words, the most frequent word occurs approximately twice as often as the second most frequent word, three times as often as the third most frequent word, and so on.\nAs we can see, our distribuions to not follow the Zipf distribution exactly. This is because the Zipf distribution is a theoretical distribution, and the actual distribution of words in a corpus is affected by various sampling factors, including the size of the corpus. The larger the corpus, the closer the distribution will be to the Zipf distribution.\n\n\n\n\n\n\n Dive deeper\nAs stated above, Zipfian distributions are typical of natural language and are observed a various linguistic levels. This is because natural language is a complex system, and complex systems tend to exhibit Zipfian distributions. Other examples of complex systems that exhibit Zipfian distributions include the size of cities, the frequency of species in ecological communities, the frequency of links in the World Wide Web, etc.\n\n\n\nThe observation captured in the Zipf distribution is key to understanding quantitative text analysis. It demonstrates that most of the types in a corpus occur (relatively) infrequently, while a small number of types occur very frequently. In fact, if we calculate the cumulative frequency of the lemmas in the masc_tbl data frame, we can see that the top 10 types account for over 20% of the lemmas used in the dataset –by 100 types that increases to over 40%, as seen in Example 8.7.\n\nExample 8.7  \n\n# Calculate cumulative frequency\nlemma_cumul_freq &lt;- \n  masc_tbl |&gt; \n  count(lemma) |&gt; \n  arrange(desc(n)) |&gt; \n  mutate(cumulative = cumsum(n)) |&gt; \n  mutate(percent = cumulative / sum(n))\n\nlemma_cumul_freq |&gt;\n  slice_head(n = 2000) |&gt; \n  ggplot(aes(x = reorder(lemma, desc(n)), y = percent)) +\n  geom_col() +\n  geom_vline(xintercept = 10, linetype = \"dashed\") +\n  geom_vline(xintercept = 100, linetype = \"dashed\") +\n  # annotate(\"text\", x = 10+10, y = 0.5, label = \"10 lemmas\") +\n  # annotate(\"text\", x = 100+10, y = 0.5, label = \"100 lemmas\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +\n  labs(x = \"Types\", y = \"Cumulative frequency percent\") +\n  theme(axis.text.x = element_blank())\n\n\n\nFigure 8.3: Cumulative frequency of lemmas in the MASC dataset\n\n\n\n\nIf we look at the types that appear within the first 100 most frequent, you can likely also appreciate another thing about language use. Let’s list the top 100 types in Example 8.8.\n\nExample 8.8  \n\n# Top 100 types\nlemma_cumul_freq |&gt; \n  slice_head(n = 100) |&gt; \n  pull(lemma) |&gt; \n  matrix(ncol = 10, byrow = TRUE) |&gt;\n  kable(col.names = NULL)\n\n\n\nTable 8.4: Top 100 lemma types in the MASC dataset.\n\n\nthe\nbe\nto\nand\nof\na\nin\ni\nthat\nyou\n\n\nhave\nit\nfor\non\ndo\nwith\nwe\nas\nthis\nnot\n\n\nat\nfrom\nhe\nbut\nby\nwill\nmy\nor\nn't\nthey\n\n\nyour\nan\nsay\nwhat\nso\nhis\nif\n's\ncan\ngo\n\n\nall\nthere\nme\nwould\nabout\nknow\nget\nmake\nout\nup\n\n\nthink\nour\nshe\nmore\ntime\njust\nno\nwhen\ntheir\nlike\n\n\nher\nwho\nwhich\nother\nsee\npeople\nnew\ns\ntake\nnow\n\n\nwork\nsome\nyear\nhow\nthem\nuse\ncome\ninto\nwell\nthan\n\n\nlook\nits\nmay\nright\nthen\ncould\nbecause\nonly\nus\nthese\n\n\nwant\nany\nalso\nneed\nway\nwhere\nback\nhim\nhere\n'\n\n\n\n\n\n\n\n\n\nFor the most part, the most frequent words are not content words, but rather function words (e.g. determiners, prepositions, pronouns, auxiliary verbs). Function words include a closed class of relatively few words that are used to express grammatical relationships between content words. It then is no surprise that they are the comprise many of the most frequent words in a corpus.\nAnother key observation is that among the most frequency content words (e.g. nouns, verbs, adjectives, adverbs) are words that are quite semantically generic –that is, they are words that are used in a wide range of contexts and take a wide range of meanings. Take for example the adjective ‘good’. It can be used to describe a wide range of nouns, such as ‘good food’, ‘good people’, ‘good times’, etc. A sometimes near-synonym of ‘good’, for example ‘good student’, is the word ‘studious’. Yet, ‘studious’ is not as frequent as ‘good’ as it is used to describe a narrower range of nouns, such as ‘studious student’, ‘studious scholar’, ‘studious researcher’, etc. In this way, ‘studious’ is more semantically specific than ‘good’.\n\n\n\n\n\n\n Consider this\nBased on what you now know about the expected distribution of words in a corpus, what if your were asked to predict what the most frequency English word used is in each U.S. State? What would you predict? How confident would you be in your prediction? What if you were asked to predict what the most frequency word used is in the language of a given country? What would you want to know before making your prediction?\n\n\n\nSo common across corpus samples, in some analyses these usual suspects of the most common words are considered irrelvant and are filtered out. In our ELL materials task, however, we might exclude them for this simple fact that it will be a given that we will teach these words given their grammatical importance. If we want to focus on the most common content words, we can filter out the function words.\nOne approach to filtering out these words is to use a pre-determined list of stopwords. The tidytext package includes a data frame stop_words of stopword lexicons for English. We can select a lexicon from stop_words and use anti_join() to filter out the words that appear in the word variable from the lemma variable in the masc_tbl data frame. In Example 8.9, I perform this filtering and then re-run the frequency analysis for the top 100 lemmas.\n\nExample 8.9  \n\n# Load package\nlibrary(tidytext)\n\n# Select stopword lexicon\nstopwords &lt;- \n  stop_words |&gt; \n  filter(lexicon == \"SMART\")\n\n# Filter out stop words\nanti_join(\n  x = masc_tbl,\n  y = stopwords,\n  by = c(\"lemma\" = \"word\")\n  ) |&gt;\n  count(lemma, sort = TRUE) |&gt; \n  slice_head(n = 100) |&gt; \n  pull(lemma) |&gt; \n  matrix(ncol = 10, byrow = TRUE) |&gt;\n  as_tibble() |&gt; \n  kable(col.names = NULL)\n\n\n\nTable 8.5: Frequency of tokens in the MASC dataset after filtering out stopwords\n\n\nn't\n's\nmake\ntime\npeople\nwork\nyear\nback\n'\nfind\n\n\ngive\nday\nthing\njack\nman\nyeah\ngood\ncall\nworld\npresident\n\n\nstate\nquestion\nservice\nchange\nlife\nleave\nsubject\nset\nlong\nplace\n\n\nwrite\nshow\nchild\nend\nfeel\nhand\nsystem\nschool\ninformation\npart\n\n\ngroup\nfollow\nrun\nsupport\ntoday\npoint\nread\nprovide\nuh\nsend\n\n\nturn\ninclude\ntalk\nfact\n&\nlive\nput\nword\nnumber\nstart\n\n\nlaw\ncase\ncompany\nmoney\ngreat\nopen\nhome\ncity\nissue\nwoman\n\n\njob\namerican\nimportant\nresult\nbook\nhear\nsparrow\nhouse\nproblem\num\n\n\namerica\nwalk\nfamily\nbegin\ncountry\ndate\nface\nfriend\nreport\nmove\n\n\norder\nhead\nid\nwatch\nform\nprogram\nmarket\nweek\narea\nfigure\n\n\n\n\n\n\n\n\n\nThe resulting list in Table 8.5 paints a different picture of the most frequent words in the dataset. The most frequent words are now content words, and included in most frequent words are more semantically specific words.\nEliminating words in this fashion, however, may not always be the best approach. Available lists of stopwords vary in their contents and are determined by other researchers for other potential uses. We may instead opt to create our own stopword list that is tailored to the task, or we may opt to use a statistical approach based on their distribution in the dataset using a combination of frequency and dispersion measures, as we will see in [the next section.]\nFor our case, however, we have another strategy to apply. Since our task is to identify relevant vocabulary, beyond the fundamental function words in English, we can use the part-of-speech tags to reduce our dataset to just the content words, that is nouns, verbs, adjectives, and adverbs. We need to consult the Penn Tagset again, to ensure we are selecting the correct tags. I will assign this data frame to masc_content_tbl to keep it separate from our main data frame masc_tbl, seen in Example 8.10.\n\nExample 8.10  \n\n# Penn Tagset for content words\n# Nouns: NN, NNS,\n# Verbs: VB, VBD, VBG, VBN, VBP, VBZ\n# Adjectives: JJ, JJR, JJS\n# Adverbs: RB, RBR, RBS\n\ncontent_pos &lt;- c(\"NN\", \"NNS\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\")\n\n# Select content words\nmasc_content_tbl &lt;- \n  masc_tbl |&gt; \n  filter(pos %in% content_pos)\n\n\nWe now have reduced the number of observations by 50% focusing on the content words. We are getting closer to identifying the vocabulary that we want to include in our ELL materials, but we will need some more tools to help us identify the most relevant vocabulary.\nDispersion\nDispersion is a measure of how evenly distributed a linguistic unit is across a dataset. This is a key concept in text analysis, as important as frequency. It is important to recognize that frequency and dispersion are measures of different characteristics. We can have two words that occur with the same frequency, but one word may be more evenly distributed across a dataset than the other. Depending on the researcher’s aims, this may be an important distinction to make. For our task, it is likely the case that we want to capture words that are well-dispersed across the dataset as words that have a high frequency and a low dispersion tend to be connected to a particular context, whether that be a particular genre, a particular speaker, a particular topic, etc. In other research, aim may be the reverse; to identify words that are highly frequent and highly concentrated in a particular context to identify words that are distinctive to that context.\nThis a wide variety of measures that can be used to estimate the distribution of types across a dataset. Let’s focus on three measures: document frequency (\\(df\\)), inverse document frequency (\\(idf\\)), and Gries’ Deviation of Proportions (\\(dp\\)).\nThe most basic measure is document frequency (\\(df\\)). This is the number of documents in which a type appears at least once. For example, if a type appears in 10 documents, then the document frequency is 10. This is a very basic measure, but it is a good starting point.\nA nuanced version of document frequency is inverse document frequency (\\(idf\\)). This measure takes the total number of documents and divides it by the document frequency. This results in a measure that is inversely proportional to the document frequency. That is, the higher the document frequency, the lower the inverse document frequency. This measure is often log-transformed to spread out the values.\nOne thing to consider about \\(df\\) and \\(idf\\) is that niether takes into account the length of the documents in which the type appears nor the spread of the type within each document. To take these factors into account, we can use Gries’ Deviation of Proportions (\\(dp\\)) measure (Gries 2023, 87–88). The \\(dp\\) measure is calculated as the difference between the proportion of a tokens in a document and tokens in the corpus. The metric can be subtracted from 1 to create a normalized measure of dispersion ranging between 0 and 1, with lower values being more dispersed.\nLet’s consider how these measures differ with three scenarios:\nImagine a type with a token frequency of 100 appears in each of the 10 documents in a corpus.\nA. Each of the documents is 100 words long. The type appears 10 times in each document. B. Each of the documents is 100 words long. But now the type appears once in 9 documents and 91 times in 1 document. C. Nine of the documents constitute 99% of the corpus. The type appears once in each of the 9 documents and 91 times in the 10th document.\nScenario A is the most dispersed, scenario B is less dispersed, and scenario C is the least dispersed. Yet, the type’s \\(df\\) and \\(idf\\) scores will be the same. But the \\(dp\\) score will reflect increasing concentration of the type from A to B to C. You may wonder why we would want to use \\(df\\) or \\(idf\\) at all. The answer is some combination of the fact that they are computationally less expensive to calculate, they are widely used (especially \\(idf\\)), and/ or in many practical situations they often highly correlated with \\(dp\\).\nSo for our task we will use \\(dp\\) as our measure of dispersion. The qtalrkit package includes the calc_type_metrics() function which calculates, among other metrics, the dispersion metrics \\(df\\), \\(idf\\), and/ or \\(dp\\). Let’s select dp and assign the result to masc_lemma_disp, as seen in Example 8.11.\n\nExample 8.11  \n\n# Load package\nlibrary(qtalrkit)\n\n# Calculate deviance of proportions (DP)\nmasc_lemma_disp &lt;- \n  masc_content_tbl |&gt; \n  calc_type_metrics(\n    type = lemma, \n    documents = doc_id, \n    dispersion = \"dp\"\n  ) |&gt; \n  arrange(dp)\n\n# Preview\nmasc_lemma_disp |&gt; \n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 3\n&gt;    type      n    dp\n&gt;    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n&gt;  1 be    19231 0.123\n&gt;  2 have   5136 0.189\n&gt;  3 not    2279 0.240\n&gt;  4 make   1149 0.266\n&gt;  5 other   882 0.270\n&gt;  6 more   1005 0.276\n&gt;  7 take    769 0.286\n&gt;  8 only    627 0.286\n&gt;  9 time    931 0.314\n&gt; 10 see     865 0.327\n\n\n\nWe would like to identify lemmas that are frequent and well-dispersed. But an important question arises, what is the threshold for frequency and dispersion that we should use to identify the lemmas that we want to include in our ELL materials?\n\n\n\n\n\n\n Consider this\nYou may be wondering why the Inverse Document Frequency is, in fact, the inverse of the document counts, instead of just a count of the documents that each type appears in. The \\(idf\\) is a very common measure in machine learning that is used in combination with (term) frequency to calculate the \\(tf-idf\\) (term frequency-inverse document frequency) measure. That is, the product of the frequency of a term and the inverse document frequency of the term. This serves as a weighting measure that lowers the \\(tf-idf\\) score for terms that are frequent across documents and increases the \\(tf-idf\\) score for terms that are infrequent across documents.\nConsider what types will end up with a high or a low \\(tf-idf\\) score. What use(s) could this measure have for distinguishing between types in a corpus?\nHint: consider the earlier discussion of stopword lists.\n\n\n\nThere are statistical approaches to identifying natural breakpoints, including clustering, but a visual inspection is often good enough for practical purposes. Let’s create a density plot to see if there is a natural break in the distribution of our dispersion measure, as seen in Figure 8.4.\n\nExample 8.12  \n\n# Density plot of dp\nmasc_lemma_disp |&gt; \n  ggplot(aes(x = dp)) +\n  geom_density() +\n  scale_x_continuous(breaks = seq(0, 1, .1)) +\n  labs(x = \"Deviation of Proportions\")\n\n\n\nFigure 8.4: Density plot of Deviation of Proportions for lemmas in the MASC dataset\n\n\n\n\nWhat we are looking for is an elbow in the distribution of dispersion measures. In Figure 8.4, we can see that there is distinctive bend in the distribution between .85 and .97. We can split the difference and use this as a threshold to filter out lemmas that are less dispersed. In Example 8.13, I filter out lemmas that have a dispersion measure less than .91. Then in Table 8.6, I preview the top and bottom 50 lemmas in the dataset.\n\nExample 8.13  \n\n# Filter for lemmas with dp &lt;= .91\nmasc_lemma_disp_thr &lt;- \n  masc_lemma_disp |&gt; \n  filter(dp &lt;= .91) |&gt; \n  arrange(desc(n))\n\n# Preview top\nmasc_lemma_disp_thr |&gt; \n  slice_head(n = 50) |&gt; \n  pull(type) |&gt;\n  matrix(ncol = 10, byrow = TRUE) |&gt; \n  kable(col.names = NULL)\n# Preview bottom\nmasc_lemma_disp_thr |&gt;\n  slice_tail(n = 50) |&gt; \n  pull(type) |&gt;\n  matrix(ncol = 10) |&gt; \n  kable(col.names = NULL)\n\n\nTable 8.6: Frequency of tokens in the MASC dataset after filtering out lemmas with a Deviation of Proportions less than .91\n\n\n\n\n\n(a) Top 50 lemmas\n\n\nbe\nhave\ndo\nnot\nn't\nsay\ngo\nknow\nget\nmake\n\n\nthink\nmore\njust\ntime\nso\nother\nsee\npeople\ntake\nnow\n\n\nwork\nyear\ncome\nuse\nwell\nlook\nthen\nright\nonly\nwant\n\n\nalso\nway\nneed\nback\nhere\nnew\nfind\ngive\nthing\ntell\n\n\nt\nfirst\nhelp\nday\nmany\nman\nask\nvery\nmuch\neven\n\n\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\n\n(b) Bottom 50 lemmas\n\n\nfortunately\nignorance\nliability\nbrave\nsummarize\nliberty\nwound\nnostalgic\naccidentally\nwax\n\n\ndump\nsting\ntuition\nunleash\nblur\ngoing\ndevote\nshy\nprotective\nfaith-based\n\n\ninstrument\nmainstream\nawaken\nprosperous\nresistance\nawkward\nalright\nproximity\npreside\ndecidedly\n\n\ntriumph\nwildly\nhook\nbuzz\nabsurd\nafterwards\nevolutionary\nsandy\nrethink\nresolute\n\n\nharsh\ndismiss\nfetch\npresume\nqualify\neve\nenvy\ninterfere\nstrictly\nevidently\n\n\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\nWe now have a good candidate list of common vocabulary that is spread well across the corpus.\nRelative frequency\nGauging frequency and dispersion across the entire corpus is a good starting point for any frequency analysis, but it is often the case that we want to compare the frequency and dispersion of linguistic units across corpora or sub-corpora.\nIn the case of the MASC dataset, for example, we may want to compare metrics across the two modalities or the various genres. Simply comparing frequency counts across these sub-corpora is not a good approach, and can be misleading, as the sub-corpora may vary in size. For example, if one sub-corpus is twice as large as another sub-corpus, then, all else being equal, the frequency counts will be twice as large in the larger sub-corpus. This is why we use relative frequency measures, which are normalized by the size of the sub-corpus.\n\n\n\n\n\n\n Consider this\nA variable in the MASC dataset that has yet to be used is the pos part-of-speech variable. How could we use this variable to refine our frequency and dispersion analysis of lemma types?\nHint: consider lemma forms that may be tagged with different parts-of-speech.\n\n\n\nTo normalize the frequency of linguistic units across sub-corpora, we can use the relative frequency measure. This is the frequency of a linguistic unit divided by the total number of linguistic units in the sub-corpus. This bakes in the size of the sub-corpus into the measure. The notion of relative frequency is key to all research working with text, as it is the basis for the statistical approach to text analysis where comparisons are made.\nThere are some field-specific terms that are used to refer to relative frequency measures. For example, in information retrieval and Natural Language Processing, the relative frequency measure is often referred to as the term frequency. In corpus linguistics, the relative frequency measure is often modified slightly to include a constant (e.g. \\(rf * 100\\)) which is known as the observed relative frequency. Athough the observed relative frequency per number of tokens is not strictly necessary, it is often used to make the values more interpretable as we can now talk about an observed relative frequency of 1.5 as a linguistic unit that occurs 1.5 times per 100 linguistic units.\nLet’s consider how we might compare the frequency and dispersion of lemmas across the two modalities in the MASC dataset, spoken and written. To make this a bit more interesting and more relevant, let’s add the pos variable to our analysis. The intent, then, will be to identify lemmas tagged with particular parts of speech that are particularly indicative of each of the modaliites.\nWe can do this by collapsing the lemma and pos variables into a single variable, lemma_pos, with the str_c() function, as seen in Example 8.14.\n\nExample 8.14  \n\n# Collapse lemma and pos into type\nmasc_content_tbl &lt;- \n  masc_content_tbl |&gt; \n  mutate(lemma_pos = str_c(lemma, pos, sep = \"_\"))\n\n# Preview\nmasc_content_tbl |&gt; \n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 8\n&gt;    doc_id modality genre   term_num term         lemma        pos   lemma_pos   \n&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;       \n&gt;  1 1      Written  Letters        3 contribution contribution NN    contributio…\n&gt;  2 1      Written  Letters        7 mean         mean         VB    mean_VB     \n&gt;  3 1      Written  Letters        8 more         more         JJR   more_JJR    \n&gt;  4 1      Written  Letters       12 know         know         VB    know_VB     \n&gt;  5 1      Written  Letters       15 help         help         VB    help_VB     \n&gt;  6 1      Written  Letters       17 see          see          VB    see_VB      \n&gt;  7 1      Written  Letters       19 much         much         JJ    much_JJ     \n&gt;  8 1      Written  Letters       21 contribution contribution NN    contributio…\n&gt;  9 1      Written  Letters       22 means        mean         VBZ   mean_VBZ    \n&gt; 10 1      Written  Letters       25 'm           be           VBP   be_VBP\n\n\n\nNow this will increase the number of lemma types in the dataset as we are now considering lemmas where the same lemma form is tagged with different parts-of-speech.\nGetting back to calculating the frequency and dispersion of lemmas in each modality, we can use the calc_type_metrics() function with lemma_pos as our type argument. We will, however, need to apply this function to each sub-corpus independently and then concatenate the two data frames. This function returns a (raw) frequency measure by default, but we can specify thefrequency argument to rf to calculate the relative frequency of the linguistic units as in Example 8.15.\n\nExample 8.15  \n\n# Calculate relative frequency\n# Spoken\nmasc_spoken_metrics &lt;- \n  masc_content_tbl |&gt; \n  filter(modality == \"Spoken\") |&gt; \n  calc_type_metrics(\n    type = lemma_pos, \n    documents = doc_id, \n    frequency = \"rf\",\n    dispersion = \"dp\"\n  ) |&gt; \n  mutate(modality = \"Spoken\") |&gt;\n  arrange(desc(n))\n\n# Written \nmasc_written_metrics &lt;- \n  masc_content_tbl |&gt; \n  filter(modality == \"Written\") |&gt; \n  calc_type_metrics(\n    type = lemma_pos, \n    documents = doc_id, \n    frequency = \"rf\",\n    dispersion = \"dp\"\n  ) |&gt; \n  mutate(modality = \"Written\") |&gt; \n  arrange(desc(n))\n\n# Concatenate spoken and written metrics\nmasc_metrics &lt;-\n  bind_rows(masc_spoken_metrics, masc_written_metrics)\n\n# Preview\nmasc_metrics |&gt; \n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 5\n&gt;    type         n      rf     dp modality\n&gt;    &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n&gt;  1 be_VBZ    2612 0.0489  0.0842 Spoken  \n&gt;  2 be_VBP    1282 0.0240  0.111  Spoken  \n&gt;  3 be_VBD    1020 0.0191  0.301  Spoken  \n&gt;  4 n't_RB     829 0.0155  0.139  Spoken  \n&gt;  5 have_VBP   766 0.0143  0.152  Spoken  \n&gt;  6 do_VBP     728 0.0136  0.180  Spoken  \n&gt;  7 be_VB      655 0.0123  0.147  Spoken  \n&gt;  8 not_RB     638 0.0119  0.137  Spoken  \n&gt;  9 just_RB    404 0.00756 0.267  Spoken  \n&gt; 10 so_RB      387 0.00725 0.357  Spoken\n\n\n\nWith the rf measure, we are now in a position to compare ‘apples to apples’, as you might say. We can now compare the relative frequency of lemmas across the two modalities. Let’s preview the top 10 lemmas in each modality, as seen in Example 8.16.\n\nExample 8.16  \n\n# Preview top 10 lemmas in each modality\nmasc_metrics |&gt; \n  group_by(modality) |&gt; \n  slice_max(n = 10, order_by = rf) |&gt;\n  ungroup()\n\n&gt; # A tibble: 20 × 5\n&gt;    type         n      rf     dp modality\n&gt;    &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n&gt;  1 be_VBZ    2612 0.0489  0.0842 Spoken  \n&gt;  2 be_VBP    1282 0.0240  0.111  Spoken  \n&gt;  3 be_VBD    1020 0.0191  0.301  Spoken  \n&gt;  4 n't_RB     829 0.0155  0.139  Spoken  \n&gt;  5 have_VBP   766 0.0143  0.152  Spoken  \n&gt;  6 do_VBP     728 0.0136  0.180  Spoken  \n&gt;  7 be_VB      655 0.0123  0.147  Spoken  \n&gt;  8 not_RB     638 0.0119  0.137  Spoken  \n&gt;  9 just_RB    404 0.00756 0.267  Spoken  \n&gt; 10 so_RB      387 0.00725 0.357  Spoken  \n&gt; 11 be_VBZ    4745 0.0248  0.230  Written \n&gt; 12 be_VBD    3317 0.0173  0.366  Written \n&gt; 13 be_VBP    2617 0.0137  0.237  Written \n&gt; 14 be_VB     1863 0.00974 0.218  Written \n&gt; 15 not_RB    1640 0.00858 0.259  Written \n&gt; 16 have_VBP  1227 0.00642 0.290  Written \n&gt; 17 n't_RB     905 0.00473 0.540  Written \n&gt; 18 have_VBD   859 0.00449 0.446  Written \n&gt; 19 have_VBZ   777 0.00406 0.335  Written \n&gt; 20 say_VBD    710 0.00371 0.609  Written\n\n\n\nWe can appreciate, now, that there are similarities and a few differences between the most frequent lemmas for each modality. First, there are similar lemmas in written and spoken modalities, such as ‘be’, ‘have’, and ‘not’. Second, the top 10 include verbs and adverbs. Now we are looking at the most frequent types, so it is not surprising that we see more in common than not. However, looking close we can see that contracted forms are more frequent in the spoken modality, such as ‘isn’t’, ‘don’t’, and ‘can’t’ and that ordering of the verb tenses differs to some degree. Whether these are important distinctions for our task is something we will need to consider.\nWe can further cull our results by filtering out lemmas that are not well-dispersed across the sub-corpora. Although it may be tempting to use the threshold we used earlier, we should consider that the size of the sub-corpora are different and the distribution of the dispersion measure may be different. With this in mind, we need to visualize the distribution of the dispersion measure for each modality, as seen in Figure 8.5.\n\n# Density plot of dp by modality\nmasc_metrics |&gt; \n  ggplot(aes(x = dp)) +\n  geom_density(alpha = .5) +\n  scale_x_continuous(breaks = seq(0, 1, .1)) +\n  labs(x = \"Deviation of Proportions\", y = \"Density\") +\n  facet_wrap(~ modality, ncol = 2, scales = \"free_x\")\n\n\n\nFigure 8.5: Density plot of Deviation of Proportions for lemmas in the MASC dataset by modality\n\n\n\nAs expected, the density plots point to different thresholds for each modality.The written subcorpus follows closely with the previous distribution, but the spoken subcorpus has more than one bend in the distribution. Why are there multiple peaks in the density plot? It points to some level of inconsistency in the spoken data, either potentially some level of context-dependent language use (genres, topics, speech styles) or it could be due to the fact that the spoken subcorpus’ size is too small to provide a reliable distribution.\nIn any case, we can estimate the threshold for the spoken corpus making use of the largest peak in the distribution as the reference point. With this approach in mind, lets maintain the \\(.91\\) threshold for the written subcorpus and use a \\(.79\\) threshold for the spoken subcorpus. Let’s filter out lemmas that have a dispersion measure less than .91 for the written subcorpus and less than .79 for the spoken subcorpus, as seen in Example 8.17.\n\nExample 8.17  \n\n# Filter for lemmas with\n# dp &lt;= .91 for written and \n# dp &lt;= .79 for spoken\nmasc_metrics_thr &lt;- \n  masc_metrics |&gt; \n  filter(\n    (modality == \"Written\" & dp &lt;= .91) | \n    (modality == \"Spoken\" & dp &lt;= .79)\n  ) |&gt; \n  arrange(desc(rf))\n\n\nFiltering the less-dispersed types reduces the dataset from 33637 to 4860 observations. This will provide us with a more succinct list of common and well-dispersed lemmas that are used in each modality.\nAs much as the frequency and dispersion measures can provide us with a good starting point, it does not provide an understanding of what types are more indicative of a particular sub-corpus, modality subcorpora in our case. We can do this by calculating the log odds ratio of each lemma in each modality.\nThe log odds ratio is a measure that quantifies the difference between the frequencies of a type in two corpora or sub-corpora. In spirit and in name, it compares the odds of a type occurring in one corpus versus the other. The values range from negative to positive infinity, with negative values indicating that the type is more frequent in the first corpus and positive values indicating that the lemma is more frequent in the second corpus. The magnitude of the value indicates the strength of the association.\nThe tidylo package provides a convenient function bind_log_odds() to calculate the log odds ratio, and a weighed variant, for each type in each sub-corpus. Let’s use this function to calculate the log odds ratio for each lemma in each modality, as seen in Example 8.18.\n\nExample 8.18  \n\n# Load package\nlibrary(tidylo)\n\n# Calculate log odds ratio\nmasc_metrics_thr &lt;- \n  masc_metrics_thr |&gt; \n  bind_log_odds(\n    set = modality,\n    feature = type,\n    n = n, \n    unweighted = TRUE\n  )\n\n# Preview (ordered by log_odds)\n# Spoken\nmasc_metrics_thr |&gt; \n  slice_max(n = 10, order_by = log_odds)\n\n&gt; # A tibble: 10 × 7\n&gt;    type                  n       rf    dp modality log_odds log_odds_weighted\n&gt;    &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n&gt;  1 understanding_NN     45 0.000843 0.649 Spoken      0.955              6.41\n&gt;  2 meeting_NNS          42 0.000786 0.702 Spoken      0.955              6.19\n&gt;  3 testimony_NN         36 0.000674 0.785 Spoken      0.955              5.73\n&gt;  4 administration_NN    34 0.000637 0.650 Spoken      0.955              5.57\n&gt;  5 trial_NN             33 0.000618 0.769 Spoken      0.955              5.49\n&gt;  6 governor_NN          28 0.000524 0.597 Spoken      0.955              5.05\n&gt;  7 intelligent_JJ       28 0.000524 0.777 Spoken      0.955              5.05\n&gt;  8 faith_NN             27 0.000506 0.524 Spoken      0.955              4.96\n&gt;  9 walk_VBZ             27 0.000506 0.761 Spoken      0.955              4.96\n&gt; 10 gun_NNS              26 0.000487 0.577 Spoken      0.955              4.87\n\n# Written\nmasc_metrics_thr |&gt; \n  slice_min(n = 10, order_by = log_odds)\n\n&gt; # A tibble: 10 × 7\n&gt;    type             n        rf    dp modality log_odds log_odds_weighted\n&gt;    &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n&gt;  1 correct_JJ      13 0.0000680 0.900 Written    -0.461             -3.75\n&gt;  2 mean_VBP        38 0.000199  0.776 Written    -0.411             -5.00\n&gt;  3 president_NN    37 0.000194  0.840 Written    -0.406             -4.81\n&gt;  4 board_NN        27 0.000141  0.830 Written    -0.405             -4.10\n&gt;  5 argument_NN     24 0.000126  0.798 Written    -0.356             -3.07\n&gt;  6 meeting_NN      44 0.000230  0.852 Written    -0.343             -3.90\n&gt;  7 question_NN     80 0.000418  0.630 Written    -0.330             -4.94\n&gt;  8 say_VBN         20 0.000105  0.767 Written    -0.325             -2.42\n&gt;  9 read_VBN        10 0.0000523 0.895 Written    -0.325             -1.71\n&gt; 10 cut_NN           9 0.0000471 0.879 Written    -0.325             -1.62\n\n\n\nThe distinctive terms in each modality from Example 8.18 may not jibe with your intuitio, and that’s understandable. This is likely because we are comparing sub-corpora of different sizes and with different document lengths. The log odds ratio is a measure that is sensitive to these differences.\nThe second measure produced by bind_log_odds() function, is the weighted log odds ratio. This measure provides a more robust and interpretable measure for comparing term frequencies across corpora, especially when term frequencies are low or when corpora are of different sizes. The weighting (or standardization) also makes it easier to identify terms that are particularly distinctive or characteristic of one corpus over another. Note that the weighted measure’s interpretation is slightly different that the log odds’s. The larger positive values in each corpus indicate that the type is more indicative of that (sub-)corpus, and the larger negative values indicate that the type is less indicative.\nLet’s imagine we would like to extract the most indicative verbs for each modality using the weighted log odds as our measure. We can do this with a little regular expression magic. Let’s use the str_subset() function to filter for lemmas that start with ‘V’ and then use slice_max() to extract the top 10 most indicative verb lemmas, as seen in Example 8.19.\n\nExample 8.19  \n\n# Preview (ordered by log_odds_weighted)\n# Spoken and written\nmasc_metrics_thr |&gt; \n  group_by(modality) |&gt; \n  filter(str_detect(type, \"_V\")) |&gt; \n  slice_max(n = 10, order_by = log_odds_weighted) |&gt;\n  select(-n, -log_odds) |&gt;\n  ungroup() \n\n&gt; # A tibble: 20 × 5\n&gt;    type                rf     dp modality log_odds_weighted\n&gt;    &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n&gt;  1 be_VBZ        0.0489   0.0842 Spoken               14.0 \n&gt;  2 do_VBP        0.0136   0.180  Spoken               10.3 \n&gt;  3 be_VBP        0.0240   0.111  Spoken                8.66\n&gt;  4 think_VBP     0.00655  0.259  Spoken                8.32\n&gt;  5 have_VBP      0.0143   0.152  Spoken                8.00\n&gt;  6 know_VBP      0.00528  0.260  Spoken                7.03\n&gt;  7 go_VBG        0.00534  0.207  Spoken                6.47\n&gt;  8 do_VBD        0.00603  0.321  Spoken                5.97\n&gt;  9 mean_VBP      0.00247  0.543  Spoken                5.94\n&gt; 10 do_VB         0.00455  0.207  Spoken                5.61\n&gt; 11 don_VB        0.000361 0.839  Written               4.04\n&gt; 12 doe_VBZ       0.000350 0.871  Written               3.98\n&gt; 13 walk_VBD      0.000319 0.790  Written               3.80\n&gt; 14 associate_VBN 0.000303 0.777  Written               3.70\n&gt; 15 reply_VBD     0.000293 0.838  Written               3.64\n&gt; 16 develop_VBG   0.000288 0.812  Written               3.60\n&gt; 17 require_VBN   0.000272 0.793  Written               3.50\n&gt; 18 fall_VBD      0.000267 0.757  Written               3.47\n&gt; 19 meet_VB       0.000241 0.729  Written               3.30\n&gt; 20 regard_VBG    0.000225 0.823  Written               3.19\n\n\n\nNote that the log odds are larger for the spoken modality than the written modality. This indicates that theses types are more strongly indicative of the spoken modality than the types in the written modality are indicative of the written modality. This is not surprising, as the written modality is typically more diverse in terms of lexical usage than the spoken modality, where the terms tend to be repeated more often, including verbs.\nCo-occurrence analysis\nMoving forward on our task, we have a good idea of the general vocabulary that we want to include in our ELL materials and can identify lemma types that are particularly indicative of each modality. Another useful approach to complement our analysis is to identify words that co-occur with our target lemmas –in particular verbs. In English it is common for verbs to appear with a preposition or adverb, such as ‘give up’, ‘look after’. These ‘phrasal verbs’ form a semantic unit that is distinct from the verb alone.\nIn cases such as this, we are aiming to do a co-occurrence analysis. Co-occurrence analysis is a set of methods that are used to identify words that appear in close proximity to a target type.\n\nAn exploratory, primarily qualitative, approach is to display the co-occurrence of words in a Keyword in Context (KWIC). This is a table that displays the target word in the center of the table and the words that appear before and after the target word. This is a useful approach for spot identifying collocations of a target word or phrase.\nThe quanteda package includes a function kwic() that can be used to create a KWIC table. It does require some transformation to the data, however. We need to collapse the lemma column into a single string for each document from the original transformed dataset, masc_tbl. Then we can apply the corpus() and then tokens function to create a quanteda tokens object. Then we can apply the kwic() function to create a KWIC table.\n\nExample 8.20  \n\n# Collapse lemma, pos into a single string\nmasc_text_tbl &lt;- \n  masc_tbl |&gt; \n  mutate(lemma_pos = str_c(lemma, pos, sep = \"_\")) |&gt;\n  group_by(doc_id, modality, genre) |&gt; \n  summarize(text = str_c(lemma_pos, collapse = \" \")) |&gt; \n  ungroup()\n\n# Load package\nlibrary(quanteda)\n\nmasc_corpus &lt;- \n  masc_text_tbl |&gt; \n  corpus(\n    text_field = \"text\",\n    docid_field = \"doc_id\"\n  )\n\nmasc_corpus |&gt; \n  tokens() |&gt; \n  kwic(\n    pattern = phrase(\"*_V* *_IN*\"),\n    window = 3\n  ) |&gt; \n  as_tibble() |&gt; \n  select(docname, pre, keyword, post) |&gt;\n  slice_sample(n = 10)\n\n&gt; # A tibble: 10 × 4\n&gt;    docname pre                               keyword             post           \n&gt;    &lt;chr&gt;   &lt;chr&gt;                             &lt;chr&gt;               &lt;chr&gt;          \n&gt;  1 171     other_JJ hollywood_NNP star_NNS   come_VBD as_IN      well_RB simply…\n&gt;  2 242     not_RB only_RB i_PRP              find_VBD out_IN     more_JJR about…\n&gt;  3 206     long_JJ have_VBP you_PRP          live_VBD in_IN      charlotte_NNP …\n&gt;  4 76      fire_NN let_VBG it_PRP            burn_VBP out_IN     by_IN itself_P…\n&gt;  5 170     s_POS paranoia_NN by_IN           flee_VBG into_IN    egypt_NNP with…\n&gt;  6 382     be_VBZ currently_RB be_VBG        examine_VBN by_IN   regulator_NNS …\n&gt;  7 217     collection_NN of_IN nestorian_NNP cross_VBZ from_IN   the_DT yuan_NN…\n&gt;  8 98      destroy_VBN save_VB for_IN        reserve_VBN for_IN  the_DT smithso…\n&gt;  9 233     secret_JJ in_NNP attorney_NNS     charge_VBN with_IN  prosecute_VBG …\n&gt; 10 153     the_DT artist_NN have_VBD         reach_VBN within_IN himself_PRP it…\n\n\n\n\nA straightforward quantitative way to explore co-occurrence is to set the unit of observation to an \\(n-gram\\) of terms. An \\(n-gram\\) is a sequence of \\(n\\) words. For example, a 2-gram is a sequence of two words, a 3-gram is a sequence of three words, and so on. Then, the frequency and dispersion metrics can be calculated for each \\(n-gram\\).\nIn general, deriving \\(n-grams\\) from a corpus is a straightforward process. The tidytext package includes a function unnest_tokens() that can be used to create \\(n-grams\\) from a corpus. The function can take a single column of untokenized text or a tokenized column in combination with a variable to use as the grouping variable. In the masc_tbl dataset, we have tokenized text in the lemma column and a grouping variable in the doc_id column. We can use the unnest_tokens() function to create a new data frame with a row for each \\(n-gram\\) in each document, as seen in Example 8.21.\n\nExample 8.21  \n\n# Load package\nlibrary(tidytext)\n\n# Create bigrams\nmasc_tbl |&gt; \n  unnest_tokens(\n    output = bigrams, \n    input = lemma, \n    token = \"ngrams\", \n    n = 2, \n    collapse = \"doc_id\"\n    ) |&gt; \n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 2\n&gt;    doc_id bigrams          \n&gt;    &lt;chr&gt;  &lt;chr&gt;            \n&gt;  1 1      december your    \n&gt;  2 1      your contribution\n&gt;  3 1      contribution to  \n&gt;  4 1      to goodwill      \n&gt;  5 1      goodwill will    \n&gt;  6 1      will mean        \n&gt;  7 1      mean more        \n&gt;  8 1      more than        \n&gt;  9 1      than you         \n&gt; 10 1      you may\n\n\n\nThe result of this operation can be joined with the original dataset using the doc_id as the key variable. Then we can calculate the frequency and dispersion metrics for each \\(n-gram\\) in each modality. However, this approach is not ideal for our task. The reason is that we are interested in identifying \\(n-grams\\) that include verbs. The unnest_tokens() function does not allow us to filter the \\(n-grams\\) by part-of-speech.\nAnother, more informative approach is to create a new variable that combines the lemma and part-of-speech for each observation before using unnest_tokens() to generate the bigrams. We can use the str_c() function from the stringr package to join the lemma and pos columns into a single string, so that we have a variable lemma_pos with the lemma and part-of-speech joined by an underscore.\nOne consideration that we need to take for our goal to identify verb particle constructions, is how we ultimately want to group our lemma_pos values. This is particularly important given the fact that our pos tags for verbs include information about the verb’s tense and person. This means that a verb in a verb particle bigram, such as ‘look after’, will be represented by multiple lemma_pos values, such as ‘look_VB’, ‘look_VBP’, ‘look_VBD’, and ‘look_VBG’. If we want this level of detail, we just proceed as described above. However, if we want to group the verb particle bigrams by a single verb value, we need to recode the pos values for verbs. We can do this with the case_match() function from the dplyr package.\nIn Example 8.22, I recode the pos values for verbs to ‘V’ and then join the lemma and pos columns into a single string.\n\nExample 8.22  \n\n# Collapse lemma into a single string\nmasc_lemma_pos_tbl &lt;- \n  masc_tbl |&gt; \n  mutate(pos = case_when(\n    str_detect(pos, \"^V\") ~ \"V\",\n    TRUE ~ pos\n  )) |&gt; \n  group_by(doc_id) |&gt; \n  mutate(lemma_pos = str_c(lemma, pos, sep = \"_\")) |&gt;\n  ungroup()\n\n# Preview\nmasc_lemma_pos_tbl |&gt; \n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 8\n&gt;    doc_id modality genre   term_num term         lemma        pos   lemma_pos   \n&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;       \n&gt;  1 1      Written  Letters        0 December     december     NNP   december_NNP\n&gt;  2 1      Written  Letters        2 Your         your         PRP$  your_PRP$   \n&gt;  3 1      Written  Letters        3 contribution contribution NN    contributio…\n&gt;  4 1      Written  Letters        4 to           to           TO    to_TO       \n&gt;  5 1      Written  Letters        5 Goodwill     goodwill     NNP   goodwill_NNP\n&gt;  6 1      Written  Letters        6 will         will         MD    will_MD     \n&gt;  7 1      Written  Letters        7 mean         mean         V     mean_V      \n&gt;  8 1      Written  Letters        8 more         more         JJR   more_JJR    \n&gt;  9 1      Written  Letters        9 than         than         IN    than_IN     \n&gt; 10 1      Written  Letters       10 you          you          PRP   you_PRP\n\nmasc_lemma_pos_tbl |&gt; \n  unnest_tokens(\n    output = bigrams, \n    input = lemma_pos, \n    token = \"ngrams\", \n    n = 2, \n    to_lower = FALSE,\n    collapse = \"doc_id\"\n  ) |&gt; \n  filter(str_detect(bigrams, \"_V.*_IN\")) |&gt; \n  calc_type_metrics(\n    type = bigrams, \n    documents = doc_id, \n    frequency = \"rf\",\n    dispersion = \"dp\"\n  ) |&gt; \n  arrange(desc(rf))\n\n&gt; # A tibble: 4,512 × 4\n&gt;    type                n      rf    dp\n&gt;    &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n&gt;  1 be_V in_IN        359 0.0259  0.366\n&gt;  2 be_V that_IN      210 0.0151  0.504\n&gt;  3 look_V at_IN      157 0.0113  0.528\n&gt;  4 say_V that_IN     134 0.00966 0.580\n&gt;  5 be_V on_IN        120 0.00865 0.523\n&gt;  6 talk_V about_IN   114 0.00821 0.622\n&gt;  7 be_V of_IN        110 0.00793 0.503\n&gt;  8 know_V that_IN     98 0.00706 0.605\n&gt;  9 think_V that_IN    90 0.00649 0.643\n&gt; 10 be_V about_IN      76 0.00548 0.608\n&gt; # ℹ 4,502 more rows\n\n\n\nWe have identified and derived frequency and dispersion metrics for \\(n-grams\\) that include verb particle construction candidates. Yet, there is a problem with this approach. The problem is that the \\(n-grams\\) are not necessarily verb particle constructions in the sense that they form a semantic unit. Second, frequency and dispersion metrics are not necessarily the best measures for identifying the co-occurrence relationship between the verb and the particle. In other words, just because a two-word sequence is frequent and well-dispersed does not mean that the two words form a semantic unit.\n\nTo address these issues, we can use a statistical measures to estimate collocational strength between two words. A collocation is a sequence of words that co-occur more often than would be expected by chance. The most common measure of collocation is the pointwise mutual information (PMI) measure. The PMI measure is calculated as the log ratio of the observed frequency of two words co-occurring to the expected frequency of the two words co-occurring. The expected frequency is calculated as the product of the frequency of each word. The PMI measure is a log ratio, so the values range from negative to positive infinity, with negative values indicating that the two words co-occur less often than would be expected by chance and positive values indicating that the two words co-occur more often than would be expected by chance. The magnitude of the value indicates the strength of the association.\nLet’s calculate the PMI for all the bigrams in the MASC dataset. We can use the calc_assoc_metrics() function from qtalrkit. We need to specify the association argument to pmi and the type argument to bigrams, as seen in Example 8.23.\n\nExample 8.23  \n\nmasc_lemma_pos_assoc &lt;- \n  masc_lemma_pos_tbl |&gt; \n  calc_assoc_metrics(\n    doc_index = doc_id, \n    token_index = term_num, \n    type = lemma_pos, \n    association = \"pmi\"\n  )\n\n# Preview \nmasc_lemma_pos_assoc |&gt; \n  arrange(desc(pmi)) |&gt;\n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 4\n&gt;    x               y                   n   pmi\n&gt;    &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n&gt;  1 #Christian_NN   bigot_NN            1  12.4\n&gt;  2 #FAIL_NN        phenomenally_RB     1  12.4\n&gt;  3 #NASCAR_NN      #indycar_NN         1  12.4\n&gt;  4 #PALM_NN        merchan_NN          1  12.4\n&gt;  5 #Twitter_NN     #growth_NN          1  12.4\n&gt;  6 #college_NN     #jobs_NN            1  12.4\n&gt;  7 #education_NN   #teaching_NN        1  12.4\n&gt;  8 #faculty_NN     #cites_NN           1  12.4\n&gt;  9 #fb_NN          siebel_NNP          1  12.4\n&gt; 10 #glitchmyass_NN reps_NNP            1  12.4\n\n\n\nOne caveat to using the PMI measure is that it is sensitive to the frequency of the words. If the words in a bigram pair are infrequent, and especially if they only occur once, then the PMI measure will be inflated. To mitigate this issue, we can apply a frequency threshold to the bigrams before calculating the PMI measure. Let’s filter out bigrams that occur less than 10 times, as seen in Example 8.24.\n\nExample 8.24  \n\n# Filter for bigrams that occur &gt;= 10 times\nmasc_lemma_pos_assoc_thr &lt;- \n  masc_lemma_pos_assoc |&gt; \n  filter(n &gt;= 10) |&gt; \n  arrange(desc(pmi))\n\n# Preview\nmasc_lemma_pos_assoc_thr |&gt; \n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 4\n&gt;    x             y                n   pmi\n&gt;    &lt;chr&gt;         &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n&gt;  1 pianista_NN   irlandesa_NN    10 10.0 \n&gt;  2 costa_NNP     rica_NNP        10  9.95\n&gt;  3 nanowrimo_NNP novel_NNP       12  9.87\n&gt;  4 bin_NN        laden_NNP       11  9.79\n&gt;  5 osama_NNP     bin_NN          11  9.79\n&gt;  6 bin_NNP       ladin_NNP       11  9.70\n&gt;  7 los_NNP       angeles_NNP     11  9.64\n&gt;  8 chilean_JJ    seabass_NNS     13  9.64\n&gt;  9 novel_NNP     ch_NNP          12  9.58\n&gt; 10 st_NNP        zip_NNP         10  9.52\n\n\n\nNow we are in a position to identify verb particle constructions. We can filter for bigrams that include a verb and a particle and that have a PMI measure greater than 0, as seen in Example 8.25.\n\nExample 8.25  \n\n# Filter for bigrams that include a verb and a particle\n# and that have a PMI measure greater than 0\nmasc_verb_part_assoc &lt;- \n  masc_lemma_pos_assoc_thr |&gt; \n  filter(str_detect(x, \"_V\")) |&gt; \n  filter(str_detect(y, \"_IN\")) |&gt;\n  filter(pmi &gt; 0) |&gt; \n  arrange(x)\n\n# Preview \nmasc_verb_part_assoc |&gt; \n  slice_head(n = 10)\n\n&gt; # A tibble: 10 × 4\n&gt;    x             y           n   pmi\n&gt;    &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n&gt;  1 account_V     for_IN     17  3.74\n&gt;  2 acknowledge_V that_IN    13  3.62\n&gt;  3 act_V         as_IN      14  2.96\n&gt;  4 agree_V       with_IN    45  3.21\n&gt;  5 agree_V       that_IN    14  1.94\n&gt;  6 appear_V      on_IN      12  1.98\n&gt;  7 appear_V      in_IN      24  1.76\n&gt;  8 argue_V       that_IN    20  3.26\n&gt;  9 arrive_V      at_IN      18  3.39\n&gt; 10 arrive_V      in_IN      10  1.48\n\n\n\nWe can clean up the results a bit by removing the part-of-speech tags from the x and y variables, up our minimum PMI value, and create a network plot to visualize the results, as seen in Figure 8.6.\n\n# Clean up results\nmasc_verb_part_assoc_plot &lt;-\n  masc_verb_part_assoc |&gt;\n  filter(pmi &gt;= 2) |&gt;\n  mutate(\n    x = str_remove(x, \"_V.*\"),\n    y = str_remove(y, \"_IN\")\n  )\n\n# Create an association network plot\n# `x` and `y` are the nodes\n# `pmi` is the edge weight\n\nlibrary(igraph)\nlibrary(ggraph)\n\nmasc_verb_part_assoc_plot |&gt;\n  graph_from_data_frame() |&gt;\n  ggraph(layout = \"nicely\") +\n  geom_edge_link(aes(color = pmi),\n    alpha = 0.8,\n    edge_width = 0.8,\n    arrow = grid::arrow()\n  ) +\n  geom_node_point(color = \"black\") +\n  geom_node_text(aes(label = name), repel = TRUE) +\n  scale_edge_color_gradient(low = \"grey90\", high = \"grey20\") +\n  theme_void()\n\n\n\nFigure 8.6: Network plot of verb particle constructions in the MASC dataset\n\n\n\nFrom this plot, and from the underlying data, we can explore verb particle constructions. We could go further and apply our co-occurrence methods to each modality separately, if we wanted to identify verb particle constructions that are distinctive to each modality. We could also apply our co-occurrence methods to other parts-of-speech, such as adjectives and nouns, to identify collocations of these parts-of-speech. There is much more to explore with co-occurrence analysis, but this should give you a good idea of the types of questions that can be addressed with co-occurrence analysis.\n\n8.2.2 Unsupervised learning\nAligned in purpose with descriptive approaches, unsupervised learning approaches to exploratory data analysis are used to identify patterns in the data from an algorithmic perspective. Common methods in text analysis include principle component analysis, clustering, and vector space modeling.\nWe will continue to use the MASC dataset as we develop materials for our ELL textbook to illustrate unsupervised learning methods. In the process, we will explore the following questions:\n\nCan we identify and group documents based on linguistic features or co-occurrence patterns of the data itself?\nDo the groups of documents map to the labels in the dataset?\nCan we estimate the semantics of words based on their co-occurrence patterns?\n\nThrough these questions we will build on our knowledge of frequency, dispersion, and co-occurrence analysis and introduce concepts and methods associated with machine learning.\nClustering\nClustering is a unsupervised learning technique that can be used to group similar items in the text data, helping to organize the data into distinct categories and discover relationships between different elements in the text. The main steps in the procedure includes identifying the relevant linguistic features to use for clustering, representing the features in a way that can be used for clustering, and applying a clustering algorithm to the data. However, it is important to consider the strengths and weaknesses of the clustering algorithm for a particular task and how the results will be evaluated.\nIn our ELL textbook task, we may very well want to explore the similiarities and/ or differences between the documents based on the distribution of linguistic features. This provides us a view to evaluate to what extent the labels in the dataset (modality and genre) map to the distribution of linguistic features. Based on this evaluation, we may want to consider re-labeling the documents, collapsing labels, or even adding new labels.\nEnter clustering. Instead of relying entirely on the labels in the MASC dataset, we can let the data itself say something about how related the documents are. Yet, a pivotal question is what features should we use, otherwise known as feature selection. We could use terms or lemmas, but we may want to consider other features, such as parts-of-speech or some co-occurrence patterns. We are not locked into using one criterion, and we can perform clustering multiple times with different features, but we should consider the implications of our feature selection for our interpretation of the results.\nAnother key question is what clustering algorithm to use. Again, we are not married to one algorithm, and we can perform clustering multiple times with different algorithms, but not all algorithms are created equal. Some algorithms are better suited for certain types of data and certain types of tasks. For example, Hierarchical clustering is a good choice when we are not sure how many clusters we want to identify, as it does not require us to specify the number of clusters from the outset. However, it is not a good choice when we have a large dataset, as it can be computationally expensive compared to some other algorithms. K-means clustering, on the other hand, is a good choice when we want to identify a pre-defined number of clusters, and the aim is to gauge how well the data fit the clusters. These two clustering techniques, therefore complement each other with Hierarchical clustering being a good choice for initial exploration and K-means clustering being a good choice for targeted evaluation.\nWith these considerations in mind, let’s start by identifying the linguistic features that we want to use for clustering. Imagine that among the various features that we are interested in associating documents, we consider lemma use and part-of-speech use. However, we need to operationalize what we mean by ‘use’. In machine learning, this process is known as feature engineering. Since we aim to compare documents it is logical for us to use the document-normalized features. So in both lemma and part-of-speech tags, we will use the relative frequency. An additional operation that we can apply to the lemma feature is to weight the relative frequency by the dispersion of the lemma. This will give us a measure of the distinctiveness of the lemma in the document. A common implementation of this approach is to use the \\(tf-idf\\) measure, which is the product of the relative frequency and the inverse document frequency.\nEach of these engineered feature sets represents a different aspect of the lexical nature of the documents. The relative frequency of lemmas represents the lexical diversity of the documents, the dispersion-weighted \\(tf-idf\\) of lemmas represents the distinctiveness of the lemmas in the documents, and the relative frequency of part-of-speech tags represents the grammatical diversity of the documents (Petrenz and Webber 2011). //FIXME CITATIONS\nThe next question to address in any analysis is how to represent the features. In our case, we want to represent the features in each document. In machine learning, the most common way to represent features is in a matrix. In our case, we want to create a matrix with the documents in the rows and the features in the columns. The values in the matrix will be the operationalization of lexical use in each document for each of our three candidate measures. This configuration is known as a document-term matrix (DTM).\nTo recast a data frame into a DTM, we can use the cast_dtm() function from the tidytext package. This function takes a data frame with a document identifier, a feature identifier, and a value for each observation and casts it into a matrix. Operations such as normalization are easily and efficiently performed in R on matrices, so initially we can cast a frequency table of lemmas and part-of-speech tags into a matrix and then normalize the matrix by documents. For the \\(tf-idf\\) measure we use the bind_tf_idf() function from the tidytext package. This function takes a DTM and calculates the \\(tf-idf\\) measure for each feature in each document. This is a normalized measure, so we do not need to normalize the matrix by documents. Let’s see how this works with the MASC dataset in Example 8.26.\n\nExample 8.26  \n\n# Load packages\nlibrary(tidytext)\n\n# Create a document-term matrix of lemmas\nmasc_lemma_dtm &lt;- \n  masc_tbl |&gt; \n  count(doc_id, lemma) |&gt; \n  cast_dtm(doc_id, lemma, n) |&gt; \n  as.matrix()\n\n# Create a document-term matrix of part-of-speech tags\nmasc_pos_dtm &lt;- \n  masc_tbl |&gt; \n  count(doc_id, pos) |&gt; \n  cast_dtm(doc_id, pos, n) |&gt; \n  as.matrix()\n\n# Create a document-term matrix of tf-idf weighted lemmas\nmasc_lemma_tfidf_dtm &lt;- \n  masc_tbl |&gt; \n  count(doc_id, lemma) |&gt; \n  bind_tf_idf(doc_id, lemma, n) |&gt; \n  cast_dtm(doc_id, lemma, tf_idf) |&gt; \n  as.matrix()\n\n\nNote preview the a subset of the contents of a matrix, such as in Example 8.26, we use bracket syntax [] instead of the head() function. Let’s take a look at the first 5 rows and 5 columns of the matrices, as seen in Example 8.27.\n\nExample 8.27  \n\n# Preview\nmasc_lemma_dtm[1:5, 1:5]\n\n&gt;      Terms\n&gt; Docs  'd 's M.  a account\n&gt;   1    1  1  1 15       1\n&gt;   10   0  0  0  7       0\n&gt;   100  0  0  0  0       0\n&gt;   101  0  0  0  2       0\n&gt;   102  0  0  0  1       0\n\nmasc_pos_dtm[1:5, 1:5]\n\n&gt;      Terms\n&gt; Docs  CC DT EX IN JJ\n&gt;   1   14 35  1 44 27\n&gt;   10  11 38  0 39 18\n&gt;   100  0  2  0  2  3\n&gt;   101  3 16  0 23  7\n&gt;   102 20 29  0 34 20\n\nmasc_lemma_tfidf_dtm[1:5, 1:5]\n\n&gt;      Terms\n&gt; Docs      'd      's    M.        a account\n&gt;   1   0.0556 0.00324 0.228 0.006549  0.0414\n&gt;   10  0.0000 0.00000 0.000 0.003320  0.0000\n&gt;   100 0.0000 0.00000 0.000 0.000000  0.0000\n&gt;   101 0.0000 0.00000 0.000 0.001064  0.0000\n&gt;   102 0.0000 0.00000 0.000 0.000485  0.0000\n\n\n\nNow we can normalize the lemma and pos matrices by documents. We can do this by dividing each feature count by the total count in each document. This is a row-wise transformation, so we can use the rowSums() function from base R to calculate the total count in each document. Then each count divided by its row’s total count, as seen in Example 8.28.\n\nExample 8.28  \n\n# Normalize lemma and pos matrices by documents\nmasc_lemma_dtm &lt;- \n  masc_lemma_dtm / rowSums(masc_lemma_dtm)\n\nmasc_pos_dtm &lt;- \n  masc_pos_dtm / rowSums(masc_pos_dtm)\n\n\nThere are two concerns to address before we can proceed with clustering. First, clustering algorithm performance tends to degrade with the number of features. If we consider either the relative frequency of lemmas or the dispersion-weighted \\(tf-idf\\) of lemmas, we are looking at over 25k features! Second, clustering algorithms perform better with more informative features. That is to say, features that are more distinct across the documents provide better information for deriving useful clusters.\nWe can address both of these concerns by reducing the number of features and increasing the informativeness of the features. To accomplish this is to use dimensionality reduction. Dimensionality reduction is a set of methods that are used to reduce the number of features in a dataset while retaining as much information as possible. The most common method for dimensionality reduction is principle component analysis (PCA). PCA is a method that transforms a set of correlated variables into a set of uncorrelated variables, known as principle components. The principle components are ordered by the amount of variance that they explain in the data. The first principle component explains the most variance, the second principle component explains the second most variance, and so on.\nWe can apply PCA to each of these features and assess how well the features account for the variation in the data. We can then use the features that account for the most variation in the data for clustering. The prcomp() function from base R can be used to perform PCA. Let’s apply PCA to each of our candidate feature matrices, as seen in Example 8.29.\n\nExample 8.29  \n\n# Set seed for reproducibility\nset.seed(123)\n\n# Apply PCA to each feature matrix\nmasc_lemma_pca &lt;- \n  masc_lemma_dtm |&gt; \n  prcomp()\n\nmasc_pos_pca &lt;- \n  masc_pos_dtm |&gt; \n  prcomp()\n\nmasc_lemma_tfidf_pca &lt;- \n  masc_lemma_tfidf_dtm |&gt; \n  prcomp()\n\n\nWe can visualize the amount of variance explained by each principle component with a scree plot. The fviz_eig() function from the factoextra package can be used to create a scree plot. The fviz_eig() function takes the output of the prcomp() function as its argument and an argument ncp = 10 to specify the number of principle components to include in the plot. Let’s create a scree plot for each of our candidate feature matrices, as seen in Figure 8.7.\n\n# Load package\nlibrary(factoextra)\n\n# Scree plot: lemma relative frequency\nfviz_eig(masc_lemma_pca, ncp = 10)\n\n# Scree plot: lemma dispersion-weighted tf-idf\nfviz_eig(masc_lemma_tfidf_pca, ncp = 10)\n\n# Scree plot: part-of-speech relative frequency\nfviz_eig(masc_pos_pca, ncp = 10)\n\n\n\n\n\n(a) Lemma relative frequency\n\n\n\n\n\n(b) Lemma dispersion-weighted tf-idf\n\n\n\n\n\n(c) Part-of-speech relative frequency\n\n\n\nFigure 8.7: Scree plot of the principle components of the MASC dataset\n\n\n\nFrom Figure 8.7, we can see the plots are different. All trend toward less variance explained as the number of dimensions increase. But for our purposes, we are interested in the largest variance explained with the fewest dimensions. To that end, the Figure 8.7 (c) plot is the most reduced and most informative. The amount of variance explained is over 30% for the first dimension alone, however, the variance explained decreases between 4 and 5 dimensions. This is a good indication that we should use 4 dimensions for our clustering algorithm.\nTo calculate the amount of variance explained by each principle component we can square the standard deviations of the principle components and divide by the sum of the squared standard deviations. Let’s calculate the amount of variance explained by each principle component for each of our candidate feature matrices, as seen in Example 8.30.\n\nExample 8.30  \n\n# Calculate variance explained for first 3 principle components\n# lemma\nmasc_lemma_pca_var &lt;-\n  masc_lemma_pca$sdev^2 / sum(masc_lemma_pca$sdev^2) * 100\n\nsum(masc_lemma_pca_var[1:4])\n\n&gt; [1] 24.3\n\n# pos\nmasc_pos_pca_var &lt;-\n  masc_pos_pca$sdev^2 / sum(masc_pos_pca$sdev^2) * 100\n\nsum(masc_pos_pca_var[1:4])\n\n&gt; [1] 68.2\n\n# lemma tf-idf\nmasc_lemma_tfidf_pca_var &lt;-\n  masc_lemma_tfidf_pca$sdev^2 / sum(masc_lemma_tfidf_pca$sdev^2) * 100\n\nsum(masc_lemma_tfidf_pca_var[1:4])\n\n&gt; [1] 7.76\n\n\n\nCombining the findings in the Scree plots and the variance explained calculations, we can see that the first four principle components of the part-of-speech features account for a good proportion of the variance. Therefore, all else being equal, we should use the part-of-speech features. As with all things exploratory, however, it is important to consider the implications of our feature selection for our interpretation of the results. In this case, the part-of-speech features approximate grammatical diversity of the documents, more so than lexical diversity. This means that the clusters that we identify will be based on a particular measure of grammatical diversity of the documents. If, for example, we want to identify clusters based on the lexical diversity of the documents, we may opt to use the lemma features, or some other operationalized measure of lexical diversity.\nBefore we leave PCA, let’s also take a look at the principle components themselves. The get_pca_var() function from the factoextra package can be used to extract the principle components from the output of the prcomp() function. The get_pca_var() function takes the output of the prcomp() function as its argument and an argument ncp = 10 to specify the number of principle components to include in the plot. Let’s create a plot of the first five principle components for the part-of-speech data, as seen in Figure 8.8.\n\n# Load package\nlibrary(factoextra)\n\n# Plot principle components: pos\nmasc_pos_pca |&gt;\n  fviz_contrib(\n    choice = \"var\",\n    axes = 1:4,\n    top = 20\n  )\n\n\n\nFigure 8.8: Feature contributions to the PCA of the MASC dataset\n\n\n\nFrom Figure 8.8, we can see that the four principle components are dominated by the relative frequency of nouns, personal pronouns, prepositions, and determiners. This information can help us better understand the results of the clustering algorithm.\nNow that we have identified the features that we want to use for clustering and we have represented the features in a way that can be used for clustering, we can apply a clustering algorithm to the data. For Hiearchical clustering, we can use the hclust() function from base R. The hclust() function takes a distance matrix as its argument and an argument method = \"average\" to specify the average linkage method. The average linkage method takes the average of the dissimilarities between all pairs in two clusters. It is less sensitive to outliers compared to other methods. Let’s apply the clustering algorithm to the part-of-speech features, as seen in Example 8.31.\n\nExample 8.31  \n\n# Extract first 4 principle components\nmasc_pos_pca_pc &lt;- \n  masc_pos_pca$x[, 1:4]\n\n# Create distance matrix\nmasc_pos_dist &lt;- \n  masc_pos_pca_pc |&gt; \n  dist(method = \"manhattan\")\n\n# Apply the clustering algorithm\nmasc_pos_hc &lt;- \n  masc_pos_dist |&gt; \n  hclust(method = \"average\")\n\n# Visualize\nmasc_pos_hc |&gt; fviz_dend(show_labels = FALSE)\n\n\n\nFigure 8.9: Hierarchical clustering of the MASC dataset\n\n\n\n\nSince we are exploring the usefulness of the 18 genre labels used in the MASC dataset we have a good idea of how many clusters we want to start with. This is a good case to employ the K-means clustering algorithm. In K-means clustering, we specify the number of clusters that we want to identify. For each cluster number, a random center is generated. Then each observation is assigned to the cluster with the nearest center. The center of each cluster is then recalculated based on the distribution of the observations in the cluster. This process is iterates either a pre-defined number of times, or until the centers converge (i.e observations stop switching clusters).\nWe can use the kmeans() function from base R to apply the K-means clustering algorithm. The kmeans() function takes the matrix of features as its first argument and the number of clusters as its second argument. We can specify the number of clusters with the centers argument. The kmeans() function also takes an argument nstart to specify the number of random starts. The K-means algorithm is sensitive to the initial starting points, so it is a good idea to run the algorithm multiple times with different starting points. The nstart argument specifies the number of random starts. The default value is 1, but we can increase this to 10 or 20 to increase the likelihood of finding a good solution.\nOur goal, then, will be to assess how well this number of clusters fits the data. If it does not fit the data well, we can try a different number of clusters. We can then compare the results of the clustering with the genre labels to see how well the clusters map to the labels and make ajustments to the way we group the labels as necessary.\nLet’s start with 18 clusters, assuming the target of the number of genres in the MASC dataset. We can apply the K-means clustering algorithm to the part-of-speech features, as seen in Example 8.32.\n\nExample 8.32  \n\n# Extract first 4 principle components\nmasc_pos_pca_pc &lt;-\n  masc_pos_pca$x[, 1:4] \n\n# K-means clustering\nmasc_pos_kmeans &lt;- \n  masc_pos_pca_pc |&gt; \n  kmeans(\n    centers = 18,\n    nstart = 25,\n    iter.max = 20\n  )\n\n\nThe factoextra package provides fviz_cluster() function for visualizing the results of clustering algorithms. The fviz_cluster() function takes the output of the kmeans() function as its first argument and the matrix of features as its second argument. The fviz_cluster() function can be used to visualize the clusters in the data, as seen in Figure 8.10.\n\n# Visualize\nmasc_pos_kmeans |&gt;  # output of kmeans()\n  fviz_cluster(\n    data = masc_pos_pca_pc, # matrix of features\n    ellipse.type = \"norm\",\n    ellipse.level = 0.95,\n    geom = \"point\",\n    pointsize = 1,\n    palette = \"grey\",\n    ggtheme = theme_qtalr()\n  )\n\n\n\nFigure 8.10: K-means clustering of the MASC dataset\n\n\n\nThe ellipses in a k-means plot represent the 95% confidence interval for each cluster. The ellipses are based on the multivariate normal distribution of the data in each cluster. The size and shape of the ellipses tell us about the variance of the data in each cluster. The larger the ellipse the greater the dispersion of values. A wide ellipse suggests high within cluster variability. The distance between the clusters also tells us about the similarity between the clusters. The closer the clusters are to each other, the more similar they are. The further the clusters are from each other, the more dissimilar they are.\nSo in Figure 8.10, we see a mix of shapes and sizes. This suggests that some clusters are more homogeneous than others. We also see separation between the large wide clusters towards the top of the plot and the smaller, more circular clusters towards the center. With 18 clusters, we have a lot of clusters, so it is difficult to interpret the results as there is a large amount of overlap. In sum, 18 clusters is likely not an optimal number for this clustering approach.\nWe could run the code in Example 8.32 for different values for \\(k\\) and plot each in turn. But a more effective way to determine the optimal number of clusters is to plot the within-cluster sum of squares (WSS) for a range of values for \\(k\\). The WSS is the sum of the squared distance between each observation and its cluster center. With a plot of the WSS for a range of values for \\(k\\), we can identify the value for \\(k\\) where the WSS begins to level off. This is known as the elbow method. The elbow method is a heuristic, so it is not always clear where the elbow is. However, it is a good starting point for identifying the optimal number of clusters.\nAgain, the factoextra package has us covered. The fviz_nbclust() function can be used to plot the WSS for a range of values for \\(k\\). The fviz_nbclust() function takes the kmeans() function as its first argument and the matrix of features as its second argument. The fviz_nbclust() function also takes arguments method = \"wss\" to specify the WSS method and k.max = 20 to specify the maximum number of clusters to plot. Let’s plot the WSS for a range of values for \\(k\\), as seen in Figure 8.11.\n\n# Determine the optimal number of clusters\nmasc_pos_pca_pc |&gt; \n  fviz_nbclust(\n    FUNcluster = kmeans,\n    method = \"wss\", # method\n    k.max = 20,\n    nstart = 25,\n    iter.max = 20\n  )\n\n\n\nFigure 8.11: Elbow method for k-means clustering of the MASC dataset\n\n\n\nIt is clear that there is significant gains in cluster fit from 1 to 4 clusters, but the gains begin to level off after 5-7 clusters. Now we can skip ahead and try 4 clusters, as seen in Example 8.33.\n\nExample 8.33  \n\n# Set seed for reproducibility\nset.seed(123)\n\n# K-means: for 5 clusters\nmasc_pos_kmeans_fit &lt;-\n  masc_pos_pca_pc |&gt;\n  kmeans(\n    centers = 4,\n    nstart = 25,\n    iter.max = 20\n  )\n\n# Visualize\nmasc_pos_kmeans_fit |&gt; \n  fviz_cluster(\n  data = masc_pos_pca_pc,\n  ellipse.type = \"norm\",\n  ellipse.level = 0.95,\n  geom = \"point\",\n  pointsize = 1,\n  palette = \"grey\",\n  ggtheme = theme_qtalr()\n)\n\n\n\nFigure 8.12: K-means clustering of the MASC dataset with 4 clusters\n\n\n\n\nThe results are much more interpretable with 4 clusters. We can see that the clusters are more homogeneous and more distinct from each other, in particular clusters 1 and 2, and are generally similar in shape and size. However, there is still some overlap between the clusters, in particular for clusters 3 and 4. We expect there to be noise as we have paired down the number of features from over 25k to 4, so this is a good working solution.\nFrom this point we can join document-cluster pairings produced by the k-means algorithm with the original dataset. We can then explore the clusters in terms of the original features. We can also explore the clusters in terms of the original labels. Let’s join the cluster assignments to the original dataset, as seen in Example 8.34.\n\nExample 8.34  \n\n# Organize k-means clusters into a tibble \nmasc_pos_cluster_tbl &lt;- \n  tibble(\n    doc_id = names(masc_pos_kmeans_fit$cluster),\n    cluster = masc_pos_kmeans_fit$cluster\n  )\n\n# Join cluster assignments to original dataset\nmasc_cluster_tbl &lt;- \n  masc_tbl|&gt; \n  left_join(\n    masc_pos_cluster_tbl,\n    by = \"doc_id\"\n  )\n\n# Preview \nmasc_cluster_tbl |&gt; \n  slice_head(n = 5)\n\n&gt; # A tibble: 5 × 8\n&gt;   doc_id modality genre   term_num term         lemma        pos   cluster\n&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;   &lt;int&gt;\n&gt; 1 1      Written  Letters        2 Your         your         PRP$        1\n&gt; 2 1      Written  Letters        3 contribution contribution NN          1\n&gt; 3 1      Written  Letters        4 to           to           TO          1\n&gt; 4 1      Written  Letters        6 will         will         MD          1\n&gt; 5 1      Written  Letters        7 mean         mean         VB          1\n\n\n\nWe now see that the cluster assignments from the k-means algorithm have been joined to the original dataset. We can now explore the clusters in terms of the original features. For example, let’s look at the distribution of the clusters across modality first, as seen in Example 8.35. To do this, we first need to reduce our dataset to the distinct combinations of modality, genre, and cluster. Then, we can use the janitor package’s tabyl() function to provided formatted percentages.\n\nExample 8.35  \n\n# Load package\nlibrary(janitor)\n\n# Reduce to distinct combinations of modality, genre, and cluster\nmasc_meta_tbl &lt;- \n  masc_cluster_tbl |&gt; \n  distinct(modality, genre, cluster)\n\n# Tabulate: cluster by modality\nmasc_meta_tbl |&gt;\n  tabyl(cluster, modality) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(digits = 1)\n\n&gt;  cluster Spoken Written\n&gt;        1  30.8%   69.2%\n&gt;        2   0.0%  100.0%\n&gt;        3   0.0%  100.0%\n&gt;        4   8.3%   91.7%\n\n\n\nFrom Example 8.35, we can see that the clusters are not evenly distributed across the modalities. In particular, cluster 1 is where the great majority of the spoken modality appears. We can also appreciate that there may be some written genres that are more similar to spoken genres than other written genres. This may be something we could like to explore further.\nLet’s dive into genres and limit our analysis to the written modality, as seen in Example 8.36. To do this, we first need to filter the dataset to the written modality. In addition to the row-wise percentages which capture the proportion of the genre that contributes to each cluster, let’s also consider the column-wise percentages to consider how each genre is distrributed across the clusters.\n\nExample 8.36  \n\n# Tabulate: cluster by genre for written modality\nmasc_meta_tbl |&gt;\n  filter(modality == \"Written\") |&gt;\n  tabyl(cluster, genre) |&gt; \n  adorn_percentages(\"row\") |&gt; \n  adorn_pct_formatting(digits = 1)\n\n&gt;  cluster  Blog Email Essay Fiction Fictlets Government Jokes Journal Letters\n&gt;        1 11.1% 11.1% 11.1%   11.1%    11.1%       0.0% 11.1%   11.1%   11.1%\n&gt;        2  0.0% 20.0% 20.0%    0.0%     0.0%       0.0%  0.0%    0.0%    0.0%\n&gt;        3 20.0% 20.0%  0.0%    0.0%     0.0%       0.0%  0.0%    0.0%   20.0%\n&gt;        4  9.1%  9.1%  9.1%    9.1%     0.0%       9.1%  0.0%    9.1%    9.1%\n&gt;  Newspaper Non-fiction Technical Travel Guide Twitter\n&gt;      11.1%        0.0%      0.0%         0.0%    0.0%\n&gt;      20.0%       20.0%     20.0%         0.0%    0.0%\n&gt;      20.0%        0.0%      0.0%         0.0%   20.0%\n&gt;       9.1%        9.1%      9.1%         9.1%    0.0%\n\n# Tabulate: cluster by genre for written modality\nmasc_meta_tbl |&gt;\n  filter(modality == \"Written\") |&gt;\n  tabyl(cluster, genre) |&gt; \n  adorn_percentages(\"col\") |&gt; \n  adorn_pct_formatting(digits = 1)\n\n&gt;  cluster  Blog Email Essay Fiction Fictlets Government  Jokes Journal Letters\n&gt;        1 33.3% 25.0% 33.3%   50.0%   100.0%       0.0% 100.0%   50.0%   33.3%\n&gt;        2  0.0% 25.0% 33.3%    0.0%     0.0%       0.0%   0.0%    0.0%    0.0%\n&gt;        3 33.3% 25.0%  0.0%    0.0%     0.0%       0.0%   0.0%    0.0%   33.3%\n&gt;        4 33.3% 25.0% 33.3%   50.0%     0.0%     100.0%   0.0%   50.0%   33.3%\n&gt;  Newspaper Non-fiction Technical Travel Guide Twitter\n&gt;      25.0%        0.0%      0.0%         0.0%    0.0%\n&gt;      25.0%       50.0%     50.0%         0.0%    0.0%\n&gt;      25.0%        0.0%      0.0%         0.0%  100.0%\n&gt;      25.0%       50.0%     50.0%       100.0%    0.0%\n\n\n\nOn the other hand, looking at the written genres, we see that there are some genres which are grouped entirely in cluster 1. In other words, these genres, such as ‘Fictlets’ and ‘Jokes’, align with spoken genres in terms of their grammatical diversity. This is an interesting finding that we may want to explore further. Furthermore, it may be of interest to explore individual documents in genres which have a signifcant proportion of documents in cluster 1. There are too many possibilities to explore here but this is a good example of how exploratory data analysis can be used to identify new questions and new variables of interest.\n\n\n\n\n\n\n Consider this\nGiven the cluster assignments derived using the distribution of part-of-speech tags, what other relationships between the clusters and the original features could one explore? What are the limitations of this approach? What are the implications of this approach for the interpretation of the results?\n\n\n\nVector space models\nIn our discussion of clustering, we targeted associations between documents based on the distribution of linguistic features. We now turn to targeting associations between linguistic features based on their distribution across documents. The technique we will introduce is known as vector space modeling. Vector space modeling aims to represent linguistic features as numerical vectors which reflect the various linguistic contexts in which the features appear. Together these vectors form a feature-context space in which features with similar contextual distributions are closer together.\nAn interesting property of vector space models is that are able to capture semantic and/ or syntactic relationships between features based on their distribution. In this way, vector space modeling can be seen as an implementation of the distributional hypothesis –that words that appear in similar linguistic contexts tend to have similar meanings (Harris 1954). As Firth (1957) states “you shall know a word by the company it keeps”.\n\n\n\n\n\n\n Case study\nGarg et al. (2018) quantify and compare gender and ethnic stereotypes over time using word embeddings. The authors explore the temporal dynamics of stereotypes using word embeddings as a quantitative measure of bias. The data used includes word embeddings from the Google News dataset for contemporary analysis, as well as embeddings from the COHA and Google Books datasets for historical analysis. Additional validation is done using embeddings from the New York Times Annotated Corpus. Several word lists representing gender, ethnicity, and neutral words are collated for analysis. The main finding is that language reflects and perpetuates cultural stereotypes, and the analysis shows consistency in the relationships between embedding bias and external metrics across datasets over time. The results also highlight the impact of historical events, such as the women’s movement of the 1960s, on the encoding of stereotypes.\n\n\n\nLet’s assume in our textbook project we are interested in gathering information about English’s expression of the semantic concepts of manner and motion. For learners of English, this can be an area of difficulty as languages differ in how these semantic properties are expressed. English is a good example of a “satellite-framed” language, that is that manner and motion are often encoded in the same verb with a particle encoding the motion path (“rush out”, “climb up”). Other languages such as Spanish, Turkish, and Japanese are “verb-framed” languages, that is that motion but not manner is encoded in the verb (“salir corriendo”, “koşarak çıkmak”, “走り出す”).\nWe can use vector space modeling to represent the distribution of verbs in the MASC dataset and then target the concepts of manner and motion to then explore how English encodes these concepts. The question will be what will our features be. They could be terms, lemmas, pos tags, etc. Or they could be some combination. Considering the task at hand which we will ultimately want to know something about verbs, it makes sense to include the part of speech information in combination with either the term or the lemma.\nIf we include term and pos then we have a feature for every morphological variant of the term (e.g. house_VB, housed_VBD, housing_VBG). This can make the model more sizeable than it needs to be. If we include lemma and pos then we have a feature for every lemma with a distinct grammatical category (e.g. house_NN, house_VB). Note that as the pos tags are from the Penn tagset, many morphological variants appear in the tag itself (e.g. house_VB, houses_VBZ, housing_VBG). This is a good example of how the choice of features can impact the size of the model. In our case, it is not clear that we need to include the morphological variants of the verbs, so we will use lemma and a simplified pos as our features.\nTo engineer these features we will need to simplify the tags. We will conflate Penn tagset distinctions between nouns, verbs, adjectives, and adverbs. This will give us a feature for every lemma with a distinct grammatical category (e.g. house_NOUN, house_VERB), and no more than that. To do this we can apply the case_when() function from the dplyr package. The case_when() function takes a series of logical statements and returns a value based on the first logical statement that is TRUE. Let’s conflate the Penn tagset distinctions between nouns, verbs, adjectives, and adverbs, as seen in Example 8.37.\n\n\nExample 8.37  \n\n# Conflate Penn tagset into Universal-like tagset\nmasc_tbl &lt;- \n  masc_tbl |&gt; \n  mutate(xpos = case_when(\n    pos %in% c(\"NN\", \"NNS\", \"NNP\", \"NNPS\") ~ \"NOUN\",\n    pos %in% c(\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\") ~ \"VERB\",\n    pos %in% c(\"JJ\", \"JJR\", \"JJS\") ~ \"ADJ\",\n    pos %in% c(\"RB\", \"RBR\", \"RBS\") ~ \"ADV\",\n    TRUE ~ pos # keep other tags\n  ))\n\n# Lemma + pos\nmasc_tbl &lt;- \n  masc_tbl |&gt; \n  mutate(lemma_pos = str_c(lemma, xpos, sep = \"_\"))\n\n# Preview\nmasc_tbl |&gt; glimpse()\n\n&gt; Rows: 439,539\n&gt; Columns: 9\n&gt; $ doc_id    &lt;chr&gt; \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", …\n&gt; $ modality  &lt;chr&gt; \"Written\", \"Written\", \"Written\", \"Written\", \"Written\", \"Writ…\n&gt; $ genre     &lt;chr&gt; \"Letters\", \"Letters\", \"Letters\", \"Letters\", \"Letters\", \"Lett…\n&gt; $ term_num  &lt;dbl&gt; 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20,…\n&gt; $ term      &lt;chr&gt; \"Your\", \"contribution\", \"to\", \"will\", \"mean\", \"more\", \"than\"…\n&gt; $ lemma     &lt;chr&gt; \"your\", \"contribution\", \"to\", \"will\", \"mean\", \"more\", \"than\"…\n&gt; $ pos       &lt;chr&gt; \"PRP$\", \"NN\", \"TO\", \"MD\", \"VB\", \"JJR\", \"IN\", \"PRP\", \"MD\", \"V…\n&gt; $ xpos      &lt;chr&gt; \"PRP$\", \"NOUN\", \"TO\", \"MD\", \"VERB\", \"ADJ\", \"IN\", \"PRP\", \"MD\"…\n&gt; $ lemma_pos &lt;chr&gt; \"your_PRP$\", \"contribution_NOUN\", \"to_TO\", \"will_MD\", \"mean_…\n\n\n\nWhen VSM is applied to words, it is known as word embedding. To calculate word embeddings there are various algorithms that can be used (BERT, word2vec, GloVe, etc.) The most common algorithm is word2vec (Mikolov et al. 2013). Word2vec is a neural network-based algorithm that learns word embeddings from a large corpus of text. In the word2vec algorithm the researcher can choose to learn embeddings from a Continuous Bag of Words (CBOW) or a Skip-gram model. The CBOW model predicts a target word based on the context words. The Skip-gram model predicts the context words based on the target word. The CBOW model is faster to train and is better for frequent words. The Skip-gram model is slower to train and is better for infrequent words.\nAnother consideration to take into account is the size of the corpus used to train the model. VSM provide more reliable results when trained on larger corpora. The MASC dataset is relatively small. We’ve simplified our features in order to have a smaller vocabulary in hopes to offset this limitation to a degree. But the choice of either CBOW or Skip-gram can also help to offset this limitation. CBOW can be better for smaller corpora as it aggregates context infomation.\nTo implement the word2vec algorithm on our lemma + pos features, we will use the word2vec package. The word2vec() function takes a text file and uses it to train the vector representations. To prepare the MASC dataset for training, we will need to write the lemma + pos features to a text file as a single character string. We can do this by first collapsing the lemma_pos variable into a single string for each document using the str_c() function from the stringr package. Then we can use the writeLines() function to write the string to a text file. Let’s prepare the MASC dataset for training, as seen in Example 8.38.\n\nExample 8.38  \n\n# Prepare data for word2vec training\nmasc_tbl |&gt; \n  summarize(text = str_c(lemma_pos, collapse = \" \")) |&gt; \n  pull(text) |&gt; \n  writeLines(\n    con = \"../data/derived/masc_word2vec.txt\"\n  )\n\n\nWith our masc_word2vect.txt file, we read in. apply the word2vec algorithm using the word2vec package, and write the model to disk. By default, te word2vec() function applies the CBOW model, with 50 dimensions, a window size of 5, and a minimum word count of 5. We can change these parameters as needed, but let’s apply the default algorithm to the text file spliting features by sentence punctuation, as seen in Example 8.39.\n\nExample 8.39  \n\n# Load package\nlibrary(word2vec)\n\n# Traing word2vec model\nmasc_model &lt;- \n  word2vec(\n    x = \"../data/derived/masc_word2vec.txt\",\n    split = c(\" \", \".?!\"),\n  )\n\n# Write model to disk\nwrite.word2vec(\n  masc_model,\n  file = \"../data/derived/masc_word2vec.bin\"\n)\n\n\n# Set seed for reproducibility\nset.seed(1234)\n\n# Load package\nlibrary(word2vec)\n\n# Traing word2vec model\nmasc_model &lt;- \n  word2vec(\n    x = \"data/masc_word2vec.txt\",\n    split = c(\" \", \".?!\"),\n  )\n\n# Write model to disk\nwrite.word2vec(\n  masc_model,\n  file = \"data/masc_word2vec.bin\"\n)\n\n\nWriting the model to disk is important as it allows us to read the model in without having to retrain it. In cases where the corpus is large, this can save a lot of computational time.\nNow that we have a trained model, we can read it in with the read.vectors() function from the wordVectors package.\n\nExample 8.40  \n\n# Load package\nlibrary(wordVectors)\n\n# Read word2vec model\nmasc_model &lt;- \n  read.vectors(\n    filename = \"../data/derived/masc_word2vec.bin\"\n  )\n\n\nThe read.vectors() function returns a matrix where each row is a term in the model and each column is a dimension in the vector space, as seen in Example 8.41.\n\nExample 8.41  \n\n# Inspect\ndim(masc_model)\n\n&gt; [1] 5892   50\n\n# Preview\nmasc_model[1:5, 1:5]\n\n&gt; A VectorSpaceModel object of  5  words and  5  vectors\n&gt;                     [,1]  [,2]   [,3]   [,4]    [,5]\n&gt; map_VERB           0.476 0.590  0.572 -0.547 -0.0146\n&gt; KGCUBS10_NOUN     -0.304 1.506 -0.407  1.518  0.1857\n&gt; MICKEY3_NOUN      -0.259 0.854 -0.925  0.759  0.1816\n&gt; amenity_NOUN       1.284 0.502 -0.921  0.488 -1.1896\n&gt; transmission_NOUN -1.580 1.493 -0.205  1.457 -1.7360\n&gt; attr(,\".cache\")\n&gt; &lt;environment: 0x7fdc08066940&gt;\n\n\n\nThe row-wise vector in the model is the vector representation of each feature. The notion is that these values can now be compared with other terms to explore distributional relatedness. We can extract specific features from the matrix using the [] operator.\nAs an example, let’s compare the vectors for noun-verb pairs for the lemmas ‘run’ and ‘walk’. To do this we extract these features from the model. To appreciate the relatedness of these features it is best to visualize them. We can do this by first reducing the dimensionality of the vectors using principal components analysis (PCA). We can then plot the first two principle components, as seen in Figure 8.13.\n\nExample 8.42  \n\n# Extract vectors\nword_vectors &lt;- \n  masc_model[c(\"run_VERB\", \"walk_VERB\", \"run_NOUN\", \"walk_NOUN\"), ] |&gt; \n  as.matrix() \n\npca &lt;- \n  word_vectors |&gt;\n  scale() |&gt;\n  prcomp()\n\npca_tbl &lt;- \n  as_tibble(pca$x[, 1:2]) |&gt; \n  mutate(word = rownames(word_vectors))\n\npca_tbl |&gt; \n  ggplot(aes(x = PC1, y = PC2, label = word)) +\n  geom_point() +\n  ggrepel::geom_text_repel()\n\n\n\nFigure 8.13: Similarity between ‘run’ and ‘walk’ in the MASC dataset\n\n\n\n\n\nFrom Figure 8.13, we can see that each of these features occupies a distinct position in the reduced vector space. But on closer inspection, we can see that there is a relationship between the lemma pairs. Remember that PCA reduces the dimensionality of the data by identifying the dimensions that capture the greatest amount of variance in the data. This means that of the 50 dimensions in the model, the PC1 and PC2 correspond to orthogonal dimensions that capture the greatest amount of variance in the data. If we look along PC1, we can see that there is a distinction between part-of-speech. Looking along PC1, we see some pariety between lemma meanings. Given these features, we can see that meaning and grammatical category can be captured in the vector space.\nAn interesting property of vector space models is that we can build up a dimension of meaning by adding vectors that are expect to approximate that meaning. For example, we can add the vectors for typical motion verbs to create a vector for motion-similarity and one for manner-similarity. We can then compare the feature vectors for all verbs and assess their motion-similarity and manner-similarity.\nTo do this let’s first subset the model to only include verbs, as in Example 8.43. We will also remove the part-of-speech tags from the rownames of the matrix as they are no longer needed.\n\nExample 8.43  \n\n# Filter to verbs\nverbs &lt;- str_subset(rownames(masc_model), \".*_VERB\")\nverb_vectors &lt;- masc_model[verbs, ]\n\n# Remove part-of-speech tags\nrownames(verb_vectors) &lt;- \n  verb_vectors |&gt;\n  rownames() |&gt;\n  str_replace_all(\"_VERB\", \"\")\n\n# Inspect\ndim(verb_vectors)\n\n&gt; [1] 1115   50\n\n# Preview\nverb_vectors[1:5, 1:5]\n\n&gt; A VectorSpaceModel object of  5  words and  5  vectors\n&gt;            [,1]  [,2]   [,3]    [,4]    [,5]\n&gt; map       0.476 0.590  0.572 -0.5469 -0.0146\n&gt; whip      0.636 0.897 -1.017 -0.0436 -0.3124\n&gt; enroll    0.600 0.580 -0.335  0.0160 -1.0247\n&gt; tuck     -0.795 1.834 -1.696 -0.3221  0.5523\n&gt; suppress -0.368 1.970 -1.438  0.6805 -0.3694\n&gt; attr(,\".cache\")\n&gt; &lt;environment: 0x7fdbef3661f8&gt;\n\n\n\nWe now have verb_vectors which includes the vector representations for all verbs 1115 in the MASC dataset. Next, let’s seed the vectors for motion-similarity and manner-similarity and calculate the vector ‘closeness’ to the motion and manner seed vectors with the closest_to() function from the wordVectors() package.\n\nExample 8.44  \n\n# Add vectors for motion-similarity and manner-similarity\nmotion &lt;-\n  c(\"go\", \"come\", \"leave\", \"arrive\", \"enter\", \"exit\", \"depart\", \"return\")\n\nmotion_similarity &lt;-\n  verb_vectors |&gt; closest_to(motion, n = Inf)\n\n# Preview\nmotion_similarity |&gt; glimpse()\n\n&gt; Rows: 1,115\n&gt; Columns: 2\n&gt; $ word                   &lt;chr&gt; \"sever\", \"hang\", \"nod\", \"splash\", \"return\", \"sl…\n&gt; $ `similarity to motion` &lt;dbl&gt; 0.781, 0.776, 0.769, 0.760, 0.755, 0.755, 0.745…\n\nmanner &lt;-\n  c(\"run\", \"walk\", \"jump\", \"crawl\", \"swim\", \"fly\", \"drive\", \"ride\")\n\nmanner_similarity &lt;-\n  verb_vectors |&gt; closest_to(manner, n = Inf)\n\n# Preview\nmanner_similarity |&gt; glimpse()\n\n&gt; Rows: 1,115\n&gt; Columns: 2\n&gt; $ word                   &lt;chr&gt; \"crawl\", \"drop\", \"step\", \"hang\", \"walk\", \"throw…\n&gt; $ `similarity to manner` &lt;dbl&gt; 0.894, 0.880, 0.867, 0.861, 0.858, 0.857, 0.850…\n\n\n\nThe motion_similarity and motion_similarity data frames each contain all the verbs with a corresponding closeness measure. We can join these two data frames by feature to create a single data frame with the motion-similarity and manner-similarity measures, as seen in Example 8.45.\n\nExample 8.45  \n\n# Join motion-similarity and manner-similarity\nmanner_motion_similarity &lt;- \n  manner_similarity |&gt; \n  inner_join(motion_similarity)\n\n# Preview\nmanner_motion_similarity |&gt; glimpse()\n\n&gt; Rows: 1,115\n&gt; Columns: 3\n&gt; $ word                   &lt;chr&gt; \"crawl\", \"drop\", \"step\", \"hang\", \"walk\", \"throw…\n&gt; $ `similarity to manner` &lt;dbl&gt; 0.894, 0.880, 0.867, 0.861, 0.858, 0.857, 0.850…\n&gt; $ `similarity to motion` &lt;dbl&gt; 0.742, 0.743, 0.692, 0.776, 0.695, 0.627, 0.702…\n\n\n\nThe result of Example 8.45 is a data frame with the motion-similarity and manner-similarity measures for all verbs in the MASC dataset. We can now visualize the distribution of motion-similarity and manner-similarity measures, as seen in Figure 8.14.\n\n\n\n\nFigure 8.14: Motion-similarity and manner-similarity of verbs in the MASC dataset\n\n\n\nFrom Figure 8.14, we can see that the manner-similarity is plotted on the x-axis and the motion-similarity on the y-axis. I’ve added horizontal and vertical lines to break the scatterplot into quadrants –the top-right corresponding to high manner- and motion-similiarity and the bottom-left corresponding to low manner- and motion-similarity. This captures the majority of the verbs in the dataset. The verbs in the top-left quadrant have high motion-similarity but lower manner similarity, and verbs in the bottom-right quadrant have high manner-similarity but lower motion-similarity.\nI’ve randomly sampled 50 verbs from the dataset and plotted them as text labels. I’ve also plotted the motion and manner seed vectors as triangle and box points, respectively. We can see that motion- and manner-similiarity seed verbs are found in the top-left quandrant together, showing that they are semantically related. Verbs in the other quadrants are either lower in motion- or manner-similarity, or both. From a qualitative point of view it appears that many of the verbs coincide with inuition. Some, however, less so. This is to be expected to some degree as the model is trained on a relatively small corpus. All in all, this serves as an example of how vector space modeling can be used to explore semantic relationships between linguistic features."
  },
  {
    "objectID": "exploration.html#summary",
    "href": "exploration.html#summary",
    "title": "8  Exploration",
    "section": "\n8.3 Summary",
    "text": "8.3 Summary\nIn this chapter we surveyed a range of methods for uncovering insights from data, particularly when we do not have a predetermined hypothesis. We broke the chapter discussion along the two central branches of exploratory data analysis: descriptive analysis and unsupervised learning. Descriptive analysis offers statistical or visual summaries of datasets through frequency, dispersion, and co-occurrence measures, while unsupervised learning utilizes machine learning techniques to uncover patterns without predefining variable relationships. Here we covered a few unsupervised learning methods including clustering, diminensionality reduction, and vector space modeling. Through either descriptive or unsupervised learning methodologies, we probe questions in a data-driven fashion and apply methods to summarize, reduce, and sort complex datasets. This in turn facilitates novel, quantitative perspectives that can subsequently be evaluated qualitatively, offering us a robust approach to exploring and generating research questions."
  },
  {
    "objectID": "exploration.html#activities",
    "href": "exploration.html#activities",
    "title": "8  Exploration",
    "section": "Activities",
    "text": "Activities\n\n\n\n\n\n\n\n Recipe\nWhat: Exploratory methods: descriptive and unsupervised learning analysis methodsHow: Read Recipe 8 and participate in the Hypothes.is online social annotation.Why: To illustrate how to prepare a dataset for descriptive and unsupervised machine learning methods and evaluate the results for exploratory data analysis.\n\n\n\n\n\n\n\n\n\n Lab\n\n\nWhat: Exploratory Data AnalysisHow: Clone, fork, and complete the steps in Lab 8.Why: To gain experience working with coding strategies to prepare, feature engineer, explore, and evaluate results from exploratory data analyses, practice transforming datasets into new object formats and visualizing relationships, and implement organizational strategies for organizing and reporting results in a reproducible fashion."
  },
  {
    "objectID": "exploration.html#questions",
    "href": "exploration.html#questions",
    "title": "8  Exploration",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\n Conceptual questions\n\nWhat is exploratory data analysis?\nHow can exploratory data analysis be used to uncover patterns and associations?\nDescribe the workflow of exploratory data analysis?\nWhat are the advantages and disadvantages of descriptive analysis?\nWhat are the advantages and disadvantages of unsupervised learning?\nWhat is the difference between supervised and unsupervised learning?\nHow does exploratory data analysis differ from traditional hypothesis testing?\n\n\n\n\n\n\n\n\n\n\n Technical questions\n\nWrite a function in R to conduct a hierarchical cluster analysis on a dataset.\nImplement a k-means algorithm in R to identify clusters within a dataset.\nImplement a Principal Component Analysis (PCA) algorithm in R to identify patterns and associations within a dataset.\nWrite a function in R to produce a descriptive summary of a dataset.\nConduct a correlation analysis in R to identify relationships between variables in a dataset.\nLoad a dataset into R and conduct a frequency analysis on the dataset.\nLoad a dataset into R and conduct a keyword in context analysis on the dataset.\nLoad a dataset into R and conduct a keyword analysis on the dataset.\nLoad a dataset into R and conduct a sentiment analysis on the dataset.\nLoad a dataset into R and conduct a topic modelling analysis on the dataset.\n\n\n\n\n\n\n\n\nFirth, John R. 1957. Papers in Linguistics. Oxford University Press.\n\n\nGarg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018. “Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes.” Proceedings of the National Academy of Sciences 115 (16): E3635–44. https://doi.org/10.1073/pnas.1720347115.\n\n\nGries, Stefan Th. 2023. “Statistical Methods in Corpus Linguistics.” In Readings in Corpus Linguistics: A Teaching and Research Guide for Scholars in Nigeria and Beyond, 78–114.\n\n\nHarris, Zellig S. 1954. “Distributional Structure.” Word 10 (2-3): 146–62. https://doi.org/10.1080/00437956.1954.11659520.\n\n\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” In Advances in Neural Information Processing Systems, 3111–19.\n\n\nPetrenz, Philipp, and Bonnie Webber. 2011. “Stable Classification of Text Genres.” Computational Linguistics 37 (2): 385–93. https://doi.org/10.1162/COLI_a_00052.\n\n\nZipf, George Kingsley. 1949. Human Behavior and the Principle of Least Effort. Oxford, England: Addison-Wesley Press."
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "10  Inference",
    "section": "",
    "text": "Caution\n\n\n\nUnder development."
  },
  {
    "objectID": "communication.html",
    "href": "communication.html",
    "title": "Communication",
    "section": "",
    "text": "In this section I cover the steps in presenting the findings of the research both as a research document and as a reproducible research project. Both research documents and reproducible projects are fundamental components of modern scientific inquiry. On the one hand a research document provides readers a detailed summary of the main import of the research study. On the other hand making the research project available to interested readers ensures that the scientific community can gain insight into the process implemented in the research and thus enables researchers to vet and extend this research to build a more robust and verifiable research base."
  },
  {
    "objectID": "reporting.html",
    "href": "reporting.html",
    "title": "11  Reports",
    "section": "",
    "text": "Caution\n\n\n\nUnder development."
  },
  {
    "objectID": "collaboration.html",
    "href": "collaboration.html",
    "title": "12  Collaboration",
    "section": "",
    "text": "Caution\n\n\n\nUnder development."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ackoff, Russell L. 1989. “From Data to Wisdom.” Journal\nof Applied Systems Analysis 16 (1): 3–9.\n\n\nÄdel, Annelie. 2020. “Corpus Compilation.” In A\nPractical Handbook of Corpus Linguistics, edited by Magali Paquot\nand Stefan Th. Gries, 3–24. Switzerland: Springer.\n\n\nAlbert, Saul, Laura E. de Ruiter, and J. P. de Ruiter. 2015.\n“CABNC: The Jeffersonian Transcription of the Spoken British\nNational Corpus.” TalkBank.\n\n\nAllaire, JJ. 2023. Quarto: R Interface to Quarto Markdown Publishing\nSystem. https://github.com/quarto-dev/quarto-r.\n\n\nAllaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier\nLuraschi, Kevin Ushey, Aron Atkins, et al. 2023. Rmarkdown: Dynamic\nDocuments for r. https://github.com/rstudio/rmarkdown.\n\n\nBaayen, R. Harald. 2011. “Corpus Linguistics and Naive\nDiscriminative Learning.” Revista Brasileira de\nLingu\\’\\istica Aplicada 11 (2): 295–328.\n\n\nBaayen, R. Harald, L. B. Feldman, and R. Schreuder. 2006.\n“Morphological Influences on the Recognition of Monosyllabic\nMonomorphemic Words.” Journal of Memory and Language 55:\n290–313. https://doi.org/10.1016/j.jml.2006.03.008.\n\n\nBao, Wang, Ning Lianju, and Kong Yue. 2019. “Integration of\nUnsupervised and Supervised Machine Learning Algorithms for Credit Risk\nAssessment.” Expert Systems with Applications 128\n(August): 301–15. https://doi.org/10.1016/j.eswa.2019.02.033.\n\n\nBengtsson, Henrik. 2023. Future: Unified Parallel and Distributed\nProcessing in r for Everyone. https://future.futureverse.org.\n\n\nBenoit, Kenneth. 2020. Quanteda.corpora: A Collection of Corpora for\nQuanteda. http://github.com/quanteda/quanteda.corpora.\n\n\nBenoit, Kenneth, David Muhr, and Kohei Watanabe. 2021. Stopwords:\nMultilingual Stopword Lists. https://github.com/quanteda/stopwords.\n\n\nBenoit, Kenneth, and Adam Obeng. 2023. Readtext: Import and Handling\nfor Plain and Formatted Text Files. https://github.com/quanteda/readtext.\n\n\nBlischak, John, Peter Carbonetto, and Matthew Stephens. 2023.\nWorkflowr: A Framework for Reproducible and Collaborative Data\nScience. https://github.com/workflowr/workflowr.\n\n\nBraginsky, Mika. 2022. Wordbankr: Accessing the Wordbank\nDatabase. https://langcog.github.io/wordbankr/.\n\n\nBresnan, Joan. 2007. “A Few Lessons from Typology.”\nLinguistic Typology 11 (1): 297–306.\n\n\nBrown, Keith. 2005. Encyclopedia of Language and Linguistics.\nVol. 1. Elsevier.\n\n\nBuckheit, Jonathan B., and David L. Donoho. 1995. “Wavelab and\nReproducible Research.” In Wavelets and Statistics,\n55–81. Springer.\n\n\nBychkovska, Tetyana, and Joseph J. Lee. 2017. “At the Same Time:\nLexical Bundles in L1 and L2 University Student Argumentative\nWriting.” Journal of English for Academic Purposes 30\n(November): 38–52. https://doi.org/10.1016/j.jeap.2017.10.008.\n\n\nCampbell, Lyle. 2001. “The History of Linguistics.” In\nThe Handbook of Linguistics, edited by Mark Aronoff and Janie\nRees-Miller, 81–104. Blackwell Handbooks in Linguistics. Blackwell\nPublishers.\n\n\nCarmi, Elinor, Simeon J. Yates, Eleanor Lockley, and Alicja Pawluczuk.\n2020. “Data Citizenship: Rethinking Data Literacy in the Age of\nDisinformation, Misinformation, and Malinformation.” Internet\nPolicy Review 9 (2).\n\n\nChambers, John M. 2020. “S, r, and Data Science.”\nProceedings of the ACM on Programming Languages 4 (HOPL): 1–17.\nhttps://doi.org/10.1145/3386334.\n\n\nChan, Sin-wai. 2014. Routledge Encyclopedia of Translation\nTechnology. Routledge.\n\n\nConway, Lucian Gideon, Laura Janelle Gornick, Chelsea Burfeind, Paul\nMandella, Andrea Kuenzli, Shannon C. Houck, and Deven Theresa Fullerton.\n2012. “Does Complex or Simple Rhetoric Win Elections? An\nIntegrative Complexity Analysis of u.s. Presidential Campaigns.”\nPolitical Psychology 33 (5): 599–618. https://doi.org/10.1111/j.1467-9221.2012.00910.x.\n\n\nCross, Nigel. 2006. “Design as a Discipline.”\nDesignerly Ways of Knowing, 95–103.\n\n\n“Data Never Sleeps 7.0 Infographic.” 2019.\nhttps://www.domo.com/learn/infographic/data-never-sleeps-7.\n\n\nDeshors, Sandra C, and Stefan Th. Gries. 2016. “Profiling Verb\nComplementation Constructions Across New Englishes.”\nInternational Journal of Corpus Linguistics. 21 (2): 192–218.\n\n\nDesjardins, Jeff. 2019. “How Much Data Is Generated Each\nDay?” Visual Capitalist.\n\n\nDonoho, David. 2017. “50 Years of Data Science.”\nJournal of Computational and Graphical Statistics 26 (4):\n745–66. https://doi.org/10.1080/10618600.2017.1384734.\n\n\nDubnjakovic, Ana, and Patrick Tomlin. 2010. A Practical Guide to\nElectronic Resources in the Humanities. Elsevier.\n\n\nEisenstein, Jacob, Brendan O’Connor, Noah A Smith, and Eric P Xing.\n2012. “Mapping the Geographical Diffusion of New Words.”\nComputation and Language, 1–13. https://doi.org/10.1371/journal.pone.0113114.\n\n\nFirth, John R. 1957. Papers in Linguistics. Oxford University\nPress.\n\n\nFrancom, Jerid. 2022. “Corpus Studies of Syntax.” In\nThe Cambridge Handbook of Experimental Syntax, edited by Grant\nGoodall, 687–713. Cambridge Handbooks in Language and Linguistics.\nCambridge University Press.\n\n\n———. 2023. Qtalrkit: Quantitative Text Analysis for Linguists\nResource Kit. https://github.com/qtalr/qtalrkit.\n\n\nGandrud, Christopher. 2015. Reproducible\nResearch with r and r Studio. Second edition. CRC Press.\n\n\nGarg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018.\n“Word Embeddings Quantify 100 Years of Gender and Ethnic\nStereotypes.” Proceedings of the National Academy of\nSciences 115 (16): E3635–44. https://doi.org/10.1073/pnas.1720347115.\n\n\nGentleman, Robert, and Duncan Temple Lang. 2007. “Statistical\nAnalyses and Reproducible Research.” Journal of Computational\nand Graphical Statistics 16 (1): 1–23.\n\n\nGilquin, Gaëtanelle, and Stefan Th Gries. 2009. “Corpora and\nExperimental Methods: A State-of-the-Art Review.” Corpus\nLinguistics and Linguistic Theory 5 (1): 1–26. https://doi.org/10.1515/CLLT.2009.001.\n\n\nGomez-Uribe, Carlos A., and Neil Hunt. 2015. “The Netflix\nRecommender System: Algorithms, Business Value, and Innovation.”\nACM Transactions on Management Information Systems (TMIS) 6\n(4): 1–19.\n\n\nGries, Stefan Th. 2021. Statistics for Linguistics with r. De\nGruyter Mouton.\n\n\n———. 2023. “Statistical Methods in Corpus Linguistics.” In\nReadings in Corpus Linguistics: A Teaching and Research Guide for\nScholars in Nigeria and Beyond, 78–114.\n\n\nGries, Stefan Th. 2013. Statistics for Linguistics with r. A\nPractical Introduction. 2nd revise.\n\n\nGries, Stefan Th., and Sandra C. Deshors. 2014. “Using Regressions\nto Explore Deviations Between Corpus Data and a Standard/Target: Two\nSuggestions.” Corpora 9 (1): 109–36. https://doi.org/10.3366/cor.2014.0053.\n\n\nGrieve, Jack, Andrea Nini, and Diansheng Guo. 2018. “Mapping\nLexical Innovation on American Social Media.” Journal of\nEnglish Linguistics 46 (4): 293–319.\n\n\nHarris, Zellig S. 1954. “Distributional Structure.”\nWord 10 (2-3): 146–62. https://doi.org/10.1080/00437956.1954.11659520.\n\n\nHay, Jennifer. 2002. “From Speech Perception to Morphology: Affix\nOrdering Revisited.” Language 78 (3): 527–55.\n\n\nHester, Jim, Hadley Wickham, and Gábor Csárdi. 2023. Fs:\nCross-Platform File System Operations Based on Libuv. https://fs.r-lib.org.\n\n\nHicks, Stephanie C., and Roger D. Peng. 2019. “Elements and\nPrinciples for Characterizing Variation Between Data Analyses.”\narXiv. https://doi.org/10.48550/arXiv.1903.07639.\n\n\nIde, Nancy, Collin Baker, Christiane Fellbaum, Charles Fillmore, and\nRebecca Passonneau. 2008. “MASC: The Manually Annotated Sub-Corpus\nof American English.” In 6th International Conference on\nLanguage Resources and Evaluation, LREC 2008, 2455–60. European\nLanguage Resources Association (ELRA).\n\n\nIgnatow, Gabe, and Rada Mihalcea. 2017. An Introduction to Text\nMining: Research Design, Data Collection, and Analysis. Sage\nPublications.\n\n\nJaeger, T Florian, and Neal Snider. 2007. “Implicit Learning and\nSyntactic Persistence: Surprisal and Cumulativity.”\nUniversity of Rochester Working Papers in the Language Sciences\n3 (1).\n\n\nKaur, Jashanjot, and P. Kaur Buttar. 2018. “A Systematic Review on\nStopword Removal Algorithms.” International Journal on Future\nRevolution in Computer Science & Communication Engineering 4\n(4): 207–10.\n\n\nKloumann, IM, CM Danforth, KD Harris, and CA Bliss. 2012.\n“Positivity of the English Language.” PloS One.\n\n\nKoehn, P. 2005. “Europarl: A Parallel Corpus for Statistical\nMachine Translation.” MT Summit X, 12–16.\n\n\nKostić, Aleksandar, Tanja Marković, and Aleksandar Baucal. 2003.\n“Inflectional Morphology and Word Meaning: Orthogonal or\nCo-Implicative Cognitive Domains?” In Morphological Structure\nin Language Processing, edited by R. Harald Baayen and Robert\nSchreuder, 1–44. De Gruyter Mouton. https://doi.org/10.1515/9783110910186.1.\n\n\nKowalski, John, and Rob Cavanaugh. 2022. TBDBr: Easy Access to\nTalkBankDB via r API. https://github.com/TalkBank/TalkBankDB-R.\n\n\nKrathwohl, David R. 2002. “A Revision of Bloom’s Taxonomy: An\nOverview.” Theory into Practice 41 (4): 212–18.\n\n\nKross, Sean, Nick Carchedi, Bill Bauer, and Gina Grdina. 2020.\nSwirl: Learn r, in r. http://swirlstats.com.\n\n\nKucera, H, and W N Francis. 1967. Computational Analysis of Present\nDay American English. Brown University Press Providence.\n\n\nLandau, William Michael. 2023. Targets: Dynamic Function-Oriented\nMake-Like Declarative Pipelines. https://docs.ropensci.org/targets/.\n\n\nLantz, Brett. 2013. Machine Learning with r. Birmingham: Packt\nPublishing.\n\n\nLeech, Geoffrey. 1992. “100 Million Words of English: The British\nNational Corpus (BNC),” no. 1991: 1–13.\n\n\nLewis, Michael. 2004. Moneyball: The Art of Winning an Unfair\nGame. WW Norton & Company.\n\n\nLiu, Kanglong, and Muhammad Afzaal. 2021. “Syntactic Complexity in\nTranslated and Non-Translated Texts: A Corpus-Based Study of\nSimplification.” Edited by Diego Raphael Amancio. PLOS\nONE 16 (6): e0253454. https://doi.org/10.1371/journal.pone.0253454.\n\n\nLozano, Cristóbal. 2009. “CEDEL2: Corpus Escrito Del Español\nL2.” Applied Linguistics Now: Understanding Language and\nMind/La Lingüística Aplicada Hoy: Comprendiendo El Lenguaje y La Mente.\nAlmería: Universidad de Almería, 197–212.\n\n\nMagueresse, Alexandre, Vincent Carles, and Evan Heetderks. 2020.\n“Low-Resource Languages: A Review of Past Work and Future\nChallenges.” arXiv. https://arxiv.org/abs/2006.07264.\n\n\nManning, Christopher. 2003. “Probabilistic Syntax.” In\nProbabilistic Linguistics, edited by Bod, Jennifer Hay, and\nJannedy, 289–341. Cambridge, MA: MIT Press.\n\n\nMarcus, Mitchell P, Beatrice Santorini, and Mary Ann Marcinkiewicz.\n1993. “Building a Large Annotated Corpus of English: The Penn\nTreebank.” Computational Linguistics 19 (2): 313–30.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging\nData Analytical Work Reproducibly Using r (and Friends).” The\nAmerican Statistician 72 (1): 80–88.\n\n\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. “Distributed Representations of Words and Phrases and\nTheir Compositionality.” In Advances in Neural Information\nProcessing Systems, 3111–19.\n\n\nMoroz, George. 2023. Lingtypology: Linguistic Typology and\nMapping. https://CRAN.R-project.org/package=lingtypology.\n\n\nMosteller, Frederick, and David L Wallace. 1963. “Inference in an\nAuthorship Problem.” Journal of the American Statistical\nAssociation 58 (302): 275–309. https://www.jstor.org/stable/2283270.\n\n\nMullen, Lincoln. 2022. Tokenizers: Fast, Consistent Tokenization of\nNatural Language Text. https://docs.ropensci.org/tokenizers/.\n\n\nMuñoz, Carmen, ed. 2006. Age and the Rate of Foreign Language\nLearning. 1st ed. Vol. 19. Second Language Acquisition Series.\nClevedon: Multilingual Matters.\n\n\nNisioi, Sergiu, Ella Rabinovich, Liviu P. Dinu, and Shuly Wintner. 2016.\n“A Corpus of Native, Non-Native and Translated Texts.” In\nProceedings of the Tenth International Conference on Language\nResources and Evaluation (LREC 2016). Portoroz̆,\nSlovenia: European Language Resources Association (ELRA).\n\n\nNivre, Joakim, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg,\nJan Hajič, Christopher D Manning, Ryan McDonald, et al. 2016.\n“Universal Dependencies V1: A Multilingual Treebank\nCollection.” Proceedings of the Tenth International\nConference on Language Resources and Evaluation (LREC’16), 1659–66.\nhttps://doi.org/?\n\n\nNivre, Joakim, Marie-Catherine De Marneffe, Filip Ginter, Jan Hajič,\nChristopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis\nTyers, and Daniel Zeman. 2020. “Universal Dependencies V2: An\nEvergrowing Multilingual Treebank Collection.” arXiv Preprint\narXiv:2004.10643. https://arxiv.org/abs/2004.10643.\n\n\nOlohan, Maeve. 2008. “Leave It Out! Using a Comparable Corpus to\nInvestigate Aspects of Explicitation in Translation.”\nCadernos de Tradução, 153–69.\n\n\nOoms, Jeroen. 2023. Jsonlite: A Simple and Robust JSON Parser and\nGenerator for r. https://jeroen.r-universe.dev/jsonlite\nhttps://arxiv.org/abs/1403.2805.\n\n\nPaquot, Magali, and Stefan Th. Gries, eds. 2020. A Practical\nHandbook of Corpus Linguistics. Switzerland: Springer.\n\n\nPetrenz, Philipp, and Bonnie Webber. 2011. “Stable Classification\nof Text Genres.” Computational Linguistics 37 (2):\n385–93. https://doi.org/10.1162/COLI_a_00052.\n\n\nR Special Interest Group on Databases (R-SIG-DB), Hadley Wickham, and\nKirill Müller. 2022. DBI: R Database Interface. https://dbi.r-dbi.org.\n\n\nRiehemann, Susanne Z. 2001. “A Constructional Approach to Idioms\nand Word Formation.” PhD thesis, Stanford.\n\n\nRinker, Tyler. 2019. Lexicon: Lexicons for Text Analysis. https://github.com/trinker/lexicon.\n\n\nRinker, Tyler, and Dason Kurkiewicz. 2019. Pacman: Package\nManagement Tool. https://github.com/trinker/pacman.\n\n\nRobinson, David, and Julia Silge. 2023. Tidytext: Text Mining Using\nDplyr, Ggplot2, and Other Tidy Tools. https://github.com/juliasilge/tidytext.\n\n\nRoediger, H. L. L, and K. B. B McDermott. 2000. “Distortions of\nMemory.” The Oxford Handbook of Memory, 149–62.\n\n\nRowley, Jennifer. 2007. “The Wisdom Hierarchy: Representations of\nthe DIKW Hierarchy.” Journal of Information Science 33\n(2): 163–80. https://doi.org/10.1177/0165551506070706.\n\n\nSaxena, Shweta, and Manasi Gyanchandani. 2020. “Machine Learning\nMethods for Computer-Aided Breast Cancer Diagnosis Using Histopathology:\nA Narrative Review.” Journal of Medical Imaging and Radiation\nSciences 51 (1): 182–93.\n\n\nSedgwick, Philip. 2015. “Units of Sampling, Observation, and\nAnalysis.” BMJ (Online) 351 (October): h5396. https://doi.org/10.1136/bmj.h5396.\n\n\nSilge, Julia. 2022. Janeaustenr: Jane Austen’s Complete Novels.\nhttps://github.com/juliasilge/janeaustenr.\n\n\nSzmrecsanyi, Benedikt. 2004. “On Operationalizing Syntactic\nComplexity.” In Le Poids Des Mots. Proceedings of the 7th\nInternational Conference on Textual Data Statistical Analysis.\nLouvain-La-Neuve, 2:1032–39.\n\n\nTalarico, Jennifer M., and David C. Rubin. 2003. “Confidence, Not\nConsistency, Characterizes Flashbulb Memories.” Psychological\nScience 14 (5): 455–61. https://doi.org/10.1111/1467-9280.02453.\n\n\nTottie, Gunnel. 2011. “Uh and Um as Sociolinguistic Markers in\nBritish English.” International Journal of Corpus\nLinguistics 16 (2): 173–97.\n\n\nUniversity of Colorado Boulder. 2008. “Switchboard Dialog Act\nCorpus. Web Download.” Linguistic Data Consortium.\n\n\nVoigt, Rob, Nicholas P. Camp, Vinodkumar Prabhakaran, William L.\nHamilton, Rebecca C. Hetey, Camilla M. Griffiths, David Jurgens, Dan\nJurafsky, and Jennifer L. Eberhardt. 2017. “Language from Police\nBody Camera Footage Shows Racial Disparities in Officer Respect.”\nProceedings of the National Academy of Sciences 114 (25):\n6521–26.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia,\nHao Zhu, and Shannon Ellis. 2022. Skimr: Compact and Flexible\nSummaries of Data. https://docs.ropensci.org/skimr/.\n\n\nWhite, John Myles. 2023. ProjectTemplate: Automates the Creation of\nNew Statistical Analysis Projects. http://projecttemplate.net.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\n———. 2022. Stringr: Simple, Consistent Wrappers for Common String\nOperations. https://stringr.tidyverse.org.\n\n\n———. 2023. Tidyverse: Easily Install and Load the Tidyverse. https://tidyverse.tidyverse.org.\n\n\nWickham, Hadley, Jennifer Bryan, Malcolm Barrett, and Andy Teucher.\n2023. Usethis: Automate Package and Project Setup. https://usethis.r-lib.org.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis\nVaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://dplyr.tidyverse.org.\n\n\nWickham, Hadley, Maximilian Girlich, and Edgar Ruiz. 2023. Dbplyr: A\nDplyr Back End for Databases. https://dbplyr.tidyverse.org/.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2023. Readr: Read\nRectangular Text Data. https://readr.tidyverse.org.\n\n\nWickham, Hadley, Jim Hester, Winston Chang, and Jennifer Bryan. 2022.\nDevtools: Tools to Make Developing r Packages Easier. https://devtools.r-lib.org/.\n\n\nWickham, Hadley, Evan Miller, and Danny Smith. 2023. Haven: Import\nand Export SPSS, Stata and SAS Files. https://haven.tidyverse.org.\n\n\nWickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023. Tidyr:\nTidy Messy Data. https://tidyr.tidyverse.org.\n\n\nWijffels, Jan. 2023. Udpipe: Tokenization, Parts of Speech Tagging,\nLemmatization and Dependency Parsing with the UDPipe ’NLP’ Toolkit.\nhttps://bnosac.github.io/udpipe/en/index.html.\n\n\nWulff, S, A Stefanowitsch, and Stefan Th. Gries. 2007. “Brutal\nBrits and Persuasive Americans.” Aspects of Meaning.\n\n\nXie, Yihui. 2023. Tinytex: Helper Functions to Install and Maintain\nTeX Live, and Compile LaTeX Documents. https://github.com/rstudio/tinytex.\n\n\nZipf, George Kingsley. 1949. Human Behavior and the Principle of\nLeast Effort. Oxford, England: Addison-Wesley Press."
  },
  {
    "objectID": "data.html#sec-data-anc",
    "href": "data.html#sec-data-anc",
    "title": "App A: Data",
    "section": "A.1 ANC",
    "text": "A.1 ANC\n\nANC"
  },
  {
    "objectID": "data.html#sec-data-brown",
    "href": "data.html#sec-data-brown",
    "title": "App A: Data",
    "section": "A.2 BNC",
    "text": "A.2 BNC\n\nBrown Corpus"
  },
  {
    "objectID": "data.html#sec-data-cabnc",
    "href": "data.html#sec-data-cabnc",
    "title": "App A: Data",
    "section": "A.3 CABNC",
    "text": "A.3 CABNC\n\nCABNC"
  },
  {
    "objectID": "data.html#sec-data-cedel2",
    "href": "data.html#sec-data-cedel2",
    "title": "App A: Data",
    "section": "A.4 CEDEL2",
    "text": "A.4 CEDEL2\n\nCEDEL2"
  },
  {
    "objectID": "data.html#sec-data-enntt",
    "href": "data.html#sec-data-enntt",
    "title": "App A: Data",
    "section": "A.5 ENNTT",
    "text": "A.5 ENNTT\n\nENNTT\nEuroparl\nFederalist Papers (LOC)\nSOTU\nSWDA\n…"
  },
  {
    "objectID": "feedback.html",
    "href": "feedback.html",
    "title": "App B: Feedback ",
    "section": "",
    "text": "Thank you for taking the time to read through this book 🫶🏻. I really value your opinion and I would love to hear your thoughts as I continue to make progress.\n\nWhere to review\nChapters that are ready for feedback will appear with the following callout.\n\n\n\n\n\n\nDraft\n\n\n\nReady for review.\n\n\nThose that are not ready will appear with the following callout.\n\n\n\n\n\n\nCaution\n\n\n\nUnder development.\n\n\nIn a few cases a chapter will be ready for review, but I’ll still be working on the exercises, callouts, etc. In those cases, the items that are still under development will be marked with the  icon.\n\n\nWhat to look for\nAs you read over the draft, I’d appreciate your feedback in the following areas:\n\nClarity and Comprehensibility\nI’d love to know if you think the content is clear and easy to understand. Do you think the concepts are broken down enough? Are the examples helpful? If anything seems too jargon-y or confusing, definitely let me know.\nConsistency\nIt’s pretty important to keep things smooth. So, keep an eye out for any inconsistent writing styles, terminology, or layout. If something seems off, I’d appreciate it if you point it out.\nRelevance\nDoes the material match the current standards and knowledge? Will it the topics and questions be of interest to linguists? If something feels outdated or irrelevant, don’t hesitate to mention it.\nEngagement\n​I​’m not looking to drop a boring read on people. So, as you’re going through it, think about whether it holds your interest. Maybe ​the prose needs more life, the examples need to be more diverse, or the exercises could be more or less challenging. If you have any ideas, I’m all ears.\n\n\n\nHow to submit feedback\nDepending on your preference, you can submit feedback in one of three ways:\n\nhypothes.is\nThis is the easiest way to submit feedback. Join the “qtal_feedback” annotation group and just highlight the text you want to comment on and click the “Annotate” button. You can also add comments to the right sidebar.\nGitHub issues\nThis book is hosted on GitHub, so you can submit feedback directly through the issues page for the repository. Just click the “New issue” button and fill out the form. You’ll need a GitHub account to do this.\nEmail me at francojc@wfu.edu\nIf you’d rather not use the other options, you can always email me directly. Just make sure to try to include references to the specific parts of the book you’re referring to. A link or section number will do.\n\n\n\nThank yous!\nI want to thank you beforehand for your willingness to help me out. I really appreciate it. I also want to thank you in print. Please give me the name you would like to appear in the Acknowledgements section. If you’d rather not be acknowledged in the final version of the book, please let me know."
  }
]