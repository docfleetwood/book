[
  {
    "objectID": "framing-research.html#keys-to-strong-research",
    "href": "framing-research.html#keys-to-strong-research",
    "title": "4  Framing research",
    "section": "\n4.1 Keys to strong research",
    "text": "4.1 Keys to strong research\nTogether a research area, problem, aim and question and the research blueprint that forms the conceptual and practical scaffolding of the project ensure from the outset that the project is solidly grounded in the main characteristics of good research. These characteristics, summarized by Cross (2006), are found in Table 4.1.\n\n\n\n\nTable 4.1: Characteristics of research (Cross, 2006).\n\n\n\n\n\nCharacteristic\nDescription\n\n\n\nPurposive\nBased on identification of an issue or problem worthy and capable of investigation\n\n\nInquisitive\nSeeking to acquire new knowledge\n\n\nInformed\nConducted from an awareness of previous, related research\n\n\nMethodical\nPlanned and carried out in a disciplined manner\n\n\nCommunicable\nGenerating and reporting results which are feasible and accessible by others\n\n\n\n\n\n\nWith these characteristics in mind, let’s get started with the first component to address –connecting with the literature."
  },
  {
    "objectID": "framing-research.html#connect",
    "href": "framing-research.html#connect",
    "title": "4  Framing research",
    "section": "\n4.2 Connect",
    "text": "4.2 Connect\n\n4.2.1 Research area\nThe area of research is the first decision to make in terms of where to make a contribution to understanding. At this point, the aim is to identify a general area of interest where a researcher wants to derive insight. For those with an established research trajectory in language, the area of research to address through text analysis will likely be an extension of their prior work. For others, which include new researchers or researcher’s that want to explore new areas of language research or approach an area through a language-based lens, the choice of area may be less obvious. In either case, the choice of a research area should be guided by a desire to contribute something relevant to a theoretical, social, and/ or practical matter of personal interest. Personal relevance goes a long way to developing and carrying out purposive and inquisitive research.\nSo how do we get started? The first step is to reflect on your own areas of interest and knowledge, be it academic, professional, or personal. Language is at the heart of the human experience and therefore found in some fashion anywhere one seeks to find it. But it is a big world and more often than not the general question about what area to explore language use is sometimes the most difficult. To get the ball rolling, it is helpful to peruse disciplinary encyclopedias or handbooks of linguistics and language-related an academic fields (e.g. Encyclopedia of Language and Linguistics (Brown 2005), A Practical Guide to Electronic Resources in the Humanities (Dubnjakovic and Tomlin 2010), Routledge encyclopedia of translation technology (Chan 2014))\nA more personal, less academic, approach is to consult online forums, blogs, etc. that one already frequents or can be accessed via an online search. For example, Reddit has a wide variety of active subreddits (r/LanguageTechnology, r/Linguistics, r/corpuslinguistics, r/DigitalHumanities, etc.). Twitter and Facebook also have interesting posts on linguistics and language-related fields worth following. Through one of these social media site you may find particular people that maintain a blog worth browsing. For example, I follow Julia Silge, Rachel Tatman, and Ted Underwood, inter alia. Perusing these resources can help spark ideas and highlight the kinds of questions that interest you.\nRegardless of whether your inquiry stems from academic, professional, or personal interest, try to connect these findings to academic areas of research. Academic research is highly structured and well-documented and making associations with this network will aid in subsequent steps in developing a research project.\n\n4.2.2 Research problem\nOnce you’ve made a rough-cut decision about the area of research, it is now time to take a deeper dive into the subject area and jump into the literature. This is where the rich structure of disciplinary research will provide aid to traverse the vast world of academic knowledge and identify a research problem. A research problem highlights a particular topic of debate or uncertainty in existing knowledge which is worthy of study.\nSurveying the relevant literature is key to ensuring that your research is informed, that is, connected to previous work. Identifying relevant research to consult can be a bit of a ‘chicken or the egg’ problem –some knowledge of the area is necessary to find relevant topics, some knowledge of the topics is necessary to narrow the area of research. Many times the only way forward is to jump in conducting searches. These can be world-accessible resources (e.g. Google Scholar) or limited-access resources that are provided through an academic institution (e.g. Linguistics and Language Behavior Abstracts), ERIC, PsycINFO, etc.). Some organizations and academic institutions provide research guides to help researcher’s access the primary literature.\nAnother avenue to explore are journals dedicated to areas in which linguistics and language-related research is published. In the following tables I’ve listed a number of highly visible journals in linguistics, digital humanities, and computational linguistics.\n\n\n\n\nTable 4.2: A list of some linguistics journals.\n\n\n\n\n\nResource\nDescription\n\n\n\nCorpora\nAn international, peer-reviewed journal of corpus linguistics focusing on the many and varied uses of corpora both in linguistics and beyond.\n\n\nCorpus Linguistics and Linguistic Theory\nCorpus Linguistics and Linguistic Theory (CLLT) is a peer-reviewed journal publishing high-quality original corpus-based research focusing on theoretically relevant issues in all core areas of linguistic research, or other recognized topic areas.\n\n\nInternational Journal of Corpus Linguistics\nThe International Journal of Corpus Linguistics (IJCL) publishes original research covering methodological, applied and theoretical work in any area of corpus linguistics.\n\n\nInternational Journal of Language Studies\nIt is a refereed international journal publishing articles and reports dealing with theoretical as well as practical issues focusing on language, communication, society and culture.\n\n\nJournal of Child Language\nA key publication in the field, Journal of Child Language publishes articles on all aspects of the scientific study of language behaviour in children, the principles which underlie it, and the theories which may account for it.\n\n\nJournal of Linguistic Geography\nThe Journal of Linguistic Geography focuses on dialect geography and the spatial distribution of language relative to questions of variation and change.\n\n\nJournal of Quantitative Linguistics\nPublishes research on the quantitative characteristics of language and text in mathematical form, introducing methods of advanced scientific disciplines.\n\n\n\n\n\n\n\n\n\n\nTable 4.3: A list of some humanities journals.\n\n\n\n\n\nResource\nDescription\n\n\n\nDigital Humanities Quarterly\nDigital Humanities Quarterly (DHQ), an open-access, peer-reviewed, digital journal covering all aspects of digital media in the humanities.\n\n\nDigital Scholarship in the Humanities\nDSH or Digital Scholarship in the Humanities is an international, peer reviewed journal which publishes original contributions on all aspects of digital scholarship in the Humanities including, but not limited to, the field of what is currently called the Digital Humanities.\n\n\nJournal of Cultural Analytics\nCultural Analytics is an open-access journal dedicated to the computational study of culture. Its aim is to promote high quality scholarship that applies computational and quantitative methods to the study of cultural objects (sound, image, text), cultural processes (reading, listening, searching, sorting, hierarchizing) and cultural agents (artists, editors, producers, composers).\n\n\n\n\n\n\n\n\n\n\nTable 4.4: A list of some computational linguistics journals.\n\n\n\n\n\nResource\nDescription\n\n\n\nComputational Linguistics\nComputational Linguistics is the longest-running publication devoted exclusively to the computational and mathematical properties of language and the design and analysis of natural language processing systems.\n\n\nLREC Conferences\nThe International Conference on Language Resources and Evaluation is organised by ELRA biennially with the support of institutions and organisations involved in HLT.\n\n\nTransactions of the Association for Computational Linguistics\nTransactions of the Association for Computational Linguistics (TACL) is an ACL-sponsored journal published by MIT Press that publishes papers in all areas of computational linguistics and natural language processing.\n\n\n\n\n\n\nTo explore research related to text analysis it is helpful to start with the (sub)discipline name(s) you identified in when selecting your research area, more specific terms that occur to you or key terms from the literature, and terms such as ‘corpus study’ or ‘corpus-based’. The results from first searches may not turn out to be sources that end up figuring explicitly in your research, but it is important to skim these results and the publications themselves to mine information that can be useful to formulate better and more targeted searches. Relevant information for honing your searches can be found throughout an academic publication (article or book). However, pay particular attention to the abstract, in articles, and the table of contents, in books, and the cited references. Abstracts and tables of contents often include discipline-specific jargon that is commonly used in the field. In some articles there is even a short list of key terms listed below the abstract which can be extremely useful to seed better and more precise search results. The references section will contain relevant and influential research. Scan these references for publications which appear to narrowing in on topic of interest and treat it like a search in its own right.\nOnce your searches begin to show promising results it is time to keep track and organize these references. Whether you plan to collect thousands of references over a lifetime of academic research or your aim is centered around one project, software such as Zotero1, Mendeley, or BibDesk provide powerful, flexible, and easy-to-use tools to collect, organize, annotate, search, and export references. Citation management software is indispensable for modern research –and often free!\nAs your list of relevant references grows, you will want to start the investigation process in earnest. Begin skimming (not reading) the contents of each of these publications, starting with the most relevant first2. Annotate these publications using highlighting features of the citation management software to identify: (1) the stated goal(s) of the research, (2) the data source(s) used, (3) the information drawn from the data source(s), (4) the analysis approach employed, and (5) the main finding(s) of the research as they pertain to the stated goal(s). Next, in your own words, summarize these five key areas in prose adding your summary to the notes feature of the citation management software. This process will allow you to efficiently gather and document references with the relevant information to guide the identification of a research problem, to guide the formation of your problem statement, and ultimately, to support the literature review that will figure in your project write-up.\nFrom your preliminary annotated summaries you will undoubtedly start to recognize overlapping and contrasting aspects in the research literature. These aspects may be topical, theoretical, methodological, or appear along other lines. Note these aspects and continue to conduct more refine searches, annotate new references, and monitor for any emerging patterns of uncertainty or debate (gaps) which align with your research interest(s). When a promising pattern takes shape, it is time to engage with a more detailed reading of those references which appear most relevant highlighting the potential gap(s) in the literature. At this point you can focus energy on more nuanced aspects of a particular gap in the literature with the goal to formulate a problem statement. A problem statement directly acknowledges a gap in the literature and puts a finer point on the nature and relevance of this gap for understanding. This statement reflects your first deliberate attempt to establish a line of inquiry. It will be a targeted, but still somewhat general, statement framing the gap in the literature that will guide subsequent research design decisions."
  },
  {
    "objectID": "framing-research.html#findings",
    "href": "framing-research.html#findings",
    "title": "4  Framing research",
    "section": "\n4.3 Findings",
    "text": "4.3 Findings\n\n4.3.1 Research aim\nWith a problem statement in hand, it is now time to consider the goal(s) of the research. A research aim frames the type of inquiry to be conducted. Will the research aim to explain, evaluate, or explore? In other words, will the research seek to test a particular relationship, assess the potential strength of a particular relationship, or uncover novel relationships? As you can appreciate, the research aim is directly related to the analysis methods we touched upon in Chapter 3.\nTo gauge how to frame your research aim, reflect on the literature that led you to your problem statement and the nature of the problem statement itself. If the gap at the center of the problem statement is a lack of knowledge, your research aim may be exploratory. If the gap concerns a conjecture about a relationship, then your research may take a predictive approach. When the gap points to the validation of a relationship, then your research will likely be inferential in nature. Before selecting your research aim it is also helpful to consult the research aims of the primary literature that led you to your research statement. Consider how your research statement relates the previous literature. Do you aim to test a hypothesis based on previous exploratory analyses? Are you looking to generate new knowledge in an (apparently) uncharted area?\nIn general, a problem statement which addresses a smaller, nuanced gap will tend to adopt similar research aims as the previous literature while a larger, more divergent gap will tend to adopt a distinct research aim. This is not a hard rule, but more of a heuristic, however, and it is important to be familiar with both the previous literature, the nature of different types of analysis, and the goals of the research to ensure that the research is best-positioned to generate findings that will contribute to the existing body of understanding in a principled way.\n\n4.3.2 Research question\nThe next step in research design is to craft the research question. A research question is clearly defined statement which identifies an aspect of uncertainty and the particular relationships that this uncertainty concerns. The research question extends and narrows the line of inquiry established in the research statement and research aim. The research statement can be seen as the content and the research aim as the form.\nThe form of a research question will vary based on the analysis approach. For inferential-based research, the research question will actually be a statement, not a question. This statement makes a testable claim about the nature of a particular relationship –i.e. asserts a hypothesis. For illustration, let’s return to one of the hypotheses we previously sketched out in Chapter 3, leaving aside the implicit null hypothesis.\nWomen use more questions than men in spontaneous conversations.\nFor predictive- and exploratory-based research, the research question is in fact a question. A reframing of the example hypothesis for a predictive-based research question might looks something like this.\nCan the number of questions used in spontaneous conversations predict if a speaker is male or female?\nAnd a similar exploratory-based research question would take this form.\nDo men and women differ in terms of the number of questions they use in spontaneous conversations?\nThe central research interest behind these hypothetical research questions is, admittedly, quite basic. But from these simplified examples, we are able to appreciate the similarities and differences between the forms of research statements that correspond to distinct research aims.\nIn terms of content, the research question will make reference to two key components. First, is the unit of analysis. The unit of analysis is the entity which the research aims to investigate. For our three example research aims, the unit of analysis is the same, namely men and women. Note, however, that the current unit of analysis is somewhat vague in the example research questions. A more precise unit of analysis would include more information about the population from which the men and women are drawn (e.g English speakers, American English speakers, American English speakers of the Southeast, etc.).\nThe second key component is the unit of observation. The unit of observation is the primary element on which the insight into the unit of analysis is derived and in this way constitutes the essential organization unit of the data to be collected. In our examples, the unit of observation, again, is unchanged and is spontaneous conversations. Note that while the unit of observation is key to identify as it forms the organizational backbone of the research, it is very common for the research to derive variables from this unit to provide evidence to investigate the research question. In the previous examples, we identified the number of conversations as part of the research question. But in other cases a researcher may seek to understand other aspects of questions in spontaneous conversations (i.e type of question, features of questions, etc.). The unit of observation, however, would remain the same."
  },
  {
    "objectID": "framing-research.html#blueprint",
    "href": "framing-research.html#blueprint",
    "title": "4  Framing research",
    "section": "\n4.4 Blueprint",
    "text": "4.4 Blueprint\nEfforts to craft a research question are a very important aspect of developing purposive, inquisitive, and informed research (returning to Cross’s characteristics of research). Moving beyond the research question in the project means developing and laying out the research design in a way such that the research is Methodical and Communicable. In this coursebook, the method to achieve these goals is through the development of a research blueprint. The blueprint includes two components: (1) the process of identifying the data, information, and methods to be used and (2) the creation of a plan to structure and document the project.\nAs Ignatow and Mihalcea (2017) point out:\n\nResearch design is essentially concerned with the basic architecture of research projects, with designing projects as systems that allow theory, data, and research methods to interface in such a way as to maximize a project’s ability to achieve its goals […]. Research design involves a sequence of decisions that have to be taken in a project’s early stages, when one oversight or poor decision can lead to results that are ultimately trivial or untrustworthy. Thus, it is critically important to think carefully and systematically about research design before committing time and resources to acquiring texts or mastering software packages or programming languages for your text mining project.\n\n\n4.4.1 Identify\nImportance of identifying and documenting the key aspects required to conduct the research cannot be understated. On the one hand this process links concept to implementation. In doing so, a researcher is better-positioned to conduct research with a clear view of what will be entailed. On the other hand, a promising research question, on paper, may present challenges that may require modification or reevaluation of the viability of the project. It is not uncommon to encounter roadblocks or even dead-ends for moving a well-founded research question forward when considering the available data, a researcher’s (current) technical and/ or research skills, and the given time frame for the project. In practice, the process of identifying the data, information, and methods of analysis are considered in tandem with the investigative work to develop a research aim and research question. In this subsection I will cover the main characteristics to consider when developing a research blueprint.\n\nThe first, and most important, part of establishing a research blueprint is to identify a viable data source. Regardless of how you find and access the data, it is essential to vet the corpus sample in light of the research question. In the case that research is inferential in nature, the sampling frame of the corpus is of primary importance as the goal is to generalize the findings to a target population. A corpus resource should align, to the extent feasible, with this target population. For predictive and exploratory research, the goal to generalize a claim is not central and for this reason the there is some freedom in terms of how representative a corpus sample is of a target population. Ideally a researcher will find and be able to model a language population of target interest. Since the goal, however, is not to test a hypothesis, but rather to explore particular or potential relationships, either in an predictive or exploratory fashion, the research can often continue with the stipulation that the results are interpreted in the light of the characteristic of the available corpus sample.\n\n\nThe second step is to identify the key variables need to conduct the research are and then ensure that this information can be derived from the corpus data. The research question will reference the unit of analysis and the unit of observation, but it is important at this point to then pinpoint what the key variables will be. If the unit of observation is spontaneous conversations. The question as to what aspects of these conversations will be used in the analysis. In the research questions presented in this chapter, we will want to envision what needs to be done to generate a variable which measures the number of questions in each of the conversations. In other research, their may be features that need to be extracted and recoded to address the research question. Other variables of importance may be non-linguistic in nature. Provided the corpus has the required meta-data for the research, variables can be normalized, recoded, and generated from the corpus itself to fit research needs. In cases where there the meta-data is incomplete for the goals of the research, it is sometimes possible to merge meta-data from other sources.\n\nThe third step is to identify a method of analysis. The selection of the analysis approach that was part of the research aim and then the research question goes a long way to narrowing the methods that a researcher must consider. But there are a number of factors which will make some methods more appropriate than others. In inferential research, the number and information values of the variables to be analyzed will be of key importance (Gries 2013). The informational value of the dependent variable will again narrow the search for the appropriate method. The number of independent variables also plays an important role. For example, a study with a categorical dependent variable with a single categorical independent variable will lead the researcher to the Chi-squared test. A study with a continuous dependent variable with multiple independent variables will lead to linear regression. Another aspect of note for inference studies is the consideration of the distribution of continuous variables –a normal distribution will use a parametric test where a non-normal distribution will use a non-parametric test. These details need not be nailed down at this point, but it is helpful to have them on your radar to ensure that when the time comes to analyze the data, the appropriate steps are taken to test for normality and then apply the correct test.\nFor predictive-based research, the informational value of the target variable is key to deciding whether the prediction will be a classification task or a numeric prediction task. This has downstream effects when it comes time to evaluate and interpret the results. Although the feature engineering process in predictive analyses means that the features do not need to be specified from the outset and can be tweaked and changed as needed during an analysis, it is a good idea to start with a basic sense of what features most likely will be helpful in developing a robust predictive model. Furthermore, while the number and informational values of the features (predictor variables) are not as important to selecting a prediction method (algorithm) as they are in inferential analysis methods, it is important to recognize that algorithms have strengths and shortcomings when working large numbers and/ or types of features (Lantz 2013).\nExploratory research is the least restricted of the three types of analysis approaches. Although it may be the case that a research will not be able to specify from the outset of a project what the exact analysis methods will be, an attempt to consider what types of analysis methods will be most promising to provide results to address the research question goes a long way to steering a project in the right direction and grounding the research. As with the other analysis approaches, it is important to be aware of what the analysis methods available and what type of information they produce in light of the research question.\nIn sum, the identification of the data, information, and analysis methods that will be used in the proposed research are key to ensuring the research is viable. Be sure to document this process in prose and describe the strengths and potential shortcomings of (1) the corpus data selected, (2) the information to be extracted for analysis, and (3) the analysis method(s) that are appropriate for the research aim and what the evaluation method will be. Furthermore, not every eventuality can be foreseen. It is helpful to include a description of aspects of this process which may pose challenges and to include potential contingency plans as part of this prose description.\n\n\n4.4.2 Plan\nThe next step in creating a research blueprint is to consider how to physically implement your project. This includes how to organize files and directories in a fashion that both provides the researcher a logical and predictable structure to work with but also ensures that the research is Communicable. On the one hand, communicable research includes a strong write-up of the research, but, on the other hand, it is also important that the research is reproducible. Reproducibility strategies are a benefit to the researcher (in the moment and in the future) as it leads to better work habits and to better teamwork and it makes changes to the project easier. Reproducibility is also of benefit to the scientific community as shared reproducible research enhances replicability and encourages cumulative knowledge development (Gandrud 2015).\nThere are a set of guiding principles to accomplish these goals (Gentleman and Temple Lang 2007; Marwick, Boettiger, and Mullen 2018).\n\nAll files should be plain text which means they contain no formatting information other than whitespace.\nThere should be a clear separation between the data, method, and output of research. This should be apparent from the directory structure.\nA separation between original data and derived data should be made. Original data should be treated as ‘read-only’. Any changes to the original data should be justified, generated by the code, and documented (see point 6).\nEach analysis file (script) should represent a particular, well-defined step in the research process.\nEach analysis script should be modular –that is, each file should correspond to a specific goal in the analysis procedure with input and output only corresponding to this step.\nAll analysis scripts should be tied together by a ‘master’ script that is used to coordinate the execution of all the analysis steps.\nEverything should be documented. This includes analysis steps, script code comments, data description in data dictionaries, information about the computing environment and packages used to conduct the analysis, and detailed instructions on how to reproduce the research.\n\nThese seven principles can be physically implemented in countless ways. In recent years, there has been a growing number of efforts to create R packages and templates to quickly generate the scaffolding and tools to facilitate reproducible research. Some notable R packages include workflowr and ProjectTemplate but there are many other resources for R included on the CRAN Task View for Reproducible Research. There are many advantages to working with pre-existing frameworks for the savvy R programmer.\n\nIn this coursebook, however, I have developed a project template (available on GitHub) which I believe simplifies and makes the process more transparent for beginning and intermediate R programmers, the directory structure is provided below.\n\n\n#> ../project_template/\n#> ├── README.md\n#> ├── _pipeline.R\n#> ├── analysis\n#> │   ├── 1_acquire_data.Rmd\n#> │   ├── 2_curate_dataset.Rmd\n#> │   ├── 3_transform_dataset.Rmd\n#> │   ├── 4_analyze_dataset.Rmd\n#> │   ├── 5_generate_article.Rmd\n#> │   ├── _session-info.Rmd\n#> │   ├── _site.yml\n#> │   ├── index.Rmd\n#> │   └── references.bib\n#> ├── data\n#> │   ├── derived\n#> │   └── original\n#> └── output\n#>     ├── figures\n#>     └── results\n\n\nLet me now describe how this template structure aligns with the seven principles of quality reproducible research.\n\nAll files are plain text (e.g. .R, .Rmd, .csv, .txt, etc.).\nThere are three main directories analysis/, data/, and ouput/.\nThe data/ directory contains sub-directories for original (‘read-only’) data and derived data.\nThe analysis/ directory contains five scripts which are numbered to correspond with their sequential role in the research process.\nEach of these analysis scripts are designed to be modular; input and output must be explicit and no intermediate objects are carried over to other analysis scripts. Dataset output should be written to and read from the data/derived/ directory. Figures and statistical results should be written to and read from output/figures/ and output/results respectively.\nAll of the analysis scripts, and therefore the entire project, are tied to the _pipeline.R script. To reproduce the entire project only this script need be run.\nDocumentation takes place at many levels. The README.md file is the first file that a researcher will consult. It contains a brief description of the project goals and how to reproduce the analysis. Analysis scripts use the Rmarkdown format (.Rmd). This format allows researchers to interleave prose description and executable code in the same script. This ensures that the rationale for the steps taken are described in prose, the code is made available to consult, and that code comments can be added to every line. The _sesssion-info.Rmd script is merged with each analysis script to provide information about the computing environment and packages used to conduct each step analysis. As this is a template, no data or datasets appear. However, once data is acquired and that data is curated and transformed, documentation for these resources should be documented for each resource in a data dictionary along side the data(set) itself.\n\nThe aspects of the project template described in points 1-7 together form the backbone for reproducible research. This template, however, includes additional functionality to enhance efficient and communicable research. The _pipeline.R script executes the analysis scripts in the analysis directory, but as a side effect also produces a working website and a journal-ready article for publishing your analysis, results, and findings to the web in HTML and PDF format. The index.Rmd file is the splash page for the website and is a good place to house your pre-analysis investigative work including your research area, problem, aim, and question and to document your research blueprint including the identification of viable data resource(s), the key variables for the analysis, the analysis method, and the method of assessment. All Rmarkdown files provide functionality for citing and organizing references. The references.bib file is where references are stored and can be used to include citations that support your research throughout your project.\n\n4.4.3 Prepare\nThis template will allow you to organize your research design and align it with implementation steps to conduct quality reproducible research. To prepare for your analysis, you will need to download or fork and clone this template from the GitHub repository and then make some adjustments to personalize this template for your research.\nTo create a local copy of this project template either:\n\nDownload and decompress the .zip file\n\nIf you have git installed on your machine and a GitHub account, fork the repository to your own GitHub account. Then open a terminal in the desired location and clone the repository. If you are using RStudio, you can setup a new RStudio Project with the clone using the ‘New Project…’ dialog, choosing ‘Version Control’, and following the steps.\n\nBefore you begin configuring and adding your project-specific details to this template. Reproduce this project ‘as-is’ to confirm that it builds on your local machine.\nIn RStudio or in R session in a Terminal application, open the console in the root directory of the project. Then run:\nsource(\"_pipeline.R\")\nIt will take some time to complete, when it does the prompt (>) in the console will return. Then navigate to and open docs/index.html in a browser.\nOnce you have confirmed that the project template builds, then you can begin to configure the template to reflect your project. There a few files to consider first. These files are places where the title of your project should appear.\n\nREADME.md\n_pipeline.R\nanalysis/index.Rmd\n\nAfter updating these files, build the project again and make sure that the new changes appear as you would like them. You are now ready to start your research project!"
  },
  {
    "objectID": "framing-research.html#summary",
    "href": "framing-research.html#summary",
    "title": "4  Framing research",
    "section": "Summary",
    "text": "Summary\nThe aim of this chapter is to provide the key conceptual and practical points to guide the development of a viable research project. Good research is purposive, inquisitive, informed, methodological, and communicable. It is not, however, always a linear process. Exploring your area(s) of interest and connecting with existing work will help couch and refine your research. But practical considerations, such as the existence of viable data, technical skills, and/ or time constrains, sometimes pose challenges and require a researcher to rethink and/ or redirect the research in sometimes small and other times more significant ways. The process of formulating a research question and developing a viable research plan is key to supporting viable, successful, and insightful research. To ensure that the effort to derive insight from data is of most value to the researcher and the research community, the research should strive to be methodological and communicable adopting best practices for reproducible research.\nThis chapter concludes the Orientation section of this coursebook. At this point the fundamental characteristics of research are in place to move a project towards implementation. The next section, Preparation, aims to cover the acquisition, curation, and transformation of data in preparation for analysis. These are the first steps in putting a research blueprint into action and by no coincidence the first components in the Data to Insight Hierarchy. Following the Preparation section our attention will turn to the implementation of the three analysis approaches we have covered: inference, prediction, and exploration. Throughout these next sections we will maintain our aim to develop methodological and communicable research by connecting our implementation process to reproducible programming strategies.\n\n\n\n\nFigure 4.1: Framing research: visual summary"
  },
  {
    "objectID": "framing-research.html#activities",
    "href": "framing-research.html#activities",
    "title": "4  Framing research",
    "section": "Activities",
    "text": "Activities\n\n\n\n\n\n\nRecipe\n\n\n\nWhat: Project management with Git, GitHub, and RStudio CloudHow: Read Recipe 5 and participate in the Hypothes.is online social annotation.Why: To learn how to use Git, GitHub, and RStudio to manage, store, and publish reproducible research projects.\n\n\n\n\n\n\n\n\nLab\n\n\n\nWhat: Project management with Git, GitHub, and RStudio CloudHow: Clone, fork, and complete the steps in Lab 5.Why: To set up a GitHub account, fork and copy a GitHub repository to RStudio Cloud, and use R, Git, and GitHub to manage, store, and publish changes to a reproducible research project."
  },
  {
    "objectID": "framing-research.html#questions",
    "href": "framing-research.html#questions",
    "title": "4  Framing research",
    "section": "Questions",
    "text": "Questions\n\n…\n…\n\n\n\n\n\nBrown, Keith. 2005. Encyclopedia of Language and Linguistics. Vol. 1. Elsevier.\n\n\nChan, Sin-wai. 2014. Routledge Encyclopedia of Translation Technology. Routledge.\n\n\nCross, Nigel. 2006. “Design as a Discipline.” Designerly Ways of Knowing, 95–103.\n\n\nDubnjakovic, Ana, and Patrick Tomlin. 2010. A Practical Guide to Electronic Resources in the Humanities. Elsevier.\n\n\nGandrud, Christopher. 2015. Reproducible Research with r and r Studio. Second edition. CRC Press.\n\n\nGentleman, Robert, and Duncan Temple Lang. 2007. “Statistical Analyses and Reproducible Research.” Journal of Computational and Graphical Statistics 16 (1): 1–23.\n\n\nGries, Stefan Th. 2013. Statistics for Linguistics with r. A Practical Introduction. 2nd revise.\n\n\nIgnatow, Gabe, and Rada Mihalcea. 2017. An Introduction to Text Mining: Research Design, Data Collection, and Analysis. Sage Publications.\n\n\nLantz, Brett. 2013. Machine Learning with r. Birmingham: Packt Publishing.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging Data Analytical Work Reproducibly Using r (and Friends).” The American Statistician 72 (1): 80–88."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Quantitative Text Analysis for Linguistics",
    "section": "Welcome",
    "text": "Welcome\nThis textbook is an introduction to the fundamental concepts and practical programming skills from Data Science that are increasingly employed in a variety of language-centered fields and sub-fields applied to the task of quantitative text analysis. It is geared towards advanced undergraduates, graduate students, and researchers looking to expand their methodological toolbox.\n\nThe content is currently under development. Feedback is welcome and can be provided through the hypothes.is service. A toolbar interface to this service is located on the right sidebar. To register for a free account and join the “text_as_data” annotation group follow this link. Suggestions and changes that are incorporated will be acknowledged.\n\nAuthor\nDr. Jerid Francom is Associate Professor of Spanish and Linguistics at Wake Forest University. His research focuses on the use of large-scale language archives (corpora) from a variety of sources (news, social media, and other internet sources) to better understand the linguistic and cultural similarities and differences between language varieties for both scholarly and pedagogical projects. He has published on topics including the development, annotation, and evaluation of linguistic corpora and analyzed corpora through corpus, psycholinguistic, and computational methodologies. He also has experience working with and teaching statistical programming with R."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Quantitative Text Analysis for Linguistics",
    "section": "License",
    "text": "License\nThis work by Jerid C. Francom is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 United States License."
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Quantitative Text Analysis for Linguistics",
    "section": "Credits",
    "text": "Credits\n\nIcons made from Icon Fonts are licensed by CC BY 3.0"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Quantitative Text Analysis for Linguistics",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nTAD has been reviewed by and suggestions and changes incorporated based on the feedback through the TAD Hypothes.is group by the following people: Andrea Bowling, Caroline Brady, Declan Golsen, Asya Little, Claudia Valdez, …"
  },
  {
    "objectID": "index.html#build-information",
    "href": "index.html#build-information",
    "title": "Quantitative Text Analysis for Linguistics",
    "section": "Build information",
    "text": "Build information\n\n\nThis version of the textbook was built with R version 4.2.2 (2022-10-31) on macOS Big Sur … 10.16 with the following packages:\n\n\n\n\npackage\nversion\nsource\n\n\n\ndplyr\n1.0.10\nCRAN (R 4.2.0)\n\n\nDT\n0.26\nCRAN (R 4.2.0)\n\n\nforcats\n0.5.2\nCRAN (R 4.2.0)\n\n\nggplot2\n3.4.0\nCRAN (R 4.2.0)\n\n\nhere\n1.0.1\nCRAN (R 4.2.0)\n\n\nknitr\n1.41\nCRAN (R 4.2.0)\n\n\npurrr\n1.0.1\nCRAN (R 4.2.2)\n\n\nreadr\n2.1.3\nCRAN (R 4.2.0)\n\n\nrmarkdown\n2.19\nCRAN (R 4.2.0)\n\n\nstringr\n1.5.0\nCRAN (R 4.2.0)\n\n\ntadr\n0.1.1\nlocal\n\n\ntibble\n3.1.8\nCRAN (R 4.2.0)\n\n\ntidyr\n1.2.1\nCRAN (R 4.2.0)\n\n\ntidytext\n0.4.1\nCRAN (R 4.2.0)\n\n\ntidyverse\n1.3.2\nCRAN (R 4.2.0)\n\n\nwebshot\n0.5.4\nCRAN (R 4.2.0)"
  },
  {
    "objectID": "transform-datasets.html#normalize",
    "href": "transform-datasets.html#normalize",
    "title": "7  Transform datasets",
    "section": "\n7.1 Normalize",
    "text": "7.1 Normalize\nThe process of normalizing datasets in essence is to santize the values of variable or set of variables such that there are no artifacts that will contaminate subsequent processing. It may be the case that non-linguistic metadata may require normalization but more often than not linguistic information is the most common target for normalization as text often includes artifacts from the acquisition process which will not be desired in the analysis.\nEuroparle Corpus\nConsider the curated Europarle Corpus dataset. I will read in the dataset. Since the dataset is quite large, I have also subsetted the dataset keeping only the first 1,000 observations for each of value of type for demonstration purposes.\n\neuroparle <- read_csv(file = \"../data/derived/europarle/europarle_curated.csv\") %>%  # read curated dataset\n  filter(sentence_id < 1001) # keep first 1000 observations for each type\n\nglimpse(europarle)\n\n\n\n#> Rows: 2,000\n#> Columns: 3\n#> $ type        <chr> \"Source\", \"Target\", \"Source\", \"Target\", \"Source\", \"Target\"…\n#> $ sentence_id <dbl> 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, …\n#> $ sentence    <chr> \"Reanudación del período de sesiones\", \"Resumption of the …\n\n\nSimply looking at the first 14 lines of this dataset, we can see that if our goal is to work with the transcribed (‘Source’) and translated (‘Target’) language, there are lines which do not appear to be of interest.\n\n\n\n\nTable 7.1: Europarle Corpus curated dataset preview.\n\n\n\n\n\n\ntype\nsentence_id\nsentence\n\n\n\nSource\n1\nReanudación del período de sesiones\n\n\nTarget\n1\nResumption of the session\n\n\nSource\n2\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\n\n\nTarget\n2\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n\n\nSource\n3\nComo todos han podido comprobar, el gran “efecto del año 2000” no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\n\n\nTarget\n3\nAlthough, as you will have seen, the dreaded ‘millennium bug’ failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n\n\nSource\n4\nSus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\n\n\nTarget\n4\nYou have requested a debate on this subject in the course of the next few days, during this part-session.\n\n\nSource\n5\nA la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\n\n\nTarget\n5\nIn the meantime, I should like to observe a minute’ s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n\n\nSource\n6\nInvito a todos a que nos pongamos de pie para guardar un minuto de silencio.\n\n\nTarget\n6\nPlease rise, then, for this minute’ s silence.\n\n\nSource\n7\n(El Parlamento, de pie, guarda un minuto de silencio)\n\n\nTarget\n7\n(The House rose and observed a minute’ s silence)\n\n\n\n\n\n\nsentence_id 1 appears to be title and sentence_id 7 reflects description of the parliamentary session. Both of these are artifacts that we would like to remove from the dataset.\nTo remove these lines we can turn to the programming strategies we’ve previously worked with. Namely we will use filter() to filter observations in combination with str_detect() to detect matches for some pattern that is indicative of these lines that we want to remove and not of the other lines that we want to keep.\nBefore we remove any lines, let’s try craft a search pattern to identify these lines, and exclude the lines we will want to keep. Condition one is lines which start with an opening parenthesis (. Condition two is lines that do not end in standard sentence punctuation (., !, or ?). I’ve added both conditions to one filter() using the logical OR operator (|) to ensure that either condition is matched in the output.\n\n# Identify non-speech lines\neuroparle %>% \n  filter(str_detect(sentence, \"^\\\\(\") | str_detect(sentence, \"[^.!?]$\")) %>% # filter lines that detect a match for either condition 1 or 2\n  slice_sample(n = 10) %>% # random sample of 10 observations\n  knitr::kable(booktabs = TRUE)\n\n\n\nTable 7.2: Non-speech lines in the Europarle dataset.\n\n\n\n\n\n\ntype\nsentence_id\nsentence\n\n\n\nTarget\n639\nAdoption of the Minutes of the previous sitting\n\n\nTarget\n673\nA5-0069/1999 by Mr von Wogau, on behalf of the Committee on Economic and Monetary Affairs, on the Commission White Paper on modernisation of the rules implementing Articles 85 and 86 of the EC Treaty [COM(1999) 101 - C5-0105/1999 - 1999/2108(COS)];\n\n\nSource\n674\nA5-0087/1999 del Sr. Jonckheer, en nombre de la Comisión de Asuntos Económicos y Monetarios, sobre el séptimo informe sobre ayudas estatales a la industria y a otros sectores en la Unión Europea (COM(1999) 148- C5-0107/1999 - 1999/2110(COS));\n\n\nSource\n294\nCoordinación Fondos estructurales/Fondo de Cohesión\n\n\nSource\n221\nTransporte de mercancías peligrosas por carretera\n\n\nTarget\n670\n(The Minutes were approved)\n\n\nSource\n134\nConsejeros de seguridad para el transporte de mercancías peligrosas\n\n\nTarget\n7\n(The House rose and observed a minute’ s silence)\n\n\nTarget\n675\nA5-0087/1999 by Mr Jonckheer, on behalf of the Committee on Economic and Monetary Affairs, on the seventh survey on state aid in the European Union in the manufacturing and certain other sectors. [COM(1999) 148 - C5-0107/1999 - 1999/2110(COS)] (Report 1995-1997);\n\n\nSource\n93\n(El Parlamento rechaza la petición) El Presidente.\n\n\n\n\n\n\nSince this search appears to match lines that we do not want to preserve, let’s move now to eliminate these lines from the dataset. To do this we will use the same regular expression patterns, but now each condition will have it’s own filter() call and the str_detect() will be negated with a prefixed !.\n\neuroparle <- \n  europarle %>% # dataset\n  filter(!str_detect(sentence, pattern = \"^\\\\(\")) %>% # remove lines starting with (\n  filter(!str_detect(sentence, pattern = \"[^.!?]$\")) # remove lines not ending in ., !, or ?\n\nLet’s look at the first 14 lines again, now that we have eliminated these artifacts.\n\n\n\n\nTable 7.3: Europarle Corpus non-speech lines removed.\n\n\n\n\n\n\ntype\nsentence_id\nsentence\n\n\n\nSource\n2\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\n\n\nTarget\n2\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n\n\nSource\n3\nComo todos han podido comprobar, el gran “efecto del año 2000” no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\n\n\nTarget\n3\nAlthough, as you will have seen, the dreaded ‘millennium bug’ failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n\n\nSource\n4\nSus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\n\n\nTarget\n4\nYou have requested a debate on this subject in the course of the next few days, during this part-session.\n\n\nSource\n5\nA la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\n\n\nTarget\n5\nIn the meantime, I should like to observe a minute’ s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n\n\nSource\n6\nInvito a todos a que nos pongamos de pie para guardar un minuto de silencio.\n\n\nTarget\n6\nPlease rise, then, for this minute’ s silence.\n\n\nSource\n8\nSeñora Presidenta, una cuestión de procedimiento.\n\n\nTarget\n8\nMadam President, on a point of order.\n\n\nSource\n9\nSabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en Sri Lanka.\n\n\nTarget\n9\nYou will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n\n\n\n\n\n\nOne further issue that we may want to resolve concerns the fact that there are whitespaces between possessive forms (i.e. “minute’ s silence”). In this case we can employ str_replace_all() inside the mutate() function to overwrite the sentence values that match an apostrophe ' with whitespace (\\\\s) before s.\n\neuroparle <- \n  europarle %>% # dataset\n  mutate(sentence = str_replace_all(string = sentence, \n                                    pattern = \"'\\\\ss\", \n                                    replacement = \"'s\")) # replace ' s with `s\n\nNow we have normalized text in the sentence column in the Europarle dataset.\nLast FM Lyrics\n\nLet’s look at another dataset we have worked with during this coursebook: the Lastfm lyrics. Reading in the lastfm_curated dataset from the data/derived/ directory we can see the structure for the curated structure.\n\nlastfm <- read_csv(file = \"../data/derived/lastfm/lastfm_curated.csv\") # read in lastfm_curated dataset\n\n\n\n\n\n\n\n\nTable 7.4: Last fm lyrics dataset preview with one artist/ song per genre and the lyrics text truncated at 200 characters for display purposes.\n\n\n\n\n\n\n\nartist\nsong\nlyrics\ngenre\n\n\n\nAlan Jackson\nLittle Bitty\nHave a little love on a little honeymoonYou got a little dish and you got a little spoonA little bitty house and a little bitty yardA little bitty dog and a little bitty car Well, it’s alright to b…\ncountry\n\n\n50 Cent\nIn Da Club\nGo, go, go, go, go, goGo, shortyIt’s your birthdayWe gon’ party like it’s your birthdayWe gon’ sip Bacardi like it’s your birthdayAnd you know we don’t give a fuck it’s not your birthday You can fi…\nhip-hop\n\n\nBlack Sabbath\nParanoid\nFinished with my woman’Cause she couldn’t help me with my mindPeople think I’m insaneBecause I am frowning all the time All day long, I think of thingsBut nothing seems to satisfyThink I’ll lose my…\nmetal\n\n\na-ha\nTake On Me\nTalking awayI don’t know whatWhat to sayI’ll say it anywayToday is another day to find youShying awayOh, I’ll be coming for your love, okay? Take On Me (Take On Me)Take me on (Take On Me)I’ll be go…\npop\n\n\n3 Doors Down\nHere Without You\nA hundred days have made me olderSince the last time that I saw your pretty faceA thousand lies have made me colderAnd I don’t think I can look at this the same But all the miles that separateDisap…\nrock\n\n\n\n\n\n\nThere are a few things that we might want to clean out of the lyrics column’s values. First, there are lines from the original webscrape where the end of one stanza runs into the next without whitespace between them (i.e. “honeymoonYou”). These reflect contiguous end-new line segments where stanzas were joined in the curation process. Second, we see that there are what appear to be backing vocals which appear between parentheses (i.e. “(Take On Me)”).\nIn both cases we will use mutate(). With contiguous end-new line segments we will use str_replace_all() inside and for backing vocals in parentheses we will use str_remove_all().\nThe pattern to match for end-new lines from the stanzas will use some regular expression magic. The base pattern includes finding a pair of lowercase-uppercase letters (i.e. “nY”, in “honeymoonYou”). For this we can use the pattern [a-z][A-Z]. To replace this pattern using the lowercase letter then a space and then the uppercase letter we take advantage of the grouping syntax in regular expressions (...). So we add parentheses around the two groups to capture like this ([a-z])([A-Z]). In the replacement argument of the str_replace_all() function we then specify to use the captured groups in the order they appear \\\\1 for the lowercase letter match and \\\\2 for the uppercase letter match.\nNow, I’ve looked more extensively at the lyrics column and found that there are other combinations that are joined between stanzas. Namely that ', !, ,, ), ?, and I also may precede the uppercase letter. To make sure we capture these possibilities as well I’ve updated the regular expression to ([a-z'!,.)?I])([A-Z]).\nNow to remove the backing vocals, the regex pattern is \\\\(.+?\\\\) –match the parentheses and everything within the parentheses. The added ? after the + operator is what is known as a ‘lazy’ operator. This specifies that the .+ will match the minimal string that is enclosed by the trailing ). If we did not include this then we would get matches that span from the first parenthesis ( all the way to the last, which would match real lyrics, not just the backing vocals.\nPutting this to work let’s clean the lyrics column.\n\nlastfm <- \n  lastfm %>% # dataset\n  mutate(lyrics = \n           str_replace_all(string = lyrics, \n                           pattern = \"([a-z'!,.)?I])([A-Z])\", # find contiguous end/ new line segments\n                           replacement = \"\\\\1 \\\\2\")) %>%  # replace with whitespace between\n  mutate(lyrics = str_remove_all(lyrics, \"\\\\(.+?\\\\)\")) # remove backing vocals (Take On Me)\n\n\n\n\n\nTable 7.5: Last fm lyrics with cleaned lyrics…\n\n\n\n\n\n\n\nartist\nsong\nlyrics\ngenre\n\n\n\nAlan Jackson\nLittle Bitty\nHave a little love on a little honeymoon You got a little dish and you got a little spoon A little bitty house and a little bitty yard A little bitty dog and a little bitty car Well, it’s alright t…\ncountry\n\n\n50 Cent\nIn Da Club\nGo, go, go, go, go, go Go, shorty It’s your birthday We gon’ party like it’s your birthday We gon’ sip Bacardi like it’s your birthday And you know we don’t give a fuck it’s not your birthday You c…\nhip-hop\n\n\nBlack Sabbath\nParanoid\nFinished with my woman’ Cause she couldn’t help me with my mind People think I’m insane Because I am frowning all the time All day long, I think of things But nothing seems to satisfy Think I’ll lo…\nmetal\n\n\na-ha\nTake On Me\nTalking away I don’t know what What to say I’ll say it anyway Today is another day to find you Shying away Oh, I’ll be coming for your love, okay? Take On Me Take me on I’ll be gone In a day or t…\npop\n\n\n3 Doors Down\nHere Without You\nA hundred days have made me older Since the last time that I saw your pretty face A thousand lies have made me colder And I don’t think I can look at this the same But all the miles that separate D…\nrock\n\n\n\n\n\n\nNow given the fact that songs are poems, there are many lines that are not complete sentences so there is no practical way to try to segment these into grammatical sentence units. So in this case, this seems like a good stopping point for normalizing the lastfm dataset."
  },
  {
    "objectID": "transform-datasets.html#recode",
    "href": "transform-datasets.html#recode",
    "title": "7  Transform datasets",
    "section": "\n7.2 Recode",
    "text": "7.2 Recode\n\nNormalizing text can be seen as an extension of dataset curation to some extent in that the structure of the dataset is maintained. In both the Europarle and Lastfm cases we saw this to be true. In the case of recoding, and other transformational steps, the aim will be to modify the dataset structure either by rows, columns, or both. Recoding processes can be characterized by the creation of structural changes which are derived from values in variables effectively recasting values as new variables to enable more direct access in our analyses.\nSwitchboard Dialogue Act Corpus\nThe Switchboard Dialogue Act Corpus dataset that was curated in the previous chapter contains a number of variables describing conversations between speakers of American English.\nLet’s read in this dataset and take a closer look.\n\nsdac <- read_csv(file = \"../data/derived/sdac/sdac_curated.csv\") # read curated dataset\n\n\n\n\nAmong a number of metadata variables, curated dataset includes the utterance_text column which contains dialogue from the conversations interleaved with a disfluency annotation scheme.\n\n\n\n\nTable 7.6: 20 randomly sampled lines of the SDAC curated dataset.\n\n\n\n\n\n\n\n\n\n\ndoc_id\ndamsl_tag\nspeaker\nturn_num\nutterance_num\nutterance_text\nspeaker_id\n\n\n\n2634\nqy\nB\n60\n1\nL A area? /\n1122\n\n\n2237\naa\nA\n239\n1\nYeah  /\n\n1043\n\n\n2552\naa\nA\n85\n1\n{D Well, } that’s right. /\n1148\n\n\n2227\nsd\nA\n121\n2\n{C And so } we just woke up /\n1167\n\n\n2445\nbf\nA\n81\n2\n{D Well, } {C so, } they evacuated it . /\n\n1214\n\n\n3994\nx\nA\n53\n1\n. /\n1512\n\n\n2305\n%\nB\n162\n1\nI, - /\n1141\n\n\n2185\nny\nB\n72\n1\nYeah. /\n1142\n\n\n2038\n%\nB\n35\n2\nyeah, /\n1039\n\n\n2896\n+\nA\n79\n1\n{C And } with rollers and, {F uh, } modern latex paints, /\n1132\n\n\n3624\nsd\nA\n63\n1\n{F Um, } {C and } it’s always a mystery, /\n1349\n\n\n2441\nsv\nA\n159\n1\n{D You know, } [ [ it’s just, + it’s, ] + it’s ] ridiculous the way unions have gone. /\n1151\n\n\n3751\nb\nA\n119\n1\n{F Oh } yeah. /\n1074\n\n\n3080\nb\nA\n43\n1\nUh-huh. /\n1284\n\n\n4366\nsd\nA\n9\n2\nHe has arthritis. /\n1614\n\n\n2154\nsd\nA\n79\n1\n[ [ It, +\n1002\n\n\n3362\nb\nA\n41\n1\nRight, /\n1426\n\n\n3473\n%\nA\n37\n1\n{D Well, } {F uh, } -/\n1378\n\n\n2476\nsd\nA\n79\n3\n{C because, } {F uh, } one of the area supervisors, {F uh, } in my area, as a matter of fact, took off, {F um, } for a couple of weeks, when his wife had their baby. /\n1018\n\n\n3682\nsd(^q)\nB\n23\n2\nPeople would come and say [ could y-, + would you ] come over and make lasagna for me . /\n\n1408\n\n\n\n\n\n\nLet’s drop a few variables from our dataset to rein in our focus. I will keep the doc_id, speaker_id, and utterance_text.\n\nsdac_simplified <- \n  sdac %>% # dataset\n  select(doc_id, speaker_id, utterance_text) # columns to retain\n\n\n\n\n\nTable 7.7: First 10 lines of the simplified SDAC curated dataset.\n\n\n\n\n\n\ndoc_id\nspeaker_id\nutterance_text\n\n\n\n4325\n1632\nOkay. /\n\n\n4325\n1632\n{D So, }\n\n\n4325\n1519\n[ [ I guess, +\n\n\n4325\n1632\nWhat kind of experience [ do you, + do you ] have, then with child care? /\n\n\n4325\n1519\nI think, ] + {F uh, } I wonder ] if that worked. /\n\n\n4325\n1632\nDoes it say something? /\n\n\n4325\n1519\nI think it usually does. /\n\n\n4325\n1519\nYou might try, {F uh, } /\n\n\n4325\n1519\nI don’t know, /\n\n\n4325\n1519\nhold it down a little longer, /\n\n\n\n\n\n\nIn this disfluency annotation system, there are various conventions used for non-sentence elements. If say, for example, a researcher were to be interested in understanding the use of filled pauses (‘uh’ or ‘uh’), the aim would be to identify those lines where the {F ...} annotation is used around the utterances ‘uh’ and ‘um’.\nTo do this we turn to the str_count() function. This function will count the number of matches found for a pattern. We can use a regular expression to identify the pattern of interest which is all the instances of {F followed by either uh or um. Since the disfluencies may start an utterance, and therefore be capitalized we need to formulate a regular expression which allows for either U or u for each disfluency type. The result from each disfluency match will be added to a new column. To create a new column we will wrap each str_count() with mutate() and give the new column a meaningful name. In this case I’ve opted for uh and um.\n\nsdac_disfluencies <- \n  sdac_simplified %>% # dataset\n  mutate(uh = str_count(utterance_text, \"\\\\{F [Uu]h\")) %>% # match {F Uh or {F uh}\n  mutate(um = str_count(utterance_text, \"\\\\{F [Uu]m\")) # match {F Um or {F um}\n\n\n\n\n\nTable 7.8: First 20 lines of SDAC dataset with counts for the disfluencies ‘uh’ and ‘um’.\n\n\n\n\n\n\n\n\ndoc_id\nspeaker_id\nutterance_text\nuh\num\n\n\n\n4325\n1632\nOkay. /\n0\n0\n\n\n4325\n1632\n{D So, }\n0\n0\n\n\n4325\n1519\n[ [ I guess, +\n0\n0\n\n\n4325\n1632\nWhat kind of experience [ do you, + do you ] have, then with child care? /\n0\n0\n\n\n4325\n1519\nI think, ] + {F uh, } I wonder ] if that worked. /\n1\n0\n\n\n4325\n1632\nDoes it say something? /\n0\n0\n\n\n4325\n1519\nI think it usually does. /\n0\n0\n\n\n4325\n1519\nYou might try, {F uh, } /\n1\n0\n\n\n4325\n1519\nI don’t know, /\n0\n0\n\n\n4325\n1519\nhold it down a little longer, /\n0\n0\n\n\n4325\n1519\n{C and } see if it, {F uh, } -/\n1\n0\n\n\n4325\n1632\nOkay . /\n\n0\n0\n\n\n4325\n1632\n<> {D Well, }\n\n0\n0\n\n\n4325\n1519\nOkay /\n0\n0\n\n\n4325\n1519\n[ I, +\n0\n0\n\n\n4325\n1632\nDoes it usually make a recording or s-, /\n0\n0\n\n\n4325\n1519\n{D Well, } I ] don’t remember. /\n0\n0\n\n\n4325\n1519\nIt seemed like it did, /\n0\n0\n\n\n4325\n1519\n{C but }  it might not. /\n\n0\n0\n\n\n4325\n1519\n[ I guess + –\n0\n0\n\n\n\n\n\n\nNow we have two new columns, uh and um which indicate how many times the relevant pattern was matched for a given utterance. By choosing to focus on disfluencies, however, we have made a decision to change the unit of observation from the utterance to the use of filled pauses (uh and um). This means that as the dataset stands, it is not in tidy format –where each observation corresponds to the observational unit. When datasets are misaligned in this particular way, there are in what is known as ‘wide’ format. What we want to do, then, is to restructure our dataset such that each row corresponds to the unit of observation –in this case each filled pause type.\nTo convert our current (wide) dataset to one where each filler type is listed and the counts are measured for each utterance we turn to the pivot_longer() function. This function creates two new columns, one in which the column names are listed and one for the values for each of the column names.\n\nsdac_disfluencies <- \n  sdac_disfluencies %>% # dataset\n  pivot_longer(cols = c(\"uh\", \"um\"), # columns to convert\n               names_to = \"filler\", # column for the column names (i.e. filler types)\n               values_to = \"count\") # column for the column values (i.e. counts)\n\n\n\n\n\nTable 7.9: First 20 lines of SDAC dataset with tidy format for fillers as the unit of observation.\n\n\n\n\n\n\n\n\ndoc_id\nspeaker_id\nutterance_text\nfiller\ncount\n\n\n\n4325\n1632\nOkay. /\nuh\n0\n\n\n4325\n1632\nOkay. /\num\n0\n\n\n4325\n1632\n{D So, }\nuh\n0\n\n\n4325\n1632\n{D So, }\num\n0\n\n\n4325\n1519\n[ [ I guess, +\nuh\n0\n\n\n4325\n1519\n[ [ I guess, +\num\n0\n\n\n4325\n1632\nWhat kind of experience [ do you, + do you ] have, then with child care? /\nuh\n0\n\n\n4325\n1632\nWhat kind of experience [ do you, + do you ] have, then with child care? /\num\n0\n\n\n4325\n1519\nI think, ] + {F uh, } I wonder ] if that worked. /\nuh\n1\n\n\n4325\n1519\nI think, ] + {F uh, } I wonder ] if that worked. /\num\n0\n\n\n4325\n1632\nDoes it say something? /\nuh\n0\n\n\n4325\n1632\nDoes it say something? /\num\n0\n\n\n4325\n1519\nI think it usually does. /\nuh\n0\n\n\n4325\n1519\nI think it usually does. /\num\n0\n\n\n4325\n1519\nYou might try, {F uh, } /\nuh\n1\n\n\n4325\n1519\nYou might try, {F uh, } /\num\n0\n\n\n4325\n1519\nI don’t know, /\nuh\n0\n\n\n4325\n1519\nI don’t know, /\num\n0\n\n\n4325\n1519\nhold it down a little longer, /\nuh\n0\n\n\n4325\n1519\nhold it down a little longer, /\num\n0\n\n\n\n\n\n\nLast fm\n\nIn the previous example, we used a matching approach to extract information embedded in one column of the dataset and recoded the dataset to maintain the fidelity between the particular unit of observation and the other metadata.\nAnother common approach for recoding datasets in text analysis projects involves recoding linguistic units as smaller units; a process known as tokenization.\nLet’s return to the lastfm object we normalized earlier in the chapter to see the various ways one can choose to tokenize linguistic information.\n\n\n\n\nTable 7.10: Last fm dataset with normalized lyrics.\n\n\n\n\n\n\n\nartist\nsong\nlyrics\ngenre\n\n\n\nAlan Jackson\nLittle Bitty\nHave a little love on a little honeymoon You got a little dish and you got a little spoon A little bitty house and a little bitty yard A little bitty dog and a little bitty car Well, it’s alright t…\ncountry\n\n\n50 Cent\nIn Da Club\nGo, go, go, go, go, go Go, shorty It’s your birthday We gon’ party like it’s your birthday We gon’ sip Bacardi like it’s your birthday And you know we don’t give a fuck it’s not your birthday You c…\nhip-hop\n\n\nBlack Sabbath\nParanoid\nFinished with my woman’ Cause she couldn’t help me with my mind People think I’m insane Because I am frowning all the time All day long, I think of things But nothing seems to satisfy Think I’ll lo…\nmetal\n\n\na-ha\nTake On Me\nTalking away I don’t know what What to say I’ll say it anyway Today is another day to find you Shying away Oh, I’ll be coming for your love, okay? Take On Me Take me on I’ll be gone In a day or t…\npop\n\n\n3 Doors Down\nHere Without You\nA hundred days have made me older Since the last time that I saw your pretty face A thousand lies have made me colder And I don’t think I can look at this the same But all the miles that separate D…\nrock\n\n\n\n\n\n\nIn the current lastfm dataset, the unit of observation is the lyrics for the entire artist, song, and genre combination. If, however, we would like to change the unit to say words, we would like each word used to appear on its own row, while still maintaining the other relevant attributes associated with each word.\nThe tidytext package includes a very useful function unnest_tokens() which allows us to tokenize some textual input into smaller linguistic units. The ‘unnest’ part of the the function name refers to the process of extracting the unit of interest while maintaining the other relevant attributes. Let’s see this in action.\n\nlastfm %>% # dataset\n  unnest_tokens(output = word, # column for tokenized output\n                input = lyrics, # input column\n                token = \"words\") %>% # tokenize unit type\n  slice_head(n = 10) %>%  # preview first 10 lines\n  kable(booktabs = TRUE)\n\n\n\nTable 7.11: First 10 observations for lastfm dataset tokenized by words.\n\nartist\nsong\ngenre\nword\n\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhave\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlove\n\n\nAlan Jackson\nLittle Bitty\ncountry\non\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhoneymoon\n\n\nAlan Jackson\nLittle Bitty\ncountry\nyou\n\n\nAlan Jackson\nLittle Bitty\ncountry\ngot\n\n\n\n\n\n\nWe can see from the output, each word appears on a separate line in the order of appearance in the input text (lyrics). Furthermore, the output is in tidy format as each of the words is still associated with the relevant attribute values (artist, song, and genre). By default the tokenized text output is lowercased and the original text input column is dropped. These can be overridden, however, if desired.\nIn addition to ‘words’, the unnest_tokens() function provides easy access to a number of common tokenized units including ‘characters’, ‘sentences’, and ‘paragraphs’.\n\nlastfm %>% # dataset\n  unnest_tokens(output = character, # column for tokenized output\n                input = lyrics, # input column\n                token = \"characters\") %>% # tokenize unit type\n  slice_head(n = 10) %>%  # preview first 10 lines\n  kable(booktabs = TRUE)\n\n\n\nTable 7.12: First 10 observations for lastfm dataset tokenized by characters.\n\nartist\nsong\ngenre\ncharacter\n\n\n\nAlan Jackson\nLittle Bitty\ncountry\nh\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\n\n\nAlan Jackson\nLittle Bitty\ncountry\nv\n\n\nAlan Jackson\nLittle Bitty\ncountry\ne\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\n\n\nAlan Jackson\nLittle Bitty\ncountry\nl\n\n\nAlan Jackson\nLittle Bitty\ncountry\ni\n\n\nAlan Jackson\nLittle Bitty\ncountry\nt\n\n\nAlan Jackson\nLittle Bitty\ncountry\nt\n\n\nAlan Jackson\nLittle Bitty\ncountry\nl\n\n\n\n\n\n\nThe other two built-in options ‘sentences’ and ‘paragraphs’ depend on punctuation and/ or line breaks to function, so in this particular dataset, these options will not work given the particular characteristics of the lyrics variable.\nThere are even other options which allow for the creation of sequences of linguistic units. Say we want to tokenize our lyrics into two-word sequences, we can specify the token as ‘ngrams’ and then add the argument n = 2 to reflect we want two-word sequences.\n\nlastfm %>% \n  unnest_tokens(output = bigram, # column for tokenized output\n                input = lyrics, # input column\n                token = \"ngrams\", # tokenize unit type\n                n = 2) %>%  # size of word sequences \n  slice_head(n = 10) %>%  # preview first 10 lines\n  kable(booktabs = TRUE)\n\n\n\nTable 7.13: First 10 observations for lastfm dataset tokenized by bigrams\n\nartist\nsong\ngenre\nbigram\n\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhave a\n\n\nAlan Jackson\nLittle Bitty\ncountry\na little\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle love\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlove on\n\n\nAlan Jackson\nLittle Bitty\ncountry\non a\n\n\nAlan Jackson\nLittle Bitty\ncountry\na little\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle honeymoon\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhoneymoon you\n\n\nAlan Jackson\nLittle Bitty\ncountry\nyou got\n\n\nAlan Jackson\nLittle Bitty\ncountry\ngot a\n\n\n\n\n\n\nThe ‘n’ in ‘ngram’ refers to the number of word-sequence units we want to tokenize. Two-word sequences are known as ‘bigrams’, three-word sequences ‘trigrams’, and so on."
  },
  {
    "objectID": "transform-datasets.html#generate",
    "href": "transform-datasets.html#generate",
    "title": "7  Transform datasets",
    "section": "\n7.3 Generate",
    "text": "7.3 Generate\nIn the process of recoding a dataset the transformation of the dataset works with information that is already explicit. The process of generation, however, aims to make implicit information explicit. The most common type of operation involved in the generation process is the addition of linguistic annotation. This process can be accomplished manually by a researcher or research team or automatically through the use of pre-trained linguistic resources and/ or software. Ideally the annotation of linguistic information can be conducted automatically.\nThere are important considerations, however, that need to be taken into account when choosing whether linguistic annotation can be conducted automatically. First and foremost has to do with the type of annotation desired. Information such as part of speech (grammatical category) and morpho-syntactic information are the the most common types of linguistic annotation that can be conducted automatically. Second the degree to which the resource that will be used to annotate the linguistic information is aligned with the language variety and/or register is also a key consideration. As noted, automatic linguistic annotation methods are contingent on pre-trained resources. The language and language variety used to develop these resources may not be available for the language under investigation, or if it does, the language variety and/ or register may not align. The degree to which a resource does not align with the linguistic information targeted for annotation is directly related to the quality of the final annotations. To be clear, no annotation method, whether manual or automatic is guaranteed to be perfectly accurate.\nLet’s take a look at annotation some of the language from the Europarle dataset we normalized.\n\neuroparle %>% \n  filter(type == \"Target\") %>% \n  slice_head(n = 10) %>% \n  kable(booktabs = TRUE)\n\n\n\nTable 7.14: First 10 lines in English from the normalized SDAC dataset.\n\n\n\n\n\n\ntype\nsentence_id\nsentence\n\n\n\nTarget\n2\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n\n\nTarget\n3\nAlthough, as you will have seen, the dreaded ‘millennium bug’ failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n\n\nTarget\n4\nYou have requested a debate on this subject in the course of the next few days, during this part-session.\n\n\nTarget\n5\nIn the meantime, I should like to observe a minute’s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n\n\nTarget\n6\nPlease rise, then, for this minute’s silence.\n\n\nTarget\n8\nMadam President, on a point of order.\n\n\nTarget\n9\nYou will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n\n\nTarget\n10\nOne of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\n\n\nTarget\n11\nWould it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament’s regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\n\n\nTarget\n12\nYes, Mr Evans, I feel an initiative of the type you have just suggested would be entirely appropriate.\n\n\n\n\n\n\nWe will use the cleanNLP package to do our linguistic annotation. The annotation process depends on the pre-trained language models. There is a list of the models available to access. The load_model_udpipe() custom function below downloads the specified language model and initialized the udpipe engine (cnlp_init_udpipe()) for conducting annotations.\n\nload_model_udpipe <- function(model_lang) {\n  # Function\n  # Download and load the specified udpipe language model\n  \n  cnlp_init_udpipe(model_lang) # to download the model, if not downloaded\nbase_path <- system.file(\"extdata\", package = \"cleanNLP\") # get the base path\n  model_name <- # extract the model_name\n    base_path %>% # extract the base path\n    dir() %>% # get the directory\n    stringr::str_subset(pattern = paste0(\"^\", model_lang)) # extract the name of the model\n  \n  udpipe::udpipe_load_model(file = file.path(base_path, model_name, fsep = \"/\")) %>% # create the path to the downloaded model stored on disk\n    return()\n}\n\nIn a test case, let’s load the ‘english’ model to annotate a sentence line from the Europarle dataset to illustrate the basic workflow.\n\neng_model <- load_model_udpipe(\"english\") # load and initialize the language model, 'english' in this case.\n\neng_annotation <- \n  europarle %>% # dataset \n  filter(type == \"Target\" & sentence_id == 6) %>% # select English and sentence_id 6\n  cnlp_annotate(text_name = \"sentence\", # input text (sentence)\n                doc_name = \"sentence_id\") # specify the grouping column (sentence_id)\n\nglimpse(eng_annotation) # preview structure\n\n#> List of 2\n#>  $ token   : tibble [11 × 11] (S3: tbl_df/tbl/data.frame)\n#>   ..$ doc_id       : num [1:11] 6 6 6 6 6 6 6 6 6 6 ...\n#>   ..$ sid          : int [1:11] 1 1 1 1 1 1 1 1 1 1 ...\n#>   ..$ tid          : chr [1:11] \"1\" \"2\" \"3\" \"4\" ...\n#>   ..$ token        : chr [1:11] \"Please\" \"rise\" \",\" \"then\" ...\n#>   ..$ token_with_ws: chr [1:11] \"Please \" \"rise\" \", \" \"then\" ...\n#>   ..$ lemma        : chr [1:11] \"please\" \"rise\" \",\" \"then\" ...\n#>   ..$ upos         : chr [1:11] \"INTJ\" \"VERB\" \"PUNCT\" \"ADV\" ...\n#>   ..$ xpos         : chr [1:11] \"UH\" \"VB\" \",\" \"RB\" ...\n#>   ..$ feats        : chr [1:11] NA \"Mood=Imp|VerbForm=Fin\" NA \"PronType=Dem\" ...\n#>   ..$ tid_source   : chr [1:11] \"2\" \"0\" \"2\" \"10\" ...\n#>   ..$ relation     : chr [1:11] \"discourse\" \"root\" \"punct\" \"advmod\" ...\n#>  $ document: tibble [1 × 2] (S3: tbl_df/tbl/data.frame)\n#>   ..$ type  : chr \"Target\"\n#>   ..$ doc_id: num 6\n#>  - attr(*, \"class\")= chr [1:2] \"cnlp_annotation\" \"list\"\n\n\nWe see that the structure returned by the cnlp_annotate() function is a list. This list contains two data frames (tibbles). One for the tokens (and there annotation information) and the document (the metadata information). We can inspect the annotation characteristics for this one sentence by targetting the $tokens data frame. Let’s take a look at the linguistic annotation information returned.\n\n\n\n\nTable 7.15: Annotation information for a single English sentence from the Europarle dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndoc_id\nsid\ntid\ntoken\ntoken_with_ws\nlemma\nupos\nxpos\nfeats\ntid_source\nrelation\n\n\n\n6\n1\n1\nPlease\nPlease\nplease\nINTJ\nUH\nNA\n2\ndiscourse\n\n\n6\n1\n2\nrise\nrise\nrise\nVERB\nVB\nMood=Imp|VerbForm=Fin\n0\nroot\n\n\n6\n1\n3\n,\n,\n,\nPUNCT\n,\nNA\n2\npunct\n\n\n6\n1\n4\nthen\nthen\nthen\nADV\nRB\nPronType=Dem\n10\nadvmod\n\n\n6\n1\n5\n,\n,\n,\nPUNCT\n,\nNA\n10\npunct\n\n\n6\n1\n6\nfor\nfor\nfor\nADP\nIN\nNA\n10\ncase\n\n\n6\n1\n7\nthis\nthis\nthis\nDET\nDT\nNumber=Sing|PronType=Dem\n8\ndet\n\n\n6\n1\n8\nminute\nminute\nminute\nNOUN\nNN\nNumber=Sing\n10\nnmod:poss\n\n\n6\n1\n9\n’s\n’s\n’s\nPART\nPOS\nNA\n8\ncase\n\n\n6\n1\n10\nsilence\nsilence\nsilence\nNOUN\nNN\nNumber=Sing\n2\nconj\n\n\n6\n1\n11\n.\n.\n.\nPUNCT\n.\nNA\n2\npunct\n\n\n\n\n\n\nThere is quite a bit of information which is returned from cnlp_annotate(). First note that the input sentence has been tokenized by word. Each token includes the token, lemma, part of speech (upos and xpos), morphological features (feats), and syntactic relationships (tid_source and relation). It is also key to note that the doc_id, sid and tid maintain the relational attributes from the original dataset –and therefore maintains our annotated dataset in tidy format.\nLet’s now annotate the same sentence from the Europarle corpus for the Source (‘Spanish’) and note the similarities and differences.\n\nspa_model <- load_model_udpipe(\"spanish\") # load and initialize the language model, 'spanish' in this case.\n\nspa_annotation <- \n  europarle %>% # dataset \n  filter(type == \"Source\" & sentence_id == 6) %>% # select Spanish and sentence_id 6\n  cnlp_annotate(text_name = \"sentence\", # input text (sentence)\n                doc_name = \"sentence_id\") # specify the grouping column (sentence_id)\n\n\n\n\n\nTable 7.16: Annotation information for a single Spanish sentence from the Europarle dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndoc_id\nsid\ntid\ntoken\ntoken_with_ws\nlemma\nupos\nxpos\nfeats\ntid_source\nrelation\n\n\n\n6\n1\n1\nInvito\nInvito\nInvito\nVERB\nNA\nGender=Masc|Number=Sing|VerbForm=Fin\n0\nroot\n\n\n6\n1\n2\na\na\na\nADP\nNA\nNA\n3\ncase\n\n\n6\n1\n3\ntodos\ntodos\ntodo\nPRON\nNA\nGender=Masc|Number=Plur|PronType=Tot\n1\nobj\n\n\n6\n1\n4\na\na\na\nADP\nNA\nNA\n7\nmark\n\n\n6\n1\n5\nque\nque\nque\nSCONJ\nNA\nNA\n4\nfixed\n\n\n6\n1\n6\nnos\nnos\nyo\nPRON\nNA\nCase=Acc,Dat|Number=Plur|Person=1|PrepCase=Npr|PronType=Prs|Reflex=Yes\n7\niobj\n\n\n6\n1\n7\npongamos\npongamos\npongar\nVERB\nNA\nMood=Ind|Number=Plur|Person=1|Tense=Pres|VerbForm=Fin\n1\nadvcl\n\n\n6\n1\n8\nde\nde\nde\nADP\nNA\nNA\n9\ncase\n\n\n6\n1\n9\npie\npie\npie\nNOUN\nNA\nGender=Masc|Number=Sing\n7\nobl\n\n\n6\n1\n10\npara\npara\npara\nADP\nNA\nNA\n11\nmark\n\n\n6\n1\n11\nguardar\nguardar\nguardar\nVERB\nNA\nVerbForm=Inf\n1\nadvcl\n\n\n6\n1\n12\nun\nun\nuno\nDET\nNA\nDefinite=Ind|Gender=Masc|Number=Sing|PronType=Art\n13\ndet\n\n\n6\n1\n13\nminuto\nminuto\nminuto\nNOUN\nNA\nGender=Masc|Number=Sing\n11\nobj\n\n\n6\n1\n14\nde\nde\nde\nADP\nNA\nNA\n15\ncase\n\n\n6\n1\n15\nsilencio\nsilencio\nsilencio\nNOUN\nNA\nGender=Masc|Number=Sing\n13\nnmod\n\n\n6\n1\n16\n.\n.\n.\nPUNCT\nNA\nNA\n1\npunct\n\n\n\n\n\n\nFor the Spanish version of this sentence, we see the same variables. However, the feats variable has morphological information which is specific to Spanish –notably gender and mood.\n\n\n\n\n\n\nTip\n\n\n\nThe rsyntax package (Welbers and van Atteveldt 2022) can be used to recode and extract patterns from the output from automatic linguistic annotations using cleanNLP. See the documentation for more information."
  },
  {
    "objectID": "transform-datasets.html#merge",
    "href": "transform-datasets.html#merge",
    "title": "7  Transform datasets",
    "section": "\n7.4 Merge",
    "text": "7.4 Merge\n\nOne final class of transformations that can be applied to curated datasets to enhance their informativeness for a research project is the process of merging two or more datasets. To merge datasets it is required that the datasets share one or more attributes. With a common attribute two datasets can be joined to coordinate the attributes of one dataset with the other effectively adding attributes and one dataset with extended information. Another approach is to join datasets with the goal of filtering one of the datasets given the matching attribute.\nLet’s see this in practice. Take the lastfm dataset. Let’s tokenize the dataset into words, using unnest_tokens() such that our unit of observation is words.\n\nlastfm_words <- \n  lastfm %>% # dataset\n  unnest_tokens(output = \"word\", # output column\n                input = \"lyrics\", # input column\n                token = \"words\") # tokenized unit (words)\n\nlastfm_words %>% # dataset\n  slice_head(n = 10) %>% # first 10 observations\n  kable(booktabs = TRUE)\n\n\n\nTable 7.17: First 10 observations for lastfm_words dataset.\n\nartist\nsong\ngenre\nword\n\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhave\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlove\n\n\nAlan Jackson\nLittle Bitty\ncountry\non\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhoneymoon\n\n\nAlan Jackson\nLittle Bitty\ncountry\nyou\n\n\nAlan Jackson\nLittle Bitty\ncountry\ngot\n\n\n\n\n\n\nConsider the get_sentiments() function which returns words which have been classified as ‘positive’- or ‘negative’-biased, if the lexicon is set to ‘bing’ (Hu and Liu 2004).\n\nsentiments_bing <- \n  tidytext::get_sentiments(lexicon = \"bing\") # get 'bing' lexicon from get_sentiments\n\nsentiments_bing %>% \n  slice_head(n = 10) # preview first 10 observations\n\n#> # A tibble: 10 × 2\n#>    word        sentiment\n#>    <chr>       <chr>    \n#>  1 2-faces     negative \n#>  2 abnormal    negative \n#>  3 abolish     negative \n#>  4 abominable  negative \n#>  5 abominably  negative \n#>  6 abominate   negative \n#>  7 abomination negative \n#>  8 abort       negative \n#>  9 aborted     negative \n#> 10 aborts      negative\n\n\nSince the sentiments_bing dataset and the lastfm_words dataset both share a column word (which has the same type of values) we can join these two datasets. The sentiments_bing dataset has 6786 unique words. Let’s check how many distinct words our lastfm_words dataset has.\n\nlastfm_words %>% # dataset\n  distinct(word) %>% # find unique words\n  nrow() # count distinct rows/ words\n\n#> [1] 4614\n\n\nOne thing to note is that the sentiments_bing dataset does not include function words, that is words that are associated with closed-class categories (pronouns, determiners, prepositions, etc.) as these words do not have semantic content along the lines of positive and negative. So many of the words that appear in the lastfm_words will not be matched. Other thing to note is that the sentiments_bing lexicon will undoubtly have words that do not appear in the lastfm_words and vice versa.\nIf we want to keep all the words in the lastfm_words and add the sentiment information for those words that do match in both datasets, we can use the left_join() function. lastfm_words will be the dataset on the ‘left’ and therefore all rows in this dataset will be retained.\n\nleft_join(lastfm_words, sentiments_bing) %>% \n  slice_head(n = 10) %>% # first 10 observations\n  kable(booktabs = TRUE)\n\n\n\nTable 7.18: First 10 observations for the lastfm_words sentiments_bing` left join.\n\nartist\nsong\ngenre\nword\nsentiment\n\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhave\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlove\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\non\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhoneymoon\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\nyou\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\ngot\nNA\n\n\n\n\n\n\nSo we see that quite a few of the words from lastfm_words are not matched. To focus in on those words in lastfm_words that do match, we’ll run the same join operation and filter for rows where sentiment is not empty (i.e. that there is a match in the sentiments_bing lexicon).\n\nleft_join(lastfm_words, sentiments_bing) %>%\n  filter(sentiment != \"\") %>% # return matched sentiments\n  slice_head(n = 10) %>% # first 10 observations\n  kable(booktabs = TRUE)\n\n\nFirst 10 observations for the lastfm_words sentiments_bing` left join. \n\nartist\nsong\ngenre\nword\nsentiment\n\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlove\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nwell\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nwell\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nwell\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nsmile\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nwell\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nwell\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nwell\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nsmile\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\ngood\npositive\n\n\n\n\n\nLet’s turn to another type of join: an anti-join. The purpose of an anti-join is to eliminate matches. This makes sense for a quick and dirty approach to removing function words (i.e. those grammatical words with little semantic content). In this case we use the get_stopwords() function to get the dataset. We’ll specify English as the language and we’ll use the default lexicon (‘Snowball’).\n\nenglish_stopwords <- \n  get_stopwords(language = \"en\") # get English stopwords from the Snowball lexicon\n\nenglish_stopwords %>% \n  slice_head(n = 10) # preview first 10 observations\n\n#> # A tibble: 10 × 2\n#>    word      lexicon \n#>    <chr>     <chr>   \n#>  1 i         snowball\n#>  2 me        snowball\n#>  3 my        snowball\n#>  4 myself    snowball\n#>  5 we        snowball\n#>  6 our       snowball\n#>  7 ours      snowball\n#>  8 ourselves snowball\n#>  9 you       snowball\n#> 10 your      snowball\n\n\nNow if we want to eliminate stopwords from our lastfm_words dataset we use anti_join(). All the observations in the lastfm_words where there is not a match in english_stopwords will be returned.\n\nanti_join(lastfm_words, english_stopwords) %>% \n  slice_head(n = 10) %>% \n  kable(booktabs = TRUE)\n\n\n\nTable 7.19: First 10 observations in lastfm_words after filtering for English stopwords.\n\nartist\nsong\ngenre\nword\n\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlove\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhoneymoon\n\n\nAlan Jackson\nLittle Bitty\ncountry\ngot\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\ndish\n\n\nAlan Jackson\nLittle Bitty\ncountry\ngot\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\nspoon\n\n\n\n\n\n\nWe can also merge datasets that we generate in our analysis or that we import from other sources. This can be useful when there are cases in which a corpus has associated metadata that is contained in files separate from the corpus itself. This is the case for the Switchboard Dialogue Act Corpus.\nOur existing, disfluency recoded, version includes the following variables.\n\nsdac_disfluencies %>% # dataset\n  slice_head(n = 10) # preview first 10 observations\n\n#> # A tibble: 10 × 5\n#>    doc_id speaker_id utterance_text                                 filler count\n#>     <dbl>      <dbl> <chr>                                          <chr>  <int>\n#>  1   4325       1632 Okay.  /                                       uh         0\n#>  2   4325       1632 Okay.  /                                       um         0\n#>  3   4325       1632 {D So, }                                       uh         0\n#>  4   4325       1632 {D So, }                                       um         0\n#>  5   4325       1519 [ [ I guess, +                                 uh         0\n#>  6   4325       1519 [ [ I guess, +                                 um         0\n#>  7   4325       1632 What kind of experience [ do you, + do you ] … uh         0\n#>  8   4325       1632 What kind of experience [ do you, + do you ] … um         0\n#>  9   4325       1519 I think, ] + {F uh, } I wonder ] if that work… uh         1\n#> 10   4325       1519 I think, ] + {F uh, } I wonder ] if that work… um         0\n\n\nThe online documentation page provides a key file caller_tab.csv which contains speaker metadata information. Included in this .csv file is a column caller_no which contains the speaker_id we currently have in the sdac_disfluencies dataset. Let’s read this file into our R session renaming caller_no to speaker_id to prepare to join these datasets.\n\nsdac_speaker_meta <- \n  read_csv(file = \"https://catalog.ldc.upenn.edu/docs/LDC97S62/caller_tab.csv\", \n           col_names = c(\"speaker_id\", # changed from `caller_no`\n                         \"pin\",\n                         \"target\",\n                         \"sex\",\n                         \"birth_year\",\n                         \"dialect_area\",\n                         \"education\",\n                         \"ti\",\n                         \"payment_type\",\n                         \"amt_pd\",\n                         \"con\",\n                         \"remarks\",\n                         \"calls_deleted\",\n                         \"speaker_partition\"))\n\nglimpse(sdac_speaker_meta)\n\n#> Rows: 543\n#> Columns: 14\n#> $ speaker_id        <dbl> 1000, 1001, 1002, 1003, 1004, 1005, 1007, 1008, 1010…\n#> $ pin               <dbl> 32, 102, 104, 5656, 123, 166, 274, 322, 445, 461, 57…\n#> $ target            <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y…\n#> $ sex               <chr> \"FEMALE\", \"MALE\", \"FEMALE\", \"MALE\", \"FEMALE\", \"FEMAL…\n#> $ birth_year        <dbl> 1954, 1940, 1963, 1947, 1958, 1956, 1965, 1939, 1932…\n#> $ dialect_area      <chr> \"SOUTH MIDLAND\", \"WESTERN\", \"SOUTHERN\", \"NORTH MIDLA…\n#> $ education         <dbl> 1, 3, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 3, 3, 2, 3…\n#> $ ti                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ payment_type      <chr> \"CASH\", \"GIFT\", \"GIFT\", \"NONE\", \"GIFT\", \"GIFT\", \"CAS…\n#> $ amt_pd            <dbl> 15, 10, 11, 7, 11, 22, 20, 3, 11, 9, 25, 9, 1, 16, 1…\n#> $ con               <chr> \"N\", \"N\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"N\", \"N…\n#> $ remarks           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ calls_deleted     <dbl> 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0…\n#> $ speaker_partition <chr> \"DN2\", \"XP\", \"XP\", \"DN2\", \"XP\", \"ET\", \"DN1\", \"DN1\", …\n\n\nNow to join the sdac_disfluencies and sdac_speaker_meta. Let’s turn to left_join() again as we want to retain all the observations (rows) from sdac_disfluencies and add the columns for sdac_speaker_meta where the speaker_id column values match.\n\nsdac_disfluencies <- \n  left_join(sdac_disfluencies, sdac_speaker_meta) # join by ``speaker_id`\n\nglimpse(sdac_disfluencies)\n\n#> Rows: 447,212\n#> Columns: 18\n#> $ doc_id            <dbl> 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325…\n#> $ speaker_id        <dbl> 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519…\n#> $ utterance_text    <chr> \"Okay.  /\", \"Okay.  /\", \"{D So, }\", \"{D So, }\", \"[ […\n#> $ filler            <chr> \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\"…\n#> $ count             <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n#> $ pin               <dbl> 7713, 7713, 7713, 7713, 775, 775, 7713, 7713, 775, 7…\n#> $ target            <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n#> $ sex               <chr> \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"F…\n#> $ birth_year        <dbl> 1962, 1962, 1962, 1962, 1971, 1971, 1962, 1962, 1971…\n#> $ dialect_area      <chr> \"WESTERN\", \"WESTERN\", \"WESTERN\", \"WESTERN\", \"SOUTH M…\n#> $ education         <dbl> 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1…\n#> $ ti                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ payment_type      <chr> \"CASH\", \"CASH\", \"CASH\", \"CASH\", \"CASH\", \"CASH\", \"CAS…\n#> $ amt_pd            <dbl> 10, 10, 10, 10, 4, 4, 10, 10, 4, 4, 10, 10, 4, 4, 4,…\n#> $ con               <chr> \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y…\n#> $ remarks           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#> $ calls_deleted     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ speaker_partition <chr> \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UN…\n\n\nNow there are some metadata columns we may want to keep and others we may want to drop as they may not be of importance for our analysis. I’m going to assume that we want to keep sex, birth_year, dialect_area, and education and drop the rest.\n\nsdac_disfluencies <- \n  sdac_disfluencies %>% # dataset\n  select(doc_id:count, sex:education) # subset key columns\n\n\n\n\n\nTable 7.20: First 10 observations for the sdac_disfluencies dataset with speaker metadata.\n\n\n\n\n\n\n\n\n\n\n\n\ndoc_id\nspeaker_id\nutterance_text\nfiller\ncount\nsex\nbirth_year\ndialect_area\neducation\n\n\n\n4325\n1632\nOkay. /\nuh\n0\nFEMALE\n1962\nWESTERN\n2\n\n\n4325\n1632\nOkay. /\num\n0\nFEMALE\n1962\nWESTERN\n2\n\n\n4325\n1632\n{D So, }\nuh\n0\nFEMALE\n1962\nWESTERN\n2\n\n\n4325\n1632\n{D So, }\num\n0\nFEMALE\n1962\nWESTERN\n2\n\n\n4325\n1519\n[ [ I guess, +\nuh\n0\nFEMALE\n1971\nSOUTH MIDLAND\n1\n\n\n4325\n1519\n[ [ I guess, +\num\n0\nFEMALE\n1971\nSOUTH MIDLAND\n1\n\n\n4325\n1632\nWhat kind of experience [ do you, + do you ] have, then with child care? /\nuh\n0\nFEMALE\n1962\nWESTERN\n2\n\n\n4325\n1632\nWhat kind of experience [ do you, + do you ] have, then with child care? /\num\n0\nFEMALE\n1962\nWESTERN\n2\n\n\n4325\n1519\nI think, ] + {F uh, } I wonder ] if that worked. /\nuh\n1\nFEMALE\n1971\nSOUTH MIDLAND\n1\n\n\n4325\n1519\nI think, ] + {F uh, } I wonder ] if that worked. /\num\n0\nFEMALE\n1971\nSOUTH MIDLAND\n1"
  },
  {
    "objectID": "transform-datasets.html#documentation",
    "href": "transform-datasets.html#documentation",
    "title": "7  Transform datasets",
    "section": "\n7.5 Documentation",
    "text": "7.5 Documentation\nDocumentation of the transformed dataset is just as important as the curated dataset. Therefore we use the same process as covered in the previous chapter. First we write the transformed dataset to disk and then we work to provide a data dictionary for this dataset. I’ve included the data_dic_starter() custom function to apply to our dataset(s).\n\ndata_dic_starter <- function(data, file_path) {\n  # Function:\n  # Creates a .csv file with the basic information\n  # to document a curated dataset\n  \n  tibble(variable_name = names(data), # column with existing variable names \n       name = \"\", # column for human-readable names\n       description = \"\") %>% # column for prose description\n  write_csv(file = file_path) # write to disk\n}\n\nLet’s apply our function to the sdac_disfluencies dataset using the R console (not part of our project script to avoid overwriting our documentation!).\n\ndata_dic_starter(data = sdac_disfluencies, file_path = \"../data/derived/sdac/sdac_disfluencies_data_dictionary.csv\")\n\ndata/derived/\n└── sdac/\n    ├── data_dictionary_sdac.csv\n    ├── sdac_curated.csv\n    ├── sdac_disfluencies.csv\n    └── sdac_disfluencies_data_dictionary.csv\nOpen the data_dictionary_sdac_disfluencies.csv file in spreadsheet software and add the relevant description of the dataset."
  },
  {
    "objectID": "transform-datasets.html#summary",
    "href": "transform-datasets.html#summary",
    "title": "7  Transform datasets",
    "section": "Summary",
    "text": "Summary\nIn this chapter we covered the process of transforming datasets. The goal is to manipulate the curated dataset to make it align better for analysis. There are four general types of transformation steps: normalization, recoding, generation, and merging. In any given research project some or all of these steps will be employed –but not necessarily in the order presented in this chapter. Furthermore there may also be various datasets generated in at this stage each with a distinct analysis focus in mind. In any case it is important to write these datasets to disk and to document them according to the principles that we have established in the previous chapter.\nThis chapter concludes the section on data/ dataset preparation. The next section we turn to analyzing datasets. This is the stage where we interrogate the datasets to derive knowledge and insight either through inference, prediction, and/ or exploratory methods."
  },
  {
    "objectID": "transform-datasets.html#activities",
    "href": "transform-datasets.html#activities",
    "title": "7  Transform datasets",
    "section": "Activities",
    "text": "Activities\n\n\n\n\n\n\nRecipe\n\n\n\nWhat: Dataset manipulation: tokenization and joining datasetsHow: Read Recipe 8 and participate in the Hypothes.is online social annotation.Why: To work with to primary types of transformations, tokenization and joins. Tokenization is the process of recasting textual units as smaller textual units. The process of joining datasets aims to incorporate other datasets to augment or filter the dataset of interest.\n\n\n\n\n\n\n\n\nLab\n\n\n\nWhat: Dataset manipulation: tokenization and joining datasetsHow: Clone, fork, and complete the steps in Lab 8.Why: To gain experience working with coding strategies for transforming datasets using tidyverse functions and regular expressions, practice reading/ writing data from/ to disk, and implement organizational strategies for organizing and documenting a dataset in reproducible fashion."
  },
  {
    "objectID": "transform-datasets.html#questions",
    "href": "transform-datasets.html#questions",
    "title": "7  Transform datasets",
    "section": "Questions",
    "text": "Questions\n\n…\n…\n\n\n\n\n\nHu, Minqing, and Bing Liu. 2004. “Mining and Summarizing Customer Reviews.” In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 168–77.\n\n\nWelbers, Kasper, and Wouter van Atteveldt. 2022. Rsyntax: Extract Semantic Relations from Text by Querying and Reshaping Syntax. https://CRAN.R-project.org/package=rsyntax."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "In this section we turn to the analysis of datasets, the evaluation of results, and the interpretation of the findings. We will outline the three main types of statistical analyses: Inferential Data Analysis (IDA), Predictive Data Analysis (PDA), and Exploratory Data Analysis (EDA). Each of these analysis types have distinct, non-overlapping aims and therefore should be determined from the outset of the research project and included as part of the research blueprint. The aim of this section is to establish a clearer picture of the goals, methods, and value of each of these approaches."
  },
  {
    "objectID": "inference.html#preparation",
    "href": "inference.html#preparation",
    "title": "8  Inference",
    "section": "\n8.1 Preparation",
    "text": "8.1 Preparation\nAt this point let’s now get familiar with the datasets and prepare them for analysis. The first dataset to consider is the dative dataset. This dataset can be loaded from the languageR package (R. H. Baayen and Shafaei-Bajestan 2019).\n\ndative <- \n  languageR::dative %>% # load the `dative` dataset  \n  as_tibble() # convert the data frame to a tibble object\n  \nglimpse(dative) # preview structure \n\n#> Rows: 3,263\n#> Columns: 15\n#> $ Speaker                <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ Modality               <fct> written, written, written, written, written, wr…\n#> $ Verb                   <fct> feed, give, give, give, offer, give, pay, bring…\n#> $ SemanticClass          <fct> t, a, a, a, c, a, t, a, a, a, a, a, t, a, c, a,…\n#> $ LengthOfRecipient      <int> 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1,…\n#> $ AnimacyOfRec           <fct> animate, animate, animate, animate, animate, an…\n#> $ DefinOfRec             <fct> definite, definite, definite, definite, definit…\n#> $ PronomOfRec            <fct> pronominal, nonpronominal, nonpronominal, prono…\n#> $ LengthOfTheme          <int> 14, 3, 13, 5, 3, 4, 4, 1, 11, 2, 3, 3, 5, 2, 8,…\n#> $ AnimacyOfTheme         <fct> inanimate, inanimate, inanimate, inanimate, ina…\n#> $ DefinOfTheme           <fct> indefinite, indefinite, definite, indefinite, d…\n#> $ PronomOfTheme          <fct> nonpronominal, nonpronominal, nonpronominal, no…\n#> $ RealizationOfRecipient <fct> NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP,…\n#> $ AccessOfRec            <fct> given, given, given, given, given, given, given…\n#> $ AccessOfTheme          <fct> new, new, new, new, new, new, new, new, accessi…\n\n\nFrom glimpse() we can see that this dataset contains 3,263 observations and 15 columns.\nThe R Documentation can be consulted using ?dative in the R Console. The description states:\n\nData describing the realization of the dative as NP or PP in the Switchboard corpus and the Treebank Wall Street Journal collection.\n\nFor a bit more context, a dative is the phrase which reflects the entity that takes the recipient role in a ditransitive clause. In English, the recipient (dative) can be realized as either a noun phrase (NP) as seen in (1) or as a prepositional phrase (PP) as seen in (2) below.\n\nThey give [you NP] a drug test.\nThey give a drug test [to you PP].\n\nTogether these two syntactic options are known as the Dative Alternation.\nThe observational unit for this dataset is RealizationOfRecipient variable which is either ‘NP’ or ‘PP’. For the purposes of this chapter I will select a subset of the key variables we will use in the upcoming analyses and drop the others.\n\ndative <- \n  dative %>% # dataset\n  select(RealizationOfRecipient, Modality, LengthOfRecipient, LengthOfTheme) %>% # select key variables\n  janitor::clean_names() # normalize variable names\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\n\n\n\n\nrealization_of_recipient\nmodality\nlength_of_recipient\nlength_of_theme\n\n\n\nNP\nwritten\n1\n14\n\n\nNP\nwritten\n2\n3\n\n\nNP\nwritten\n1\n13\n\n\nNP\nwritten\n1\n5\n\n\nNP\nwritten\n2\n3\n\n\nNP\nwritten\n2\n4\n\n\nNP\nwritten\n2\n4\n\n\nNP\nwritten\n1\n1\n\n\nNP\nwritten\n1\n11\n\n\nNP\nwritten\n1\n2\n\n\n\n\n\nIn Table 8.1 I’ve created a data dictionary describing the variables in our new dative dataset based on the variable descriptions in the languageR::dative documentation.\n\n\n\n\nTable 8.1: Data dictionary for the dative dataset.\n\n\n\n\n\n\nvariable_name\nname\ndescription\n\n\n\nrealization_of_recipient\nRealization of Recipient\nA factor with levels NP and PP coding the realization of the dative.\n\n\nmodality\nLanguage Modality\nA factor with levels spoken, written.\n\n\nlength_of_recipient\nLength of Recipient\nA numeric vector coding the number of words comprising the recipient.\n\n\nlength_of_theme\nLength of Theme\nA numeric vector coding the number of words comprising the theme.\n\n\n\n\n\n\nThe second dataset that we will use in this chapter is the sdac_disfluencies dataset that we worked to derived in the previous chapter. Let’s read in the dataset and preview the structure.\n\nsdac_disfluencies <- \n  read_csv(file = \"../data/derived/sdac/sdac_disfluencies.csv\") # read transformed dataset\n\nglimpse(sdac_disfluencies) # preview structure\n\n\n\n#> Rows: 447,212\n#> Columns: 9\n#> $ doc_id         <dbl> 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4…\n#> $ speaker_id     <dbl> 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519, 1…\n#> $ utterance_text <chr> \"Okay.  /\", \"Okay.  /\", \"{D So, }\", \"{D So, }\", \"[ [ I …\n#> $ filler         <chr> \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"…\n#> $ count          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0…\n#> $ sex            <chr> \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMA…\n#> $ birth_year     <dbl> 1962, 1962, 1962, 1962, 1971, 1971, 1962, 1962, 1971, 1…\n#> $ dialect_area   <chr> \"WESTERN\", \"WESTERN\", \"WESTERN\", \"WESTERN\", \"SOUTH MIDL…\n#> $ education      <dbl> 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1…\n\n\nWe prepared a data dictionary that reflects this transformed dataset. Let’s read that file and then view it in Table 8.2.\n\nsdac_disfluencies_dictionary <- read_csv(file = \"../data/derived/sdac/sdac_disfluencies_data_dictionary.csv\") # read data dictionary\n\n\n\n\n\n\n\n\nTable 8.2: Data dictionary for the sdac_disfluencies dataset.\n\n\n\n\n\n\nvariable_name\nname\ndescription\n\n\n\ndoc_id\nDocument ID\nUnique identifier for each conversation file.\n\n\nspeaker_id\nSpeaker ID\nUnique identifier for each speaker in the corpus.\n\n\nutterance_text\nUtterance Text\nTranscribed utterances for each conversation. Includes disfluency annotation tags.\n\n\nfiller\nFiller\nFiller type either uh or um.\n\n\ncount\nCount\nNumber of fillers for each utterance.\n\n\nsex\nSex\nSex for each speaker either male or female.\n\n\nbirth_year\nBirth Year\nThe year each speaker was born.\n\n\ndialect_area\nDialect Area\nRegion from the US where the speaker spent first 10 years.\n\n\neducation\nEducation\nHighest educational level attained: values 0, 1, 2, 3, and 9.\n\n\n\n\n\n\nFor our analysis purposes we will reduce this dataset, as we did for the dative dataset, retaining only the variables of interest for the upcoming analyses.\n\nsdac_disfluencies <- \n  sdac_disfluencies %>% # dataset\n  select(speaker_id, filler, count, sex, birth_year, education) # select key variables\n\nLet’s preview this simplified sdac_disfluencies dataset.\n\n\n\n\nTable 8.3: First 10 observations of simplified sdac_disfluencies dataset.\n\nspeaker_id\nfiller\ncount\nsex\nbirth_year\neducation\n\n\n\n1632\nuh\n0\nFEMALE\n1962\n2\n\n\n1632\num\n0\nFEMALE\n1962\n2\n\n\n1632\nuh\n0\nFEMALE\n1962\n2\n\n\n1632\num\n0\nFEMALE\n1962\n2\n\n\n1519\nuh\n0\nFEMALE\n1971\n1\n\n\n1519\num\n0\nFEMALE\n1971\n1\n\n\n1632\nuh\n0\nFEMALE\n1962\n2\n\n\n1632\num\n0\nFEMALE\n1962\n2\n\n\n1519\nuh\n1\nFEMALE\n1971\n1\n\n\n1519\num\n0\nFEMALE\n1971\n1\n\n\n\n\n\n\nNow the sdac_disfluencies dataset needs some extra transformation to better prepare it for statistical interrogation. On the one hand the variables birth_year and education are not maximally informative. First it would be more ideal if birth_year would reflect the age of the speaker at the time of the conversation(s) and furthermore the coded values of education are not explicit as far what the numeric values refer to.\nThe second issue has to do with preparing the sdac_disfluencies dataset for statistical analysis. This involves converting our column types to the correct vector types for statistical methods. Specifically we need to convert our categorical variables to the R type ‘factor’ (fct). This includes of our current variables which are character vectors, but also the speaker_id and education which appear as numeric but do not reflect a continuous variables; one is merely a code which uniquely labels each speaker and the other is an ordinal list of educational levels.\nThis will be a three step process, first we will normalize the birth_year to reflect the age of the speaker, second we will convert all the relevant categorical variables to factors, and third we will convert the education variable to a factor adding meaningful labels for the levels of this factor.\nConsulting the online manual for this corpus, we see that the recording date for these conversations took place in 1992, so we can simply subtract the birth_year from 1992 to get each participant’s age. We’ll rename this new column age and drop the birth_year column.\n\nsdac_disfluencies <- \n  sdac_disfluencies %>% # dataset\n  mutate(age = (1992 - birth_year)) %>% # calculate age\n  select(-birth_year) # drop `birth_year` column\n\nNow let’s convert all the variables which are character vectors. We can do this using the the factor() function; first on speaker_id and then, with the help of mutate_if(), to all the other variables which are character vectors.\n\nsdac_disfluencies <- \n  sdac_disfluencies %>% # dataset\n  mutate(speaker_id = factor(speaker_id)) %>% # convert numeric to factor\n  mutate_if(is.character, factor) # convert all character to factor\n\nWe know from the data dictionary that the education column contains four values (0, 1, 2, 3, and 9). Again, consulting the corpus manual we can see what these values mean.\nEDUCATION    COUNT\n--------------------\n\n0            14      less than high school\n1            39      less than college\n2            309     college\n3            176     more than college\n9            4       unknown\nSo let’s convert education to a factor adding these descriptions as factor level labels. The function factor() can take an argument labels = which we can manually assign the label names for the factor levels in the order of the factor levels. Since the original values were numeric, the factor level ordering defaults to ascending order.\n\nsdac_disfluencies <- \n  sdac_disfluencies %>% # dataset\n  mutate(education = factor(education, \n                            labels = c(\"less than high school\", # value 0\n                                       \"less than college\", # value 1\n                                       \"college\", # value 2\n                                       \"more than college\", # value 3 \n                                       \"unknown\"))) # value 9\n\nSo let’s take a look at the sdac_disfluencies dataset we’ve prepared for analysis.\n\nglimpse(sdac_disfluencies)\n\n#> Rows: 447,212\n#> Columns: 6\n#> $ speaker_id <fct> 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519, 1519,…\n#> $ filler     <fct> uh, um, uh, um, uh, um, uh, um, uh, um, uh, um, uh, um, uh,…\n#> $ count      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n#> $ sex        <fct> FEMALE, FEMALE, FEMALE, FEMALE, FEMALE, FEMALE, FEMALE, FEM…\n#> $ education  <fct> college, college, college, college, less than college, less…\n#> $ age        <dbl> 30, 30, 30, 30, 21, 21, 30, 30, 21, 21, 30, 30, 21, 21, 21,…\n\n\nNow the datasets dative and sdac_disfluencies are ready to be statistically interrogated."
  },
  {
    "objectID": "inference.html#univariate-analysis",
    "href": "inference.html#univariate-analysis",
    "title": "8  Inference",
    "section": "\n8.2 Univariate analysis",
    "text": "8.2 Univariate analysis\nIn what follows I will provide a description of inferential data analysis when only one variable is to be interrogated. This is known as a univariate analysis, or one-variable analysis. We will consider a case when the variable is categorical and the other continuous.\n\n8.2.1 Categorical\nAs an example of a univariate analysis where the variable used in the analysis is categorical we will look at the dative dataset. In this analysis we may be interested in knowing whether the recipient role in a ditransitive construction is realized more as an ‘NP’ or ‘PP’.\nDescriptive assessment\nThe realization_of_recipient variable contains the relevant information. Let’s take a first look using the skimr package.\n\ndative %>% # dataset\n  select(realization_of_recipient) %>% # select the variable\n  skim() %>% # get data summary\n  yank(\"factor\") # only show factor-oriented information\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\nrealization_of_recipient\n0\n1\nFALSE\n2\nNP: 2414, PP: 849\n\n\n\n\nThe output from skim() produces various pieces of information that can be helpful. On the one hand we get diagnostics that tell us if there are missing cases (NA values), what the proportion of complete cases is, if the the factor is ordered, how many distinct levels the factor has, as well as the level counts.\nLooking at the top_counts we can see that of the 3,263 observations, in 2,414 the dative is expressed as an ‘NP’ and 849 as ‘PP’. Numerically we can see that there is a difference between the use of the alternation types. A visualization is often helpful for descriptive purposes in statistical analysis. In this particular case, however, we are considering a single categorical variable with only two levels (values) so a visualization is not likely to be more informative than the numeric values we have already obtained. But for demonstration purposes and to get more familiar with building plots, let’s create a visualization.\n\ndative %>% # dataset\n  ggplot(aes(x = realization_of_recipient)) + # mapping\n  geom_bar() + # geometry\n  labs(x = \"Realization of recipient\", y = \"Count\") # labels\n\n\n\nBarplot visualizing the realization of recipient\n\n\n\n\nThe question we want to address, however, is whether this numerical difference is in fact a statistical difference.\nStatistical interrogation\n\nTo statistical assess the distribution for a categorical variable, we will turn to the Chi-squared test. This test aims to gauge whether the numerical differences between ‘NP’ and ‘PP’ counts observed in the data is greater than what would be expected by chance. Chance in the case where there are only two possible outcome levels is 50/50. For our particular data where there are 3,263 observations half would be ‘NP’ and the other half ‘PP’ –specifically 1631.5 for each.\nTo run this test we first will need to create a cross-tabulation of the variable. We will use the xtabs() function to create the table.\n\nror_table <- \n  xtabs(formula = ~ realization_of_recipient, # formula selecting the variable\n        data = dative) # dataset\n\nror_table # preview\n\n#> realization_of_recipient\n#>   NP   PP \n#> 2414  849\n\n\nNo new information here, but the format (i.e. an object of class ‘table’) is what is important for the input argument for the chisq.test() function we will use to run the test.\n\nc1 <- chisq.test(x = ror_table) # apply the chi-squared test to `ror_table`\n\nc1 # preview the test results\n\n#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  ror_table\n#> X-squared = 750.61, df = 1, p-value < 2.2e-16\n\n\nThe preview of the c1 object reveals the main information of interest including the Chi-squared statistic, the degrees of freedom, and the \\(p\\)-value (albeit in scientific notation). However, the c1 is an ‘htest’ object an includes a number of other pieces information about the test.\n\nnames(c1) # preview column names\n\n#> [1] \"statistic\" \"parameter\" \"p.value\"   \"method\"    \"data.name\" \"observed\" \n#> [7] \"expected\"  \"residuals\" \"stdres\"\n\n\nFor our purposes let’s simply confirm that the \\(p\\)-value is lower than the standard .05 threshold for statistical significance.\n\nc1$p.value < .05 # confirm p-value below .05\n\n#> [1] TRUE\n\n\nOther information can be organized in a more readable format using the broom package’s augment() function.\n\nc1 %>% # statistical result\n  augment() # view detailed statistical test information\n\n#> # A tibble: 2 × 6\n#>   realization_of_recipient .observed .prop .expected .resid .std.resid\n#>   <fct>                        <int> <dbl>     <dbl>  <dbl>      <dbl>\n#> 1 NP                            2414 0.740     1632.   19.4       27.4\n#> 2 PP                             849 0.260     1632.  -19.4      -27.4\n\n\nHere we can see the observed and expected counts and the proportions for each level of realization_of_recipient. We also get additional information concerning residuals, but we will leave these aside.\nEvaluation\nAt this point we may think we are done. We have statistically interrogated the realization_of_recipient variable and found that the difference between ‘NP’ and ‘PP’ realization in the datives in this dataset is statistically significant. However, we need to evaluate the size (‘effect size’) and the reliability of the effect (‘confidence interval’). The effectsize package provides a function effectsize() that can provide us both the effect size and confidence interval.\n\neffects <- \n  effectsize(c1) # evaluate effect size and generate a confidence interval (fei type given 2x1 contingency table)\n\neffects # preview effect size and confidence interval\n\n#> Fei  |       95% CI\n#> -------------------\n#> 0.48 | [0.45, 1.00]\n#> \n#> - Adjusted for uniform expected probabilities.\n#> - One-sided CIs: upper bound fixed at [1.00].\n\n\neffectsize() recognizes the type of test results in c1 and calculates the appropriate effect size measure and generates a confidence interval. Since the effect statistic (“Fei”) falls between the 95% confidence interval this suggests the results are reliably interpreted (chances of Type I (false positive) or Type II (false negative) are low).\nNow, the remaining question is to evaluate whether the significant result here is a strong effect or not. To do this we can pass the effect size measure to the interpret_r() function.\n\ninterpret_r(effects$Fei) # interpret the effect size \n\n#> [1] \"very large\"\n#> (Rules: funder2019)\n\n\nTurns out we have a strong effect; the realization of dative alternation heavily favors the ‘NP’ form in our data. The potential reasons why are not considered in this univariate analysis, but we will return to this question later as we add independent variables to the statistical analysis.\n\n8.2.2 Continuous\nNow let’s turn to a case when the variable we aim to interrogate is non-categorical. For this case we will turn to the sdac_disfluencies dataset. Specifically we will aim to test whether the use of fillers is normally distributed across speakers.\n\n\n\n\n\n\nTip\n\n\n\nThis is an important step when working with numeric dependent variables as the type of distribution will dictate decisions about whether we will use parametric or non-parametric tests if we consider the extent to which an independent variable (or variables) can explain the variation of the dependent variable.\n\n\nSince the dataset is currently organized around fillers as the observational unit, I will first transform this dataset to sum the use of fillers for each speaker in the dataset.\n\nsdac_speaker_fillers <- \n  sdac_disfluencies %>% # dataset\n  group_by(speaker_id) %>% # group by each speaker\n  summarise(sum = sum(count)) %>% # add up all fillers used\n  ungroup() # remove grouping parameter\n\n\n\n\n\nTable 8.4: First 10 observations of sdac_speaker_fillers dataset.\n\nspeaker_id\nsum\n\n\n\n155\n28\n\n\n1000\n45\n\n\n1001\n264\n\n\n1002\n54\n\n\n1004\n45\n\n\n1005\n129\n\n\n1007\n0\n\n\n1008\n27\n\n\n1010\n2\n\n\n1011\n54\n\n\n\n\n\n\nDescriptive assessment\nLet’s perform some descriptive assessement of the variable of interest sum. First let’s apply the skim() function and retrieve just the relevant numeric descriptors with yank(). One twist here, however, is that I’ve customized the skim() function using the skim_with() to remove the default histogram and add the Interquartile Range (IQR) to the output. This new skim function num_skim() will take the place of skim().\n\nnum_skim <- \n  skim_with(numeric = sfl(hist = NULL, # remove hist skim\n                                   iqr = IQR)) # add IQR to skim\n\nsdac_speaker_fillers %>% # dataset\n  select(sum) %>% # variable of interest\n  num_skim() %>% # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\niqr\n\n\nsum\n0\n1\n87.09\n108.34\n0\n16\n45\n114\n668\n98\n\n\n\n\nWe see here that the mean use of fillers is 87.1 across speakers. However, the standard deviation and IQR are large relative to this mean which indicates that the dispersion is quite large, in other words this suggests that there are large differences between speakers. Furthermore, since the median (p50) is smaller than the mean, the distribution is right skewed.\nLet’s look a couple visualizations of this distribution to appreciate these descriptives. A histogram will provide us a view of the distribution using the counts of the values of sum and a density plot will provide a smooth curve which represents the scaled distribution of the observed data.\n\n\np1 <- \n  sdac_speaker_fillers %>% # dataset\n  ggplot(aes(sum)) + # mapping\n  geom_histogram() +  # geometry\n  labs(x = \"Fillers\", y = \"Count\")\n\np2 <- \n  sdac_speaker_fillers %>% # dataset\n  ggplot(aes(sum)) + # mapping\n  geom_density() + # geometry\n  geom_rug() +  # visualize individual observations\n  labs(x = \"Fillers\", y = \"Density\")\n\np1 + p2 + plot_annotation(\"Filler count distributions.\")\n\n\n\nFigure 8.1: ?(caption)\n\n\n\n\nFrom the plots in Figure 8.1 we see that our initial intuitions about the distribution of sum are correct. There is large dispersion between speakers and the data distribution is right skewed.\n\n\n\n\n\n\nTip\n\n\n\nNote that I’ve used the patchwork package for organizing the display of plots and including a plot annotation label.\n\n\nSince our aim is to test for normality, we can generate a Quantile-Quantile plots (QQ Plot).\n\nsdac_speaker_fillers %>% # dataset\n  ggplot(aes(sample = sum)) + # mapping\n  stat_qq() + # calculate expected quantile-quantile distribution\n  stat_qq_line() # plot the qq-line\n\n\n\n\n\n\n\nSince many points do not fall on the expected normal distribution line we have even more evidence to support the notion that the distribution of sum is non-normal.\nStatistical interrogation\nAlthough the descriptives and visualizations strongly suggest that we do not have normally distributed data let’s run a normality test. For this we turn to the shapiro.test() function which performs the Shapiro-Wilk test of normality. We pass the sum variable to this function to run the test.\n\ns1 <- shapiro.test(sdac_speaker_fillers$sum) # apply the normality test to `sum`\n\ns1 # preview the test results\n\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  sdac_speaker_fillers$sum\n#> W = 0.75079, p-value < 2.2e-16\n\n\nAs we saw with the results from the chisq.test() function, the shapiro.test() function produces an object with information about the test including the \\(p\\)-value. Let’s run our logical test to see if the test is statistically significant.\n\ns1$p.value < .05 # \n\n#> [1] TRUE\n\n\nEvaluation\nThe results from the Shapiro-Wilk Normality Test tell us that the distribution of sum is statistically found to differ from the normal distribution. So in this case, statistical significance suggests that sum cannot be used as a parametric dependent variable. For our aims this is all the evaluation required. Effect size and confidence intervals are not applicable.\nIt is of note, however, that the expectation that the variable sum would conform to the normal distribution was low from the outset as we are working with count data. Count data, or frequencies, are in a strict sense not continuous, but rather discrete –meaning that they are real numbers (whole numbers which are always positive). This is a common informational type to encounter in text analysis."
  },
  {
    "objectID": "inference.html#bivariate-analysis",
    "href": "inference.html#bivariate-analysis",
    "title": "8  Inference",
    "section": "\n8.3 Bivariate analysis",
    "text": "8.3 Bivariate analysis\nA more common scenario in statistical analysis is the consideration of the relationship between two-variables, known as bivariate analysis.\n\n8.3.1 Categorical\nLet’s build on our univariate analysis of realization_of_recipient and include an explanatory, or independent variable which we will explore to test whether it can explain our earlier finding that ‘NP’ datives are more common that ‘PP’ datives. The question to test, then, is whether modality explains the distribution of the realization_of_recipient.\nDescriptive assessment\nBoth the realization_of_recipient and modality variables are categorical, specifically nominal as we can see by using skim().\n\ndative %>% \n  select(realization_of_recipient, modality) %>% # select key variables\n  skim() %>% # get custom data summary\n  yank(\"factor\") # only show factor-oriented information\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nrealization_of_recipient\n0\n1\nFALSE\n2\nNP: 2414, PP: 849\n\n\nmodality\n0\n1\nFALSE\n2\nspo: 2360, wri: 903\n\n\n\n\n\nFor this reason measures of central tendency are not applicable and we will turn to a contingency table to summarize the relationship. The janitor package has a set of functions, the primary function being tabyl(). Other functions used here are to adorn the contingency table with totals, percentages, and to format the output for readability.\n\ndative %>% \n  tabyl(realization_of_recipient, modality) %>% # cross-tabulate\n  adorn_totals(c(\"row\", \"col\")) %>% # provide row and column totals\n  adorn_percentages(\"col\") %>% # add percentages to the columns\n  adorn_pct_formatting(rounding = \"half up\", digits = 0) %>% # round the digits\n  adorn_ns() %>% # add observation number\n  adorn_title(\"combined\") %>% # add a header title\n  kable(booktabs = TRUE) # pretty table)\n\n\n\nTable 8.5: Contingency table for realization_of_recipient and modality.\n\n\n\n\n\n\n\nrealization_of_recipient/modality\nspoken\nwritten\nTotal\n\n\n\nNP\n79% (1859)\n61% (555)\n74% (2414)\n\n\nPP\n21% (501)\n39% (348)\n26% (849)\n\n\nTotal\n100% (2360)\n100% (903)\n100% (3263)\n\n\n\n\n\n\nTo gain a better appreciation for this relationship let’s generate a couple plots one which shows cross-tabulated counts and the other calculated proportions.\n\np1 <- \n  dative %>% # dataset\n  ggplot(aes(x = realization_of_recipient, fill = modality)) + # mappings\n  geom_bar() + # geometry\n  labs(y = \"Count\", x = \"Realization of recipient\") # labels\n\np2 <- \n  dative %>% # dataset\n  ggplot(aes(x = realization_of_recipient, fill = modality)) + # mappings\n  geom_bar(position = \"fill\") + # geometry, with fill for proportion plot\n  labs(y = \"Proportion\", x = \"Realization of recipient\", fill = \"Modality\") # labels\n\np1 <- p1 + theme(legend.position = \"none\") # remove legend from left plot\n\np1 + p2 + plot_annotation(\"Relationship between Realization of recipient and Modality.\")\n\n\n\n\n\n\n\nLooking at the count plot (in the left pane) we see that large difference between the realization of the dative as an ‘NP’ or ‘PP’ obscures to some degree our ability to see to what degree modality is related to the realization of the dative. So, a proportion plot (in the right pane) standardizes each level of realization_of_recipient to provide a more comparable view. From the proportion plot we see that there appears to be a trend towards more use of ‘PP’ than ‘NP’ in the written modality.\nStatistical interrogation\nAlthough the proportion plot is visually helpful, we use the raw counts to statistically analyze this relationship. Again, as we are working with categorical variables, now for a dependent and independent variable, we use the Chi-squared test. And as before we need to create the cross-tabulation table to pass to the chisq.test() to perform the test.\n\nror_mod_table <- \n  xtabs(formula = ~ realization_of_recipient + modality, # formula \n        data = dative) # dataset\n\nc2 <- chisq.test(ror_mod_table) # apply the chi-squared test to `ror_mod_table`\n\nc2 # # preview the test results\n\n#> \n#>  Pearson's Chi-squared test with Yates' continuity correction\n#> \n#> data:  ror_mod_table\n#> X-squared = 100.76, df = 1, p-value < 2.2e-16\n\nc2$p.value < .05 # confirm p-value below .05\n\n#> [1] TRUE\n\n\nWe can preview the result and provide a confirmation of the \\(p\\)-value. This evidence suggests that there is a difference between the distribution of dative realization according to modality.\nWe can also see more details about the test.\n\nc2 %>% # statistical result\n  augment() # view detailed statistical test information\n\n#> # A tibble: 4 × 9\n#>   realization_of_…¹ modal…² .obse…³ .prop .row.…⁴ .col.…⁵ .expe…⁶ .resid .std.…⁷\n#>   <fct>             <fct>     <int> <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>\n#> 1 NP                spoken     1859 0.570   0.770   0.788   1746.   2.71    10.1\n#> 2 PP                spoken      501 0.154   0.590   0.212    614.  -4.56   -10.1\n#> 3 NP                written     555 0.170   0.230   0.615    668.  -4.37   -10.1\n#> 4 PP                written     348 0.107   0.410   0.385    235.   7.38    10.1\n#> # … with abbreviated variable names ¹​realization_of_recipient, ²​modality,\n#> #   ³​.observed, ⁴​.row.prop, ⁵​.col.prop, ⁶​.expected, ⁷​.std.resid\n\n\nEvaluation\nNow we want to calculate the effect size and the confidence interval to provide measures of assurance that our finding is robust.\n\neffects <- effectsize(c2) # evaluate effect size and generate a confidence interval\n\neffects # preview effect size and confidence interval\n\n#> Cramer's V (adj.) |       95% CI\n#> --------------------------------\n#> 0.18              | [0.15, 1.00]\n#> \n#> - One-sided CIs: upper bound fixed at [1.00].\n\ninterpret_r(effects$Cramers_v) # interpret the effect size\n\n#> [1] \"small\"\n#> (Rules: funder2019)\n\n\nWe get effect size and confidence interval information. Note that the effect size, reflected by Cramer’s V, for this relationship is weak. This points out an important aspect to evaluation of statistical tests. The fact that a test is significant does not mean that it is meaningful. A small effect size suggests that we should be cautious about the extent to which this significant finding is robust in the population from which the data is sampled.\n\n8.3.2 Continuous\nFor a bivariate analysis in which the dependent variable is not categorical, we will turn to the sdac_disfluencies dataset. The question we will pose to test is whether the use of fillers is related to the type of filler (‘uh’ or ‘um’).\nDescriptive assessment\nThe key variables to assess in this case are the variables count and filler. But before we start to explore this relationship we will need to transform the dataset such that each speaker’s use of the levels of filler is summed. We will use group_by() to group speaker_id and filler combinations and then use summarize() to then sum the counts for each filler type for each speaker\n\nsdac_fillers <- \n  sdac_disfluencies %>% # dataset\n  group_by(speaker_id, filler) %>% # grouping parameters\n  summarize(sum = sum(count)) %>% # summed counts for each speaker-filler combination\n  ungroup() # remove the grouping parameters\n\nLet’s preview this transformation.\n\n\n\n\nTable 8.6: First 10 observations from sdac_fillers dataset.\n\nspeaker_id\nfiller\nsum\n\n\n\n155\nuh\n28\n\n\n155\num\n0\n\n\n1000\nuh\n37\n\n\n1000\num\n8\n\n\n1001\nuh\n262\n\n\n1001\num\n2\n\n\n1002\nuh\n34\n\n\n1002\num\n20\n\n\n1004\nuh\n30\n\n\n1004\num\n15\n\n\n\n\n\n\nLet’s take a look at them together by grouping the dataset by filler and then using the custom skim function num_skim() for the numeric variablecount.\n\nsdac_fillers %>% # dataset\n  group_by(filler) %>% # grouping parameter\n  num_skim() %>% # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nfiller\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\niqr\n\n\n\nsum\nuh\n0\n1\n71.37\n91.48\n0\n14\n39\n91\n661\n77\n\n\nsum\num\n0\n1\n15.72\n31.03\n0\n0\n4\n16\n265\n16\n\n\n\n\n\nWe see here that the standard deviation and IQR for both ‘uh’ and ‘um’ are relatively large for the respective means (71.4 and 15.7) suggesting the distribution is quite dispersed. Let’s take a look at a boxplot to visualize the counts in sum for each level of filler.\n\np1 <- \n  sdac_fillers %>% # dataset\n  ggplot(aes(x = filler, y = sum)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  labs(x = \"Filler\", y = \"Counts\") # labels\n\np2 <- \n  sdac_fillers %>% # dataset\n  ggplot(aes(x = filler, y = sum)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  ylim(0, 100) + # scale the y axis to trim outliers\n  labs(x = \"Filler\", y = \"\") # labels\n\np1 + p2\n\n\n\n\n\n\n\nIn the plot in the left pane we see a couple things. First, it appears that there is in fact quite a bit of dispersion as there are quite a few outliers (dots) above the lines extending from the boxes. Recall that the boxes represent the first and third quantile, that is the IQR and that the notches represent the confidence interval. Second, when we compare the boxes and their notches we see that there is little overlap (looking horizontally). In the right pane I’ve zoomed in a bit trimming some outliers to get a better view of the relationship between the boxes. Since the overlap is minimal and in particular the notches do not overlap at all, this is a good indication that there is a significant trend.\nFrom the descriptive statistics and the visual summary it appears that the filler ‘uh’ is more common than ‘um’. It’s now time to submit this to statistical interrogation.\nStatistical interrogation\n\nIn a bivariate (and multivariate) analysis where the dependent variable is non-categorical we apply Linear Regression Modeling (LM). The default assumption of linear models, however, is that the dependent variable is normally distributed. As we have seen our variable sum does not conform to the normal distribution. We know this because of our tests in the univariate case, but as mentioned at the end of that section, we are working with count data which by nature is understood as discrete and not continuous in a strict technical sense. So instead of using the linear model for our regression analysis we will use the Generalized Linear Model (GLM) (R. Harald Baayen 2008; Gries 2013).\nThe function glm() implements generalized linear models. In addition to the formula (sum ~ filler) and the dataset to use, we also include an appropriate distribution family for the dependent variable. For count and frequency data the appropriate family is the “Poisson” distribution.\n\nm1 <- \n  glm(formula = sum ~ filler, # formula\n      data = sdac_fillers, # dataset\n      family = \"poisson\") # distribution family\n\nsummary(m1) # preview the test results\n\n#> \n#> Call:\n#> glm(formula = sum ~ filler, family = \"poisson\", data = sdac_fillers)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -11.948   -5.607   -3.937    0.801   41.991  \n#> \n#> Coefficients:\n#>              Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  4.267936   0.005637   757.2   <2e-16 ***\n#> fillerum    -1.513077   0.013268  -114.0   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 72049  on 881  degrees of freedom\n#> Residual deviance: 55071  on 880  degrees of freedom\n#> AIC: 58524\n#> \n#> Number of Fisher Scoring iterations: 6\n\n\nLet’s focus on the coefficients, specifically for the ‘fillerum’ line. Since our factor filler has two levels one level is used as the reference to contrast with the other level. In this case by default the first level is used as the reference. Therefore the coefficients we see in ‘fillerum’ are ‘um’ in contrast to ‘uh’. Without digging into the details of the other parameter statistics, let’s focus on the last column which contains the \\(p\\)-value. A convenient aspect of the summary() function when applied to regression model results is that it provides statistical significance codes. In this case we can see that the contrast between ‘uh’ and ‘um’ is signficant at \\(p < .001\\) which of course is lower than our standard threshold of \\(.05\\).\nTherefore we can say with some confidence that the filler ‘uh’ is more frequent than ‘um’.\nEvaluation\nGiven we have found a significant effect for filler, let’s look at evaluating the effect size and the confidence interval. Again, we use the effectsize() function. We then can preview the effects object. Note that effect size of interest is in the second row of the coefficient (Std_Coefficient) so we subset this column to extract only the effect coefficient for the filler contrast.\n\neffects <- effectsize(m1) # evaluate effect size and generate a confidence interval\n\neffects # preview effect size and confidence interval\n\n#> # Standardization method: refit\n#> \n#> Parameter   | Std. Coef. |         95% CI\n#> -----------------------------------------\n#> (Intercept) |       4.27 | [ 4.26,  4.28]\n#> fillerum    |      -1.51 | [-1.54, -1.49]\n#> \n#> - Response is unstandardized.\n\ninterpret_r(effects$Std_Coefficient[2]) # interpret the effect size\n\n#> [1] \"very large\"\n#> (Rules: funder2019)\n\n\nThe coefficient statistic falls within the confidence interval and the effect size is strong so we can be confident that our findings are reliable given this data."
  },
  {
    "objectID": "inference.html#multivariate-analysis",
    "href": "inference.html#multivariate-analysis",
    "title": "8  Inference",
    "section": "\n8.4 Multivariate analysis",
    "text": "8.4 Multivariate analysis\nThe last case to consider is when we have more than one independent variable we want to use to assess their potential relationship to the dependent variable. Again we will consider a categorical and non-categorical dependent variable. But, in this case the implementation methods are quite similar, as we will see.\n\n8.4.1 Categorical\nFor the categorical multivariate case we will again consider the dative dataset and build on the previous analyses. The question to be posed is whether modality in combination with the length of the recipient (length_of_recipient) together explain the distribution of the realization of the recipient (realization_of_recipient).\nDescriptive assessment\nNow that we have three variables, there is more to summarize to get our descriptive information. Luckily, however, the same process can be applied to three (or more) variables using the group_by() function and then passed to skim(). In this case we have two categorical variables and one numeric variable. So we will group by both the categorical variables and then pass the numeric variable to the custom num_skim() function –pulling out only the relevant descriptive information for numeric variables with yank().\n\ndative %>% # dataset\n  select(realization_of_recipient, modality, length_of_recipient) %>% # select key variables\n  group_by(realization_of_recipient, modality) %>% # grouping parameters\n  num_skim() %>% # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nrealization_of_recipient\nmodality\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\niqr\n\n\n\nlength_of_recipient\nNP\nspoken\n0\n1\n1.14\n0.60\n1\n1\n1\n1\n12\n0\n\n\nlength_of_recipient\nNP\nwritten\n0\n1\n1.95\n1.59\n1\n1\n2\n2\n17\n1\n\n\nlength_of_recipient\nPP\nspoken\n0\n1\n2.30\n2.04\n1\n1\n2\n3\n15\n2\n\n\nlength_of_recipient\nPP\nwritten\n0\n1\n4.75\n4.10\n1\n2\n4\n6\n31\n4\n\n\n\n\n\nThere is much more information now that we are considering multiple independent variables, but if we look over the measures of dispersion we can see that the median and the IQR are relatively similar to their respective means suggesting that there are fewer outliers and relativley little skew.\nLet’s take a look at a visualization of this information. Since we are working with a categorical dependent variable and there is one non-categorical variable we can use a boxplot. The addition here is to include a color mapping which will provide a distinct box for each level of modality (‘written’ and ‘spoken’).\n\np1 <- \n  dative %>% # dataset\n  ggplot(aes(x = realization_of_recipient, y = length_of_recipient, color = modality)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  labs(x = \"Realization of recipient\", y = \"Length of recipient (in words)\", color = \"Modality\") # labels\n\np2 <- \n  dative %>% # dataset\n  ggplot(aes(x = realization_of_recipient, y = length_of_recipient, color = modality)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  ylim(0, 15) + # scale the y axis to trim outliers\n  labs(x = \"Realization of recipient\", y = \"\", color = \"Modality\") # labels\n\np1 <- p1 + theme(legend.position = \"none\") # remove the legend from the left pane plot\n\np1 + p2\n\n\n\n\n\n\n\nIn the left pane we see the entire visualization including all outliers. From this view it appears that there is a potential trend that the length of the recipient is larger when the realization of the recipient is ‘PP’. There is also a potential trend for modality with written language showing longer recipient lengths overall. The pane on the right is scaled to get a better view of the boxes by scaling the y-axis down and as such trimming the outliers. This plot shows more clearly that the length of the recipient is longer when the recipient is realized as a ‘PP’. Again, the contrast in modality is also a potential trend, but the boxes (of the same color), particularly for the spoken modality overlap to some degree.\nSo we have some trends in mind which will help us interpret the statistical interrogation so let’s move there next.\nStatistical interrogation\nOnce we involve more than two variables, the choice of statistical method turns towards regression. In the case that the dependent variable is categorical, however, we will use Logistic Regression. The workhorse function glm() can be used for a series of regression models, including logistic regression. The requirement, however, is that we specify the family of the distribution. For logistic regression the family is “binomial”. The formula includes the dependent variable as a function of our other two variables, each are separated by the + operator.\n\nm1 <- glm(formula = realization_of_recipient ~ modality + length_of_recipient, # formula\n          data = dative, # dataset\n          family = \"binomial\") # distribution family\n\nsummary(m1) # preview the test results\n\n#> \n#> Call:\n#> glm(formula = realization_of_recipient ~ modality + length_of_recipient, \n#>     family = \"binomial\", data = dative)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -4.3932  -0.5979  -0.5979   0.1318   1.9238  \n#> \n#> Coefficients:\n#>                     Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)         -2.33919    0.07969 -29.354   <2e-16 ***\n#> modalitywritten     -0.04834    0.10693  -0.452    0.651    \n#> length_of_recipient  0.70810    0.04200  16.859   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 3741.1  on 3262  degrees of freedom\n#> Residual deviance: 3104.7  on 3260  degrees of freedom\n#> AIC: 3110.7\n#> \n#> Number of Fisher Scoring iterations: 5\n\n\nThe results from the model again provide a wealth of information. But the key information to focus on is the coefficients. In particular the coefficients for the independent variables modality and length_of_recipient. What we notice, is that the \\(p\\)-value for length_of_recipient is significant, but the contrast between ‘written’ and ‘spoken’ for modality is not. If you recall, we used this same dataset to explore modality as a single indpendent variable earlier –and it was found to be significant. So why now is it not? The answer is that when multiple variables are used to explain the distribution of a measure (dependent variable) each variable now adds more information to explain the dependent variable –each has it’s own contribution. Since length_of_recipient is significant, this suggests that the explanatory power of modality is weak, especially when compared to length_of_recipient. This make sense as we saw in the earlier model the fact that the effect size for modality was not strong and that is now more evident that the length_of_recipient is included in the model.\nEvaluation\nNow let’s move on and gauge the effect size and calculate the confidence interval for length_of_recipient in our model. We apply the effectsize() function to the model and then use interpret_r() on the coefficient of interest (which is in the fourth row of the Std_Coefficients column).\n\neffects <- effectsize(m1) # evaluate effect size and generate a confidence interval\n\neffects # preview effect size and confidence interval\n\n#> # Standardization method: refit\n#> \n#> Parameter           | Std. Coef. |         95% CI\n#> -------------------------------------------------\n#> (Intercept)         |      -1.03 | [-1.15, -0.92]\n#> modalitywritten     |      -0.05 | [-0.26,  0.16]\n#> length_of_recipient |       1.46 | [ 1.30,  1.64]\n#> \n#> - Response is unstandardized.\n\ninterpret_r(effects$Std_Coefficient[4]) # interpret the effect size\n\n#> [1] NA\n#> (Rules: funder2019)\n\n\nWe see we have a coefficient that falls within the confidence interval and the effect size is large. So we can saw with some confidence that the length of the recipient is a significant predictor of the use of ‘PP’ as the realization of the recipient in the dative alternation.\n\n8.4.2 Continuous\nThe last case we will consider here is when the dependent variable is non-categorical and we have more than one independent variable. The question we will pose is whether the type of filler and the sex of the speaker can explain the use of fillers in conversational speech.\nWe will need to prepare the data before we get started as our current data frame sdac_fillers has filler and the sum count for each filler grouped by speaker –but it does not include the sex of each speaker. The sdac_disfluencies data frame does have the sex column, but it has not been grouped by speaker. So let’s transform the sdac_disfluencies summarizing it to only get the speaker_id and sex combinations. This should result in a data frame with 441 observations, one observation for each speaker in the corpus.\n\nsdac_speakers_sex <- \n  sdac_disfluencies %>% # dataset\n  distinct(speaker_id, sex) # summarize for distinct `speaker_id` and `sex` values\n\nLet’s preview the first 10 observations form this transformation.\n\n\n\n\nTable 8.7: First 10 observations of the sdac_speakers_sex data frame.\n\nspeaker_id\nsex\n\n\n\n155\nNA\n\n\n1000\nFEMALE\n\n\n1001\nMALE\n\n\n1002\nFEMALE\n\n\n1004\nFEMALE\n\n\n1005\nFEMALE\n\n\n1007\nFEMALE\n\n\n1008\nFEMALE\n\n\n1010\nMALE\n\n\n1011\nFEMALE\n\n\n\n\n\n\nGreat, now we have each speaker_id and sex for all 441 speakers. One thing to note, however, is that speaker ‘155’ does not have a value for sex –this seems to be an error in the metadata that we will need to deal with before we proceed in our analysis. Let’s move on to join our new sdac_speakers_sex data frame and the sdac_fillers data frame.\nNow that we have a complete dataset with speaker_id and sex we will now join this dataset with our sdac_fillers dataset effectively adding the column sex. We want to keep all the observations in sdac_fillers and add the column sex for observations that correspond between each data frame for the column speaker_id so we will use a left join with the function left_join() with the sdac_fillers dataset on the left.\n\nsdac_fillers_sex <- \n  left_join(sdac_fillers, sdac_speakers_sex) # join\n\nNow let’s preview the first observations in this new sdac_fillers_sex data frame.\n\n\n\n\nTable 8.8: First 10 observations of the sdac_fillers_sex data frame.\n\nspeaker_id\nfiller\nsum\nsex\n\n\n\n155\nuh\n28\nNA\n\n\n155\num\n0\nNA\n\n\n1000\nuh\n37\nFEMALE\n\n\n1000\num\n8\nFEMALE\n\n\n1001\nuh\n262\nMALE\n\n\n1001\num\n2\nMALE\n\n\n1002\nuh\n34\nFEMALE\n\n\n1002\num\n20\nFEMALE\n\n\n1004\nuh\n30\nFEMALE\n\n\n1004\num\n15\nFEMALE\n\n\n\n\n\n\nAt this point let’s drop this speaker from the sdac_speakers_sex data frame.\n\nsdac_fillers_sex <- \n  sdac_fillers_sex %>% # dataset\n  filter(speaker_id != \"155\") # drop speaker_id 155\n\nWe are now ready to proceed in our analysis.\nDescriptive assessment\nThe process by now should be quite routine for getting our descriptive statistics: select the key variables, group by the categorical variables, and finally pull the descriptives for the numeric variable.\n\nsdac_fillers_sex %>% # dataset\n  select(sum, filler, sex) %>% # select key variables\n  group_by(filler, sex) %>% # grouping parameters\n  num_skim() %>% # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nfiller\nsex\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\niqr\n\n\n\nsum\nuh\nFEMALE\n0\n1\n63.22\n76.54\n0\n12.00\n39.0\n81.75\n509\n69.75\n\n\nsum\nuh\nMALE\n0\n1\n78.74\n102.61\n0\n15.25\n37.5\n101.50\n661\n86.25\n\n\nsum\num\nFEMALE\n0\n1\n22.38\n36.27\n0\n1.00\n9.0\n28.00\n265\n27.00\n\n\nsum\num\nMALE\n0\n1\n9.92\n24.24\n0\n0.00\n1.0\n8.00\n217\n8.00\n\n\n\n\n\nLooking at these descriptives, it seems like there is quite a bit of variability for some combinations and not others. In short, it’s a mixed bag. Let’s try to make sense of these numbers with a boxplot.\n\np1 <- \n  sdac_fillers_sex %>% # dataset\n  ggplot(aes(x = filler, y = sum, color = sex)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  labs(x = \"Filler\", y = \"Counts\", color = \"Sex\") # labels\n\np2 <- \n  sdac_fillers_sex %>% # dataset\n  ggplot(aes(x = filler, y = sum, color = sex)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  ylim(0, 200) + # scale the y axis to trim outliers\n  labs(x = \"Filler\", y = \"\", color = \"Sex\") # labels\n\np1 <- p1 + theme(legend.position = \"none\") # drop the legend from the left pane plot\n\np1 + p2\n\n\n\n\n\n\n\nWe can see that ‘uh’ is used more than ‘um’ overall. But that whereas men and women use ‘uh’ in similar ways, women use more ‘um’ than men. This is known as an interaction. So we will approach our statistical analysis with this in mind.\nStatistical interrogation\nWe will again use a generalized linear model with the glm() function to conduct our test. The distribution family will be the same has we are again using the sum as our dependent variable which contains discrete count values. The formula we will use, however, is new. Instead of adding a new variable to our independent variables, we will test the possible interaction between filler and sex that we noted in the descriptive assessment. To encode an interaction the * operator is used. So our formula will take the form sum ~ filler * sex. Let’s generate the model and view the summary of the test results as we have done before.\n\nm1 <- \n  glm(formula = sum ~ filler * sex, # formula\n      data = sdac_fillers_sex, # dataset\n      family = \"poisson\") # distribution family\n\nsummary(m1) # preview the test results\n\n#> \n#> Call:\n#> glm(formula = sum ~ filler * sex, family = \"poisson\", data = sdac_fillers_sex)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -12.549   -6.205   -3.640    1.080   40.598  \n#> \n#> Coefficients:\n#>                   Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)       4.146596   0.008763  473.20   <2e-16 ***\n#> fillerum         -1.038272   0.017137  -60.59   <2e-16 ***\n#> sexMALE           0.219546   0.011448   19.18   <2e-16 ***\n#> fillerum:sexMALE -1.033438   0.027906  -37.03   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 71956  on 879  degrees of freedom\n#> Residual deviance: 53543  on 876  degrees of freedom\n#> AIC: 56994\n#> \n#> Number of Fisher Scoring iterations: 6\n\n\nAgain looking at the coefficients we something new. First we see that there is a row for the filler contrast and the sex contrast but also the interaction between filler and sex (‘fillerum:sexMALE’). All rows show significant effects. It is important to note that when an interaction is explored and it is found to be significant, the other simple effects, known as main effects (‘fillerum’ and ‘sexMALE’), are ignored. Only the higer-order effect is considered significant.\nNow what does the ‘fillerum:sexMALE’ row mean. It means that there is an interaction between filler and sex. the directionality of that interaction should be interpreted using our descriptive assessment, in particular the visual boxplots we generated. In sum, women use more ‘um’ than men or stated another way men use ‘um’ less than women.\nEvaluation\nWe finalize our analysis by looking at the effect size and confidence intervals.\n\neffects <- effectsize(m1) # evaluate effect size and generate a confidence interval\n\neffects # preview effect size and confidence interval\n\n#> # Standardization method: refit\n#> \n#> Parameter        | Std. Coef. |         95% CI\n#> ----------------------------------------------\n#> (Intercept)      |       4.15 | [ 4.13,  4.16]\n#> fillerum         |      -1.04 | [-1.07, -1.00]\n#> sexMALE          |       0.22 | [ 0.20,  0.24]\n#> fillerum:sexMALE |      -1.03 | [-1.09, -0.98]\n#> \n#> - Response is unstandardized.\n\ninterpret_r(effects$Std_Coefficient[4]) # interpret the effect size\n\n#> [1] \"very large\"\n#> (Rules: funder2019)\n\n\nWe can conclude, then, that there is a strong interaction effect for filler and sex and that women use more ‘um’ than men."
  },
  {
    "objectID": "inference.html#summary",
    "href": "inference.html#summary",
    "title": "8  Inference",
    "section": "\n8.5 Summary",
    "text": "8.5 Summary\nIn this chapter we have discussed various approaches to conducting inferential data analysis. Each configuration, however, always includes a descriptive assessment, statistical interrogation, and an evaluation of the results. We considered univariate, bivariate, and multivariate analyses using both categorical and non-categorical dependent variables to explore the similarities and differences between these approaches."
  },
  {
    "objectID": "inference.html#activities",
    "href": "inference.html#activities",
    "title": "8  Inference",
    "section": "Activities",
    "text": "Activities\n\n\n\n\n\n\nRecipe\n\n\n\nWhat: Statistical inference: prep, assess, interrogate, evaluate, and reportHow: Read Recipe 9 and participate in the Hypothes.is online social annotation.Why: To illustrate some common coding strategies for preparing datasets for inferential data analysis, as well as the steps conduct descriptive assessment, statistical interrogation, and evaluation and reporting of results.\n\n\n\n\n\n\n\n\nLab\n\n\n\nWhat: Statistical inference: prep, assess, interrogate, evaluate, and reportHow: Clone, fork, and complete the steps in Lab 9.Why: To gain experience working with coding strategies to prepare, assess, interrogate, evaluate, and report results from an inferential data analysis, practice transforming datasets and visualizing relationships, and implement organizational strategies for organizing and reporting results in a reproducible fashion."
  },
  {
    "objectID": "inference.html#questions",
    "href": "inference.html#questions",
    "title": "8  Inference",
    "section": "Questions",
    "text": "Questions\n\n…\n…\n\n\n\n\n\nBaayen, R. Harald. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics Using r. Cambridge Univ Pr.\n\n\nBaayen, R. H., and Elnaz Shafaei-Bajestan. 2019. languageR: Analyzing Linguistic Data: A Practical Introduction to Statistics. https://CRAN.R-project.org/package=languageR.\n\n\nGries, Stefan Th. 2013. Statistics for Linguistics with r. A Practical Introduction. 2nd revise."
  },
  {
    "objectID": "prediction.html",
    "href": "prediction.html",
    "title": "9  Prediction",
    "section": "",
    "text": "… Introduction …\n\n1 + 1 # adds 1 and 1\n\n#> [1] 2\n\n# create a data frame with two columns x and y which generate the normal distribution\n\nStrengths: 1. Neural networks are able to capture complex relationships between words and phrases in text. 2. Neural networks can learn from large amounts of data and can be trained to classify text with high accuracy. 3. Neural networks can be used to classify text into multiple categories. 4. Neural networks can be used to identify patterns in text that are not easily detected by traditional methods.\nWeaknesses: 1. Neural networks require a large amount of data to train and can be computationally expensive. 2. Neural networks can be prone to overfitting if the data is not properly preprocessed. 3. Neural networks can be difficult to interpret and explain due to their complexity. 4. Neural networks can be sensitive to noise and outliers in the data.\n\n\n\n\n\n\nSwirl\n\n\n\nWhat: Supervised LearningHow: In the R Console pane load swirl, run swirl(), and follow prompts to select the lesson.Why: …"
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "10  Exploration",
    "section": "",
    "text": "… Introduction …\n\n\n\n\n\n\nSwirl\n\n\n\nWhat: Unsupervised LearningHow: In the R Console pane load swirl, run swirl(), and follow prompts to select the lesson.Why: To …\n\n\n…"
  },
  {
    "objectID": "communication.html",
    "href": "communication.html",
    "title": "Communication",
    "section": "",
    "text": "In this section I cover the steps in presenting the findings of the research both as a research document and as a reproducible research project. Both research documents and reproducible projects are fundamental components of modern scientific inquiry. On the one hand a research document provides readers a detailed summary of the main import of the research study. On the other hand making the research project available to interested readers ensures that the scientific community can gain insight into the process implemented in the research and thus enables researchers to vet and extend this research to build a more robust and verifiable research base."
  },
  {
    "objectID": "reporting.html",
    "href": "reporting.html",
    "title": "11  Reporting",
    "section": "",
    "text": "First we will discuss the purpose of a research document and how the structure of this document supports and clearly documents the project’s rationale, goals, procedure, results, and findings. Then I will turn to incorporating citations and references and figures and tables in R Markdown. Finally, we will explore how to apply field-specific and publishing house formats to various document formats including Word, PDF, HTML, and ePub.\n\n\n\n\n\n\nSwirl\n\n\n\nWhat: Rendering QuartoHow: In the R Console pane load swirl, run swirl(), and follow prompts to select the lesson.Why: To …\n\n\n…"
  },
  {
    "objectID": "collaboration.html",
    "href": "collaboration.html",
    "title": "12  Collaboration",
    "section": "",
    "text": "Swirl\n\n\n\nWhat: Compiling Research ProjectsHow: In the R Console pane load swirl, run swirl(), and follow prompts to select the lesson.Why: To …\n\n\nWhether for other researchers or for your future self, creating research that is well-documented and reproducible is a fundamental part of conducting modern scientific inquiry. In this chapter we will emphasize the importance of this endeavor and outline strategies for ensuring your research project is reproducible. This will include directory and file structure, key documentation files as well as how to effectively use existing software resources and frameworks for publishing your research (either for private use, journal requirements, or general public consumption) on popular repositories such as GitHub and Open Science Framework (OSF)."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ackoff, Russell L. 1989. “From Data to Wisdom.” Journal\nof Applied Systems Analysis 16 (1): 3–9.\n\n\nÄdel, Annelie. 2020. “Corpus Compilation.” In A\nPractical Handbook of Corpus Linguistics, edited by Magali Paquot\nand Stefan Th. Gries, 3–24. Switzerland: Springer.\n\n\nAlmeida, Tiago A, Jos’e María G’omez Hildago, and Akebo Yamakami. 2011.\n“Contributions to the Study of SMS Spam Filtering: New Collection\nand Results.” In Proceedings of the 2011 ACM Symposium on\nDocument Engineering (DOCENG’11), 4. Mountain View, CA.\n\n\nBaayen, R. Harald. 2004. “Statistics in Psycholinguistics: A\nCritique of Some Current Gold Standards.” Mental Lexicon\nWorking Papers 1 (1): 1–47.\n\n\n———. 2008. Analyzing Linguistic Data: A Practical Introduction to\nStatistics Using r. Cambridge Univ Pr.\n\n\n———. 2011. “Corpus Linguistics and Naive Discriminative\nLearning.” Revista Brasileira de Lingu\\’\\istica Aplicada\n11 (2): 295–328.\n\n\nBaayen, R. H., and Elnaz Shafaei-Bajestan. 2019. languageR:\nAnalyzing Linguistic Data: A Practical Introduction to Statistics.\nhttps://CRAN.R-project.org/package=languageR.\n\n\nBao, Wang, Ning Lianju, and Kong Yue. 2019. “Integration of\nUnsupervised and Supervised Machine Learning Algorithms for Credit Risk\nAssessment.” Expert Systems with Applications 128\n(August): 301–15. https://doi.org/10.1016/j.eswa.2019.02.033.\n\n\nBenoit, Kenneth. 2020. Quanteda.corpora: A Collection of Corpora for\nQuanteda. http://github.com/quanteda/quanteda.corpora.\n\n\nBroman, Karl W., and Kara H. Woo. 2018. “Data Organization in\nSpreadsheets.” The American Statistician 72 (1): 2–10.\nhttps://doi.org/10.1080/00031305.2017.1375989.\n\n\nBrown, Keith. 2005. Encyclopedia of Language and Linguistics.\nVol. 1. Elsevier.\n\n\nBuckheit, Jonathan B., and David L. Donoho. 1995. “Wavelab and\nReproducible Research.” In Wavelets and Statistics,\n55–81. Springer.\n\n\nBychkovska, Tetyana, and Joseph J. Lee. 2017. “At the Same Time:\nLexical Bundles in L1 and L2 University Student Argumentative\nWriting.” Journal of English for Academic Purposes 30\n(November): 38–52. https://doi.org/10.1016/j.jeap.2017.10.008.\n\n\nCarmi, Elinor, Simeon J. Yates, Eleanor Lockley, and Alicja Pawluczuk.\n2020. “Data Citizenship: Rethinking Data Literacy in the Age of\nDisinformation, Misinformation, and Malinformation.” Internet\nPolicy Review 9 (2).\n\n\nChambers, John M. 2020. “S, r, and Data Science.”\nProceedings of the ACM on Programming Languages 4 (HOPL): 1–17.\nhttps://doi.org/10.1145/3386334.\n\n\nChan, Sin-wai. 2014. Routledge Encyclopedia of Translation\nTechnology. Routledge.\n\n\nConway, Lucian Gideon, Laura Janelle Gornick, Chelsea Burfeind, Paul\nMandella, Andrea Kuenzli, Shannon C. Houck, and Deven Theresa Fullerton.\n2012. “Does Complex or Simple Rhetoric Win Elections? An\nIntegrative Complexity Analysis of u.s. Presidential Campaigns.”\nPolitical Psychology 33 (5): 599–618. https://doi.org/10.1111/j.1467-9221.2012.00910.x.\n\n\nCross, Nigel. 2006. “Design as a Discipline.”\nDesignerly Ways of Knowing, 95–103.\n\n\n“Data Never Sleeps 7.0 Infographic.” 2019.\nhttps://www.domo.com/learn/infographic/data-never-sleeps-7.\n\n\nDeshors, Sandra C, and Stefan Th. Gries. 2016. “Profiling Verb\nComplementation Constructions Across New Englishes.”\nInternational Journal of Corpus Linguistics. 21 (2): 192–218.\n\n\nDesjardins, Jeff. 2019. “How Much Data Is Generated Each\nDay?” Visual Capitalist.\n\n\nDonoho, David. 2017. “50 Years of Data Science.”\nJournal of Computational and Graphical Statistics 26 (4):\n745–66. https://doi.org/10.1080/10618600.2017.1384734.\n\n\nDubnjakovic, Ana, and Patrick Tomlin. 2010. A Practical Guide to\nElectronic Resources in the Humanities. Elsevier.\n\n\nEisenstein, Jacob, Brendan O’Connor, Noah A Smith, and Eric P Xing.\n2012. “Mapping the Geographical Diffusion of New Words.”\nComputation and Language, 1–13. https://doi.org/10.1371/journal.pone.0113114.\n\n\nGandrud, Christopher. 2015. Reproducible\nResearch with r and r Studio. Second edition. CRC Press.\n\n\nGentleman, Robert, and Duncan Temple Lang. 2007. “Statistical\nAnalyses and Reproducible Research.” Journal of Computational\nand Graphical Statistics 16 (1): 1–23.\n\n\nGilquin, Gaëtanelle, and Stefan Th Gries. 2009. “Corpora and\nExperimental Methods: A State-of-the-Art Review.” Corpus\nLinguistics and Linguistic Theory 5 (1): 1–26. https://doi.org/10.1515/CLLT.2009.001.\n\n\nGomez-Uribe, Carlos A., and Neil Hunt. 2015. “The Netflix\nRecommender System: Algorithms, Business Value, and Innovation.”\nACM Transactions on Management Information Systems (TMIS) 6\n(4): 1–19.\n\n\nGries, Stefan Th. 2013. Statistics for Linguistics with r. A\nPractical Introduction. 2nd revise.\n\n\nGrieve, Jack, Andrea Nini, and Diansheng Guo. 2018. “Mapping\nLexical Innovation on American Social Media.” Journal of\nEnglish Linguistics 46 (4): 293–319.\n\n\nHead, Megan L., Luke Holman, Rob Lanfear, Andrew T. Kahn, and Michael D.\nJennions. 2015. “The Extent and Consequences of p-Hacking in\nScience.” PLOS Biology 13 (3): e1002106. https://doi.org/10.1371/journal.pbio.1002106.\n\n\n“How to Make a Data Dictionary.” 2021. OSF Guides.\nhttps://help.osf.io/hc/en-us/articles/360019739054-How-to-Make-a-Data-Dictionary.\n\n\nHu, Minqing, and Bing Liu. 2004. “Mining and Summarizing Customer\nReviews.” In Proceedings of the Tenth ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining,\n168–77.\n\n\nIgnatow, Gabe, and Rada Mihalcea. 2017. An Introduction to Text\nMining: Research Design, Data Collection, and Analysis. Sage\nPublications.\n\n\nJaeger, T Florian, and Neal Snider. 2007. “Implicit Learning and\nSyntactic Persistence: Surprisal and Cumulativity.”\nUniversity of Rochester Working Papers in the Language Sciences\n3 (1).\n\n\nJurafsky, Daniel, and James H. Martin. 2020. Speech and Language\nProcessing.\n\n\nKearney, Michael W., Lluís Revilla Sancho, and Hadley Wickham. 2023.\nRtweet: Collecting Twitter Data. https://CRAN.R-project.org/package=rtweet.\n\n\nKerr, Norbert L. 1998. “HARKing: Hypothesizing After the Results\nAre Known.” Personality and Social Psychology Review 2\n(3): 196–217.\n\n\nKloumann, IM, CM Danforth, KD Harris, and CA Bliss. 2012.\n“Positivity of the English Language.” PloS One.\n\n\nKowsari, Kamran, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana\nMendu, Laura E. Barnes, and Donald E. Brown. 2019. “Text\nClassification Algorithms: A Survey.” Information 10\n(4): 150. https://doi.org/10.3390/info10040150.\n\n\nKucera, H, and W N Francis. 1967. Computational Analysis of Present\nDay American English. Brown University Press Providence.\n\n\nLantz, Brett. 2013. Machine Learning with r. Birmingham: Packt\nPublishing.\n\n\nLewis, Michael. 2004. Moneyball: The Art of Winning an Unfair\nGame. WW Norton & Company.\n\n\nLozano, Crist’obal. 2009. “CEDEL2: Corpus Escrito Del Español\nL2.” Applied Linguistics Now: Understanding Language and\nMind/La Lingüística Aplicada Hoy: Comprendiendo El Lenguaje y La Mente.\nAlmería: Universidad de Almería, 197–212.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging\nData Analytical Work Reproducibly Using r (and Friends).” The\nAmerican Statistician 72 (1): 80–88.\n\n\nMillikan, Robert A. 1923. The Electron and the Light-Quant from the\nExperimental Point of View. Nobel Prize Acceptance Speech.\n\n\nMosteller, Frederick, and David L Wallace. 1963. “Inference in an\nAuthorship Problem.” Journal of the American Statistical\nAssociation 58 (302): 275–309.\n\n\nOlohan, Maeve. 2008. “Leave It Out! Using a Comparable Corpus to\nInvestigate Aspects of Explicitation in Translation.”\nCadernos de Tradução, 153–69.\n\n\nPaquot, Magali, and Stefan Th. Gries, eds. 2020. A Practical\nHandbook of Corpus Linguistics. Switzerland: Springer.\n\n\nRoediger, H. L. L, and K. B. B McDermott. 2000. “Distortions of\nMemory.” The Oxford Handbook of Memory, 149–62.\n\n\nSaxena, Shweta, and Manasi Gyanchandani. 2020. “Machine Learning\nMethods for Computer-Aided Breast Cancer Diagnosis Using Histopathology:\nA Narrative Review.” Journal of Medical Imaging and Radiation\nSciences 51 (1): 182–93.\n\n\nTalarico, Jennifer M., and David C. Rubin. 2003. “Confidence, Not\nConsistency, Characterizes Flashbulb Memories.” Psychological\nScience 14 (5): 455–61. https://doi.org/10.1111/1467-9280.02453.\n\n\nVoigt, Rob, Nicholas P. Camp, Vinodkumar Prabhakaran, William L.\nHamilton, Rebecca C. Hetey, Camilla M. Griffiths, David Jurgens, Dan\nJurafsky, and Jennifer L. Eberhardt. 2017. “Language from Police\nBody Camera Footage Shows Racial Disparities in Officer Respect.”\nProceedings of the National Academy of Sciences 114 (25):\n6521–26.\n\n\nWelbers, Kasper, and Wouter van Atteveldt. 2022. Rsyntax: Extract\nSemantic Relations from Text by Querying and Reshaping Syntax. https://CRAN.R-project.org/package=rsyntax.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\n———. 2022. Rvest: Easily Harvest (Scrape) Web Pages. https://CRAN.R-project.org/package=rvest.\n\n\nWulff, S, A Stefanowitsch, and Stefan Th. Gries. 2007. “Brutal\nBrits and Persuasive Americans.” Aspects of Meaning."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Appendix A — Data",
    "section": "",
    "text": "…"
  }
]