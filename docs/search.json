[
  {
    "objectID": "approaching-analysis.html#sec-diagnostic-measures",
    "href": "approaching-analysis.html#sec-diagnostic-measures",
    "title": "3  Approaching analysis",
    "section": "\n3.1 Diagnostic measures",
    "text": "3.1 Diagnostic measures\n\nThe purpose of diagnostic measures is to inspect your data to ensure its quality and understand its characteristics. There are two primary types of diagnostic measures: verfication and description. Verification methods are applied to catch missing or erroneous data while descriptive methods are used to gain a better understanding of the data. Although treated in two separate sections, in practice these methods are complementary and are often addressed in tandem.\nTo ground this discussion I will introduce a new dataset. This dataset is drawn from the Barcelona English Language Corpus (BELC) (Muñoz 2006), which is found in the TalkBank repository. I’ve selected the “Written composition” task from this corpus which contains 80 writing samples from 36 second language learners of English at different ages. Participants were given the task of writing for 15 minutes on the topic of “Me: my past, present and future”. Data was collected for participants from one to three times over the course of seven years (at 10, 12, 16, and 17 years of age).\nIn Table 3.1 we see the data dictionary for the BELC dataset which reflects structural and transformational steps I’ve done so we start with a tidy dataset with word as the unit of observation.\n\n\n\n\nTable 3.1: Data dictionary for the BELC dataset.\n\n\n\n\n\n\n\nvariable\nname\ndescription\nvariable_type\n\n\n\npart_id\nParticipant ID\nUnique identifier for each participant\ncategorical\n\n\nsex\nParticipant’s sex\nSex of the participant\ncategorical\n\n\ngroup\nTime group\nLongitudinal group to which the participant belongs\nordinal\n\n\nmonth_age\nParticipant’s age in months\nAge of the participant in months\nnumeric\n\n\nutt_id\nUtterance ID\nUnique identifier for each utterance\nnumeric\n\n\nword_id\nWord ID\nUnique identifier for each word within an utterance\nnumeric\n\n\nword\nWord\nThe word spoken by the participant\ncategorical\n\n\nlemma\nWord lemma\nBase form of the word\ncategorical\n\n\npos\nPart of speech\nGrammatical category of the word\ncategorical\n\n\n\n\n\n\nThe data dictionary provides a easily accessible overview of the dataset. This includes a human-readable mapping from variable names to variable descriptions. Further, it provides information about the type of variable (e.g., categorical, ordinal, numeric). As we will see the informational type of variables is key to diagnostic measures, as well as all other components of analysis.\n\n3.1.1 Verification process\n\nAlthough a dataset has undergone curation and transformation, it is still important to verify the data. This is a process of checking the data to ensure that it is accurate and complete. In the case that it is not, consideration should be given to how to address the issues.\n\nThe most basic and usually the first step is to check for missing data. In Table 3.2, there are missing values for the lemma and pos variables in the BELC dataset.\n\n\n\n\nTable 3.2: Summary output for missing values in the BELC dataset.\n\nvariable\ntype\nn_missing\ncomplete_rate\n\n\n\npart_id\ncharacter\n0\n1.000\n\n\nsex\ncharacter\n0\n1.000\n\n\ngroup\ncharacter\n0\n1.000\n\n\nword\ncharacter\n0\n1.000\n\n\nlemma\ncharacter\n79\n0.985\n\n\npos\ncharacter\n23\n0.996\n\n\nmonth_age\nnumeric\n0\n1.000\n\n\nutt_id\nnumeric\n0\n1.000\n\n\nword_id\nnumeric\n0\n1.000\n\n\n\n\n\n\nThere are two primary approaches to dealing with missing data: deletion and recoding. Since these missing values account for only 1.5% and 0.4% of the data respectively, we might be safe to remove these observations. Another approach is to recode the missing values by either applying a unique value for missing values (e.g., NULL) or by imputing values. Imputing values is usually done by replacing missing values with some middle-of-the-road value (e.g., mean, median, mode), but other, more nuanced approaches are possible.\n\n\n\n\n\n\n Dive deeper\nFor more information on missing data, see the missing data chapter in this book.\n\n\n\nIn either case, it is important to consider the implications of missing data for the analysis. For example, if the missing data is not at random or include a sizeable portion of the values of interest, then the analysis may be biased.\n\nValue coding schemes, annotation errors, or other issues may result in anomalies in the data. These are values that are unexpected or inconsistent with the rest of the data or effect the treatment of the data for the particular analysis to be performed.\nFor categorical variables, this may include values that are not expected or are not in the set of values that are expected. A summary of the values for a given variable can be used as a first step to identify anomalies. In Table 3.3, we see the minimum and maximum number of characters and the number of unique values for each categorical variable in the BELC dataset.\n\n\n\n\nTable 3.3: Summary output for categorical variables in the BELC dataset.\n\nvariable\nmin_chars\nmax_chars\nnum_unique\n\n\n\npart_id\n3\n3\n36\n\n\nsex\n4\n6\n2\n\n\ngroup\n2\n2\n4\n\n\nword\n1\n20\n913\n\n\nlemma\n1\n20\n774\n\n\npos\n1\n9\n38\n\n\n\n\n\n\nFrom our knowledge of the data, we can gauge whether these values are expected. For example, sex has two values; likely corresponding to some coding of ‘male’ and ‘female’. The variable part_id has 36 distinct values, which is expected since there are 36 participants and group has four, corresponding to the longitudinal time groups. It is also possible to gauge the expected values for lemma as we know that these are the base words and should be less than the number of words in the dataset.\nFurther verfication of the categorical variables is need, of course. This may include aggregating the data to see the distribution of values and/ or checking the values against the documentation.\nLet’s now consider numeric variables. Numeric variables, by their very nature, do not lend themselves to the same type of summary used for categorical variables (i.e. character lengths, number of unique values, or aggregation) to detect anomalies. For numeric variables there are two types of anomalies that we will consider: outliers and errors in coding. Outliers are anomalies that are extreme values that are not representative of the rest of the data. To determine what is extreme, we need to consider the distribution of the data, that is, the range of values and the frequency of values. It is rarely the case that we can eyeball the distribution of the data based on raw values. Instead, a combination of summary statistics and visualizations are used to determine the distribution of the data. For this reason, the detection of outliers is often carried out as part of the descriptive assessment of the data, as we will see in Section 3.1.2.\nOn the other hand, coding anomalies are values that are not expected or are not in the set of values that are expected. These can sometimes be detected by visual inspection of the data. For example, in Table 3.4, we see the first 10 observations for each variable in the BELC dataset.\n\n\n\n\nTable 3.4: First 10 observations for variables in the BELC dataset.\n\n\n\n\n\n\n\n\n\n\n\n\npart_id\nsex\ngroup\nmonth_age\nutt_id\nword_id\nword\nlemma\npos\n\n\n\nL01\nfemale\nT2\n153\n0\n0\nI\nI\npro:sub\n\n\nL01\nfemale\nT2\n153\n0\n1\nwas\nbe\ncop\n\n\nL01\nfemale\nT2\n153\n0\n2\nborn\nborn\nadj\n\n\nL01\nfemale\nT2\n153\n0\n3\nin\nin\nprep\n\n\nL01\nfemale\nT2\n153\n0\n4\nBarcelona\nBarcelona\nn:prop\n\n\nL01\nfemale\nT2\n153\n0\n5\nand\nand\ncoord\n\n\nL01\nfemale\nT2\n153\n0\n6\nI\nI\npro:sub\n\n\nL01\nfemale\nT2\n153\n0\n7\nlive\nlive\nv\n\n\nL01\nfemale\nT2\n153\n0\n8\nin\nin\nprep\n\n\nL01\nfemale\nT2\n153\n0\n9\nBarcelona\nBarcelona\nn:prop\n\n\n\n\n\n\nLeaving month_age aside, we see that the other two numeric variables utt_id and word_id index utterances and words respectively. However, in contrast to part_id which is a categorical variable as it serves as a unique identifier for each participant, these variables are numeric as they serve to not only index utterances and words but also to provide a measure of how many utterances or words have been produced. Seen in this light, 0 for the first value of utt_id and word_id is unexpected. To adjust for this, we can add 1 to each value of these variables.\n\n3.1.2 Descriptive statistics\n\nThe goal of descriptive statistics is to summarize the data in order to understand and prepare the data for the analysis approach to be performed. This is accomplished through a combination of statistic measures and/ or tabular or graphic summaries. The choice of descriptive statistics is guided by the type of data, as well as the question(s) being asked of the data.\nTo that end, let’s consider a reconfiguration of the BELC dataset, in Table 3.5, which will provide a more illustrative dataset.\n\n\n\n\nTable 3.5: First 10 observations of the reconfigured BELC dataset.\n\nessay_id\npart_id\nsex\ngroup\ntokens\ntypes\nttr\nprop_l2\n\n\n\nE1\nL01\nfemale\nT2\n79\n46\n0.582\n0.987\n\n\nE2\nL02\nfemale\nT1\n18\n18\n1.000\n0.667\n\n\nE3\nL02\nfemale\nT3\n101\n53\n0.525\n1.000\n\n\nE4\nL05\nfemale\nT1\n20\n17\n0.850\n0.900\n\n\nE5\nL05\nfemale\nT3\n158\n80\n0.506\n0.987\n\n\nE6\nL05\nfemale\nT4\n184\n94\n0.511\n0.995\n\n\nE7\nL07\nmale\nT3\n98\n60\n0.612\n1.000\n\n\nE8\nL07\nmale\nT4\n134\n84\n0.627\n0.978\n\n\nE9\nL10\nfemale\nT1\n38\n28\n0.737\n0.974\n\n\nE10\nL10\nfemale\nT3\n118\n74\n0.627\n1.000\n\n\n\n\n\n\nIn this new configuration, the unit of observation is now essay_id. Each of the following variable are attributes or measures of this variable. The new variables in this dataset are aggregates of the previous BELC dataset: tokens is the number of total words, types is the number of unique words, ttr is the ratio of unique words to total words. This is known as the Type-Token Ratio and it is a standard metric for measuring lexical diversity. Finally, the proportion of L2 words (English) to the total words (tokens) is provided in prop_l2.\nIn descriptive statistics, there are four basic questions that are asked of each of the variables in the dataset. Each correspond to a different type of descriptive statistic.\n\nCentral Tendency: Where do the data points tend to be located?\nDispersion: How spread out are the data points?\nDistribution: What is the shape of the distribution of the data points?\nInterdependence: How are these data points related to other data?\n\nCentral tendency\n\n\nThe central tendency is measure which aims to summarize the data points in a variable as the most representative, middle or most typical value. There are three common measures of central tendency: the mode, mean and median. Each differ in how they summarize the data points.\nThe mode is the value, or values, that appears most frequently in a set of values. If there are multiple values with the highest frequency, then the variable is said to be multimodal. The most versatile of the central tendency measures as it can be applied to all levels of measurement, the mode is not often used for numeric variables as it is not as informative as other measures.\nThe more common measures for numeric variables are the mean and the median. The mean is a summary statistic calculated by summing all the values and dividing by the number of values. The median is calculated by sorting all the values in the variable and then selecting the middle value. Given that the mean and median are calculated differently, they will not always yield the same result. Differences that appear between the mean and median will be of interest to us later in this chapter.\nDispersion\n\nThe mean, median, and mode provide summary information where data points tend to be located. However, they do not provide us with any understanding as to how representative this value is. To provide this context, the spread of the values around the central tendency, or dispersion, is calculated.\nFor categorical variables, the spread is framed in terms of how balanced the values are across the levels. One way to do this is to calculate the (normalized) entropy. Entropy is a measure of uncertainty. The more balanced the values are across the levels, the higher the entropy. The less balanced the values are across the levels, the lower the entropy. Normalized entropy scores range from 0 to 1, with 0 indicating that all the values are the same and 1 indicating that all the values are different.\nThe most common measure of dispersion for numeric variables is the standard deviation. The standard deviation is calculated by taking the square root of the variance. The variance is the average of the squared differences from the mean. So, more succinctly, the standard deviation is a measure of the spread of the values around the mean. Where the standard deviation is anchored to the mean, the interquartile range (IQR) is tied to the median. The median represents the sorted middle of the values, in other words the 50th percentile. The IQR is the difference between the 75th percentile and the 25th percentile. Again, just as the mean and the median, the standard deviation and the IQR are calculated in different ways, they are not always the same.\nLet’s now consider the relevant central tendency and dispersion of the variables in the BELC dataset in Table 3.6.\n\n\nTable 3.6: Central tendency and dispersion of the variables in the BELC dataset\n\n\n\n\n\n\n(a) Categorical variables\n\nvariable\ntop_counts\nnorm_entropy\n\n\n\nessay_id\nE1: 1, E10: 1, E11: 1, E12: 1\n1.000\n\n\npart_id\nL05: 3, L10: 3, L11: 3, L12: 3\n0.983\n\n\nsex\nfem: 48, mal: 32\n0.971\n\n\ngroup\nT1: 25, T3: 24, T2: 16, T4: 15\n0.981\n\n\n\n\n\n\n\n\n\n\n(b) Numeric variables\n\nvariable\nmean\nmedian\nsd\niqr\n\n\n\ntokens\n67.62\n56.50\n44.20\n61.25\n\n\ntypes\n41.85\n38.50\n23.03\n31.50\n\n\nttr\n0.68\n0.66\n0.13\n0.15\n\n\nprop_l2\n0.96\n0.99\n0.10\n0.03\n\n\n\n\n\n\n\n\nIn Table 3.6 (a) we see the measures for categorical variables. The top_counts variable gives us a short list of the most frequent levels of the variable. From top_count we can gather whether the variable has one mode or is multimodel. Both essay_id and part_id have the same most frequent value for the levels listed. On the other hand, sex and group have a single mode. We can also appreciate the dispersion of these variables based on the norm_entropy of each variable. essay_id is completely balanced across the levels, so it has a normalized entropy of 1. the other variables are not as balanced, but still quite balanced as the normalized entropy is close to 1.\nIn Table 3.6 (b) the numeric variables have a column for the mean, median, standard deviation, and IQR for each. The variable tokens has a larger difference between the mean and median than the other variables and the standard deviation is relatively large suggesting that the values are more spread out around the mean. In the case of ttr the mean and median are quite close and the standard deviation is relatively small suggesting that the values are more tightly clustered around the mean.\nWhen interpreting these summary values, it is important to only directly compare column-wise. That is, focusing only on a single variable, not across variables. Each variable, as is, is measured on a different scale and only relative to itself can we make sense of the values.\nHowever, we can transform the central tendency and dispersion scores for numeric variables to make them more comparable by standardizing the scale of the values. Standardization is a scale-based transformation that changes the scale of the values to a common scale, or z-scores. It involves two separate transformations: centering and scaling. Centering is a transformation that subtracts the mean or median from each value. The result is a mean and median of zero. Scaling is a transformation that divides each value by the standard deviation or IQR.\nIn Table 3.7 we see the same summary statistics as in Table 3.6 (b), but the values have been standardized for the mean and standard deviation. The mean is now zero and the standard deviation is one. This allows us to compare the median and IQR of the variables more directly.\n\n\n\n\nTable 3.7: Standardized central tendency and dispersion of numeric variables\n\nvariable\nmean\nmedian\nsd\niqr\n\n\n\ntokens\n0\n-0.25\n1\n1.39\n\n\ntypes\n0\n-0.15\n1\n1.37\n\n\nttr\n0\n-0.19\n1\n1.14\n\n\nprop_l2\n0\n0.25\n1\n0.27\n\n\n\n\n\n\n\n\nOne more caveat to keep in mind is that we need to be mindful of the nature of the data being standardized and what the standardized values mean. For example, the variables tokens and types were originally counts. But the standardized values are not interpretable as counts, they are now on a different scale –specifically a z-score scale. In the same way since the ttr and prop_l2 variables were originally proportions, the standardized values are also not interpretable as proportions. One additional twist, however, is that the original scales for these pairs of variables were not the same: tokens and types were counts, but ttr and prop_l2 were proportions. So, even though the standardized values are on the same scale, they are not directly comparable.\nBeyond comparing central tendency and dispersion across variables, standarization is useful for analytic statistics to mitigate the influence of variables with large values. In some cases, the statistical method will require standardization of variables before analysis.\nDistributions\n\nSummary statistics of the central tendency and dispersion of a variable provide a sense of the most representative value and how spread out the data is around this value. However, to gain a more comprehensive understanding of the variable, it is key to consider the frequencies of all the data points. The distribution of a variable is the pattern or shape of the data that emerges when the frequencies of all data points are considered. This can reveal patterns that might not be immediately apparent from summary statistics alone. Understanding the frequency and distribution of data points is vital as it informs subsequent choices of statistical analysis and evaluative methods, ensuring they are appropriate for the specific characteristics of the data.\nWhen assessing the distribution of categorical variables, we can use a frequency table or bar plot. A frequency table is a useful method to display the frequency and proportion of each level in a categorical variable in a clear and concise manner. In Table 3.8 we see the frequency table for the variable sex.\n\n\n\n\n\n\nTable 3.8: Frequency table for the variable sex.\n\nsex\nfrequency\nproportion\n\n\n\nfemale\n48\n0.6\n\n\nmale\n32\n0.4\n\n\n\n\n\n\nA bar plot is a type of plot where the x-axis is a categorical variable and the y-axis is the frequency of the values. The frequency is represented by the height of the bar. The variables can be ordered by frequency, alphabetically, or some other order. Figure 3.1 is a bar chart for the variables sex, group, and part_id, ordered alphabetically.\n\n\n\n\n\n\n(a) Sex\n\n\n\n\n\n(b) Time group\n\n\n\n\n\n(c) Participant ID\n\n\n\nFigure 3.1: Bar plots for categorical variables sex, group, part_id in the BELC dataset.\n\n\nSo for a frequency table or barplot, we can see the frequency of each level of a categorical variable. This gives us some knowledge about the BELC dataset: there are more girls in the dataset, more essays appear in first and third time groups, and the number of essays written by each participant is scattered from one to three. If we were to see any clearly loopsided categories, this would be a sign of imbalance in the data and we would need to consider how this might impact our analysis.\n\n\n\n\n\n\n Consider this\nThe goal of descriptive statistics is to summarize the data in a way that is meaningful and interpretable. With this in mind, compare the frequency table in 3.8 and bar plot in 3.1 (a). Does one provide a more interpretable summary of the data? Why or why not? Are there any other ways you might communicate this distribution more effectively?\n\n\n\nFor numeric variables, understanding the distribution is more complex, and also more important. In essence, however, we are assessing two things: the appearance of outliers in relation to and the overall shape of the distribution.\n\nNow, a frequency table, as in Table 3.8, does not summarize the distribution of a numeric variable in a concise, readily human-consumable format. Instead, the distribution of a numeric variable is best understood visually.\nThe most common visualizations of the distribution of a numeric variable are histograms and density plots. Histograms are a type of bar plot where the x-axis is a numeric variable and the y-axis is the frequency of the values falling within a determined range of values, or bins. The frequency of values within each bin is represented by the height of the bars. Density plots are a smoothed version of histograms. The y-axis of a density plot is the probability of the values. When frequent values appear closely together, the plot line is higher. When the frequency of values is lower or more spread out, the plot line is lower. An example of these plots is show in Figure 3.2 for the variable tokens.\n\n\n\n\n\n(a) Histogram\n\n\n\n\n\n(b) Density plot\n\n\n\nFigure 3.2: Distribution plots for the variable tokens.\n\n\nBoth the histogram in Figure 3.2 (a) and the density plot in Figure 3.2 (b) show the distribution of the variable tokens in slightly different ways which translate into trade-offs in terms of interpretability.\nThe histogram shows the frequency of the values in bins. The number of bins and/ or binwidth can be changed for more or less granularity. A rough grain histogram shows the general shape of the distribution, but it is difficult to see the details of the distribution. A fine grain histogram shows the details of the distribution, but it is difficult to see the general shape of the distribution. The density plot shows the general shape of the distribution, but it hides the details of the distribution. Given this trade-off, it is often useful explore outliers with histograms and the overall shape of the distribution with density plots.\nIn Figure 3.3 we see histograms for the variables tokens, types, and ttr.\n\n\n\n\n\n(a) Number of tokens\n\n\n\n\n\n(b) Number of types\n\n\n\n\n\n(c) Type-token ratio score\n\n\n\nFigure 3.3: Histograms for numeric variables tokens, types, and ttr.\n\n\nFocusing on the details captured in the histogram we are better able to detect potential outliers. Outliers can reflect valid values that are simply extreme or they can reflect something erroneous in the data. To distinguish between these two possibilities, it is important to know the context of the data. Take, for example, Figure 3.3 (c). We see that there is a bin near the value 1.0. Given that the type-token ratio is a ratio of the number of types to the number of tokens, it is unlikely that the type-token ratio would be exactly 1.0 as this would mean that every word in an essay is unique. Another, less dramatic, example is the bin to the far right of Figure 3.3 (a). In this case, the bin represents the number of tokens in an essay. An uptick in the number of essays with a large number of tokens is not surprising and would not typically be considered an outlier. On the other hand, consider the bin near the value 0 in the same plot. It is unlikely that a true essay would have 0, or near 0, words and therefore a closer look at the data is warranted.\nIt is important to recognize that outliers contribute undue influence to overall measures of central tendency and dispersion. To appreciate this, let’s consider another helpful visualization called a boxplot. A boxplot is a visual representation which aims to represent the central tendency, dispersion, and distribution of a numeric variable in one plot.\n\n\n\n\n\nFigure 3.4: Boxplot for the variable ttr.\n\n\n\nIn Figure 3.4 we see a boxplot for ttr variable. The box in the middle of the plot represents the interquartile range (IQR) which is the range of values between the first quartile and the third quartile. The solid line in the middle of the box represents the median. The lines extending from the box are called ‘whiskers’ and provide the range of values which are within 1.5 times the IQR. Values outside of this range are plotted as individual points.\nNow let’s consider boxplots from another angle. In Figure 3.5 (b) I’ve plotted the boxplot horizontally, right below the histogram in Figure 3.5 (a). In this view, we can see that a boxplot is a simplifed histogram augmented with central tendency and dispersion statistics. While histograms focus on the frequency distribution of data points, boxplots focus on the data’s quartiles and potential outliers.\n\n\n\n\n\n(a) Histogram\n\n\n\n\n\n\n\n(b) Boxplot (horizontal)\n\n\n\nFigure 3.5: Histogram and boxplot for the variable ttr.\n\n\nI’ve added a dashed line in Figure 3.5 (a) and Figure 3.5 (b) to signal the mean in this set of plots, but it is not typically included. I include the dashed line to make a point: the mean is more sensitive to outliers than the median. As I pointed out in Section 3.1.2.1, the mean is the sum of all values divided by the number of values. If there are extreme values, the mean will be pulled in the direction of the extreme values. The median, however, is the middle value and a few extreme values have less effect. So, when central tendency is reported, if there is a sizeable difference between the mean and the median, measures of dispersion will be larger and the direction of the difference can be used to infer the presence of outliers.\nReturning to outliers, it is important to address them to safeguard the accuracy of the analysis. There are two main ways to address outliers: 1) transform the data and 2) eliminate observations with outliers (trimming). Trimming is more extreme as it removes data but can be the best approach for true outliers. Transforming the data is an approach to mitigating the influence of extreme but valid values. Transformation involves applying a mathematical function to the data which changes the scale and/ or shape of the distribution, but does not remove data nor does it change the relative order of the values.\nIn Figure 3.8, we see two boxplots. Figure 3.6 (a) is the original ttr data and Figure 3.6 (b) reflects the data trimmed to remove outliers. In this case, we have removed essays with a type-token ratio of 1.\n\n\n\n\n\n(a) Type-token ratio score\n\n\n\n\n\n(b) Type-token ratio score (trimmed)\n\n\n\nFigure 3.6: Boxplots for ttr before and after trimming.\n\n\nWe can now appreciate the effect that the outliers had on the mean value of the ttr variable as the difference between the mean and median is now smaller.\n\nThe exploration the data points with histograms and boxplots has helped us to identify outliers. Now we turn to the question of the overall shape of the distribution. The key question is whether the observed distribution of each variable approximates the Normal Distribution, or not.\nThe Normal Distribution is a theoretical distribution where the values are symmetrically dispersed around the central tendency (mean/ median). In terms we can now understand, this means that the mean and median are the same. The Normal Distribution is important because many statistical tests assume that the data distribution is normal or near normal.\nStepping away from our BELC dataset, I’ve created simulated data that fit normal and non-normal, or skewed, distributions. I present each of these distributions as density plots with mean and median line overlays in Figure 3.7.\n\n\n\n\n\n(a) Left skewed distribution\n\n\n\n\n\n(b) Normal distribution\n\n\n\n\n\n(c) Right skewed distribution\n\n\n\nFigure 3.7: Mean and median for normal and skewed distributions.\n\n\nA Normal Distribution, illustrated in Figure 3.7 (b), is a distribution where the values are symmetrically dispersed around the central tendency (mean/ median). This means that in a theoretical distribution that the mean and median are the same. The Normal Distribution is also known as the Gaussian Distribution or the Bell Curve, for the hallmark bell shape of the distribution. In this distribution, extreme values are less likely than values near the center.\nA skewed distribution is not a specific type of distribution but rather a characteristic than many distributions can exhibit where the values are not symmetrically dispersed around the central tendency. A distribution in which values tend to disperse to the left of the central tendency is left skewed as in Figure 3.7 (a) and dispersion to the right is right skewed as in Figure 3.7 (c).\nData that are normally, or near-normally distributed are often analyzed using parametric tests while data that exhibit a skewed distributed are often analyzed using non-parametric tests. Divergence from normality is not a binary distinction. Rather, it is a matter of degree. A visual inspection is usually sufficient for experienced researchers to determine whether a distribution is normal or skewed. However, for those who are less experienced or if you want to be more precise, there are two primary measures which can help ascertain the degree to which a distribution is normal: skewness and kurtosis. Skewness is a measure of the degree to which a distribution is asymmetrical. Kurtosis is a measure of the degree to which a distribution is peaked.\nIn Table 3.10 I provide the skewness and kurtosis scores for our simulated distributions along with central tendency measures for context.\n\n\n\n\nTable 3.9: Skewness and kurtosis for normal and skewed distributions.\n\ndistribution\nmean\nmedian\nhistogram\nskewness\nkurtosis\n\n\n\nLeft skew\n0.746\n0.767\n▁▂▅▇▆\n-0.711\n3.27\n\n\nNormal\n0.016\n0.009\n▁▅▇▃▁\n0.065\n2.93\n\n\nRight skew\n0.254\n0.233\n▆▇▅▂▁\n0.711\n3.27\n\n\n\n\n\n\nAll things distribution are matters of degree, so there are no hard and fast rules for determining whether a distribution is normal or skewed. However, there are some general guidelines that can be used to determine the degree to which a distribution is normal or skewed, as shown in Table 3.10.\n\n\nTable 3.10: Rules of thumb for skewness and kurtosis scores.\n\n\n\n\n(a) Skewness scores\n\nScore Range\nEvaluation\n\n\n\n-0.5 to 0.5\nApproximately symmetric\n\n\n-1 to -0.5 (or) 0.5 to 1\nModerately skewed\n\n\n&lt; -1 (or) &gt; 1\nHighly skewed\n\n\n\n\n\n\n(b) Kurtosis scores\n\nScore Range\nEvaluation\n\n\n\n&lt; 3\nLess peaked than normal\n\n\nEqual to 3\nNormal peak\n\n\n&gt; 3\nMore peaked than normal\n\n\n\n\n\n\n\n\n\n\n\n\n Dive deeper\nAnother approach for visually summarizing a single numeric variable is the Empirical Cumulative Distribution Function, or ECDF. An ECDF plot is a summary of the cummulative proportion of each of the values of a numeric variable. In addition to providing insight into the distribution of a variable, ECDF plots can be useful in determing what proportion of the values fall above or below a certain percentage of the data.\n\n\n\nThe question is which type of distribution does each numeric variable in the BELC dataset fit? Comparing the variables ttr, types and prop_l2 in Figure 3.8 to the three distributions in Figure 3.7, we see that all three numeric variables in the BELC dataset are skewed to some degree.\n\n\n\n\n\n(a) Type-token ratio score\n\n\n\n\n\n(b) Number of types\n\n\n\n\n\n(c) Proportion of L2 words\n\n\n\nFigure 3.8: Histogram/ Density plots for numeric variables in the BELC dataset.\n\n\nFigure 3.8 (a) for ttr has some right skewing but not as much as types in Figure 3.8 (b). prop_l2 in Figure 3.8 (c) is the most skewed of the three variables. As mentioned earlier, skewed distributions can take many forms, some are more skewed than others.\nTo view statistics on our three variables in Figure 3.8, we can calculate the skewness and kurtosis.\n\n\n\n\nTable 3.11: Skewness and kurtosis for numeric variables in the BELC dataset.\n\ndistribution\nmean\nmedian\nhistogram\nskewness\nkurtosis\n\n\n\nttr\n0.655\n0.648\n▂▇▇▆▁\n0.319\n2.90\n\n\ntypes\n46.044\n46.000\n▅▇▇▃▂\n0.407\n2.45\n\n\ntokens\n75.338\n77.000\n▇▇▇▃▂\n0.669\n2.98\n\n\nprop_l2\n0.986\n0.990\n▁▁▂▃▇\n-1.273\n4.13\n\n\n\n\n\n\n\n\nGiven the characteristics of the numeric variables in the BELC dataset, although none of them are perfectly normal, but only prop_l2 is highly skewed. Therefore, if we intend to use these variables ‘as-is’ in statistical measures or tests, we now know whether to choose parametric or non-parametric alternatives.\nIn the case that a variable is highly skewed, it is often useful to attempt transform the variable to reduce the skewness. In contrast to scale-based transformations (e.g. centering and scaling), shape-based transformations change the scale and the shape of the distribution. The most common shape-based transformation is the logarithmic transformation. The logarithmic transformation (log-transformation) takes the log (typically base 10) of each value in a variable. The log-transformation is useful for reducing the skewness of a variable as it compresses large values and expands small values. If the skewness is due to these factors, the log-transformation can help.\nIt is important to note, however, that if scale-based transformations are to be applied to a variable, they should be applied after the log-transformation as the log of negative values is undefined.\nInterdependence\n\nWe have covered the first three of the four questions we are interested in asking in a descriptive analysis. The fourth, and last, question is whether there is mutual dependence between variables. If so, what is the directionality and how strong is the dependence? Knowing the answers to these questions will help frame our approach to analysis.\nTo assess interdependence, the number and information types of the variables under consideration are important. Let’s start by considering two variables. If we are working with two variables, we are dealing with a bivariate relationship. Given there are three informational types (categorical, ordinal, and numeric), there are six logical bivariate combinations: categorical-categorical, categorical-ordinal, categorical-numeric, ordinal-ordinal, ordinal-numeric, and numeric-numeric.\nThe directionality of a relationship will take the form of a tabular or graphic summary depending on the informational value of the variables involved. In Table 3.12, we see the appropriate summary types for each of the six bivariate combinations.\n\n\nTable 3.12: Appropriate summary types for different combinations of variable types.\n\n\n\n\n\n\n\n\nCategorical\nOrdinal\nNumeric\n\n\n\nCategorical\nContingency table\nContingency table/ Bar plot\nPivot table/ Boxplot\n\n\nOrdinal\n-\nContingency table/ Bar plot\nPivot table/ Boxplot\n\n\nNumeric\n-\n-\nScatterplot\n\n\n\n\n\nLet’s first start with the combinations that include a categorical or ordinal variable. Categorical and ordinal variables reflect measures of class-type information, with add meaningful ranks to ordinal variables. To assess a relationship with these variable types, a table is always a good place to start. When combined together, a contingency table is the appropriate table. A contingency table is a cross-tabulation of two class-type variables, basically a two-way frequency table. This means that three of the six bivariate combinations are assessed with a contingency table: categorical-categorical, categorical-ordinal, and ordinal-ordinal.\nIn Table 3.13 we see contingency tables for the categorical variable sex and ordinal variable group in the BELC dataset.\n\n\n\n\nTable 3.13: Contingency tables for categorical variable sex and ordinal variable group in the BELC dataset.\n\n\n\n\n\n\n(a) Counts\n\ngroup\nfemale\nmale\nTotal\n\n\n\nT1\n7\n9\n16\n\n\nT2\n11\n4\n15\n\n\nT3\n13\n10\n23\n\n\nT4\n9\n5\n14\n\n\nTotal\n40\n28\n68\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Percentages\n\ngroup\nfemale\nmale\nTotal\n\n\n\nT1\n43.75%\n56.25%\n100.00%\n\n\nT2\n73.33%\n26.67%\n100.00%\n\n\nT3\n56.52%\n43.48%\n100.00%\n\n\nT4\n64.29%\n35.71%\n100.00%\n\n\nTotal\n58.82%\n41.18%\n100.00%\n\n\n\n\n\n\n\n\n\n\nA contingency table may include only counts, as in Table 3.13 (a), or may include proportions or percentages in an effort to normalize the counts and make them more comparable, as in Table 3.13 (b).\nIt is sometimes helpful to visualize a contingency table as a bar plot when there are a larger number of levels in either or both of the variables. Again, looking at the relationship between sex and group, we see that we can plot the counts or the proportions. In Figure 3.9, we see both.\n\n\n\n\n\n\n(a) Counts\n\n\n\n\n\n(b) Proportions\n\n\n\nFigure 3.9: Bar plots for the relationship between sex and group in the BELC dataset.\n\n\nTo summarize and assess the relationship between a categorical or an ordinal variable and a numeric variable, we cannot use a contingency table. Instead, this type of relationship is best summarized in a table using a summary statistic in a pivot table. A pivot table is a table in which a class-type variable is used to group a numeric variable by some summary statistic appropriate for numeric variables, e.g. mean, median, standard deviation, etc.\nIn Table 3.14, we see a pivot table for the relationship between group and tokens in the BELC dataset. Specifically, we see the mean number of tokens by group.\n\n\n\n\n\n\nTable 3.14: Pivot table for the relationship between group and tokens in the BELC dataset.\n\ngroup\nmean_tokens\n\n\n\nT1\n35.4\n\n\nT2\n62.5\n\n\nT3\n85.0\n\n\nT4\n118.9\n\n\n\n\n\n\n\n\nWe see that the mean number of tokens increases from Group T1 to T4, which is consistent with the idea that the students in the higher groups are writing longer essays.\nAlthough a pivot table may be appropriate for targeted numeric summaries, a visualization is often more informative for assessing the dispersion and distribution of a numeric variable by a categorical or ordinal variable. There are two main types of visualizations for this type of relationship: a boxplot and a violin plot. A violin plot is a visualization that summarizes the distribution of a numeric variable by a categorical or ordinal variable, adding the overall shape of the distribution, much as a density plot does for histograms.\nIn Figure 3.10, we see both a boxplot and a violin plot for the relationship between group and tokens in the BELC dataset.\n\n\n\n\n\n\n(a) Boxplot\n\n\n\n\n\n(b) Violin plot\n\n\n\nFigure 3.10: Boxplot and violin plot for the relationship between group and tokens in the BELC dataset.\n\n\nFrom the boxplot in Figure 3.10 (a), we see that the general trend towards more tokens used by students in higher groups. But we can also appreciate the dispersion of the data within each group looking at the boxes and whiskers. On the surface it appears that the data for groups T1 and T3 are closer to each other than groups T2 and T4, in which there is more variability within these groups. Furthermore, we can see outliers in groups T1 and T3, but not in groups T2 and T4. From the violin plot in Figure 3.10 (b), we can see the same information, but we can also see the overall shape of the distribution of tokens within each group. In this plot, it is very clear that group T4 includes a wide range of token counts.\n\nThe last bivariate combination is numeric-numeric. To summarize this type of relationship a scatterplot is used. A scatterplot is a visualization that plots each data point as a point in a two-dimensional space, with one numeric variable on the x-axis and the other numeric variable on the y-axis. Depending on the type of relationship you are trying to assess, you may want to add a trend line to the scatterplot. A trend line is a line that summarizes the overall trend in the relationship between the two numeric variables. To assess the extent to which the relationship is linear, a straight line is drawn which minimizes the distance between the line and the points.\nIn Figure 3.11, we see a scatterplot and a scatterplot with a trend line for the relationship between ttr and types in the BELC dataset.\n\n\n\n\n\n\n(a) Points\n\n\n\n\n\n(b) Points with a linear trend line\n\n\n\nFigure 3.11: Scatter plot for the relationship between ttr and types in the BELC dataset.\n\n\nWe see that there is an apparent positive relationship between these two variables, which is consistent with the idea that as the number of types increases, the type-token ratio increases. In other words, as the number of unique words increases, so does the lexical diversity of the text. Since we are evaluating a linear relationship, we are assessing the extent to which there is a correlation between ttr and types. A correlation simply means that as the values of one variable change, the values of the other variable change in a consistent manner.\n\nOnce a sense of the directionality of a relationship can be established, the next step is to gauge the relative strength, or association. Association refers to any relationship in which there is a dependency between two variables. Quantitative measures of association, in combination with tabular and visual summaries, can provide a more complete picture of the relationship between two variables.\nThere are a number of measures of assocation, depending on the types of variables being assessed and, for numeric variables, whether the distribution is normal (parametric) or non-normal (non-parametric), as seen in Table 3.15.\n\n\nTable 3.15: Measures of association or correlation strength for different combinations of variable types.\n\n\n\n\n\n\n\n\n\nCategorical\nOrdinal\nNumericNon-parametric\n\nNumericParametric\n\n\n\n\nCategorical\nChi-square \\(\\chi^2\\), Cramér’s \\(V\\)\n\nGoodman and Kruskal’s \\(\\gamma\\)\n\nRank biserial Correlation\nPoint-biseral Correlation\n\n\nOrdinal\n-\nKendall’s \\(\\tau\\)\n\nKendall’s \\(\\tau\\)\n\nPearson’s \\(r\\)\n\n\n\n\nNumericNon-parametric\n\n-\n-\nKendall’s \\(\\tau\\)\n\nPearson’s \\(r\\)\n\n\n\n\nNumericParametric\n\n-\n-\n-\nPearson’s \\(r\\)\n\n\n\n\n\nAssociation measures often are expressed as a number between -1 and 1, where 0 indicates no association, -1 indicates a perfect negative association, and 1 indicates a perfect positive association. The closer the number is to 0, the weaker the association. The closer the number is to -1 or 1, the stronger the association. Association statistics are often accompanied by a confidence interval (CI), which is a range of values that is likely to contain the true value of the association in the population. The confidence interval is expressed as a percentage, such as 95%, which means that if we were to repeat the study 100 times, 95 of those studies would produce a confidence interval that contains the true value of the association in the population. If the range between the lower and higher bounds of the confidence interval contains 0, then the association is likely no different than chance.\nGiven these measures and interpretations, let’s consider the different types of bivariate relationships we have seen so far in the BELC dataset. The first interdependence we explored involved the categorical variable sex and the ordinal variable group. This relationship may not be of primary interest to a study on L2 writing, but it is a good example of how to assess the strength of an association between a categorical and ordinal variable. Furthermore, it could be the case that we want to assess whether we have widely unbalanced female/ male proportions in our time groups.\nUsing Table 3.15, we see that we can use Goodman and Kruskal’s \\(\\gamma\\) (gamma) to assess the strength of the association between these two variables. The measures of association in Table 3.16 suggest that the proportion of male participants is higher in group T1 and lower in group T2. However, these associations are moderately strong, as the gamma value is near \\(\\pm\\) 0.4.\n\n\n\n\n\nTable 3.16: Gamma for the relationship between sex and group in the BELC dataset.\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\n\n\n\nsex.female\ngroup.T1\n-0.381\n0.95\n-0.568\n-0.157\n\n\nsex.female\ngroup.T2\n0.389\n0.95\n0.167\n0.575\n\n\nsex.male\ngroup.T1\n0.381\n0.95\n0.157\n0.568\n\n\nsex.male\ngroup.T2\n-0.389\n0.95\n-0.575\n-0.167\n\n\n\n\n\n\n\n\nWhen paired with Figure 3.9 we can appreciate that groups T1 and T2 have contrasting proportions of females to males and that groups T3 and T4 are more closely proportioned. This observation should be considered when approaching statistical analyses in which categorical variables required (near) equal proportions of categories.\n\nNow let’s take a look at a more interesting relationship, the one between the ordinal variable group and the numeric variable tokens. Since we determined that tokens was near normally distributed, we can choose the parametric version of our association measure, Pearson’s \\(r\\). The measures of association in Table 3.17 suggest that there is a negative association between group T1 and a positive one betwen group T4 and tokens, which is consistent with the idea that as the group number increases, the number of tokens increases. These associations are moderate to strong, as the Pearson’s \\(r\\) values are near \\(\\pm\\) 0.5. However, the other groups (T2 and T3) have very weak assocations with tokens and the CI includes 0, which means that the association is likely no different than chance.\n\n\n\n\nTable 3.17: Pearson’s r for the relationship between group and tokens in the BELC dataset.\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\n\n\n\ngroup.T1\ntokens\n-0.520\n0.95\n-0.675\n-0.322\n\n\ngroup.T2\ntokens\n-0.160\n0.95\n-0.384\n0.082\n\n\ngroup.T3\ntokens\n0.161\n0.95\n-0.080\n0.385\n\n\ngroup.T4\ntokens\n0.521\n0.95\n0.322\n0.675\n\n\n\n\n\n\n\n\nThese association measures suggest that there is a relationship between group and tokens, but that the relationship is not the same for all groups. This may due to a number of factors, such as the number of participants in each group, the effect of outliers within particular levels, etc. or may simply underscore that the relationship between group and tokens is not linear. What we do with this information will depend on our research aims. Whatever the case, we can use these measures to inform our next steps, as we will see in the next section.\n\nFinally, let’s look at the relationship between the numeric variables ttr and types. Since we determined both ttr and types are normally distributed, we can choose the parametric version of our association measure, Pearson’s \\(r\\). The measures of association in Table 3.18 suggest that there is a negative association between ttr and types, which is consistent with the idea that as the number of types increases, the type-token ratio decreases. This association is strong, as Pearson’s \\(r\\) value is near 0.6.\n\n\n\n\nTable 3.18: Pearson’s r for the relationship between ttr and types in the BELC dataset.\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\n\n\nttr\ntypes\n-0.606\n0.95\n-0.738\n-0.43\n\n\n\n\n\n\n\nBefore moving on to the next section, it is important to remember than through the process of diagnostic measures, we gain a thorough understanding of our data’s characteristics and quality, preparing us for the next step in our analysis. However, remember that these measures do not exist in isolation. The decisions we make at this stage, from handling missing data to understanding the distribution of our variables, can have significant implications on our subsequent analysis. So, this initial step of data analysis deserves our careful attention and scrutiny."
  },
  {
    "objectID": "approaching-analysis.html#analytic-methods",
    "href": "approaching-analysis.html#analytic-methods",
    "title": "3  Approaching analysis",
    "section": "\n3.2 Analytic methods",
    "text": "3.2 Analytic methods\nHaving ensured that our dataset is clean, valid, and thoroughly understood, we can proceed to the next key stage of our data analysis process - employing analytic methods. The goal of analysis, generally, is to generate knowledge from information. The type of knowledge generated and the process by which it is generated, however, differ and can be broadly grouped into three analysis types: exploratory, predictive, and inferential.\nIn this section I will provide an overview of how each of these analysis types are tied to research aims and how the general purpose of each type affect: (1) how to identify the variables of interest, (2) how to interrogate these variables, and (3) how to interpret the results. I will structure the discussion of these analysis types moving from the least structured (inductive) to most structured (deductive) approach to deriving knowledge from information with the aim to provide enough information for you to identify these research approaches in the literature and to make appropriate decisions as to which approach your research should adopt.\n\n\n\n3.2.1 Exploratory data analysis (EDA)\n\nIn exploratory data analysis, we use a variety of methods to identify patterns, trends, and relations within and between variables. The goal of EDA is uncover insights in an inductive, data-driven manner. That is to say, that we do not enter into EDA with a fixed hypothesis in mind, but rather we explore intuition, probe anecdote, and follow hunches to identify patterns and relationships and to evaluate whether and why they are meaningful. We are admittedly treading new or unfamiliar terrain letting the data guide our analysis. This means that we can use and reuse the same data to explore different angles and approaches adjusting our methods and measures as we go. In this way, EDA is an iterative, meaning generating process.\n\nIn line with the investigative nature of EDA, the identification of variables of interest is a discovery process. We most likely have a intution about the variables we would like to explore, but we are able to adjust our variables as need be to suit our research aims. When the identification and selection of variables is open, the process is known as feature engineering. A process that is much an art as a science, feature engineering leverages a mixture of relevant domain knowledge, intuition, and trial and error to identify features that serve to best represent the data and to best serve the research aims. Furthermore, the roles of features in EDA are fluid –no variable has a special status. We will see that in other types of analysis, some or all the roles of the variables are fixed.\n\n\n\n\nFigure 3.12: Roles of variables in EDA.\n\n\n\nFor illustrative purposes let’s consider the State of the Union Corpus (SOTU) (Benoit 2020). The presidential addresses and a set of metadata variables are included in the corpus. I’ve subsetted this corpus to only include U.S. presidents since 1946. A tabular preview of the first 10 addresses (truncated for display) can be found in Table 3.19.\n\n\nChange to data dictionary view?\n\n\n\n\n\nTable 3.19: First ten addresses from the SOTU Corpus.\n\n\n\n\n\n\n\n\npresident\ndate\ndelivery\nparty\naddresses\n\n\n\nTruman\n1946-01-21\nwritten\nDemocratic\nTo the Congress of the United States: A quarter…\n\n\nTruman\n1947-01-06\nspoken\nDemocratic\nMr. President, Mr. Speaker, Members of the Cong…\n\n\nTruman\n1948-01-07\nspoken\nDemocratic\nMr. President, Mr. Speaker, and Members of the …\n\n\nTruman\n1949-01-05\nspoken\nDemocratic\nMr. President, Mr. Speaker, Members of the Cong…\n\n\nTruman\n1950-01-04\nspoken\nDemocratic\nMr. President, Mr. Speaker, Members of the Cong…\n\n\nTruman\n1951-01-08\nspoken\nDemocratic\nMr. President, Mr. Speaker, Members of the Cong…\n\n\nTruman\n1952-01-09\nspoken\nDemocratic\nMr. President, Mr. Speaker, Members of the Cong…\n\n\nTruman\n1953-01-07\nwritten\nDemocratic\nTo the Congress of the United States: I have th…\n\n\nEisenhower\n1953-02-02\nspoken\nRepublican\nMr. President, Mr. Speaker, Members of the Eigh…\n\n\nEisenhower\n1954-01-07\nspoken\nRepublican\nMr. President, Mr. Speaker, Members of the Eigh…\n\n\n\n\n\n\nA dataset such as this one could serve as a starting point to explore many different types of research questions. In order to maintain research coherence so our efforts to not careen into a free-for-all, we need to tether our feature engineering to a unit of analysis that is relevant to the research question. A unit of analysis is the entity that we are interested in studying. Not to be confused with the unit of observation, which is the entity that we are able to observe and measure.\nFor example, in the SOTU dataset, the the unit of analysis could be the language of particular presidents, party ideology, or political rhetoric in general and the unit of observation could be individual words, phrases, sentences, etc. In some cases the unit of analysis and the unit of observation are the same. For example, if we were interested in potential changes use of the word “terrorist” over time in SOTU addresses, the unit of analysis and the unit of observation would be the same –individual addresses. So, depending on the perspective we are interested in investigating, the choice of how to approach engineering features to gain insight will vary.\n\nBy the same token, approaches for interrogating the dataset can differ significantly, between research projects and within the same project, but for instructive purposes, let’s draw a distinction between descriptive methods and unsupervised learning methods, as seen in Table 3.20.\n\n\n\n\nTable 3.20: Some common EDA methods\n\nDescriptive methods\nUnsupervised learning methods\n\n\n\nFrequency analysis\nCluster analysis\n\n\nKeyness analysis\nTopic Modeling\n\n\nCo-occurence analysis\nVector Space Models\n\n\n\n\n\n\n\n\nThe first group, descriptive methods can be seen as a (more robust) extenstion of the descriptive statistics covered earlier in this chapter including statistic, tabular, and visual techniques. For example, a frequency analysis of the SOTU dataset could be used to identify the most common words used by U.S. political parties in their addresses, in Figure 3.13 (a), or a co-occurence analysis could be used to identify the most common words the appear after the term “free”, in Figure 3.13 (b), in the dataset.\n\n\n\n\n\n\n(a) Frequency analysis of the 20 most frequent terms by party.\n\n\n\n\n\n(b) Co-occurence analysis of the terms that appear after the term ‘free’.\n\n\n\nFigure 3.13: Example of descriptive methods applied to the SOTU dataset.\n\n\nThe second group, unsupervised learning is a subtype of machine learning in which an algorithm is used to find patterns within and between variables in the data without any guidance (supervision). In this way, the algorithm, or machine learner, is left to make connections and associations wherever they may appear in the input data. If we were interested in finding word-use continuities and discontinuities between presidents, we could use a clustering algorithm, seen in Figure 3.14 (a). Or if we wanted to uncover themes … [ADD: modify plot] we could use a vector space model, as in Figure 3.14 (b).\n\n\n\n\n\n\n(a) Hierarchical clustering of the SOTU corpus.\n\n\n\n\n\n(b) Word embedding space in the SOTU corpus.\n\n\n\nFigure 3.14: Example of unsupervised learning methods applied to the SOTU dataset.\n\n\n\nEither through descriptive, unsupervised learning methods, or a combination of both, EDA employs quantitative methods to summarize, reduce, and sort complex datasets in order to provide the researcher novel perspective to be qualitatively assessed. These qualitative assessments may prove useful to spur identify new variables of interest, to generate new predictions, or to test new hypotheses.\nIn contrast to the other analysis types we will cover, EDA makes no claims about what relationship(s) to explore, what variables to include, or what series methods to employ. This provides the researcher with a great deal of flexibility to explore a research question, but also requires a great deal of care and attention to ensure that the results of EDA are not over-interpreted. In other words, EDA is a powerful tool for generating hypotheses, but it is not a tool for testing hypotheses and generalizing conclusions to the sample population.\n\n3.2.2 Predictive data analysis (PDA)\n\nPredictive Data Analysis (PDA) employs a variety of techniques to examine and evaluate the association strength between a variable or set of variables, with a specific focus on predicting a target variable. The aim of PDA is to construct models that can accurately forecast future outcomes, using either data-driven or theory-driven approaches. In this process, supervised learning methods, where the machine learning algorithm is guided (supervised) by a target outcome variable, are used. This means we don’t begin PDA with a completely open-ended exploration, but rather with an objective - accurate predictions. However, the path to achieving this objective can be flexible, allowing us freedom to adjust our models and methods. Unlike EDA, where the entire dataset can be reused for different approaches, PDA requires a portion of the data to be reserved for evaluation, safeguarding the validity of our predictive models. Thus, PDA is an iterative process that combines the flexibility of exploratory analysis with the rigor of confirmatory analysis.\n\nThere are two types of variables in PDA: the outcome variable and the predictor variables, or features. The outcome variable is the variable that the researcher is trying to predict. It is the only variable that is necessarily fixed as part of the research question. The features are the variables that are used to predict the outcome variable. Feature selection can be either data-driven or theory-driven. Data-driven features are those that are engineered to enhance predictive power, while theory-driven features are those that are selected based on theoretical relevance.\n\n\n\n\nFigure 3.15: Roles of variables in PDA.\n\n\n\nLet’s consider the Europarl corpus of native, non-native and translated texts (ENNTT) (Nisioi et al. 23-28, 2016-05). This is a monolingual English corpus of translated and non-translated texts from the European Parliament.\n\n\n\n\nTable 3.21: Data dictionary of the ENNTT corpus.\n\nvariable\nname\nvariable_type\ndescription\n\n\n\nsession_id\nSession ID\ncategorical\nUnique identifier for each session\n\n\nseq_speaker_id\nSequential Speaker ID\nordinal\nUnique numeric identifier for each speaker\n\n\nstate\nState\ncategorical\nCountry of the session speaker\n\n\nlanguage\nLanguage\ncategorical\nOriginal language in which the sentence was uttered\n\n\ntype\nType\ncategorical\nCategory of the speaker: natives, nonnatives, or translations\n\n\ntext\nText\ncategorical\nText spoken in the session\n\n\n\n\n\n\n\n\nNow depending on our research question, we will have a different outcome variable. If we want to examine the potential linguistic differences between native and non-native speakers, we will select our outcome variable to be the utterance type (natives/ nonnatives). The features selected to use to predict utterance type depend on our research question. If our research is guided by data, we will choose features that are specifically designed to boost the ability to predict. On the other hand, if our research is steered by theory, we will opt for features that are chosen due to their theoretical significance. In either case, the original dataset will likely need to be transformed.\nThe approach to interrogating the dataset includes three main steps: feature engineering, model selection, and model evaluation. We’ve discussed feature engineering, so what is model selection and model evaluation? And how do we go about performing these steps?\nModel selection is the process of choosing the combination of features and machine learning algorithm that produces the best prediction accuracy for the outcome variable. To refine our approach such that we arrive at the best combination, we need to train our machine learner on a variety of combinations and evaluate the accuracy of each. We don’t want to train and evaluate on the same data, as this would be cheating, and likely would not produce a model that generalizes well to new data. Instead, we need to split our data into two sets: a training set and a test set. The training set is used to train the machine learner, while the test set is used to evaluate the accuracy of the model. Model evaluation is the process of assessing the accuracy of the model on the test set, which is a proxy for how well the model will generalize to new data.\n\nThe elephant in the room is, what type of machine learning algorithm do I use? Well, there are many different types of machine learning algorithms, each with their own strengths and weaknesses. The first rough cut is to decide what type of outcome variable we are predicting: categorical or numeric. If the outcome variable is categorical, we are performing a classification task, and if the outcome variable is numeric, we are performing a regression task. As we see in Table 3.22, there are various algorithms that can be used for each task.\n\n\nTable 3.22: Some common supervised learning algorithms used in PDA.\n\n\n\n\n\n\nClassification\nRegression\nLearner type\n\n\n\nLogistic Regression\nLinear Regression\nInterpretable\n\n\nDecision Tree\nRegression Tree\nInterpretable\n\n\nSupport Vector Machine\nSupport Vector Regression\nBlack box\n\n\nMultilayer Perceptron\nMultilayer Perceptron\nBlack box\n\n\n\n\nI’ve included a column in Table 3.22 that charaterizes a second consideration which is whether we want an interpretable model or a black box model. When talking about whether a model is interpretable or not, we are not referring to the evaluation of the model. Rather, we are referring to the inner workings of the model that allow us to understand how the model is making its predictions. An interpretable model is one that can be understood and explored by humans, while a black box model is one whose inner workings are not trivial to understand. The advantage of an interpretable model is that it researchers can go beyond prediction accuracy and probe theory-driven associations. On the other hand, if the goal is to simply boost prediction accuracy, having access to a black box model that outperforms other models is more desirable.\nFinally, there are a number of algorithm-specific strengths and weaknesses that should be considered in the process of model selection. These hinge on characteristics of the data, such as the size of the dataset, the number of features, the type of features, and the expected type of relationships between features or on computing resources, such as the amount of time available to train the model or the amount of memory available to store the model.\n\nModel evaluation is performed quantitatively by calculating the accuracy of the model on the training, and ultimately, the test set. The accuracy of a model is calculated by comparing the predicted values to the actual values. For the results of classification tasks, this results in a contingency table, known as a confusion matrix. A confusion matrix juxtaposes predicted and actual values allowing various metrics to be calculated, for example in Table 3.23. Since regression tasks predict numeric values, the accuracy of the model is calculated by comparing the difference between the predicted and actual values. Whether the accuracy metrics are good is to some degree qualitative judgement. For example, classification accuracy overall may be relatively high, but the model may be performing poorly on one of the classes. In this case, the model may not be useful for the task at hand, despite the overall accuracy.\n\n\nTable 3.23: Confusion matrix for the utterance type classification task.\n\n\n\n\n\n\n\nPredicted: natives\nPredicted: nonnatives\n\n\n\nActual: natives\n26294 (90% of 29215)\n2921 (10% of 29215)\n\n\nActual: nonnatives\n730 (10% of 7304)\n6574 (90% of 7304)\n\n\n\n\nIn the end, PDA offers a versitle path to discover data-driven insights, to probe theory-driven associations, or even simply to perform tasks that are too complex or time-consuming for humans to perform.\n\n3.2.3 Inferential data analysis (IDA)\nInferential data analysis (IDA) aims to explain relationships between variables and the population from which the sample was drawn. IDA is a … method that tests a relationship between features. Rigorous statistical assumptions and data selection requirements aim to ensure that the findings are generalizable to the population from which the sample was drawn.\n\n\n\n\n\n\nFigure 3.16: Roles of variables in IDA."
  },
  {
    "objectID": "approaching-analysis.html#evaluation-strategies",
    "href": "approaching-analysis.html#evaluation-strategies",
    "title": "3  Approaching analysis",
    "section": "\n3.3 Evaluation strategies",
    "text": "3.3 Evaluation strategies\nInterpret your data to assess the quality, relevance, and usefulness of the results in relation to your objectives.\n\n3.3.1 Qualitative leaning\nEDA (Exploratory Data Analysis) often involves a mix of qualitative and quantitative evaluation methods. While visualizations and manual inspection of the data can be considered qualitative, there are also quantitative measures like correlation coefficients, summary statistics, and distribution metrics that can be used in EDA.\n\nsummaries/ visualizations\n\ndistributions\n\n\nconnections/ relationships\n\nsimilarity/ dissimilarity\n\n\n\n3.3.2 Quantitative leaning\nPDA (Predictive Data Analysis) typically focuses on quantitative evaluation methods, such as accuracy, precision, recall, F1-score, and other performance metrics for predictive models. However, qualitative evaluation can also play a role in understanding the model’s behavior, interpreting results, and identifying potential biases or limitations.\nIDA (Inferential Data Analysis) usually relies on quantitative evaluation methods, such as hypothesis testing, confidence intervals, and effect sizes, to draw conclusions about populations based on samples. Nonetheless, qualitative evaluation can be useful in interpreting the results and providing context for the findings.\n\nmodel evaluation\n\nBootstrapping, cross-validation\nConfidence intervals\n\n\nmodel performance\n\nAccuracy, precision, recall, F1-score/ RMSE, MAE, MSE\nEffect sizes\n\n\n\nWhile EDA may lean more towards qualitative evaluation and PDA and IDA tend to focus on quantitative evaluation, all three types of analysis can benefit from a combination of both qualitative and quantitative evaluation methods."
  },
  {
    "objectID": "approaching-analysis.html#communicate-results",
    "href": "approaching-analysis.html#communicate-results",
    "title": "3  Approaching analysis",
    "section": "\n3.4 Communicate results",
    "text": "3.4 Communicate results\nCommunicate your data to share your findings with others. Whether part of a journal, blog, presentation, report, article, book, etc. it is important to document your analysis process and results in a way that is reproducible and transparent.\n\nDocument your process\n\nCreate a codebook. This documents the key decisions you made in your analysis, including the variables you used, how you transformed them, and any other important information about your data.\nShare your code. This allows others to reproduce your analysis and verify your results.\nComment your code. This helps others understand your code and your thought process.\n\n\nDocument your results\n\nCreate a report. This allows you to share your findings with others.\n\nA report will contain a narrative of your analysis, including the research question, the data you used, the methods you applied, and the results you obtained.\n\n\n\n\n\n\n\n\n\n\n## Descriptive assessment {#sec-aa-descriptive}\n\n\n\n\n\nA descriptive assessment of the dataset includes a set of diagnostic measures and tabular and visual summaries which provide researchers a better understanding of the structure of a dataset, prepare the researcher to make decisions about which statistical methods and/ or tests are most appropriate, and to safeguard against false assumptions (missing data, data distributions, etc.).\n\n\n\n\n\n::: {.cell layout-align=“center” hash=‘approaching-analysis_cache/html/tbdbr-dataset-belc-compositions_d86904a92b502462a9bdf6fcbbdbbef1’}\n\n\n:::\n\n\n::: {.cell layout-align=“center” hash=‘approaching-analysis_cache/html/read-tbdbr-dataset-belc-compositions_0439d9a51694ba6ddddc6806a781cdf5’}\n\n\n:::\n\n\nTo ground this discussion I will introduce a new dataset. This dataset is drawn from the Barcelona English Language Corpus (BELC), which is found in the TalkBank repository. I’ve selected the “Written composition” task from this corpus which contains writing samples from second language learners of English at different ages. Participants were given the task of writing for 15 minutes on the topic of “Me: my past, present and future”. Data was collected for many (but not all) participants up to four times over the course of seven years. In … I’ve included the first 10 observations from the dataset which reflects structural and transformational steps I’ve done so we start with a tidy dataset.\n\n\n::: {#tbl-belc-overview .cell layout-align=“center” tbl-cap=‘First 10 observations of the BELC dataset for demonstration.’ hash=‘approaching-analysis_cache/html/tbl-belc-overview_45c8e00fdb9861f853f34c2fda4f25c2’} ::: {.cell-output-display} |participant_id |age_group |sex | num_tokens| num_types| ttr| |:————–|:————|:——|———-:|———:|—–:| |L02 |10-year-olds |female | 48| 12| 0.250| |L05 |10-year-olds |female | 72| 15| 0.208| |L10 |10-year-olds |female | 144| 26| 0.181| |L11 |10-year-olds |female | 40| 8| 0.200| |L12 |10-year-olds |female | 164| 23| 0.140| |L16 |10-year-olds |female | 52| 12| 0.231| |L22 |10-year-olds |female | 188| 30| 0.160| |L27 |10-year-olds |female | 32| 8| 0.250| |L28 |10-year-olds |female | 336| 34| 0.101| |L29 |10-year-olds |female | 212| 34| 0.160| ::: :::\n\n\nThe entire dataset includes 79 observations from 36 participants. Each observation in the BELC dataset corresponds to an individual learner’s composition. It includes which participant wrote the composition (participant_id), the age group they were part of at the time (age_group), their sex (sex), and the number of English words they produced (num_tokens), the number of unique English words they produced (num_types). The final variable (ttr) is the calculated ratio of number of unique words (num_types) to total words (num_tokens) for each composition. This is known as the Type-Token Ratio and it is a standard metric for measuring lexical diversity.\n\n\nIn most cases it is preferred to use the highest informational of a variable. Simplifying data results in a loss of information –which will result in a loss of information and hence statistical power which may lead to results that obscure meaningful patterns in the data (Baayen 2004).\n\n\n3.4.1 Summaries\nIt is always key to gain insight into shape of the information through numeric, tabular and/ or visual summaries before jumping in to analytic statistical approaches. The most appropriate form of summarizing information will depend on the number and informational value(s) of our target variables. To get a sense of how this looks, let’s continue to work with the BELC dataset and pose different questions to the data with an eye towards seeing how various combinations of variables are descriptively explored.\nSingle variables\n\nThe way to statistically summarize a variable into a single measure is to derive a measure of central tendency. For a continuous variable the most common measure is the (arithmetic) mean, or average, which is simply the sum of all the values divided by the number of values. As a measure of central tendency, however, the mean can be less-than-reliable as it is sensitive to outliers which is to say that data points in the variable that are extreme relative to the overall distribution of the other values in the variable affect the value of the mean depending on how extreme the deviate. One way to assess the effects of outliers is to calculate a measure of dispersion. The most common of these is the standard deviation which estimates the average amount of variability between the values in a continuous variable. Another way to assess, or rather side-step, outliers is to calculate another measure of central tendency, the median. A median is calculated by sorting all the values in the variable and then selecting the value which falls in the middle of all the other values. A median is less sensitive to outliers as extreme values (if there are few) only indirectly affect the selection of the middle value. Another measure of dispersion is to calculate quantiles. A quantile slices the data in four percentile ranges providing a five value numeric summary of the spread of the values in a continuous variable. The spread between the first and third quantile is known as the Interquartile Range (IQR) and is also used as a single statistic to summarize variability between values in a continuous variable.\nBelow is a list of central tendency and dispersion scores for the continuous variables in the BELC dataset Table 3.24.\n\n\n\n\nTable 3.24: Central tendency and dispersion measures for the continuous variables in the BELC dataset.\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nmean\nsd\np0\np25\np50\np75\np100\niqr\n\n\n\nnum_tokens\n264.911\n175.611\n4.000\n116.000\n220.00\n360.000\n740.00\n244.000\n\n\nnum_types\n40.253\n22.801\n1.000\n22.000\n38.00\n54.000\n97.00\n32.000\n\n\nttr\n0.167\n0.032\n0.101\n0.144\n0.16\n0.182\n0.25\n0.039\n\n\n\n\n\n\nIn the above summary, we see the mean, standard deviation (sd), and the quantiles (the five-number summary, p0, p25, p50, p75, and p100). The middle quantile (p50) is the median and the IQR is listed last.\nThese are important measures for assessing the central tendency and dispersion and will be useful for reporting purposes, but to get a better feel of how a variable is distributed, nothing beats a visual summary. A boxplot graphically summarizes many of these metrics. In Figure 3.17 we see the same three continuous variables, but now in graphical form.\n\n\n\n\nFigure 3.17: Boxplots for each of the continuous variables in the BELC dataset.\n\n\n\nIn a boxplot, the bold line is the median. The surrounding box around the median is the interquantile range. The extending lines above and below the IQR mark the largest and lowest value that is within 1.5 times either the 3rd (top of the box) or 1st (bottom of the box). Any values that fall outside, above or below, the extending lines are considered statistical outliers and are marked as dots. 1\nBoxplots provide a robust and visually intuitive way of assessing central tendency and variability in a continuous variable but this type of plot can be complemented by looking at the overall distribution of the values in terms of their frequencies. A histogram provides a visualization of the frequency (and density in this case with the blue overlay) of the values across a continuous variable binned at regular intervals.\nIn Figure 3.18 I’ve plotted histograms in the top row and density plots in the bottom row for the same three continuous variables from the BELC dataset.\n\n\n\n\nFigure 3.18: Histograms and density plots for the continuous variables in the BELC dataset.\n\n\n\n\n\n\n\nFigure 3.19: Histogram and boxplot for a simulated normal distribution.\n\n\n\n\n\n\n\n\n\nHistograms and Boxplots\nA histogram is a graphical representation of the frequency distribution of a dataset. It is created by dividing the data into intervals or “bins” and counting the number of data points that fall into each bin. The height of the bars in a histogram represents the frequency or count of data points in each bin. The shape of the histogram can give us insights into the distribution’s central tendency, spread, and skewness.\nA boxplot, on the other hand, is a graphical representation of the five-number summary of a dataset: the minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum. It provides a visual summary of the dataset’s central tendency, dispersion, and potential outliers. The box in a boxplot represents the interquartile range (IQR), which contains the middle 50% of the data. The whiskers extend from the box to the minimum and maximum values within 1.5 * IQR, and any data points beyond the whiskers are considered potential outliers.\nRelating histograms and boxplots:\n\nBoth histograms and boxplots provide information about the central tendency and spread of the data. The peak(s) in a histogram can give us a sense of where the data is centered, similar to the median in a boxplot. The width of the histogram can indicate the spread of the data, akin to the IQR in a boxplot.\nThe shape of a histogram can provide insights into the skewness of a dataset. A histogram that is symmetric with a single peak can correspond to a roughly symmetric boxplot. A positively skewed histogram (with a long tail to the right) can correspond to a boxplot where Q3 - Q2 &gt; Q2 - Q1, and a negatively skewed histogram (with a long tail to the left) can correspond to a boxplot where Q2 - Q1 &gt; Q3 - Q2.\nWhile histograms focus on the frequency distribution of data points, boxplots focus on the data’s quartiles and potential outliers. Both types of plots can be used in tandem to get a more comprehensive understanding of a dataset’s distribution.\n\nIn summary, histograms and boxplots are related in that they both offer graphical representations of data distributions. They provide different perspectives on the data’s central tendency, spread, and skewness, and when used together, they can offer a more complete understanding of a dataset.\n\n\n\nHistograms provide insight into the distribution of the data. For our three continuous variables, the distributions happen not to be too strikingly distinct. They are, however, not the same either. When we explore continuous variables with histograms we are often trying to assess whether there is skew or not. There are three general types of skew, visualized in Figure 3.20.\n\n\n\n\nFigure 3.20: Examples of skew types in density plots.\n\n\n\n\n\n\n\nFigure 3.21: Examples of skew types in density plots.\n\n\n\nIn histograms/ density plots in which the distribution is either left or right, the median and the mean are not aligned. The mode, which indicates the most frequent value in the variable is also not aligned with the other two measures. In a left-skewed distribution the mean will be to the left of the median which is left of the mode whereas in a right-skewed distribution the opposite occurs. In a distribution with absolutely no skew these three measures are the same. In practice these measures rarely align perfectly but it is very typical for these three measures to approximate alignment. It is common enough that this distribution is called the Normal Distribution 2 as it is very common in real-world data.\nAnother and potentially more informative way to inspect the normality of a distribution is to create Quantile-Quantile plots (QQ Plot). In Figure 3.22 I’ve created QQ plots for our three continuous variables. The line in each plot is the normal distribution and the more points that fall off of this line, the less likely that the distribution is normal.\n\n\n\n\nFigure 3.22: QQ Plots for the continuous variables in the BELC dataset.\n\n\n\nA visual inspection can often be enough to detect non-normality, but in cases which visually approximate the normal distribution (such as these) we can perform the Shapiro-Wilk test of normality. This is an inferential test that compares a variable’s distribution to the normal distribution. The likelihood that the distribution differs from the normal distribution is reflected in a \\(p\\)-value. A \\(p\\)-value below the .05 threshold suggests the distribution is non-normal. In Table 3.25 we see that given this criterion only the distribution of num_types is normally distributed.\n\n\n\n\nTable 3.25: Results from Shapiro-Wilk test of normality for continuous variables in the BELC dataset.\n\nvariable\nstatistic\np_value\n\n\n\nNumber of tokens\n0.942\n0.001\n\n\nNumber of types\n0.970\n0.058\n\n\nType-Token Ratio\n0.948\n0.003\n\n\n\n\n\n\nDownstream in the analytic analysis, the distribution of continuous variables will need to be taken into account for certain statistical tests. Tests that assume ‘normality’ are parametric tests, those that do not are non-parametric. Distributions which approximate the normal distribution can sometimes be transformed to conform to the normal distribution either by outlier trimming or through statistical procedures (e.g. square root, log, or inverse transformation), if necessary. At this stage, however, the most important thing is to recognize whether the distributions approximate or wildly diverge from the normal distribution.\nBefore we leave continuous variables, let’s consider another approach for visually summarizing a single continuous variable. The Empirical Cumulative Distribution Function, or ECDF, is a summary of the cumulative proportion of each of the values of a continuous variable over the domain of possible values. An ECDF plot can be useful in determining what proportion of the values fall above or below a certain percentage of the data.\nIn Figure 3.23 we see ECDF plots for our three continuous variables.\n\n\n\n\nFigure 3.23: ECDF plots for the continuous variables in the BELC dataset.\n\n\n\nTake, for example, the number of tokens (num_tokens) per composition. The ECDF plot tells us that 50% of the values in this variable are 56 words or less. In the three variables plotted, the cumulative growth is quite steady. In some cases it is not. When it is not, an ECDF goes a long way to provide us a glimpse into key bends in the proportions of values in a variable.\nNow let’s turn to the descriptive assessment of categorical variables. For categorical variables, central tendency can be calculated as well but only a subset of measures given the reduced informational value of categorical variables. For nominal variables where there is no relationship between the levels the central tendency is simply the mode. The levels of ordinal variables, however, are relational and therefore the median, in addition to the mode, can also be used as a measure of central tendency. Note that a variable with one mode is unimodal, two modes, bimmodal, and in variables that have two or more modes multimodal.\nBelow is a list of the central tendency metrics for the categorical variables in the BELC dataset.\n\n\nVariable type: factor\n\n\nskim_variable\nordered\nn_unique\ntop_counts\n\n\n\nparticipant_id\nFALSE\n36\nL05: 3, L10: 3, L11: 3, L12: 3\n\n\nage_group\nTRUE\n4\n10-: 24, 16-: 24, 12-: 16, 17-: 15\n\n\nsex\nFALSE\n2\nfem: 48, mal: 31\n\n\n\n\n\nIn practice when a categorical variable has few levels it is common to simply summarize the counts of each level in a table to get an overview of the variable. With ordinal variables with more numerous levels, the five-score summary (quantiles) can be useful to summarize the distribution. In contrast to continuous variables where a graphical representation is very helpful to get perspective on the shape of the distribution of the values, the exploration of single categorical variables is rarely enhanced by plots.\nMultiple variables\nIn addition to the single variable summaries (univariate), it is very useful to understand how two (bivariate) or more variables (multivariate) are related to add to our understanding of the shape of the relationships in the dataset. Just as with univariate summaries, the informational values of the variables frame our approach.\nTo explore the relationship between two continuous variables we can statistically summarize a relationship with a coefficient of correlation which is a measure of effect size between continuous variables. If the continuous variables approximate the normal distribution Pearson’s r is used, if not Kendall’s tau is the appropriate measure. A correlation coefficient ranges from -1 to 1 where 0 is no correlation and -1 or 1 is perfect correlation (either negative or positive). Let’s assess the correlation coefficient for the variables num_tokens and ttr. Since these variables are not normally distributed, we use Kendall’s tau. Using this measure the correlation coefficient is \\(-0.565\\) suggesting there is a correlation, but not a particularly strong one.\nCorrelation measures are important for reporting but to really appreciate a relationship it is best to graphically represent the variables in a scatterplot. In Figure 3.24 we see the relationship between num_tokens and ttr.\n\n\n\n\nFigure 3.24: Scatterplot…\n\n\n\nIn both plots ttr is on the y-axis and num_tokens on the x-axis. The points correspond to the intersection between these variables for each single observation. In the left pane only the points are represented. Visually (and given the correlation coefficient) we can see that there is a negative relationship between the number of tokens and the Type-Token ratio: in other words, the more tokens a composition has the lower the Type-Token Ratio. In this case this trend is quite apparent, but in other cases is may not be. To provide an additional visual cue a trend line is often added to a scatterplot. In the right pane I’ve added a linear trend line. This line demarcates the optimal central tendency across the relationship, assuming a linear relationship. The steeper the line, or slope, the more likely the correlation is strong. The band, or ribbon, around this trend line indicates the confidence interval which means that real central tendency could fall anywhere within this space. The wider the ribbon, the larger the variation between the observations. In this case we see that the ribbon widens when the number of tokens is either low or high. This means that the trend line could be potentially be drawn either steeper (more strongly correlated) or flatter (less strongly correlated).\n\n\n\n\n\n\n Tip\nIn plots comparing two or more variables, the choice of which variable to plot on the x- and y-axis is contingent on the research question and/ or the statistical approach. The language varies between statistical approaches: in inferential methods the x-axis is used to plot what is known as the dependent variable and the y-axis an independent variable. In predictive methods the dependent variable is known as the outcome and the independent variable a predictor. Exploratory methods do not draw distinctions between variables along these lines so the choice between which variable to plot along the x- and y-axis is often arbitrary.\n\n\n\nLet’s add another variable to the mix, in this case the categorical variable sex, taking our bivariate exploration to a multivariate exploration. Again each point corresponds to an observation where the values for num_tokens and ttr intersect. But now each of these points is given a color that reflects which level of sex it is associated with.\n\n\n\n\nFigure 3.25: Scatterplot visualizing the relationship between num_tokens and ttr.\n\n\n\nIn this multivariate case, the scatterplot without the trend line is more difficult to interpret. The trend lines for the levels of sex help visually understand the variation of the relationship of num_tokensand ttr much better. But it is important to note that when there are multiple trend lines there is more than one slope to evaluate. The correlation coefficient can be calculated for each level of sex (i.e. ‘male’ and ‘female’) independently but the relationship between the each slope can be visually inspected and provide important information regarding each level’s relative distribution. If the trend lines are parallel (ignoring the ribbons for the moment), as it appears in this case, this suggests that the relationship between the continuous variables is stable across the levels of the categorical variable, with males showing more lexical diversity than females declining at a similar rate. If the lines were to cross, or suggest that they would cross at some point, then there would be a potentially important difference between the levels of the categorical variable (known as an interaction). Now let’s consider the meaning of the ribbons. Since the ribbons reflect the range in which the real trend line could fall, and these ribbons overlap, the differences between the levels of our categorical variable are likely not distinct. So at a descriptive level, this visual summary would suggest that there are no differences between the relationship between num_tokens and ttr for the distinct levels of sex.\nCharacterizing the relationship between two continuous variables, as we have seen is either performed through a correlation coefficient metric or visually. The approach for summarizing a bivariate relationship which combines a continuous and categorical variable is distinct. Since a categorical variable is by definition a class-oriented variable, a descriptive evaluation can include a tabular representation, with some type of summary statistic. For example, if we consider the relationship between num_tokens and age_group we can calculate the mean for num_tokens for each level of age_group. To provide a metric of dispersion we can include either the standard error of the mean (SEM) and/ or the confidence interval (CI).\n\n\n\n\nTable 3.26: Summary table for tokens by age_group.\n\nage_group\nmean_num_tokens\nsem\nci\n\n\n\n10-year-olds\n111\n14.8\n24.3\n\n\n12-year-olds\n230\n28.5\n46.9\n\n\n16-year-olds\n327\n24.6\n40.4\n\n\n17-year-olds\n450\n51.9\n85.4\n\n\n\n\n\n\nThe SEM is a metric which summarizes variation based on the number of values and the CI, as we have seen, summarizes the potential range of in which the mean may fall given a likelihood criterion (usually the same as the \\(p\\)-value, .05).\nBecause we are assessing a categorical variable in combination with a continuous variable a table is an available visual summary. But as I have said before, a graphic summary is hard to beat. In the following figure (Figure 3.26) a barplot is provided which includes the means of num_tokens for each level of age_group. The overlaid bars represent the confidence interval for each mean score.\n\n\n\n\nFigure 3.26: Barplot comparing the mean num_tokens by age_group from the BELC dataset.\n\n\n\nWhen CI ranges overlap, just as with ribbons in scatterplots, the likelihood that the differences between levels are ‘real’ is diminished.\nTo gauge the effect size of this relationship we can use Kendall’s τ for rank-based coefficients. The score is 0.708 indicating that the relationship between age_group and num_tokens is quite strong. 3\nNow, if we want to explore a multivariate relationship and add sex to the current descriptive summary, we can create a summary table, but let’s jump straight to a barplot.\n\n\n\n\nFigure 3.27: Barplot comparing the mean num_tokens by age_group and sex from the BELC dataset.\n\n\n\nWe see in Figure 3.27 that on the whole, the appears to be general trend towards more tokens in a composition for more advanced learner levels. However, the non-overlap in CI bars for the ‘12-year-olds’ for the levels of sex (‘male’ and ‘female’) suggest that 12-year-old females may produce more tokens per composition than males –a potential divergence from the overall trend.\nBarplots are a familiar and common visualization for summaries of continuous variables across levels of categorical variables, but a boxplot is another useful visualization of this type of relationship.\n\n\n\n\nFigure 3.28: Boxplot of the relationship between age_group and num_tokens from the BELC dataset.\n\n\n\nAs seen when summarizing single continuous variables, boxplots provide a rich set of information concerning the distribution of a continuous variable. In this case we can visually compare the continuous variable num_tokens with the categorical variable age_group. The plot in the right pane includes ‘notches’. Notches represent the confidence interval, in boxplots this interval surrounds the median. When compared horizontally across levels of a categorical variable the overlap of notched spaces suggest that the true median may be within the same range. Additionally, when the confidence interval goes outside the interquantile range (the box) the notches hinge back to the either the 1st (lower) or the 3rd (higher) IQR range and suggests that the variability is high.\nWe can also add a third variable to our exploration. As in the barplot in Figure 3.27, the boxplot in Figure 3.29 suggests that there is an overall trend towards more tokens per composition as a learner advances in experience, except at the ‘12-year-old’ level where there appears to be a difference between ‘males’ and ‘females’.\n\n\n\n\nFigure 3.29: Boxplot of the relationship between age_group, num_tokens and sex from the BELC dataset.\n\n\n\nUp to this point in our exploration of multiple variables we have always included at least one continuous variable. The central tendency for continuous variables can be summarized in multiple ways (mean, median, and mode) and when calculating means and medians, measures of dispersion are also provide helpful information summarize variability. When working with categorical variables, however, measures of central tendency and dispersion are more limited. For ordinal variables central tendency can be summarized by the median or mode and dispersion can be assessed with an interquantile range. For nominal variables the mode is the only measure of central tendency and dispersion is not applicable. For this reason relationships between categorical variables are typically summarized using contingency tables which provide cross-variable counts for each level of the target categorical variables.\nLet’s explore the relationship between the categorical variables sex and age_group. In Table 3.27 we see the contingency table with summary counts and percentages.\n\n\n\n\nTable 3.27: Contingency table for age_group and sex.\n\n\n\n\n\n\n\n\n\nsex/age_group\n10-year-olds\n12-year-olds\n16-year-olds\n17-year-olds\nTotal\n\n\n\nfemale\n58% (14)\n69% (11)\n54% (13)\n67% (10)\n61% (48)\n\n\nmale\n42% (10)\n31% (5)\n46% (11)\n33% (5)\n39% (31)\n\n\nTotal\n100% (24)\n100% (16)\n100% (24)\n100% (15)\n100% (79)\n\n\n\n\n\n\nAs the size of the contingency table increases, visual inspection becomes more difficult. As we have seen, a graphical summary often proves more helpful to detect patterns.\n\n\n\n\nFigure 3.30: Barplot…\n\n\n\nIn Figure 3.30 the left pane shows the counts. Counts alone can be tricky to evaluate and adjusting the barplot to account for the proportions of males to females in each group, as shown in the right pane, provides a clearer picture of the relationship. From these barplots we can see there were more females in the study overall and particularly in the 12-year-olds and 17-year-olds groups. To gauge the association strength between sex and age_group we can calculate Cramer’s V which, in spirit, is like our correlation coefficients for the relationship between continuous variables. The Cramer’s V score for this relationship is 0 which is low, suggesting that there is not a strong association between sex and age_group –in other words, the relationship is stable.\nLet’s look at a more complex case in which we have three categorical variables. Now the dataset, as is, does not have a third categorical variable for us to explore but we can recast the continuous num_tokens variable as a categorical variable if we bin the scores into groups. I’ve binned tokens into three score groups with equal ranges in a new variable called rank_tokens.\nAdding a second categorical independent variable ups the complexity of our analysis and as a result our visualization strategy will change. Our numerical summary will include individual two-way cross-tabulations for each of the levels for the third variable. In this case it is often best to use the variable with the fewest levels as the third variable, in this case sex.\n\n\n\n\nTable 3.28: Contingency table for age_group, rank_tokens, and sex (female).\n\n\n\n\n\n\n\n\n\nrank_tokens/age_group\n10-year-olds\n12-year-olds\n16-year-olds\n17-year-olds\nTotal\n\n\n\nlow\n27% (13)\n10% (5)\n4% (2)\n6% (3)\n48% (23)\n\n\nmid\n2% (1)\n13% (6)\n21% (10)\n6% (3)\n42% (20)\n\n\nhigh\n0% (0)\n0% (0)\n2% (1)\n8% (4)\n10% (5)\n\n\nTotal\n29% (14)\n23% (11)\n27% (13)\n21% (10)\n100% (48)\n\n\n\n\n\n\n\n\n\n\nTable 3.29: Contingency table for age_group, rank_tokens, and sex (male).\n\n\n\n\n\n\n\n\n\nrank_tokens/age_group\n10-year-olds\n12-year-olds\n16-year-olds\n17-year-olds\nTotal\n\n\n\nlow\n32% (10)\n13% (4)\n13% (4)\n3% (1)\n61% (19)\n\n\nmid\n0% (0)\n3% (1)\n23% (7)\n6% (2)\n32% (10)\n\n\nhigh\n0% (0)\n0% (0)\n0% (0)\n6% (2)\n6% (2)\n\n\nTotal\n32% (10)\n16% (5)\n35% (11)\n16% (5)\n100% (31)\n\n\n\n\n\n\nContingency tables with this many levels are notoriously difficult to interpret. A plot that is often used for three-way contingency table summaries is a mosaic plot. In Figure 3.31 I have created a mosaic plot for the three categorical variables in the previous contingency tables.\n\n\n\n\nFigure 3.31: Mosaic plot for three categorical variables age_group, rank_tokens, and sex in the BELC dataset.\n\n\n\nThe mosaic plot suggests that the number of tokens per composition increase as the learner age group increases and that females show more tokens earlier.\n\nIn sum, a dataset is information but when the observations become numerous or complex they are visually difficult to inspect and understand at a pattern level. The descriptive methods described in this section are indispensable for providing the researcher an overview of the nature of each variable and any (potential) relationships between variables in a dataset. Importantly, the understanding derived from this exploration underlies all subsequent investigation and will counted on to frame your approach to analysis regardless of the research goals and the methods employed to derive more substantial knowledge."
  },
  {
    "objectID": "approaching-analysis.html#aa-types-of-analysis",
    "href": "approaching-analysis.html#aa-types-of-analysis",
    "title": "3  Approaching analysis",
    "section": "\n3.5 Types of analysis",
    "text": "3.5 Types of analysis\n\nFrom identifying a target population, to selecting a data sample that represents that population, and then to structuring the sample into a dataset, the goals of a research project inform and frame the process. So it will be unsurprising to know that the process of selecting an approach to analysis is also intimately linked with a researcher’s objectives. The goal of analysis, generally, is to generate knowledge from information. The type of knowledge generated and the process by which it is generated, however, differ and can be broadly grouped into three analysis types: exploratory, predictive, and inferential. In this section I will provide an overview of how each of these analysis types are tied to research goals and how the general goals of each type affect: (1) how to identify the variables of interest, (2) how to interrogate these variables, and (3) how to interpret the results. I will structure the discussion of these analysis types moving from the least structured (inductive) to most structured (deductive) approach to deriving knowledge from information with the aim to provide enough information to the would-be-researcher to identify these research approaches in the literature and to make appropriate decisions as to which approach their research should adopt.\n\n\n\n3.5.1 Inferential data analysis\n\nThe most commonly recognized of the three data analysis approaches, inferential data analysis (IDA) is the bread-and-butter of science. IDA is a deductive, or top-down, approach to investigation in which every step in research stems from a premise, or hypothesis, about the nature of a relationship in the world and then aims to test whether this relationship is statistically supported given the evidence. The aim is to infer conclusions about a certain relationship in the population based on a statistical evaluation of a (corpus) sample. So, if a researcher’s aim is to draw conclusions that generalize, then, this is the analysis approach a researcher will take.\n\nGiven the fact that this approach aims at making claims that can be generalized to the larger population, the IDA approach has the most rigorous set of methodological restrictions. First and foremost of these is the fact that a testable hypothesis must be formulated before research begins. The hypothesis guides the collection of data, the organization of the data into a dataset and the transformation, selection of the variables to be used to address the hypothesis, and the interpretation of the results. To conduct an analysis and then draw a hypothesis which conforms to the results is known as “Hypothesis After Result is Known” (HARKing) (Kerr 1998) and this practice violates the principles of significance testing. A second key stipulation is that the reliability of the sample data, the corpus in text analysis, to provide evidence to test the hypothesis must be representative of the population. A corpus used in a study which is misaligned with the hypothesis undermines the ability of the researcher to make valid claims about the population. In essence, IDA is only as good as the primary data is is based on.\n\nAt this point, let me elaborate on the potentially counterintuitive nature of hypothesis formulation and testing. The IDA, or Null-Hypothesis Significance Testing (NHST), paradigm is in fact approached by proposing two mutually exclusive hypotheses. The first is the Alternative Hypothesis (\\(H_1\\)). \\(H_1\\) is a precise statement grounded in the previous literature outlining a predicted relationship (and in some cases the directionality of a relationship). This is the effect that the research aims to investigate. The second hypothesis is the Null Hypothesis (\\(H_0\\)). \\(H_0\\) is the flip-side of the hypothesis testing coin and states that there is no difference or relationship. Together \\(H_1\\) and \\(H_0\\) cover all logical outcomes.\nSo to provide an example consider a hypothetical study which is aimed at investigating the claim that men and women differ in terms of the number of questions they use in spontaneous conversations. The Alternative Hypothesis would be formulated in this way:\n\\(H_1\\): Men and women differ in the frequency of the use of questions in spontaneous conversations.\nThe Null Hypothesis, then, would be a statement describing the remaining logical outcomes. Formally:\n\\(H_0\\): There is no difference between how men and women use questions in spontaneous conversations.\nNote that stated in this way our hypothesis makes no prediction about the directionality of the difference between men and women, only that there is a difference. It is a likely scenario that a hypothesis will stake a claim on the direction of the difference. A directional hypothesis would look like this:\n\\(H_1\\): Women use more questions than men in spontaneous conversations.\n\\(H_0\\): There is no difference between how men and women use questions in spontaneous conversations or men use more questions than women.\n\nA further aspect which may run counter to expectations is that the aim of hypothesis testing is not to find evidence in support of \\(H_1\\), but rather the aim is to assess the likelihood that we can reliably reject \\(H_0\\). The default assumption is that \\(H_0\\) is true until there is sufficient evidence to reject it and accept \\(H_1\\), the alternative. The metric used to determine if there is sufficient evidence is based on the probability that given the nature of the relationship and the characteristics of the data, the likelihood of there being no difference or relationship is low. The threshold for likelihood has traditionally been summarized in the p-value statistic. In the Social Sciences, a p-value lower that .05 is considered statistically significant which when interpreted correctly means that there is more than a 95% chance that the observed relationship would not be predicted by \\(H_0\\). Note that we are working in the realm of probability, not in absolutes, therefore an analysis that produces a significant result does not prove \\(H_1\\) is correct or that \\(H_0\\) is incorrect, for that matter. A margin of error is always present.\n\nLet’s now turn to the identification of variables, the statistical interrogation of these variables, and the interpretation of the statistical results. First, since a clearly defined and testable hypothesis is at the center of the IDA approach, the variables are in some sense pre-defined. The goal of the researcher is to select data and curate that data to produce variables that are operationalized (practically measured) to test the hypothesis. A second consideration are the roles that the variables will play in the analysis. In standard IDA one variable will be the dependent variable and one or more variables will be independent variables. The dependent variable, sometimes referred to as the outcome or response variable, is the variable which contains the information which is predicted to depend on the information in the independent variable(s). It is the variable whose variation a research study seeks to explain. An independent variable, sometimes referred to as a predictor or explanatory variable, is a variable whose variation is predicted to explain the variation in the dependent variable.\nReturning to our hypothetical study on the use of questions between men and women in spontaneous conversation, the frequency of questions used by each speaker would be our dependent variable and the biological sex of the speakers our independent variable. This is so because hypothesis (\\(H_1\\)) states the proposition that a speaker’s sex will predict the frequency of questions used.\nIn our hypothetical study we’ve identified two variables, one dependent and one independent. It is important keep in mind that there can be multiple independent variables in cases where the dependent variable’s variation is predicted to be related to multiple variables. This relationship would need to be explicitly part of the original hypothesis, however.\nSay we formulate a more complex relationship where the educational level of our speakers is also related to the number of questions. We can update our hypothesis to reflect such a scenario.\n\\(H_1\\): Less educated women use more questions than men in spontaneous conversations.\n\\(H_0\\): There is no difference between how men and women use questions in spontaneous conversations regardless of educational level, or more educated women use more questions than less educated women, or men use more questions than women.\nThe hypothesis we have described predicts what is known as an interaction; the relationship between our independent variables predict different variational patterns in the dependent variable. As you most likely can appreciate the more independent variables we include in our hypothesis, and by extension our analysis, the more difficult it becomes to interpret. Due to the increasing difficulty for interpretation, in practice, IDA studies rarely include more than two or three independent variables in the same analysis.\nIndependent variables add to the complexity of a study because they are part of our research focus, specifically our hypothesis. It is, however, common to include other variables which are not of central focus, but are commonly assumed to contribute to the explanation of the variation of the dependent variable. Let’s assume that the background literature suggests that the age of speakers also plays a role in the number of questions that men and women use in spontaneous conversation. Let’s also assume that the data we have collected includes information about the age of speakers. If we would like to factor out the potential influence of age on the use of questions and focus on the particular independent variables we’ve defined in our hypothesis, we can include the age of speakers as a control variable. A control variable will be added to the statistical analysis and documented in our report but it will not be included in the hypothesis nor interpreted in our results.\n\n\n\n\nFigure 3.32: Variable roles in inferential analysis.\n\n\n\nAt this point let’s look at the main characteristics that need to be taken into account to statistically interrogate the variables we have chosen to test our hypothesis. The type of statistical test that one chooses is based on (1) the informational value of the dependent variable and (2) the number of independent variables included in the analysis. Together these two characteristics go a long way in determining the appropriate class of statistical test, but other considerations about the distribution of particular variables (i.e. normality), relationships between variables (i.e. independence), and expected directionality of the predicted effect may condition the appropriate method to be applied.\nAs you can imagine, there are a host of combinations and statistical tests that apply in particular scenarios, too many to consider in given the scope of this coursebook (see Gries (2013) and Paquot and Gries (2020) for a more exhaustive description). Below I’ve summarized some common statistical scenarios and their associated tests which focus on the juxtaposition of informational values and the number of variables, leaving aside alternative tests which deal with non-normal distributions, ordinal variables, non-independent variables, etc.\nIn Table 3.30 we see monofactorial tests, tests with only one independent variable.\n\n\n\n\nTable 3.30: Common monofactorial tests.\n\nDependent\nIndependent\nTest\n\n\n\nCategorical\nCategorical\nPearson’s Chi-squared test\n\n\nContinuous\nCategorical\nStudent’s t-Test\n\n\nContinuous\nContinuous\nPearson’s correlation test\n\n\n\n\n\n\nTable 3.31 includes a listing of multifactorial tests, tests with more than one independent and/ or control variables.\n\n\n\n\nTable 3.31: Common multifactorial tests.\n\nDependent\nIndependent\nControl\nTest\n\n\n\nCategorical\nvaried\nvaried\nLogistic regression\n\n\nContinuous\nvaried\nvaried\nLinear regression\n\n\n\n\n\n\nOne key point to make before we turn to how to interpret the statistical results is concerns the use of the data in IDA. In contrast to the other two analysis methods we will cover, the data in IDA is only used once. That is to say, that the entire dataset is used a single time to statistically interrogate the relationship(s) of interest. The resulting confidence metrics (p-values, etc.) are evaluated and the findings are interpreted. The practice of running multiple tests until a statistically significant result is found is called “p-hacking” (Head et al. 2015) and like HARKing (described earlier) violates statistical hypothesis testing practice. For this reason it is vital to identify your statistical approach from the outset of your research project.\nNow let’s consider how to approach interpreting the results from a statistical test. As I have now made reference to multiple times, the results of statistical procedure in hypothesis testing will result in a confidence metric. The most standard and widely used of these confidence metrics is the p-value. The p-value provides a probability that the results of our statistical test could be explained by the null hypothesis. When this probability crosses below the threshold of .05, the result is considered statistically significant, otherwise we have a ‘null result’ (i.e. non-significant). However, this sets up a binary distinction that can be problematic. On the one hand what is one to do if a test returns a p-value of .051 or something ‘marginally significant’? According to standard practice these results would not be statistically significant. But it is important to note that a p-value is sensitive to the sample size. A small sample may return a non-significant result, but a larger sample size with the same underlying characteristics may very well return a significant result. On the other hand, if we get a statistically significant result, do we move on –case closed? As I just pointed out the sample size plays a role in finding statistically significant results, but that does not mean that the results are ‘important’ for even small effects in large samples can return a significant p-value.\nIt is important to underscore that the purpose of IDA is to draw conclusions from a dataset which are generalizable to the population. These conclusions require that there are rigorous measures to ensure that the results of the analysis do not overgeneralize (suggest there is a relationship when there is not one) and balance that with the fact that we don’t want to undergeneralize (miss the fact that there is an relationship in the population, but our analysis was not capable of detecting it). Overgeneralization is known as Type I error or false positive and undergeneralization is a Type II error or false negative.\nFor these reasons it is important to calculate the size and magnitude of the result to gauge the uncertainty of our result in standardized, sample size-independent way. This is performed by analyzing the effect size and reporting a confidence interval (CI) for the results. The wider the CI the more uncertainty surrounds our statistical result, and therefore the more likely that our significant p-value could be the result of Type I error. A non-significant p-value and large effect size could be the result of Type II error. In addition to vetting our p-value, the CI and effect size can help determine if a significant result is reliable and ‘important’. Together effect size and CIs aid in our ability to realistically interpret confidence metrics in statistical hypothesis testing.\n\n\n3.5.2 Predictive data analysis\n\nPredictive data analysis (PDA) is the first of the two types of statistical approaches we will cover that fall under machine learning. A branch of artificial intelligence (AI), machine learning aims to develop computer algorithms that can essentially learn patterns from data automatically. In the case of PDA, also known as supervised learning, the learning process is guided (supervised) by directing an algorithm to associate patterns in a variable or set of variables to single particular variable. The particular variable is analogous to some degree to a dependent variable in IDA, but in the machine learning literature this variable is known as the target variable. The other variable or (more often than not) variables are known as features. The goal of PDA is to develop a statistical generalization that can accurately predict the values of a target variable using the values of the feature variables. PDA can be seen as a mix of deductive (top-down) and inductive (bottom-up) methods in that the target variable is determined by a research goal but the feature variables and choice of statistical method (algorithm) are not fixed and can vary depending on their usefulness in effectively predicting the target variable. PDA is a versatile method that often employed to derive intelligent action from data, but it can also be used for hypothesis generation and even hypothesis testing, under certain conditions. If a researcher’s aim is to create model that can perform a language related task, explore association strength between a target variable and various types and combinations of features, or to perform emerging alternative approaches to hypothesis testing 4, this is the analysis approach a researcher will take.\n\nAt this point let’s consider some departures from the inferential data analysis (IDA) approach we covered in the last subsection that are important to highlight to orient our overview of PDA. First, while the cornerstone of IDA is the hypothesis, in PDA this is typically not the case. A research question which identifies a source of potential uncertainty in an area and outlines a strategy for addressing this uncertainty is sufficient groundwork to embark on an analysis. A second divergence, is the fact that the data is used in a very distinct way. In IDA the entire dataset is statistically interrogated once and only once. In PDA the dataset is (minimally) partitioned into a training set and a test set. The training set is used to train a statistical model and the test set is left to test the accuracy of the statistical model. The training set typically constitutes a larger portion of the data (typically around 75%) and serves as the test bed for iteratively applying one or more algorithms and/ or feature combinations to produce the most successful learning model. The test set is reserved for a final evaluation of the model’s performance. Depending on the application and the amount of available data, a third development set is sometimes created as a pseudo test set to facilitate the testing of multiple approaches on data outside the training set before the final evaluation on the test set is performed. In this scenario the proportions of the partitions vary, but a good rule of thumb is to reserve 60% of the data for training, 20% for development, and 20% for testing.\nLet’s now turn to the identification of variables, the statistical interrogation of these variables, and the interpretation of the statistical results. In IDA the variables (features) are pre-determined by the hypothesis and the informational values and number of these variables plays a significant role in selecting a statistical procedure (algorithm). Lacking a hypothesis, a PDA approach’s main goal is to make accurate predictions on the target variable and is free to explore any number of features and feature combinations to that end. The target variable is the only variable which necessarily fixed and in this light pre-determined.\nTo give an example, let’s consider a language task in which the goal is to take text messages (SMS) and develop a language model that predict if a message is spam or not. Minimally we would need data which includes individual text messages and each of these text message will need to be labeled as being either spam or legitimate messages (‘ham’ in this case). In Table 3.32 we see the first ten of 5574 observations from the SMS Spam Collection (v.1) dataset collected by Almeida, Gómez Hildago, and Yamakami (2011).\n\n\n\n\nTable 3.32: First ten observations from the SMS Spam Collection (v.1)\n\n\n\n\n\nsms_type\nmessage\n\n\n\nham\nGo until jurong point, crazy.. Available only in bugis n great world la e buffet… Cine there got amore wat…\n\n\nham\nOk lar… Joking wif u oni…\n\n\nspam\nFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C’s apply 08452810075over18’s\n\n\nham\nU dun say so early hor… U c already then say…\n\n\nham\nNah I don’t think he goes to usf, he lives around here though\n\n\nspam\nFreeMsg Hey there darling it’s been 3 week’s now and no word back! I’d like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n\n\nham\nEven my brother is not like to speak with me. They treat me like aids patent.\n\n\nham\nAs per your request ‘Melle Melle (Oru Minnaminunginte Nurungu Vettam)’ has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n\n\nspam\nWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n\n\nspam\nHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n\n\n\n\n\n\nAs it stands we have two variables; sms_type is clearly the target and message contain the full messages. The question is how best to transform the information in the message variable such that it will provide an algorithm useful information to predict each value of sms_type. Since the informational value of sms_type is categorical we will call the values classes. The process of deciding on how to transform the information in message into useful features is called feature engineering and it is a process which is much an art as a science. On the creative side of things it is often helpful to have a mixture of relevant domain knowledge and clever hacking skills to envision what features may work best. The more logistic side of things requires some knowledge about the strengths and weaknesses of various learning algorithms when dealing with certain number and informational value feature combinations.\nLeaving the choice of algorithm aside, let’s focus on feature engineering. Since each message value is a unique message, the chance that using message as it is, is not likely to help us make reliable predictions about the status of new message (‘spam’ or ‘ham’). A simple first-pass approach to decomposing message to draw out similarities and distinctions between the classes may be to break each message into words. Now SMS messages are not your average type of text –there are many non-standard forms. So our definition of word may simply be character groupings broken apart by whitespace. To avoid confusion between our common-sense understanding of word and the types of character strings, it is often the case that language feature values are called terms. Other term types may work better, n-grams, character sequences, stems/lemmas, or even combinations of these. Certain terms may be removed that are potentially uninformative either based on their class (stopwords, numerals, punctuation, etc.) or due to their distribution. The process of systematic isolation of terms which are more informative than others is called dimensionality reduction (Kowsari et al. 2019). With experience a research will become more adept a recognizing advantages and potential issues and alternative ways of approaching the creation of features but there is almost always some level of trial and error in the process. Feature engineering is very much an exploratory process. It is also iterative. You can try a set of features with an algorithm and produce a language model and test it on the training set –if is accurate, great. If not, you can brainstorm some more –you are free to try further engineer the features trying new features or feature measures (term weights) and/ or change the learning algorithm.\n\n\n\n\nFigure 3.33: Variable roles in predictive analysis.\n\n\n\n\nLet’s now turn to some considerations to take into account when selecting a statistical algorithm. First, just as in IDA, variable informational value plays a role in algorithm selection, specifically the informational value of the target variable. If the target variable is categorical, then we are looking for a classification algorithm. If the target variable is continuous, we will employ a regression algorithm. 5 Some common classification algorithms are listed in Table 3.22.\n\n\n\n\nTable 3.33: Some common supervised learning algorithms.\n\nClassification\nRegression\n\n\n\nLogistic Regression\nLinear Regression\n\n\nSupport Vector Machine\nSupport Vector Regression\n\n\nNaïve Bayes Classifier\nPoisson Regression\n\n\nNeural Network\n\n\n\nDecision Tree\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother consideration to take into account is the whether the researcher aims to go beyond simply using an algorithm to make accurate predictions, but also wants to understand how the algorithm made its predictions and what contribution features made in the process. There are algorithms that produce models that allow the researcher to peer into and understand its inner workings (e.g. logistic regression, naïve bayes classifiers, inter alia) and those that do not (e.g. neural networks, support vector machines, inter alia). Those that do not are called ‘black-box’ algorithms. Neither type assures the best prediction accuracy. Important trade-offs need to be considered, however, if the best prediction comes from a black-box method, but the goal of the research is to understand the contribution of the features to the model’s predictions.\n\nOnce we have identified our target variable, engineered a promising set of features, and selected an algorithm to employ that meets our research goals, it is now time to interrogate the dataset. The first step is to partition the dataset into a training and test set. The training set is the dataset we will use to try out different features and/ or algorithms with the aim of developing a model which can most accurately predict the target variable values in this training set. This is the second step and it’s done by first training an algorithm to associate the features with the (actual) target values. Next, the resulting model is then applied to the same training data, yet with the target variable removed, or hidden, from the machine learner. The target values predicted by the model for each observation are compared to the actual target values. The more predicted and actual values for the target variable coincide, the more accurate the model. If the model shows high accuracy, then we are ready to move to evaluate this model on the test set (again removing the target variable). If the model accuracy is low, it’s back to the drawing board either returning to feature engineering and/ or algorithm selection in hopes to improve model performance. In this way, the training data can be used multiple times, a clear divergence from standard IDA methods in which the data is interrogated and analyzed once and only once.\n\n\n\n\nFigure 3.34: Phases in predictive analysis.\n\n\n\n\nFor all applications of PDA the interpretation of the prediction model includes some metric or metrics of accuracy comparing the extent to which the models predictions and the actual targets align. In cases in which the inner workings of the model are of interest, a researcher can dive into features and their contributions to the prediction model in an exploratory fashion according to the research goals. The exploration of features, then, varies, so at this time let’s focus on the metrics of prediction accuracy.\nThe standard form for evaluating a model’s performance differs between classification models (naive bayes) and regression models (linear regression). For classification models, a cross-tabulation of the predicted and actual classes results in a contingency table which can be used to calculate accuracy which is the sum of all the correctly predicted observations divided by the total number of observations in the test set. In addition to accuracy, there are various other measures which aim to assess a model’s performance to gain more insight into the potential over- or under-generalization of the model (Precision and Recall). For regression models, differences between predicted and actual values can be assessed using a coefficient of correlation (typically \\(R^2\\)). Again, more fine-grained detail about the model’s performance can be calculated (Root Mean Square Error).\nAnother component worthy of consideration when evaluating a model’s performance is how do we determine if the performance is actually good. One the one hand, accuracy rates into the 90+% range on the test set is usually a good sign that the model is performing well. No model will perform with perfect accuracy, however, and depending on the goal of the research particular error patterns may be more important, and problematic, than the overall prediction accuracy. On the other hand, another eventuality is that the model performs very well on the training set but that on the test set (new data) the performance drops significantly. This is a sign that during the training phrase the machine learning algorithm learned nuances in the data (‘noise’) that obscure the signal pattern to be learned. This problem is called overfitting and to avoid it researchers iteratively run evaluations of the training data using resampling. The two most common resampling methods are bootstrapping (resampling with replacement) and cross-validation (resampling without replacement). The performance of these multiple models are summarized and the error between them is assessed. The goal is to minimize the performance differences between the models while maximizing the overall performance. These measures go a long way to avoiding overfitting and therefore maximizing the chance that the training phase will produce a model which is robust.\n\n3.5.3 Exploratory data analysis\nThe last of the three analysis types, exploratory data analysis (EDA) includes a wide range of methods whose objective is to identify structure in datasets using only the data itself. In this way, EDA is an inductive, bottom-up approach to data analysis, which does not make any formal assumptions about the relationship(s) between variables. EDA can be roughly broken into two subgroups of analysis. Unsupervised learning, like supervised learning (PDA), is a subtype of machine learning. However, unlike prediction, unsupervised learning does not include a target variable to guide associations. The second subgroup of EDA methods can be seen as a (more robust) extension of the descriptive analysis methods covered earlier in this chapter. Either through unsupervised learning or descriptive methods, EDA employs quantitative methods to summarize, reduce, and sort complex datasets and statistically and visually interrogate a dataset in order to provide the researcher novel perspective to be qualitatively assessed. These qualitative assessments may prove useful to generate hypotheses or to generate groupings to be used in predictive analyses. So, if a researcher’s aim is to probe a dataset in order to explore potential relationships in an area where predictions and/ or hypotheses cannot be clearly made, this is the analysis approach to choose.\n\nIn contrast to both IDA and even PDA in which there are assumptions made about what relationship(s) to explore, EDA makes no such assumptions. Furthermore, given the exploratory nature of the process, EDA is not an approach which can in itself be used to make conclusive generalizations about the populations from which the (corpus) sample in which it is drawn. For IDA the fidelity of the sample and the process of selection of the variables is of utmost importance to ensure that the statistical results are reliably generalizable. Even in the case of PDA, the sample and variables selected are key to building a robust predictive model. However, in contrast to IDA, but similar to PDA, EDA methods may reuse the data selecting different variables and/or methods as research goals dictate. If a machine learning approach to EDA is adopted, the dataset can be partitioned into training and test sets, in a similar fashion to PDA. And as with PDA, the training set is used for refining statistical measures and the test set is used to evaluate the refined measures. Although the evaluation results still cannot be used to generalize, the insight can be taken as stronger evidence that there is a potential relationship, or set of relationships, worthy of further study.\nAnother notable point of contrast concerns the interpretation of EDA results. Although quantitative in nature, exploratory methods involve a high level of human interpretation. Human interpretation is a part of each stage of data analysis, and each statistical approach, in general, but exploratory methods produce results that require associative thinking and pattern detection which is distinct from the other two analysis approaches, in particular, IDA.\nAgain, as we have done for the other two analysis approaches, let’s turn to the process of variable identification, data interrogation, and interpretation methods. As in the case of PDA, EDA only requires a research goal. But in PDA, the research goal centered around predicting a target variable. In EDA, there is no such focus. The research goal may in fact be less defined and a researcher may consider various relationships in turn or simultaneously. The curation of the variables, however, does overlap in spirit to the process of feature engineering that we touched on for creating variables for predictive models. But in EDA the measure to gauge whether the engineered variables are good, is left to the qualitative evaluation of the researcher.\n\n\n\n\nFigure 3.35: Variable roles in exploratory analysis.\n\n\n\nFor illustrative purposes let’s consider the State of the Union Corpus (SOTU) (Benoit 2020). The presidential addresses and a set of meta-data variables are included in the corpus. I’ve subsetted this corpus to only include U.S. presidents since 1946. A tabular preview of the first 10 addresses (truncated for display) can be found in Table 3.19.\n\n\n\n\nTable 3.34: First ten addresses from the SOTU Corpus.\n\npresident\ndate\ndelivery\nparty\naddresses\n\n\n\nTruman\n1946-01-21\nwritten\nDemocratic\nTo the Congress of the United States: A quarter...\n\n\nTruman\n1947-01-06\nspoken\nDemocratic\nMr. President, Mr. Speaker, Members of the Cong...\n\n\nTruman\n1948-01-07\nspoken\nDemocratic\nMr. President, Mr. Speaker, and Members of the ...\n\n\nTruman\n1949-01-05\nspoken\nDemocratic\nMr. President, Mr. Speaker, Members of the Cong...\n\n\nTruman\n1950-01-04\nspoken\nDemocratic\nMr. President, Mr. Speaker, Members of the Cong...\n\n\nTruman\n1951-01-08\nspoken\nDemocratic\nMr. President, Mr. Speaker, Members of the Cong...\n\n\nTruman\n1952-01-09\nspoken\nDemocratic\nMr. President, Mr. Speaker, Members of the Cong...\n\n\nTruman\n1953-01-07\nwritten\nDemocratic\nTo the Congress of the United States: I have th...\n\n\nEisenhower\n1953-02-02\nspoken\nRepublican\nMr. President, Mr. Speaker, Members of the Eigh...\n\n\nEisenhower\n1954-01-07\nspoken\nRepublican\nMr. President, Mr. Speaker, Members of the Eigh...\n\n\n\n\n\n\n\n\nA dataset such as this one could be leveraged to explore many different types of research questions. Key to guiding the engineering of features, however, is to clarify from the outset of the research project what the entity of study is, or unit of analysis. In IDA and PDA approaches, the unit of analysis forms an explicit part of the research hypothesis or goal. In EDA the research question may have multiple fronts, which may be reflected in differing units of analysis. For example, based on the SOTU dataset, we could be interested in political rhetoric, language of particular presidents, party ideology, etc. Depending on the perspective we are interested in investigating, the choice of how to approach engineering features to gain insight will vary.\nBy the same token, approaches for interrogating the dataset can vary widely, between and within the same research project, but for instructive purposes we can draw a distinction between descriptive methods and unsupervised learning methods.\n\n\n\n\nTable 3.35: Some common EDA analysis methods\n\nDescriptive\nUnsupervised learning\n\n\n\nTerm frequency analysis\nCluster analysis\n\n\nTerm keyness analysis\nTopic Modeling\n\n\nCollocation analysis\nDimensionality reduction\n\n\n\n\n\n\nEDA leans heavily on visual representations of both descriptive and unsupervised learning methods. Visualizations enable humans to identify and extrapolate associative patterns. Visualizations range from standard barplots and scatterplots to network graphs and dendrograms and more. Some sample visualizations based on the SOTU Corpus are found in Figure 3.36.\n\n\n\n\nFigure 3.36: Sample visualizations from the SOTU Corpus (1946-2020).\n\n\n\nJust as feature selection and analysis method, the interpretation of the results in EDA are much more varied than in the other analysis methods. EDA methods provide information which requires much more human intervention and associative interpretation. In this way, EDA can be seen as a quantitatively informed qualitative assessment approach. The results from one approach can be used as the input to another. Findings can lead to further exploration and probing of nuances in the data. Speculative as they are the results from exploratory methods can be highly informative and lead to new insight and inspire further study in directions that may not have been expected."
  },
  {
    "objectID": "approaching-analysis.html#reporting",
    "href": "approaching-analysis.html#reporting",
    "title": "3  Approaching analysis",
    "section": "\n3.6 Reporting",
    "text": "3.6 Reporting\nMuch of the necessary reporting for an analysis features in prose as part of the write-up of a report or article. This will include descriptive summaries, a blueprint of the method(s) used, and the results. Descriptive summaries will often include assessments of individual variables and/ or relationships between variables (central tendency, dispersion, association strength, etc.). Any procedures applied to diagnose or to correct the data should also be included in the final report. This information is key to helping readers assess the results from the analysis. A blueprint of the methods used will describe the variable selection process, how the variables were used in the statistical analysis, and any other information that is relevant for a reader to understand what was done and why it was done. Reporting results from an analysis will depend on the type of analysis and the particular method(s) employed. For inferential analyses this will include the test statistic(s) (\\(X^2\\), \\(R^2\\), etc.) and some measure of confidence (\\(p\\)-value, confidence interval, effect size). In predictive analyses accuracy results and related information will need to be reported. For exploratory analyses, the reporting of results will vary and often include visualizations and metrics that require more human interpretation than the other analysis types.\nWhile a good article write-up will include the most vital information to understand the procedures taken in an analysis, there are many more details which do not traditionally appear in prose. If a research project was conducted programmatically, however, the programming files (scripts) used to generate the analysis can (and should) be shared. While the scripts themselves are highly useful for other researchers to consult and understand in fine-grained detail the steps that were taken, it is important to also recognize that the research project should be well documented –through organized project directory and file structure as well as through code commenting. This description and instructions on how to run the analysis form a research compendium which ensure that the research conducted is easily understood and able to be reproduced and/ or enhanced by other researchers."
  },
  {
    "objectID": "approaching-analysis.html#summary",
    "href": "approaching-analysis.html#summary",
    "title": "3  Approaching analysis",
    "section": "Summary",
    "text": "Summary\nIn this chapter we have focused on description and analysis –the third component of DIKI Hierarchy. This process is visually summarized in Figure 3.37.\n\n\n\n\nFigure 3.37: Approaching analysis: visual summary\n\n\n\nBuilding on the strategies covered in Chapter 2 “Understanding data” to derive a rich relational dataset, in this chapter we outlined key points in approaching analysis. The first key step in any analysis is to perform a descriptive assessment of the individual variables and relationships between variables. To select the appropriate descriptive measures we covered the various informational values that a variable can take. In addition to providing key information for reporting purposes, descriptive measures are important to explore so the researcher can get a better feel for the dataset before conducting an analysis.\nWe covered three data analysis types in this chapter: inferential, predictive, and exploratory. Each of these embodies very distinct approaches to deriving knowledge from data. Ultimately the choice of analysis type is highly dependent on the goals of the research. Inferential analysis is centered around the goal of testing a hypothesis, and for this reason it is the most highly structured approach to analysis. This structure is aimed at providing the mechanisms to draw conclusions from the results that can be generalized to the target population. Predictive analysis has a less-ambitious but at times more relevant goal of discovering the extent to which a given relationship can be extrapolated from the data to provide a model of language that can accurately predict an outcome using new data. While many times predictive analysis is used to perform language tasks, it can also be a highly effective methodology for applying different algorithmic approaches and exploring relationships a target variable and various configurations of variables. The ability to explore the data in multiple ways, is also a key strength of employing an exploratory analysis. The least structured and most variable of the analysis types, exploratory analyses are a powerful approach to deriving knowledge from data in an area where clear predictions cannot be made.\nI rounded out this chapter with a short description of the importance of reporting the metrics, procedures, and results from analysis. Reporting, in its traditional form, is documented in prose in an article. This reporting aims to provide the key information that a reader will need to understand what was done, how it was done, and why it was done. This information also provides the necessary information for reader’s with a critical eye to understand the analysis in more detail. Yet even the most detailed reporting in a write-up still leaves many practical, but key, points of the analysis obscured. A programming approach provides the procedural steps taken that when shared provide the exact methods applied. Together with the write-up a research compendium which provides the scripts to run the analysis and documentation on how to run the analysis forms an integral part of creating reproducible research."
  },
  {
    "objectID": "approaching-analysis.html#activities",
    "href": "approaching-analysis.html#activities",
    "title": "3  Approaching analysis",
    "section": "Activities",
    "text": "Activities\n\n\n\n\n\n\n\n Recipe\n\nWhat: Descriptive assessment of datasetsHow: Read Recipe 4 and participate in the Hypothes.is online social annotation.Why: To explore appropriate methods for summarizing variables in datasets given the number and informational values of the variable(s).\n\n\n\n\n\n\n\n\n\n Lab\n\nWhat: Descriptive assessment of datasetsHow: Clone, fork, and complete the steps in Lab 4.Why: To identify and apply the appropriate descriptive methods for a vector’s informational value and to assess both single variables and multiple variables with the appropriate statistical, tabular, and/ or graphical summaries."
  },
  {
    "objectID": "approaching-analysis.html#questions",
    "href": "approaching-analysis.html#questions",
    "title": "3  Approaching analysis",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\n\nConceptual questions\n\nWhat are the key differences between assessment and analysis?\nWhat are the potential measures of central tendency and dispersion for a variable? Does it depend on the informational value of the variable?\nConsider the following variables: \\(X\\) = number of children, \\(Y\\) = number of siblings, \\(Z\\) = number of siblings who are older than the participant. Which of these variables are categorical, ordinal, numeric? What are the measures of central tendency and dispersion for each variable?\nWhat type(s) of tables or plots are appropriate for summarizing a variable? What type(s) of tables or plots are appropriate for summarizing the relationship between two variables?\nIn the following variables and informational values, identify if the plots are appropriate for summarizing the relationship.\nWhat are the key differences between exploratory, predictive, and inferential analysis?\nHow do the goals of the research influence the choice of analysis type?\nGiven the following research questions, identify which type of analysis is most appropriate and why:\nGiven the following research questions, identify which type of analysis is most appropriate and why:\nGiven the following research questions, identify which type of analysis is most appropriate and why:\nHow are the results of inferential, predictive, and exploratory analysis evaluated?\nResearch compendia are an important part of reproducible research. What are the key components of a research compendium? What are the benefits of sharing a research compendium?\n\n\n\n\n\n\n\n\n\n\n\nTechnical exercises\n\nCreate a contingency table for the following variables:\nCreate a plot for the following variables:\nReport these tables and plots with a short interpretation of what they show.\n…\n\n\n\n\n\n\n\n\n\nAlmeida, Tiago A, José María Gómez Hildago, and Akebo Yamakami. 2011. “Contributions to the Study of SMS Spam Filtering: New Collection and Results.” In Proceedings of the 2011 ACM Symposium on Document Engineering (DOCENG’11), 4. Mountain View, CA.\n\n\nBaayen, R. Harald. 2004. “Statistics in Psycholinguistics: A Critique of Some Current Gold Standards.” Mental Lexicon Working Papers 1 (1): 1–47.\n\n\n———. 2011. “Corpus Linguistics and Naive Discriminative Learning.” Revista Brasileira de Lingu\\’\\istica Aplicada 11 (2): 295–328.\n\n\nBenoit, Kenneth. 2020. Quanteda.corpora: A Collection of Corpora for Quanteda. http://github.com/quanteda/quanteda.corpora.\n\n\nDeshors, Sandra C, and Stefan Th. Gries. 2016. “Profiling Verb Complementation Constructions Across New Englishes.” International Journal of Corpus Linguistics. 21 (2): 192–218.\n\n\nGries, Stefan Th. 2013. Statistics for Linguistics with r. A Practical Introduction. 2nd revise.\n\n\nHead, Megan L., Luke Holman, Rob Lanfear, Andrew T. Kahn, and Michael D. Jennions. 2015. “The Extent and Consequences of p-Hacking in Science.” PLOS Biology 13 (3): e1002106. https://doi.org/10.1371/journal.pbio.1002106.\n\n\nKerr, Norbert L. 1998. “HARKing: Hypothesizing After the Results Are Known.” Personality and Social Psychology Review 2 (3): 196–217.\n\n\nKowsari, Kamran, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana Mendu, Laura E. Barnes, and Donald E. Brown. 2019. “Text Classification Algorithms: A Survey.” Information 10 (4): 150. https://doi.org/10.3390/info10040150.\n\n\nMuñoz, Carmen, ed. 2006. Age and the Rate of Foreign Language Learning. 1st ed. Vol. 19. Second Language Acquisition Series. Clevedon: Multilingual Matters.\n\n\nNisioi, Sergiu, Ella Rabinovich, Liviu P. Dinu, and Shuly Wintner. 23-28, 2016-05. “A Corpus of Native, Non-Native and Translated Texts.” In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016). Portoroz̆, Slovenia: European Language Resources Association (ELRA).\n\n\nPaquot, Magali, and Stefan Th. Gries, eds. 2020. A Practical Handbook of Corpus Linguistics. Switzerland: Springer."
  },
  {
    "objectID": "approaching-analysis.html#footnotes",
    "href": "approaching-analysis.html#footnotes",
    "title": "3  Approaching analysis",
    "section": "",
    "text": "Note that each of these three variables are to be considered separately here (vertically). Later we will see the use of boxplots to compare a continuous variable across levels of a categorical variable (horizontally).↩︎\nformally known as a Gaussian Distribution↩︎\nTo calculate effect sizes for the difference between two means, Cohen’s d is used.↩︎\nsee Deshors and Gries (2016) and Baayen (2011)↩︎\nThe name regression can be a bit confusing given a very common classification algorithm is “Logistic Regression”.↩︎"
  },
  {
    "objectID": "preface.html#rationale",
    "href": "preface.html#rationale",
    "title": "Preface",
    "section": "Rationale",
    "text": "Rationale\nData science, an interdisciplinary field that combines knownledge and skills from statistics, computer science, and domain-specific expertise to extract meaningful insight from structured and unstructured data, has emerged as an exciting and rapidly growing field in recent years, driven in large part by the increase in computing power available to the average individual and the abundance of electronic data now available through the internet. These advances have become an integral part of the modern scientific landscape, with data-driven insights now being used to inform decision-making in a wide variety of academic fields, including linguistics and language-related disciplines.\nThis textbook seeks to meet this growing demand by providing an introduction to the fundamental concepts and practical programming skills from data science applied to the task of quantitative text analysis. It is intended primarily for undergraduate students, but may also be useful for graduates and researchers seeking to expand their methodological toolbox. The textbook takes a pedagogical approach which assumes no prior experience with statistics or programming, making it an accessible resource for novices beginning their exploration of quantitative text analysis methods."
  },
  {
    "objectID": "preface.html#aims",
    "href": "preface.html#aims",
    "title": "Preface",
    "section": "Aims",
    "text": "Aims\nThe overarching goal of this textbook is to provide readers with foundational knowledge and practical skills to conduct and evaluate quantitative text analysis using the R programming language and other open source tools and technologies. The specific aims are to develop the reader’s proficiency in three main areas:\n\n\nData literacy: Identify, interpret and evaluate data analysis procedures and results\n\n\nThroughout this textbook we will explore topics which will help you understand how data analysis methods derive insight from data. In this process you will be encouraged to critically evaluate connections across linguistic and language-related disciplines using data analysis knowledge and skills. Data literacy is an invaluable skillset for academics and professionals but also is an indispensable aptitude for in the 21st century citizens to navigate and actively participate in the ‘Information Age’ in which we live (Carmi et al. 2020).\n\n\n\nResearch skills: Design, implement, and communicate quantitative research\n\n\nThis aim does not differ significantly, in spirit, from common learning outcomes in a research methods course. However, working with text will incur a series of key steps in the selection, collection, and preparation of the data that are unique to text analysis projects. In addition, I will stress the importance of research documentation and creating reproducible research as an integral part of modern scientific inquiry (Buckheit and Donoho 1995).\n\n\n\nProgramming skills: Apply programmatic strategies to develop and collaborate on reproducible research projects\n\n\nModern data analysis, and by extension, text analysis is conducted using programming. There are various key reasons for this: a programming approach (1) affords researchers unlimited research freedom –if you can envision it, you can program it, (2) underlies well-documented and reproducible research (Gandrud 2015), and (3) invites researchers to engage more intimately with the data and the methods for analysis.\n\nThese aims are important for linguistics students because they provide a foundation for concepts and in the skills required to succeed in the rapidly evolving landscape of 21st-century research. These abilities enable researchers to evaluate and conduct high-quality empirical investigation across linguistic fields on a wide variety of topics. Moreover, these skills go beyond linguistics research; they are widely applicable across many disciplines where quantitative data analysis and programming are becoming increasingly important. Thus, this textbook provides students with a comprehensive introduction to quantitative text analysis that is relevant to linguistics research and that equips them with valuable skills for their future careers."
  },
  {
    "objectID": "preface.html#approach",
    "href": "preface.html#approach",
    "title": "Preface",
    "section": "Approach",
    "text": "Approach\nThe approach taken in this textbook is designed to accomodate linguistics students and researchers with little to no prior experience with programming or quantitative methods. With this in mind the objective is connect conceptual understanding with practical application. Real-world data and research tasks relevant to linguistics are used thoughtout the book to provide context and to motivate the learning process1. Furthermore, as an introduction to the field, the textbook focuses on the most common and fundamental methods and techniques for quantitative text analysis and prioritizes breadth over depth and intuitive understanding over technical explanations. On the programming side, the Tidyverse approach to programming in R will be adopted. This approach provides a consistent syntax across different packages and is known for its legibility, making it easier for readers to understand and write code. Together, these strategies form an approach that is intended to provide readers with an accessible resource to gain a foothold in the field and to equip them with the knowledge and skills to apply quantitative text analysis in their own research."
  },
  {
    "objectID": "preface.html#structure",
    "href": "preface.html#structure",
    "title": "Preface",
    "section": "Structure",
    "text": "Structure\nThe aims and approach described above is reflected in the overall structure of the book and each chapter.\nBook level\nAt the book level, there are five interdependent parts:\nPart I “Orientation” provides the necessary background knowledge to situate quantitative text analysis in the wider context of data analysis and linguistic research and to provide a clearer picture of what text analysis entails and its range of applications.\nThe subsequent parts are directly aligned with the data analysis process. The building blocks of this process are reflected in ‘Data to Insight Hierarchy (DIKI)’ visualized in Figure 12.\n\n\n\n\nFigure 1: Data to Insight Hierarchy (DIKI)\n\n\n\nThe DIKI Hierarchy highlights the stages and intermediate steps required to derive insight from data. Part II “Foundations” provides a conceptual introduction to the DIKI Hierarchy and establishes foundational knowledge about data, information, knowledge, and insight which is fundamental to developing a viable research plan.\nParts III “Preparation” and IV “Analysis” focus on the implementation process. Part III covers the steps involved in preparing data for analysis, including data acquisition, curation, and transformation. Part IV covers the steps involved in conducting analysis, including exploratory, predictive, and inferential data analysis.\nThe final part, Part V “Communication”, covers the final stage of the data analysis process, which is to communicate the results of the analysis. This includes the structure and content of research reports as well as the process of publishing, sharing, and collaborating on research.\nChapter level\nAt the chapter level, both conceptual and programming skills are developed in stages3. The chapter-level structure is consistent across chapters and can be seen in Table 1.\n\n\nTable 1: The general structure of a chapter including: the component, its purpose, where to find the resource, and the target learning stage.\n\n\n\n\n\n\n\nComponent\nPurpose\nResource\nStage\n\n\n\nOutcomes\nIdentify the learning objectives for the chapter\nTextbook\nIntroduction\n\n\nOverview\nProvide a brief introduction to the chapter topic\nTextbook\nIntroduction\n\n\nCoding Lessons\nTeach programming techniques with hands-on interactive exercises\nGitHub\nSkills\n\n\nContent\nCombine conceptual discussions and programming skills, incorporating thought-provoking questions, relevant studies, and advanced topic references\nTextbook\nKnowledge\n\n\nRecipes\nOffer step-by-step programming examples related to the chapter\nResources website\nComprehension\n\n\nLabs\nAllow readers to apply chapter-specific concepts and techniques\nGitHub\nApplication\n\n\nSummary\nReview the key concepts and skills covered in the chapter\nTextbook\nReview\n\n\nQuestions\nAssess and expand the reader’s knowledge and abilities\nTextbook\nAssessment\n\n\n\n\nEach chapter will begin with a list of key learning outcomes followed by a brief introduction to the chapter’s content. The goal is to orient the reader to the chapter. Next there will be a prompt to complete the interactive coding lesson(s) to introduce reader’s to key programming concepts related to the chapter though hands-on experience and then the main content of the chapter will follow. The content will be a combination of conceptual discussions and programming skills, incorporating thought-provoking questions (‘Consider this’), relevant studies (‘Case study’), and advanced topic references (‘Dive deeper’). Together these components form the skills and knowledge phase. The next phase is the application phase. This phase will include step-by-step programming demonstrations related to the chapter (Recipes) and lab exercises that allow readers to apply their knowledge and skills chapter-related tasks. Finally the chapter concludes with a summary of the key concepts and skills covered in the chapter and a set of questions to assess and expand the reader’s knowledge and abilities."
  },
  {
    "objectID": "preface.html#sec-resouces",
    "href": "preface.html#sec-resouces",
    "title": "Preface",
    "section": "Resources",
    "text": "Resources\nThere are three main resources available to support the aims and approach of this textbook. Firstly, the textbook itself provides prose discussion, figures/ tables, R code, case studies, and thought and practical exercises. Secondly, there is a companion R package called qtalrkit (Francom 2023), which includes functions for accessing data and datasets, as well as various useful functions developed specifically for this textbook. In addition, there is a comprehensive website Quantitative Text Analysis for Linguistics Resources(qtalr website) that includes programming tutorials and demonstrations to enhance the reader’s recognition of how programming strategies are implemented. Finally, a GitHub repository is provided which contains both a set of interactive R programming lessons (Swirl) and lab exercises designed to guide the reader through practical hands-on programming applications. The companion qtalrkit package and the GitHub repository are both under active development and will be updated regularly to ensure that supplementary materials remain relevant to the content of the text4."
  },
  {
    "objectID": "preface.html#sec-getting-started",
    "href": "preface.html#sec-getting-started",
    "title": "Preface",
    "section": "Getting started",
    "text": "Getting started\nBefore jumping in to this and subsequent chapter’s textbook activities, it is important to prepare your computing environment and understand how to take advantage of the resources available, both those directly and indirectly associated with the textbook.\nR and IDEs\n\n\nConsider whether to advocate for RStudio Desktop/ Cloud or include VS Code as an alternative.\n\nProgramming is the backbone for modern quantitative research. Among the many programming languages available, R is a popular open-source language and software environment for statistical computing. R is popular with statisticians and has been adopted as the de facto language by many other fields in natural and social sciences, including linguistics. It is freely downloadable from The R Project for Statistical Programming website and is available for macOS, Linux, and Windows operating systems.\nSuccessfully installing R is rarely the last step in setting up your R-enabled computing environment. The majority of R users also install an integrated development environment (IDE). An IDE, such as RStudio or Visual Studio Code, provide a graphical user interface (GUI) for working with R. In effect, IDEs provide a dashboard for working with R and are designed to make it easier to write and execute R code. IDEs also provide a number of other useful features such as syntax highlighting, code completion, and debugging. IDEs are not required to work with R but they are highly recommended.\nChoosing to install R and an IDE on your personal computer, which is know as your local environment, is not the only option to work with R. You can also choose to work with R in the cloud, a remote environment. There are a number of cloud-based options for working with R, including RStudio Cloud and Microsoft Azure. These options provide a pre-configured R environment that you can access from any computer with an internet connection. The advantage of working in the cloud is that you do not need to install R or an IDE on your local computer. The disadvantage is that you will need to be connected to the internet to work with R and the free tiers for these services are limited. If you are new to R, you may want to consider working in the cloud to get started. If you plan to continue to work with R in the future, you will most likely want to install R and an IDE on your local computer or explore using a virtual environment. Virtual environments, such as Docker, provide a way to use a pre-configured computing environment or create your own that you can share with others. Virtual environments are a good option if you want to ensure that everyone in your research group is working with the same computing environment. Pre-configured virtual environments exist for R through the Rocker project and can be used locally or in the cloud.\nThere are trade-offs in terms of cost, convenience, and flexibility when choosing to work with R in a local, remote, or virtual environment. The choice is yours and you can always change your mind later. The important thing is to get started and begin learning R. Furthermore, any of the approaches described here will be compatible with this textbook.\nFor more information and instructions on setting up an R environment consult the following guides.\n\n\n\n\n\n\n Guides\n\nInstalling R\nChoosing and setting up an IDE\nWorking with R in remote and virtual environments\n\n\n\n\nR packages\nThroughout your R programming journey you will take advantage of code created by other R users in the form of packages. A package is a downloadable set of functions and/ or datasets which aim to accomplish a given cohesive set of related tasks. There are official R package repositories such as CRAN (Comprehensive R Archive Network) and other packages are available on code-sharing repositories such as GitHub.\n\n\n\n\n\n\n Consider this\nThe Comprehensive R Archive Network (CRAN) includes groupings of popular packages related to a given applied programming task called Task Views. Explore the available CRAN Task Views listings. Note the variety of areas (tasks) that are covered in this listing. Now explore in more detail one of the following task views which are directly related to topics covered in this textbook noting the associated packages and their descriptions: (1) Cluster, (2) MachineLearning, (3) NaturalLanguageProcessing, or (4) ReproducibleResearch.\n\n\n\nYou will download a number of packages at different stages of this textbook, but there is a set of packages that will be key to have from the get go. Once you have access to a working R/ RStudio environment, you can proceed to install the following packages.\nInstall the following packages from CRAN.\n\n\ntidyverse (Wickham 2023)\n\n\nrmarkdown (Allaire et al. 2023)\n\n\nquarto (Allaire 2022)\n\n\ntinytex (Xie 2023)\n\n\ndevtools (Wickham et al. 2022)\n\n\nusethis (Wickham et al. 2023)\n\n\nswirl (Kross et al. 2020)\n\n\nYou can do this by running the following code in an R console:\n # install key packages from CRAN\ninstall.packages(c(\"tidyverse\", \"rmarkdown\", \"quarto\", \"tinytex\", \"devtools\", \"usethis\", \"swirl\"))\nFor instructions on how to install the qtalrkit package from GitHub and download and use the interactive R programming lessons for this textbook, see the following guides.\n\n\n\n\n\n\n Guides\n\nGetting started\n\n\n\n\nGit and GitHub\n\n\nMove the labs to qtlar and then create a starred list to link here\n\nGitHub is a code sharing website. Modern computing is highly collaborative and GitHub is a very popular platform for sharing and collaborating on coding projects. The lab exercises for this textbook are shared on GitHub. To access and complete these exercises you will need to sign up for a (free) GitHub account and then set up the version control software git on your computing environment. git is the conduit to interfacing GitHub and for many git will already be installed on your computer (or cloud computing environment).\nFor more information and instructions on setting up version control consult the following guide.\n\n\n\n\n\n\n Guides\n\nSetting up Git and GitHub\n\n\n\n\nGetting help\nThe technologies employed in this approach to text analysis will include a somewhat steep learning curve. And in all honesty, the learning never stops! Experienced programmers and novices alike require support. Fortunately there is a very large community of programmers who have developed many official support resources and who actively contribute to unofficial discussion forums. Together these resources provide ample methods for overcoming any challenge.\nThe first place to look for help with R is the official documentation of the R package you are using. You can access this documentation by running help(package = \"package_name\") in an R console or using the ? operator and then the package or function name. Many R packages often include “Vignettes” (long-form documentation and demonstrations). These can be accessed either by running browseVignettes() in an R console with the package name in quotes (e.g. browseVignettes(\"tidyverse\")). You can also search the web for package documentation and vignettes. A popular site for this purpose is R-Universe.\nIf you are using the RStudio IDE, the easiest and most convenient place to get help with either R or RStudio is through the RStudio “Help” toolbar menu. There you will find links to help resources, guides, and manuals.\nThere are a number of very popular discussion forum websites where the programming community asks and answers questions to real-world issues. These sites often have subsections dedicated to particular programming languages or software. The most popular of these sites is Stack Overflow. There are also R-specific discussion forums such as RStudio Community.\nIf you post a question on one of these communities ensure that if your question involves some coding issue or error that you provide enough background such that the community will be able to help you. This is often referred to as a reproducible example or “reprex”. A reprex is a minimal piece of code that demonstrates the issue you are having. It is a very useful tool for both asking and answering questions.\nFor information on how to create a reprex consult the following guide.\n\n\n\n\n\n\n Guides\n\nCreating reproducible examples\n\n\n\n\nThe take-home message here is that you are not alone. There are many people world-wide that are learning to program and/ or contribute to the learning of others. The more you engage with these resources and communities the more successful your learning will be. As soon as you are able, pay it forward. Posting questions and offering answers helps the community and engages and refines your skills –a win-win."
  },
  {
    "objectID": "preface.html#conventions",
    "href": "preface.html#conventions",
    "title": "Preface",
    "section": "Conventions",
    "text": "Conventions\nTo facilitate the learning process, this textbook will employ a number of conventions. These conventions are intended to help the reader navigate the text and to signal the reader’s attention to important concepts and information.\nProse\nThe following typographic conventions are used throughout the text:\n\n\nItalics\n\nFilenames, file extensions, directory paths, and URLs.\n\n\n\nFixed-width\n\nPackage names, function names, variable names, and in-line code including expressions and operators.\n\n\n\nBold\n\nKey concepts when first introduced.\n\n\n\nLinked text\n\nLinks to internal and external resources, footnotes, and citations including references to R packages when first introduced.\n\n\nCode blocks\nMore lengthy code will be presented in code blocks as seen below.\n\n# A function that takes a name and returns a greeting\ngreet &lt;- function(name) { # function definition\n  paste(\"Hello\", name) # print greeting\n} # end function definition\n\ngreet(name = \"Jerid\") # apply function to a name\n\n&gt; [1] \"Hello Jerid\"\n\n\nThere are a couple of things to note about this code block. First, this code block shows the code that is run in R as well as the ouput that is returned. The code will appear in a box and the output will appear below the box. Both code and output will appear in fixed-width font. Output which is text will be prefixed with &gt;. Second, the # symbol is used to signal a code comment, a human-facing description. Everything right of a # is not run as code. In this textbook you will see code comments above code on a separate line and to the right of code on the same line. It is good practice to comment your code to enhance readability and to help others understand what your code is doing.\n\n\n\n\n\n\n Tip\nSince you are reading this textbook in a web browser there are two more features that you should be aware of. First, you can click on the code block to copy the code to your clipboard. Second, you can click on a function name to see the help documentation for that function.\n\n\n\nAll figures, tables, and images in this textbook are generated by code chunks but only code for those elements that are relevant for discussion will be shown. However, if you wish to see the code for any element in this textbook, you can visit the GitHub repository https://qtalr.github.io/book/.\nCallouts\nCallouts are used to signal the reader’s attention to content, activity, and other important sections. The following callouts are used in this textbook:\nContent\n\n\n\n\n\n\n Outcomes\nLearning outcomes for the chapter appear here.\n\n\n\n\n\n\n\n\n\n Consider this\nPoints for you to consider and questions to explore appear here.\n\n\n\n\n\n\n\n\n\n Case study\nCase studies for applying conceptual knowledge and coding skills covered in the chapter appear here.\n\n\n\n\n\n\n\n\n\n Dive deeper\nLinks to additional resources for diving deeper into the topic appear here.\n\n\n\nActivities\n\n\n\n\n\n\n Swirl lesson\nLinks to swirl lessons for practicing coding skills for the chapter appear here.\n\n\n\n\n\n\n\n\n\n Recipe\nLinks to demonstration programming tasks on the qtalr site for the chapter appear here.\n\n\n\n\n\n\n\n\n\n Lab\nLinks to lab exercises for applying conceptual knowledge and coding skills on the qtalr GitHub repository for the chapter appear here.\n\n\n\nOther\n\n\n\n\n\n\n Tip\nTips for using R and related tools appear here.\n\n\n\n\n\n\n\n\n\n Warning\nWarnings for using R and related tools appear here."
  },
  {
    "objectID": "preface.html#to-the-instructor",
    "href": "preface.html#to-the-instructor",
    "title": "Preface",
    "section": "To the instructor",
    "text": "To the instructor\nDepending on the experience level and expectations of your readers, you may want to consider adopting one of the following course designs for using this textbook.\n\n\nRefine the ‘To the instructor’ section.\n\nBasic Introduction\n\nCover chapters 1-5 in sequence to give your readers a foundational understanding of quantitative text analysis.\nCulminate the course with a research proposal assignment that requires them to identify an interesting linguistic problem, propose ways of solving it using the methods covered in class, and identify potential data sources.\nIf your readers have little to no experience with R, you may want to consider using the RStudio Cloud platform to host the course. This will provide them with a pre-installed R environment and allow them to focus on learning the material rather than troubleshooting.\nIntermediate Introduction\n\nCover chapters 1, 5-10 in sequence to give your readers a deeper understanding of quantitative text analysis methods. Explore additional case studies or dataset examples throughout the course if you wish to supplement your lectures.\nCulminate the course with a research project assignment that allows your readers to apply what they’ve learned to linguistic content of their choice.\nYou may consider using the RStudio Cloud platform to host the course, but ensure that your readers have access to R and RStudio on their own computers as well.\nAdvanced Introduction\n\nCover all 12 chapters to give your readers a thorough understanding of quantitative text analysis concepts and techniques. Devote more time chapters 5-10 providing demonstrations of how to approach different problems and evaluating alternative approaches.\nCulminate the course with a collaborative research project that requires your readers to work in groups to conduct a comprehensive analysis of a given dataset.\nEnsure that your readers install R and RStudio on their own computers as they will need full control over their coding environment.\n\nFor all course designs, it is strongly recommend that you evaluate the readers’ success in understanding the material by providing a combination of quizzes, lab assignments, programming exercises, and written reports. Additionally, encourage your readers to ask questions5, collaborate with peers, and seek help from the ample resources available online when they encounter scope-limited programming problems.\nFor more information on how to use this textbook in your course, visit the Instructor Guide on the compansion website."
  },
  {
    "objectID": "preface.html#activities",
    "href": "preface.html#activities",
    "title": "Preface",
    "section": "Activities",
    "text": "Activities\n\nAt this point you should have a working R environment with the core packages including qtalrkit installed. You should also have verified that you have a working Git environment and that you have a GitHub account. If you have not completed these tasks, return to the guides listed above in “Getting started” of this Preface and complete them before proceeding.\nThe following activities are designed to help you become familiar with the tools and resources that you will be using throughout this textbook. These and subsequent activities are designed to be completed in the order that they are presented in this textbook.\n\n\n\n\n\n\n Swirl lesson\n\nWhat: Intro to SwirlHow: In the R console load swirl, run swirl(), and follow prompts to select the lesson.Why: To familiarize you with navigating, selecting, and completing swirl lessons.\n\n\n\n\n\n\n\n\n\n Recipe\n\nWhat: Literate programming IHow: Read Recipe 0 and participate in collaborative discussion with peers.Why: To introduce the concept of Literate Programming and how to create literate documents using R and Quarto.\n\n\n\n\n\n\n\n\n\n Lab\n\nWhat: Literate programming IHow: Clone, fork, and complete the steps in Lab 0.Why: To put literate programming techniques covered in Recipe 0 into practice. Specifically, you will create and edit a Quarto document and render a report in PDF format."
  },
  {
    "objectID": "preface.html#summary",
    "href": "preface.html#summary",
    "title": "Preface",
    "section": "Summary",
    "text": "Summary\nIn the Preface, we lay the groundwork by introducing the textbook’s underlying principles, learning goals, teaching methods, and target audience. The chapter also offers advice on how to navigate the book’s layout, comprehend its subject matter, and make use of supplementary materials. Crucial insights from this section involve grasping the book’s objectives and aims, which center around instructing readers on quantitative text analysis for linguistics using R while emphasizing reproducible research. This chapter assists readers in setting up a working R development environment ensuring they can effectively engage with the material. Moreover, the Preface provides guidance on how to get help with R and other related software tools and deciphering conventions in the text. With this foundation, you’re now prepared to delve into the captivating realm of text analysis in the subsequent chapter, titled “Text Analysis in Context.”"
  },
  {
    "objectID": "preface.html#questions",
    "href": "preface.html#questions",
    "title": "Preface",
    "section": "Questions",
    "text": "Questions\n\n\n\nRevise/ add questions.\n\n\n\n\n\n\n\nConceptual questions\n\nHow is the textbook designed to be accessible for both novice and seasoned practitioners in the area of quantitative text analysis?\nWhat is the purpose of the textbook and what are the three areas it aims to scaffold?\nWhat are the main components of each chapter, and how are they structured to support learning outcomes?\nHow does the structure of the textbook and associated resources work to support learning and proficiency in areas?\nWhat is the role of programmatic approaches in quantitative text analysis?\nWhat is the relationship between R and an IDE (e.g. RStudio, VS Code)?\nWhat is the relationship between R and a version control system (e.g. Git)?\n\n\n\n\n\n\n\n\n\n\nTechnical exercises\n\nInstall the latest version of R by following the instructions for your operating system. https://cran.r-project.org/\n\nInstall RStudio https://www.rstudio.com/products/rstudio/download/\n\nInstall Git, a version control system that allows you to track changes to files and collaborate with others. https://git-scm.com/downloads\n\nCreate a GitHub account. &lt;…&gt; \n\nInstall the tidyverse package by running install.packages(\"tidyverse\") in the R Console pane.\nInstall the swirl package by running install.packages(\"swirl\") in the R Console pane.\nOpen RStudio and create a new project for this textbook. This will help you keep your code and files organized.\n\n\n\n\n\n\n\n\nAckoff, Russell L. 1989. “From Data to Wisdom.” Journal of Applied Systems Analysis 16 (1): 3–9.\n\n\nAllaire, JJ. 2022. Quarto: R Interface to Quarto Markdown Publishing System. https://github.com/quarto-dev/quarto-r.\n\n\nAllaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, et al. 2023. Rmarkdown: Dynamic Documents for r. https://CRAN.R-project.org/package=rmarkdown.\n\n\nBuckheit, Jonathan B., and David L. Donoho. 1995. “Wavelab and Reproducible Research.” In Wavelets and Statistics, 55–81. Springer.\n\n\nCarmi, Elinor, Simeon J. Yates, Eleanor Lockley, and Alicja Pawluczuk. 2020. “Data Citizenship: Rethinking Data Literacy in the Age of Disinformation, Misinformation, and Malinformation.” Internet Policy Review 9 (2).\n\n\nFrancom, Jerid. 2023. Qtalrkit: Quantitative Text Analysis for Linguists Resource Kit.\n\n\nGandrud, Christopher. 2015. Reproducible Research with r and r Studio. Second edition. CRC Press.\n\n\nKrathwohl, David R. 2002. “A Revision of Bloom’s Taxonomy: An Overview.” Theory into Practice 41 (4): 212–18.\n\n\nKross, Sean, Nick Carchedi, Bill Bauer, and Gina Grdina. 2020. Swirl: Learn r, in r. http://swirlstats.com.\n\n\nRowley, Jennifer. 2007. “The Wisdom Hierarchy: Representations of the DIKW Hierarchy.” Journal of Information Science 33 (2): 163–80. https://doi.org/10.1177/0165551506070706.\n\n\nWickham, Hadley. 2023. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Jennifer Bryan, Malcolm Barrett, and Andy Teucher. 2023. Usethis: Automate Package and Project Setup. https://CRAN.R-project.org/package=usethis.\n\n\nWickham, Hadley, Jim Hester, Winston Chang, and Jennifer Bryan. 2022. Devtools: Tools to Make Developing r Packages Easier. https://CRAN.R-project.org/package=devtools.\n\n\nXie, Yihui. 2023. Tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents. https://github.com/rstudio/tinytex."
  },
  {
    "objectID": "preface.html#footnotes",
    "href": "preface.html#footnotes",
    "title": "Preface",
    "section": "",
    "text": "Research data and questions are primarily based on English for wide accessibility as it is the de facto language of academics and research. However, the methods and techniques presented in this textbook are applicable to many other languages.↩︎\nAdapted from Ackoff (1989) and Rowley (2007).↩︎\nThese stages attempt to capture the general progression of learning reflected in Bloom’s Taxonomy (see Krathwohl (2002) for a description and revision).↩︎\nErrata for the textbook is found on the qtalr website.↩︎\nIf you are using this textbook in a course, consider using a CMS (e.g. Canvas, Blackboard, etc.) or the web-based social annotation tool Hypothes.is to facilitate reader questions and discussion.↩︎"
  },
  {
    "objectID": "orientation.html",
    "href": "orientation.html",
    "title": "Orientation",
    "section": "",
    "text": "Update the overview of Part I “Orientation” to reflect the new structure of the chapter.\n\nIn this section the aims are to: 1) provide an overview of quantitative research and their applications, by both highlighting visible applications and notable research in various fields, 2) consider how quantitative research contributes to language research, and 3) layout the main types of research and situate quantitative text analysis inside these."
  },
  {
    "objectID": "text-analysis.html#text-making-sense-of-a-complex-world",
    "href": "text-analysis.html#text-making-sense-of-a-complex-world",
    "title": "1  Text analysis in context",
    "section": "\n1.1 Making sense of a complex world",
    "text": "1.1 Making sense of a complex world\n\n\n1.1.1 Heuristic Understanding\n\nThe world around us is full of actions and interactions so numerous that it is difficult to really comprehend. As each individual sees and experiences this world, we gain knowledge and build up heuristic understanding about how it works and how we can interact with it. This happens regardless of your educational background. As humans we are built for this. Our minds process countless sensory inputs. They underlie skills and abilities that we take for granted like being able to predict what will happen if you see someone about to knock a wine glass off a table and onto a concrete floor. You’ve never seen this object before and this is the first time you’ve been to this winery, but somehow and from somewhere you ‘instinctively’ make an effort to warn the would-be-glass-breaker before it is too late. You most likely have not stopped to consider where this predictive knowledge comes from, or if you have, you may have just chalked it up to ‘common sense’. As common as it may be, it is an incredible display of the brain’s capacity to monitor your environment, relate the events and observations that take place, and store that information all the time not making a big fuss to tell your conscious mind what it’s up to.\nSo wait, this is a textbook on text analysis, right? So what does all this have to do with that? Well, there are two points to make that are relevant for framing our journey: (1) the world is constantly churning out data in real-time at a scale that is daunting and (2) for all the power of the brain that works so efficiently behind the scene making sense of the world, we are one individual living one life that has a limited view of the world at large. Let me expand on these two points a little more.\nFirst let’s be clear. There is no way for anyone to experience all things at all times. But even extremely reduced slices of reality are still vastly outside of our experiential capacity, at least in real-time. One can make the point that since the inception of the internet an individual’s ability to experience larger slices of the world has increased. But could you imagine reading, watching, and listening to every file that is currently accessible on the web? Or has been? (See the Wayback Machine.) Scale this down even further; let’s take Wikipedia, the world’s largest encyclopedia. Can you imagine reading every wiki entry? As large as a resource such as Wikipedia is 1, it is still a small fragment of the written language that is produced on the web, just the web 2. Consider that for a moment.\nTo my second framing point, which is actually two points in one. I underscored the efficiency of our brain’s capacity to make sense of the world. That efficiency comes from some clever evolutionary twists that lead our brain to take in the world but it makes some shortcuts that compress the raw experience into heuristic understanding. What that means is that the brain is not a supercomputer. It does not store every experience in raw form, we do not have access to the records of our experience like we would imagine a computer would have access to the records logged in a database. Where our brains do excel is in making associations and predictions that help us (most of the time) navigate the complex world we inhabit. This point is key –our brains are doing some amazing work, but that work can give us the impression that we understand the world in more detail that we actually do. Let’s do a little thought experiment. Close your eyes and think about the last time you saw your best friend. What were they wearing? Can you remember the colors? If your like me, or any other human, you probably will have a pretty confident feeling that you know the answers to these questions and there is a chance you a right. But it has been demonstrated in numerous experiments on human memory that our confidence does not correlate with accuracy (Talarico and Rubin 2003; Roediger and McDermott 2000). You’ve experienced an event, but there is no real reason that we should bet our lives on what we experienced. It’s a little bit scary, for sure, but the magic is that it works ‘good enough’ for practical purposes.\nSo here’s the deal: as humans we are (1) clearly unable to experience large swaths of experience by the simple fact that we are individuals living individual lives and (2) the experiences we do live are not recorded with precision and therefore we cannot ‘trust’ our intuitions, at least in an absolute sense.\n\n\n\n\n\n\n Consider this\nHow might your own experiences and biases influence your understanding of the world? Language? What are some ways that you can mitigate these biases? Is ever possible to be completely objective?\n\n\n\n\n1.1.2 Science to advance understanding\n\nWhat does that mean for our human curiosity about the world around us and our ability to reliably make sense of it? In short it means that we need to approach understanding our world with the tools of science. Science starts with a question, identifies and collects data, careful selected slices of the complex world, submits this data to analysis through clearly defined and reproducible procedures, and reports the results for others to evaluate. This process is repeated, modifying, and manipulating the procedures, asking new questions and positing new explanations, all in an effort to make inroads to bring the complex into tangible view.\nIn essence what science does is attempt to subvert our inherent limitations in understanding by drawing on carefully and purposefully collected slices of observable experience and letting the analysis of these observations speak, even if it goes against our intuitions (those powerful but sometime spurious heuristics that our brains use to make sense of the world)."
  },
  {
    "objectID": "text-analysis.html#data-analysis",
    "href": "text-analysis.html#data-analysis",
    "title": "1  Text analysis in context",
    "section": "\n1.2 Data analysis",
    "text": "1.2 Data analysis\n\n\n1.2.1 Emergence of data science\n\nAt this point I’ve sketched an outline strengths and limitations of humans’ ability to make sense of the world and why science is used to address these limitations. This science I’ve described is the one you are familiar with and it has been an indispensable tool to make sense of the world. If you are like me, this description of science may be associated with visions of white coats, labs, and petri dishes. While science’s foundation still stands strong in the 21st century, a series of intellectual and technological events mid-20th century set in motion changes that have changed aspects about how science is done, not why it is done. We could call this Science 2.0, but let’s use the more popularized term data science. The recognized beginnings of data science are attributed to work in the “Statistics and Data Analysis Research” department at Bell Labs during the 1960s. Although primarily conceptual and theoretic at the time, a framework for quantitative data analysis took shape that would anticipate what would come: sizable datasets which would “[…] require advanced statistical and computational techniques […] and the software to implement them.” (Chambers 2020) This framework emphasized both the inference-based research of traditional science, but also embraced exploratory research and recognized the need to address practical considerations that would arise when working with and deriving insight from an abundance of machine-readable data.\nFast-forward to the 21st century a world in which machine-readable data is truly in abundance. With increased computing power and innovative uses of this technology the world wide web took flight. To put this in perspective, in 2019 it was estimated that every minute 511 thousand tweets were posted, 18.1 million text messages were sent, and 188 million emails were sent (“Data Never Sleeps 7.0 Infographic” 2019). The data flood has not been limited to language, there are more sensors and recording devices than ever before which capture evermore swaths of the world we live in (Desjardins 2019). Where increased computing power gave rise to the influx of data, it is also one of the primary methods for gathering, preparing, transforming, analyzing, and communicating insight derived from this data (Donoho 2017). The vision laid out in the 1960s at Bell Labs had come to fruition.\n\n1.2.2 Computing skills, statistical knowledge, and domain knowledge\n\nData science is not predicated on data alone. Turning data into insight takes computing skills (i.e. programming), statistical knowledge, and domain expertise. This triad has been popularly represented as a Venn diagram such as in Figure 1.1.\n\n\n\n\nFigure 1.1: Data Science Venn Diagram adapted from Drew Conway.\n\n\n\nThe computing skills component of data science is the ability to write code to perform the data analysis process. This is the primary approach for working with data at scale. The statistical knowledge component of data science is the ability to apply statistical methods to data to derive insight. Domain expertise provides researchers insight at key junctures in the development of a research project and aid researchers in evaluating results.\nThis triad of skills in combination with reproducible research practices is the foundational toolbelt of data science (Hicks and Peng 2019). Reproducible research entails the use of computational tools to automate the process of data analysis. This automation is achieved by writing code that can be executed to replicate the data analysis. This code can then be shared through code sharing repositories, such as GitHub, where it can be viewed, downloaded, and executed by others. This adds transparency to the process and allows others to build on previous work. This is in contrast to traditional approaches where data analysis is performed (semi-)manually, results are reported in a static document such as a report or journal article, and the data analysis process is not shared. This approach is not reproducible because the data analysis process is not transparent and cannot be replicated. This is problematic because it is difficult to evaluate the results and build on previous work. Reproducible research practices are a key component of data science and are emphasized throughout this book.\n\n1.2.3 Applications of data science\nEquipped with the data science toolbelt, the interest in deriving insight from the available data is now almost ubiquitous. The science of data has now reached deep into all aspects of life where making sense of the world is sought. Predicting whether a loan applicant will get a loan (Bao, Lianju, and Yue 2019), whether a lump is cancerous (Saxena and Gyanchandani 2020), what films to recommend based on your previous viewing history (Gomez-Uribe and Hunt 2015), what players a sports team should sign (Lewis 2004) all now incorporate a common set of data analysis tools.\nThe data science toolbelt also underlies well-known public-facing language applications. From the language-capable chat applications, plagiarism detection software, machine translation algorithms, and search engines, tangible results of quantitative approaches to language are becoming standard fixtures in our lives.\n\n\n\nAdd OpenAI’s GPT to this list\n\n\n\n\n\nFigure 1.2: Well-known language applications\n\n\n\nThe spread of quantitative data analysis too has taken root in academia. Even in areas that on first blush don’t appear readily approachable in a quantitative manner, such as fields in the social sciences and humanities, data science is making important and sometimes disciplinary changes to the way that academic research is conducted. This textbook focuses in on a domain that cuts across many of these fields; namely language. At this point let’s turn to quantitative approaches to language analysis as we work closer to contextualizing text analysis."
  },
  {
    "objectID": "text-analysis.html#language-analysis",
    "href": "text-analysis.html#language-analysis",
    "title": "1  Text analysis in context",
    "section": "\n1.3 Language analysis",
    "text": "1.3 Language analysis\nLanguage is a defining characteristic of our species. Since antiquity, language has attracted interest across disciplines and schools of thought. In the early 20th century, the development of the rigorous approach to study of language as a field in its own right took root (Campbell 2001), yet a plurality of theoretical views and methodological approaches remained. Contemporary linguistics bares this complex history and is far from theoretically and methodologically unified.\nEither based on the tenets of theoretical frameworks and/or the objects of study of particular fields, approaches to language research vary. On the one hand some language research commonly applies qualitative assessment of language structure and/ or use. Qualitative approaches describe and account for characteristics, or “qualities”, that can be observed, but not measured (e.g. introspective methods, ethnographic methods, etc.)\nOn the other hand other language research programs employ quantitative research methods either out of necessity given the object of study (phonetics, psycholinguistics, etc.) or based on theoretical principles (Cognitive Linguistics, Connectionism, etc.). Quantitative approaches involve measurements of properties of language that can be observed and measured (e.g. frequency of use, reaction time, etc.).\nThese latter research areas and theoretical paradigms employ methods that share much of the common data analysis toolbox described in the previous section. In effect, this establishes a common methodological language between other language research fields but also with research outside of linguistics.\nHowever, there is never a one-size-fits all approach to anything –much less data analysis. And even in quantitative language analysis there is a key methodological distinction that has downstream effects in terms of procedure but also in terms of interpretation. The key distinction that we need to make at this point, which will provide context for our introduction to quantitative text analysis, comes down to the approach to collecting language data and the nature of that data. This distinction is between experimental data and observational data.\nExperimental approaches start with a intentionally designed hypothesis and lay out a research methodology with appropriate instruments and a plan to collect data that shows promise for shedding light on the validity of the hypothesis. Experimental approaches are conducted under controlled contexts, usually a lab environment, in which participants are recruited to perform a language related task with stimuli that have been carefully curated by researchers to elicit some aspect of language behavior of interest. Experimental approaches to language research are heavily influenced by procedures adapted from psychology. This link is logical as language is a central area of study in cognitive psychology. This approach looks much like the white-coat science that we made reference to earlier but, as in most quantitative research, has now taken advantage of the data analysis toolbelt to collect and organize much larger quantities of data and conduct statistically more robust analysis procedures and communicate findings more efficiently.\nObservational approaches are a bit more of a mixed bag in terms of the rationale for the study; they may either start with a testable hypothesis or in other cases may start with a more open-ended research question to explore. But a more fundamental distinction between the two is drawn in the amount of control the researcher has on contexts and conditions in which the language behavior data to be collected is produced. Observational approaches seek out records of language behavior that is produced by language speakers for communicative purposes in natural(istic) contexts. This may take place in labs (language development, language disorders, etc.), but more often than not, language is collected from sources where speakers are performing language as part of their daily lives –whether that be posting on social media, speaking on the telephone, making political speeches, writing class essays, reporting the latest news for a newspaper, or crafting the next novel destined to be a New York Times best-seller. What is more, data collected from the ‘wild’ varies more in structure relative to data collected in experimental approaches and requires a number of steps to prepare the data to sync up with the data analysis toolbelt.\nI liken this distinction between experimental and observational data collection to the difference between farming and foraging. Experimental approaches are like farming; the groundwork for a research plan is designed, much as a field is prepared for seeding, then the researcher performs as series of tasks to produce data, just as a farmer waters and cares for the crops, the results of the process bear fruit, data in our case, and this data is harvested. Observational approaches are like foraging; the researcher scans the available environmental landscape for viable sources of data from all the naturally existing sources, these sources are assessed as to their usefulness and value to address the research question, the most viable is selected, and then the data is collected.\nThe data acquired from both of these approaches have their trade-offs, just as farming and foraging. Experimental approaches directly elicit language behavior in highly controlled conditions. This directness and level of control has the benefit of allowing researchers to precisely track how particular experimental conditions effect language behavior. As these conditions are an explicit part of the design and therefore the resulting language behavior can be more precisely attributed to the experimental manipulation. The primary shortcoming of experimental approaches is that there is a level of artificialness to this directness and control. Whether it is the language materials used in the task, the task itself, or the fact that the procedure takes place under supervision the language behavior elicited can diverge quite significantly from language behavior performed in natural communicative settings. Observational approaches show complementary strengths and shortcomings. Whereas experimental approaches may diverge from natural language use, observational approaches strive to identify and collected language behavior data in natural, uncontrolled, and unmonitored contexts. In this way observational approaches do not have to question to what extent the language behavior data is or is not performed as a natural communicative act. On the flipside, the contexts in which natural language communication take place are complex relative to experimental contexts. Language collected from natural contexts are nested within the complex workings of a complex world and as such inevitably include a host of factors and conditions which can prove challenging to disentangle from the language phenomenon of interest but must be addressed in order to draw reliable associations and conclusions.\n\n\n\n\ngraph TD\nstyle A fill:#aaa,stroke:#333,stroke-width:1px\nclassDef tier2 fill:#ccc,stroke:#333,stroke-width:1px;\nclassDef tier3 fill:#ddd,stroke:#333,stroke-width:1px;\nclassDef tier4 fill:#eee,stroke:#333,stroke-width:1px;\nA{{Approaches}} --&gt; B[Experimental]:::tier2\n  B --&gt; D[Strengths]:::tier3\n    D --&gt; H[Controlled conditions]:::tier4\n  B --&gt; E[Shortcomings]:::tier3\n    E --&gt; J[Level of artificialness]:::tier4\nA --&gt; C[Observational]:::tier2\n  C --&gt; F[Strengths]:::tier3\n    F --&gt; L[Level of naturalness]:::tier4\n  C --&gt; G[Shortcomings]:::tier3\n    G --&gt; N[Uncontrolled conditions]:::tier4\n\n\nFigure 1.3: Experimental and observational data collection methods.\n\n\n\nThe upshot, then, is twofold: (1) data collection methods matter for research design and interpretation and (2) there is no single best approach to data collection, each have their strengths and shortcomings. In the ideal, a robust science of language will include insight from both experimental and observational approaches (Gilquin and Gries 2009). And evermore there is greater appreciation for the complementary nature of experimental and observational approaches and a growing body of research which highlights this recognition. Given their particular trade-offs observational data is often used as an exploratory starting point to help build insight and form predictions that can then be submitted to experimental conditions. In this way studies based on observational data serve as an exploratory tool to gather a better and more externally valid view of language use which can then serve to make prediction that can be explored with more precision in an experimental paradigm. However, this is not always the case; observational data is also often used in hypothesis-testing contexts as well. And furthermore, some in some language-related fields, a hypothesis-testing is not the ultimate goal for deriving knowledge and insight."
  },
  {
    "objectID": "text-analysis.html#text-analysis",
    "href": "text-analysis.html#text-analysis",
    "title": "1  Text analysis in context",
    "section": "\n1.4 Text analysis",
    "text": "1.4 Text analysis\n\n\nRevise this section.\n\nIn a nutshell, text analysis is the process of leveraging the data science toolbelt to derive insight from textual data collected through observational methods. In the next subsections, I will unpack this definition and discuss the primary components that make up text analysis including research appoaches and technical implementation, as well as practical applications.\n\n1.4.1 Approaches\nText analysis is a multifacited research methodology. It can be used use facilitate the qualitative exploration of smaller, human-digestable textual information, but is more often employed quantitatively to bring to the surface patterns and relationships in large samples of textual data that would be otherwise difficult, if not impossible, to identify manually.\nText being text, there are a series of data prepration steps that must be taken to ready the data for analysis. In addition to collecting the data, the data must be organized, cleaned, and transformed into a format that is amenable to statistical analysis.\nThe statistical and evaluative approach employed in the analysis is dependent on the aim of the research. For research aimed at exploring and uncovering patterns and relationships in the data, exploratory data analysis (EDA) is employed. EDA combines descriptive statistics, visualizations, and statistical learning methods in an iterative and interactive way to provide the researcher the ability to identify patterns and relationships and to evaluate whether and why they are meaningful.\nFor research aimed at predicting some target outcome variable, predictive data analysis (PDA) is employed. PDA is a supervised machine learning task that uses a set of features to predict a target outcome variable. The burden of evaluation is on the predictive power of the model.\nFor research aimed at explaining relationships between variables and the population from which the sample was drawn, inferential data analysis (IDA) is employed. IDA is a statistical learning task that uses a set of features to predict a target outcome variable. The burden of evaluation is on the explanatory power of the model.\nIn this way, text analysis can be used for a variety of purposes; from data-driven exploration and discovery to hypothesis testing and generalization.\n\n1.4.2 Implementation\nTo ensure that the results of text analysis projects are replicable and transparent, programming languages form play an integral role at each stage of the implementation of a research project. While there are a number of programming languages that can be used for text analysis, R is the most popular and widely used, particularly in linguistics. R is a free and open-source programming language that is specifically designed for statistical computing and graphics. It has a large and active community of users and developers, and a robust ecosystem of packages which make it a powerful and flexible language that is well-suited for core text analysis tasks: data collection, organization, transformation, analysis, and visualization. When combined with Quarto for literate programming and GitHub for version control and collaboration, R provides a robust and reproducible workflow for text analysis.\n\n1.4.3 Applications\nSo what are some applications of text analysis? Most public facing applications stem from Computational Linguistic research, often known as Natural Language Processing (NLP) by practitioners. Whether it be using search engines, online translators, submitting your paper to plagiarism detection software, etc. the text analysis methods we will cover are at play. These uses of text analysis are production-level applications aimed at performing practical tasks for consumers.\n\n\n\n\n\n\n Consider this\nWhat are some other public facing applications of text analysis that you are aware of?\n\n\n\nIn academia the use of quantitative text analysis is even more widespread, despite the lack of public fanfare. In linguistics, text analysis can be applied to a wide range of topics and research questions in both theoretical and applied subfields.\n… theoretical examples …\n\n\n\n\n\n\n Case study\n\nmorphology?\nsyntax?\n\nWasow paper Wasow and Arnold (2005) (I used this in the Corpus Studies of Syntax, chapter)\nManning paper Manning (2003)\n\n\n\n\n\n\n\n\n\n\n\n\n\n Case study\nManning (2003) discusses the use of probabilistic models in syntax to account for the variability in language usage and the presence of both hard and soft constraints in grammar. The paper touches on the statistical methods in text analysis, the importance of distinguishing between external and internal language, and the limitations of Generative Grammar. Overall, the paper suggests that usage-based and formal syntax can learn from each other to better understand language variation and change.\n\n\n\n\n\n\n\n\n\n Case study\n… applied examples …\n\nBychkovska and Lee (2017) investigates possible differences between L1-English and L1-Chinese undergraduate students’ use of lexical bundles, multiword sequences which are extended collocations (i.e. as the result of), in argumentative essays. The authors used the Michigan Corpus of Upper-Level Student Papers (MICUSP) corpus using the argumentative essay section for L1-English and the Corpus of Ohio Learner and Teacher English (COLTE) for the L1-Chinese English essays. They found that L1-Chinese writers used more than 2 times as many bundle types than L1-English peers which they attribute to L1-Chinese writers attempt to avoid uncommon expressions and/or due to their lack of register awareness (conversation has more bundles than writing generally).\n\n\n\n\n\n\n\n\n\n Case study\n\nWulff, Stefanowitsch, and Gries (2007) explore differences between British and American English at the lexico-syntactic level in the into-causative construction (ex. ‘He tricked me into employing him.’). The analysis uses newspaper text (The Guardian and LA Times) and the findings suggest that American English uses this construction in verbal persuasion verbs whereas British English uses physical force verbs.\n\n\n\n\nWhat is the role of text analysis in linguistics research?\nMeans to an end: text analysis as a tool for other language research methods (hypothesis generation, external valid data collection, linguistic annotation, methodological triangulation, etc. (Francom 2022))\n\n… examples of text analysis to support other research methods …\nBaayen, Feldman, and Schreuder (2006)\n\nEnd in itself: text analysis as the research method to gain a deeper understanding of language structure, function, variation, and acquisition, which can contribute to both theoretical (phonetics, morphology, syntax, semantics) and applied research (language acquisition, sociolinguistics, computational linguistics, psycholinguistics).\n\n… examples of text analysis as the research method\n\n\nAdd citations to a few examples of text analysis in linguistics research, instead of the abstracts?\n\n… research from areas such as translation, stylistics, language variation, dialectology, psychology, psycholinguistics, political science, and sociolinguistics which highlights the diversity of fields and subareas which employ quantitative text analysis.\n\n\n\n\n\n\n Consider this\nLanguage is a key component of human communication and interaction. What are some other areas of research in and outside linguistics that you think could benefit from the use of text analysis?\n\n\n\nConway et al. (2012) investigate whether the established drop in language complexity of rhetoric in election seasons is associated with election outcomes.\n\n\n\nMosteller and Wallace (1963) provide a method for solving the authorship debate surrounding The Federalist papers.\n\n\n\nKloumann et al. (2012) explore the extent to which languages are positively, neutrally, or negatively biased.\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nCase study\nEisenstein et al. (2012) track the geographic spread of neologisms from city to city in the United States using Twitter data collected between 6/2009 and 5/2011. They only used tweets with geolocation data and then associated each tweet with a zip code using the US Census. The most populous metropolitan areas were used. They also used the demographics from these areas to make associations between lexical innovations and demographic attributes. From this analysis they are able to reconstruct a network of linguistic influence. One of the main findings is that demographically-similar cities are more likely to share linguistic influence. At the individual level, there is a strong, potentially stronger role of demographics than geographical location.\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nCase study\nVoigt et al. (2017) explore potential racial disparities in officer respect in police body camera footage. The dataset is based on body camera footage from the Oakland Police Department during April 2014. At total of 981 stops by 245 different officers were included (black 682, white 299) and resulted in 36,738 officer utterances. The authors found evidence for racial disparities in respect but not formality of utterances, with less respectful language used with the black community members.\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nCase study\nJaeger and Snider (2007) use a corpus study to investigate the phenomenon of syntactic persistence, the increased tendency for speakers to use a particular syntactic form over an alternate when the syntactic form is recently processed. The authors attempt to distinguish between two competing explanations for the phenomenon: (1) transient activation, where the increased tendency is short-lived and time-bound and (2) implicit learning, where the increased tendency is a reflect of learning mechanisms. The use of a speech corpora (Switchboard and spoken BNC) were used to avoid the artificialness that typically occurs in experimental settings. The authors investigated the ditransitive alternation (NP PP/ NP NP), voice alternation (active/ passive), and complementizer/ relativizer omission. In these alternations structural bias was established by measuring the probability for a verb form to appear in one of the two syntactic forms. Then the probability that that form (target) would change given previous exposure to the alternative form (prime) was calculated; what the authors called surprisal. Distance between the prime structure and the target verb were considered in the analysis. In these alternations, the less common structure was used in the target more often when the when it corresponded to the prime form (higher surprisal) suggesting that implicit learning underlies syntactic persistence effects.\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nCase study\nOlohan (2008) investigate the extent to which translated texts differ from native texts. In particular the author explores the notion of explicitation in translated texts (the tendency to make information in the source text explicit in the target translation). The study makes use of the Translational English Corpus (TEC) for translation samples and comparable sections of the British National Corpus (BNC) for the native samples. The results suggest that there is a tendency for syntactic explicitation in the translational corpus (TEC) which is assumed to be a subconscious process employed unwittingly by translators."
  },
  {
    "objectID": "text-analysis.html#summary",
    "href": "text-analysis.html#summary",
    "title": "1  Text analysis in context",
    "section": "Summary",
    "text": "Summary\nIn this chapter I started with some general observations about the difficulty of making sense of a complex world. The standard approach to overcoming inherent human limitations in sense making is science. In the 21st century the toolbelt for doing scientific research and exploration has grown in terms of the amount of data available, the statistical methods for analyzing the data, and the computational power to manage, store, and share the data, methods, and results from quantitative research. The methods and tools for deriving insight from data have made significant inroads in and outside academia, and increasingly figure in the quantitative investigation of language. Text analysis is a particular branch of this enterprise based on observational data from real-world language and is used in a wide variety of fields.\nIn the end I hope that you enjoy this exploration into text analysis. Although the learning curve at times may seem steep –the experience you will gain will not only improve your data literacy, research skills, and programmings skills but also enhance your appreciation for the richness of human language and its important role in our everyday lives."
  },
  {
    "objectID": "text-analysis.html#actitivies",
    "href": "text-analysis.html#actitivies",
    "title": "1  Text analysis in context",
    "section": "Actitivies",
    "text": "Actitivies\n\n\nAdd summary of the goals and outcomes of these activities.\n\n\n\n\n\n\n\n Recipe\n\nWhat: Literate programming IIHow: Read Recipe 1 and participate in the Hypothes.is online social annotation.Why: To explore additional functionality in Quarto: numbered sections, table of contents, in-line citations and a document-final references list, and cross-referenced tables and figures.\n\n\n\n\n\n\n\n\n\n Lab\n\nWhat: Literate programming IIHow: Clone, fork, and complete the steps in Lab 1.Why: To put into practice R Markdown functionality to communicate the aim(s) and main finding(s) from a primary research article and to interpret a related plot."
  },
  {
    "objectID": "text-analysis.html#questions",
    "href": "text-analysis.html#questions",
    "title": "1  Text analysis in context",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\n\nConceptual questions\n\nDiscriminate between quantitative and non-quantitative research.\nIdentify research in an area of interest in linguistics that has taken a quantitative approach to text analysis.\nWhat are the benefits of reproducible research in data science?\nIn your own words, define literate programming?\nWhat are the benefits of literate programming?\nWhat are the benefits of using R and Quarto for literate programming?\n\n\nWhat is the importance of science in making sense of a complex world?\nHow has the tool belt for scientific research and exploration changed in the 21st century?\nWhat is text analysis and how is it used in various fields?\nWhat are the three fundamental areas that this textbook aims to develop knowledge and skills in?\nWhat are the benefits of learning text analysis beyond just improving data literacy, research skills, and programming skills?\nHow does the structure of the textbook lead to the development of knowledge and skills in data literacy, research skills, and programming skills?\nHow does text analysis contribute to our understanding of human language and its role in our everyday lives?\n\n\n\n\n\n\n\n\n\n\nTechnical exercises\n\nCreate various data types in R (e.g. vectors, matrices, data frames, lists, etc.) and explore their properties.\nCreate a literate programming document in Quarto. Edit the yaml header to reflect details of the work and add your work with the data types in R to code chunks. Add, commit, and push the project to GitHub.\nFork a quantitative text analysis project on GitHub and clone it to your local machine. Run the code and explore the results. \n\n…\nExplore the following resources and identify a quantitative text analysis project. Rpubs, GitHub, DataCamp, Kaggle, R-bloggers.\nThe repository “Text Mining and Sentiment Analysis of Twitter Data” located at https://github.com/anantavijay/Text-Mining-and-Sentiment-Analysis-of-Twitter-Data contains a real-world quantitative text analysis study with reproducible code in R. The repository contains code for sentiment analysis of Twitter data and for text-mining techniques such as tokenization, stemming, and sentiment analysis. The repository also contains an R script that can be used to reproduce the results of the analysis.\n\n\n\n\n\n\n\n\n\nBaayen, R. Harald, L. B. Feldman, and R. Schreuder. 2006. “Morphological Influences on the Recognition of Monosyllabic Monomorphemic Words.” Journal of Memory and Language 55: 290–313. https://doi.org/10.1016/j.jml.2006.03.008.\n\n\nBao, Wang, Ning Lianju, and Kong Yue. 2019. “Integration of Unsupervised and Supervised Machine Learning Algorithms for Credit Risk Assessment.” Expert Systems with Applications 128 (August): 301–15. https://doi.org/10.1016/j.eswa.2019.02.033.\n\n\nBychkovska, Tetyana, and Joseph J. Lee. 2017. “At the Same Time: Lexical Bundles in L1 and L2 University Student Argumentative Writing.” Journal of English for Academic Purposes 30 (November): 38–52. https://doi.org/10.1016/j.jeap.2017.10.008.\n\n\nCampbell, Lyle. 2001. “The History of Linguistics.” In The Handbook of Linguistics, edited by Mark Aronoff and Janie Rees-Miller, 81–104. Blackwell Handbooks in Linguistics. Blackwell Publishers.\n\n\nChambers, John M. 2020. “S, r, and Data Science.” Proceedings of the ACM on Programming Languages 4 (HOPL): 1–17. https://doi.org/10.1145/3386334.\n\n\nConway, Lucian Gideon, Laura Janelle Gornick, Chelsea Burfeind, Paul Mandella, Andrea Kuenzli, Shannon C. Houck, and Deven Theresa Fullerton. 2012. “Does Complex or Simple Rhetoric Win Elections? An Integrative Complexity Analysis of u.s. Presidential Campaigns.” Political Psychology 33 (5): 599–618. https://doi.org/10.1111/j.1467-9221.2012.00910.x.\n\n\n“Data Never Sleeps 7.0 Infographic.” 2019. https://www.domo.com/learn/infographic/data-never-sleeps-7.\n\n\nDesjardins, Jeff. 2019. “How Much Data Is Generated Each Day?” Visual Capitalist.\n\n\nDonoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734.\n\n\nEisenstein, Jacob, Brendan O’Connor, Noah A Smith, and Eric P Xing. 2012. “Mapping the Geographical Diffusion of New Words.” Computation and Language, 1–13. https://doi.org/10.1371/journal.pone.0113114.\n\n\nFrancom, Jerid. 2022. “Corpus Studies of Syntax.” In The Cambridge Handbook of Experimental Syntax, edited by Grant Goodall, 687–713. Cambridge Handbooks in Language and Linguistics. Cambridge University Press.\n\n\nGilquin, Gaëtanelle, and Stefan Th Gries. 2009. “Corpora and Experimental Methods: A State-of-the-Art Review.” Corpus Linguistics and Linguistic Theory 5 (1): 1–26. https://doi.org/10.1515/CLLT.2009.001.\n\n\nGomez-Uribe, Carlos A., and Neil Hunt. 2015. “The Netflix Recommender System: Algorithms, Business Value, and Innovation.” ACM Transactions on Management Information Systems (TMIS) 6 (4): 1–19.\n\n\nHicks, Stephanie C., and Roger D. Peng. 2019. “Elements and Principles for Characterizing Variation Between Data Analyses.” arXiv. https://doi.org/10.48550/arXiv.1903.07639.\n\n\nJaeger, T Florian, and Neal Snider. 2007. “Implicit Learning and Syntactic Persistence: Surprisal and Cumulativity.” University of Rochester Working Papers in the Language Sciences 3 (1).\n\n\nKloumann, IM, CM Danforth, KD Harris, and CA Bliss. 2012. “Positivity of the English Language.” PloS One.\n\n\nLewis, Michael. 2004. Moneyball: The Art of Winning an Unfair Game. WW Norton & Company.\n\n\nManning, Christopher. 2003. “Probabilistic Syntax.” In Probabilistic Linguistics, edited by Bod, Jennifer Hay, and Jannedy, 289–341. Cambridge, MA: MIT Press.\n\n\nMosteller, Frederick, and David L Wallace. 1963. “Inference in an Authorship Problem.” Journal of the American Statistical Association 58 (302): 275–309. https://www.jstor.org/stable/2283270.\n\n\nOlohan, Maeve. 2008. “Leave It Out! Using a Comparable Corpus to Investigate Aspects of Explicitation in Translation.” Cadernos de Tradução, 153–69.\n\n\nRoediger, H. L. L, and K. B. B McDermott. 2000. “Distortions of Memory.” The Oxford Handbook of Memory, 149–62.\n\n\nSaxena, Shweta, and Manasi Gyanchandani. 2020. “Machine Learning Methods for Computer-Aided Breast Cancer Diagnosis Using Histopathology: A Narrative Review.” Journal of Medical Imaging and Radiation Sciences 51 (1): 182–93.\n\n\nTalarico, Jennifer M., and David C. Rubin. 2003. “Confidence, Not Consistency, Characterizes Flashbulb Memories.” Psychological Science 14 (5): 455–61. https://doi.org/10.1111/1467-9280.02453.\n\n\nVoigt, Rob, Nicholas P. Camp, Vinodkumar Prabhakaran, William L. Hamilton, Rebecca C. Hetey, Camilla M. Griffiths, David Jurgens, Dan Jurafsky, and Jennifer L. Eberhardt. 2017. “Language from Police Body Camera Footage Shows Racial Disparities in Officer Respect.” Proceedings of the National Academy of Sciences 114 (25): 6521–26.\n\n\nWasow, Thomas, and Jennifer Arnold. 2005. “Intuitions in Linguistic Argumentation.” Lingua 115 (11): 1481–96.\n\n\nWulff, S, A Stefanowitsch, and Stefan Th. Gries. 2007. “Brutal Brits and Persuasive Americans.” Aspects of Meaning."
  },
  {
    "objectID": "text-analysis.html#footnotes",
    "href": "text-analysis.html#footnotes",
    "title": "1  Text analysis in context",
    "section": "",
    "text": "As of 22 July 2021, there are 6,341,359 articles in the English Wikipedia containing over 3.9 billion words occupying around 19 gigabytes of information.↩︎\nFor reference, Common Crawl has millions of gigabytes collected since 2008.↩︎"
  },
  {
    "objectID": "foundations.html",
    "href": "foundations.html",
    "title": "Foundations",
    "section": "",
    "text": "Before working on the specifics of a data project, it is important to establish a fundamental understanding of the characteristics of each of the levels in the “Data, Information, Knowledge, and Insight Hierarchy (DIKI)” (see Figure 1) and the roles each of these levels have in deriving insight from data. In Chapter 2 we will explore the Data and Information levels drawing a distinction between two main types of data (populations and samples) and then cover how data is structured and transformed to generate information (datasets) that is fit for statistical analysis. In Chapter 3 I will outline the importance and distinct types of statistical procedures (descriptive and analytic) that are commonly used in text analysis. Chapter 4 aims to tie these concepts together and cover the required steps for preparing a research blueprint to conduct an original text analysis project."
  },
  {
    "objectID": "understanding-data.html#sec-ud-data",
    "href": "understanding-data.html#sec-ud-data",
    "title": "2  Understanding data",
    "section": "\n2.1 Data",
    "text": "2.1 Data\nData is data, right? The term ‘data’ is so common in popular vernacular it is easy to assume we know what we mean when we say ‘data’. But as in most things, where there are common assumptions there are important details that require more careful consideration. Let’s turn to the first key distinction that we need to make to start to break down the term ‘data’: the difference between populations and samples.\n\n\n2.1.1 Populations\n\nThe first thing that comes to many people’s mind when the term population is used is human populations (derived from Latin ‘populus’). Say for example we pose the question –What’s the population of Milwuakee? When we speak of a population in these terms we are talking about the total sum of individuals living within the geographical boundaries of Milwaukee. In concrete terms, a population an idealized set of objects or events in reality which share a common characteristic or belong to a specific category. The term to highlight here is idealized. Although we can look up the US Census report for Milwaukee and retrieve a figure for the population, this cannot truly be the population. Why is that? Well, whatever method that was used to derive this numerical figure was surely incomplete. If not incomplete, by the time someone recorded the figure some number of residents of Milwaukee moved out, moved in, were born, or passed away. In either case, this example serves to point out that populations are not fixed and are subject to change over time.\nLikewise when we talk about populations in terms of language we dealing with an idealized aspect of linguistic reality. Let’s take the words of the English language as an analog to our previous example population. In this case the words are the people and English is the grouping characteristic. Just as people, words move out, move in, are born, and pass away. Any compendium of the words of English at any moment is almost instananeously incomplete. This is true for all populations, save those relatively rare cases in which the grouping characteristics select a narrow slice of reality which is objectively measurable and whose membership is fixed (the complete works of Shakespeare, for example).\nIn sum, (most) populations are amorphous moving targets. We subjectively hold them to exist, but in practical terms we often cannot nail down the specifics of populations. So how do researchers go about studying populations if they are theoretically impossible to access directly? The strategy employed is called sampling.\n\n2.1.2 Samples\n\nA sample is the product of a subjective process of selecting a finite set of observations from an idealized population with the goal of capturing the relevant characteristics of the target population. The degree of representativeness of a sample is the extent to which the sample reflects the characteristics of the population. The degree of representativeness is crucial for research as it directly impacts of any findings based on the sample.\n\nTo maximize the representativeness of a sample, researchers employ a variety of strategies. One of the first and sometimes the easiest strategy is to increase the sample size. A larger sample will always be more representative than a smaller sample. Sample size, however, is often not enough. It is not hard to imagine a large sample which by chance captures only a subset of the features of the population. Another step to enhance sample representativeness is to apply random sampling. Together a large random sample has an even better chance of reflecting the main characteristics of the population better than a large or random sample. But, random as random is, we still run the risk of acquiring a skewed sample (i.e. a sample with low representativeness).\nTo help mitigate these issues, there are two more strategies that can be applied to improve sample representativeness. Note, however, that while size and random samples can be applied to any sample with few assumptions about internal characteristics of the population, these next two strategies require decisions depend on the presumed internal characteristics of the population. The first of these more informed sampling strategies is called stratified sampling. Stratified samples make (educated) assumptions about sub-components within the population of interest. With these sub-populations in mind, large random samples are acquired for each sub-population, or strata. At a minimum, stratified samples can be no less representative than random sampling alone, but the chances that the sample is better increases. Can there be problems in the approach? Yes, and on two fronts. First knowledge of the internal components of a population are often based on a limited or incomplete knowledge of the population (remember populations are idealized). In other words, strata are selected subjectively by researchers using various heuristics some of which are based on some sense of ‘common knowledge’.\nThe second front on which stratified sampling can err concerns the relative sizes of the sub-components relative to the whole population. Even if the relevant sub-components are identified, their relative size adds another challenge which researchers must address in order to maximize the representativeness of a sample. To attempt to align, or balance, the relative sizes of the samples for the strata is the second population-informed sampling strategy.\n\n2.1.3 Corpora\nA key feature of a sample is that it is purposely selected to model a target population. In text analysis, a purposely sampled collection of texts, of the type defined here, is known as a corpus (pl. corpora). A set of texts or documents which have not been selected purposely selected lack a sampling frame, and therefore is not a corpus. The sampling frame, hence the populations modeled, in any given corpus will vary. It is key to vet corpora to ensure that the resource’s sampling frame and the research project’s target populations align as closely as possible to safeguard the integrity of research findings later in the research process.\n\n\n\n\n\n\n Consider this\nThe ‘Standard Sample of Present-Day American English’ (known commonly as the Brown Corpus) is widely recognized as one of the first large, machine-readable corpora. Compiled by Kucera and Francis (1967), the corpus is comprised of 1,014,312 words from edited English prose published in the United States in 1961.\nGiven the sampling frame for this corpus visualized in Figure 2.1:\n\n\n\n\nFigure 2.1: Overview of the sampling frame of the Brown Corpus.\n\n\n\nCan you determine what language population this corpus aims to represent? What types of research might this corpus support or not support?\n\n\n\nTypes\nLet’s take a look at some key characteristics, attributes, and features that distinguish corpora.\nReference\nThe least common and most ambitious corpus resources are those which aim to model the characteristics of a language population. These are known as reference corpora. These are projects designed with wide sampling frames, and require significant investments of time in corpus design and implementation (and continued development) that are usually undertaken by research teams (Ädel 2020).\nThe American National Corpus (ANC) or the British National Corpus (BNC) are corpora which aim to model the general characteristics of a variety of the English language, the former of American English and the later British English. Reference corpora exist for other languages as well: Spanish Reference Corpus of Present-Day Spanish (CREA), German The German Reference Corpus (DeReKo), Turkish Turkish National Corpus (TNC), and many others.\n\n\n\n\n\n\n Consider this\nOf note is the fact that, at present, most of the world’s languages lack reference corpus resources, or any corpus resources whatsoever. “Low-resourced” languages are often less studied, resource scarce, less available in born-digital formats, etc. (Magueresse, Carles, and Heetderks 2020).\nVisit the Clarin overview on reference corpora and then visit LRE Map. Can you find a reference corpus for a language you speak or are interested in studying? If not, consider what can be done to address this gap in the research community.\n\n\n\nSpecialized\n\nSpecialized corpora aim to represent more specific populations. The population may be defined either by modality, genre, time, location, or speaker-oriented characteristics, or some combination thereof. What specialized corpora lack in breadth of coverage, they make up for in depth of coverage by providing a more targeted representation of specific language populations.\nThe Santa Barbara Corpus of Spoken American English (SBCSAE), as you can imagine from the name of the resource, aims to model spoken American English. No claim to written English is included. There are even more specific types of corpora which attempt to model other types of sub-populations such as academic writing, computer-mediated communication (CMC), language use in specific regions of the world, a country, a region of a country, etc.\n\n\n\n\n\n\n\n Consider this\nGrieve, Nini, and Guo (2018) compiled a 8.9 billion-word corpus of geotagged posts from Twitter between 2013-2014 in the United States. The authors provide a search interface to explore relationship between lexical usage and geographic location. Explore this corpus searching for terms related to slang (“hella”, “wicked”), geographical (“mountain”, “river”), meteorological (“snow”, “rain”), and/ or any other term types. What types of patterns do you find? What are the benefits and/ or limitations of this type of data and/ or interface?\n\n\n\nAnother set of specialized corpora are resources which aim to compile texts from different languages or different language varieties for direct or indirect comparison. Corpora that are directly comparable, that is they include source and translated texts, are called parallel corpora. Parallel corpora include different languages or language varieties that are indexed and aligned at some linguistic level (i.e. word, phrase, sentence, paragraph, or document), see OPUS. Corpora that are compiled with different languages or language varieties but are not directly aligned are called comparable corpora. The comparable language or language varieties are sampled with the same or similar sampling frame, for example Brown and LOB corpora.\nThe aim of the quantitative text researcher is to select the corpus, or corpora, which best align with the purpose of the research. For example, a general corpus such as the American National Corpus may be better suited to address a question dealing with the way American English works, but this general resource may lack detail in certain areas, such as medical language, that may be vital for a research project aimed at understanding changes in medical terminology. Furthermore, a researcher studying spoken language might collect a corpus of transcribed conversations from a particular community or region, such as the SBCSAE. While this would not include every possible spoken utterance produced by members of that group, it could be considered a representative sample of the population of speech in that context.\nSources\n\nPublished\nThe most common source of data used in contemporary quantitative research is the internet. On the web an investigator can access corpora published for research purposes. Many organizations exist around the globe that provide access to published corpora in browsable catalogs, or repositories. There are repositories dedicated to language research, in general, such as the Language Data Consortium or that specialize in specific domains, such as the spoken language repository TalkBank. It is always advisable to start looking for the available language data in a repository. The advantage of beginning your data search in repositories is that a repository, especially those geared towards the linguistic community, will make identifying language corpora faster than through a general web search. Furthermore, repositories often require certain standards for corpus format and documentation for publication.\nRepositories are by no means the only source of published corpora on the web. Researchers from around the world provide access to corpora and datasets on their own sites or through data sharing platforms. Corpora of various sizes and scopes will often be accessible on a dedicated homepage or appear on the homepage of a sponsoring institution. These resources may be available for download or via search inferaces. Finding these resources is often a matter of doing a web search with the word ‘corpus’ and a list of desired attributes, including language, modality, register, etc.\nAs part of a general movement towards reproducibility, more corpora are available on data sharing platforms such as GitHub, Zenodo, Re3data, OSF, etc. These platforms enable researchers to securely store, manage, and share data with others. Support is provided for various types of data, including documents and code, and as such they are a good place to look as they often include reproducible research projects as well.\nCustom-built\nLanguage corpora prepared by researchers and research groups listed on repositories or hosted by the researchers themselves is often the first place to look for data. The web, however, contains a wealth of language and language-related data that can be accessed by researcher to compile their own corpus. There are two primary ways to attain language data from the web. The first is through an Application Programming Interface (API). APIs are, as the title suggests, programming interfaces which allow access, under certain conditions, to information that a website or database accessible via the web contains.\nThe second, more involved, way to acquire data from the web is is through the process of web scraping. Web scraping is the process of harvesting data from the public-facing web. Language texts may be found on sites as uploaded files, such as pdf or doc (Word) documents, or found displayed as the primary text of a site. Given the wide variety of documents uploaded and language behavior recorded daily on news sites, blogs and the like, compiling a corpus has never been easier. Having said that, how the data is structured and how much data needs to be retrieved can pose practical obstacles to collecting data from the web, particularly if the approach is to acquire the data by manually instead of automating the task.\n\n\n\n\n\n\n Dive deeper\nThe process of corpus development is a topic in and of itself. For a more in-depth discussion of the process, see Ädel (2020).\n\n\n\n\n\n\n\n\n\n Consider this\nExplore some of the resources listed on the qtalrkit compansion site and consider their sampling frames. Can you think of a research question or questions that this resource may be well-suited to support research into? What types of questions would be less-than-adequate for a given resource?\n\n\n\nEthical considerations\nJust because data is available on the web does not mean it is free to use. Repositories, APIs, and individual data resources often have licensing agreements and terms of use, ranging from public domain to proprietary licenses. Public domain licenses, such as those found in Project Gutenberg, allow anyone to use the data for any purpose. Creative Commons licenses, like those used by the American National Corpus, Wikipedia, and TalkBank, span from public domain to more restrictive uses, including requirements for attribution or prohibiting commercial use. Even more restrictive licenses, such as those for the Corpus of Contemporary American English and the British National Corpus, may require a fee to access and use the data, even for research purposes.\nRespecting intellectual property rights is crucial when working with corpus data. Violating these rights can lead to legal and ethical issues, including lawsuits, fines, and damage to one’s professional reputation. To avoid these problems, researchers must ensure they have the necessary permissions to use copyrighted works in their corpora. Obtaining permissions involves contacting the author or publisher and requesting consent to use their work for research purposes. Documenting all obtained permissions and providing attribution and/ or citation is essential respecting the intellectual property rights of others.\nFormats\nWhether you are using a published corpus or developing your own, it is important to understand how the data you want to work with is formatted. When referring to the format of a corpus, this includes the folder and file structure, the file types, the internal structure of the files themselves, and how file content is encoded electronically.\nFolder and file structure\nSome corpus resources are contained in a single file, such as a spreadsheet or a text file, but more often than not a corpus will be comprised of multiple files and folders. The folder and file structure will reflect the organization of the corpus and may include sub-folders for different types or groupings of data. In addition to the corpus data itself, metadata and documentation will often be included in the corpus folder structure. The corpus data may be grouped by language, modality, register, or other attributes such as types of linguistic annotation.\nTo illustrate, in Example 2.1 we have the file and folder structure of a toy corpus.\n\nExample 2.1 Toy corpus structure\n\n\ncorpus/\n├── documentation/\n│ ├── README.md\n│ ├── LICENSE\n├── metadata/\n│ ├── speakers.csv\n├── data/\n│ ├── spoken/\n│ │ ├── inter-09-a.xml\n│ │ ├── inter-09-b.xml\n│ │ ├── convo-09-a.xml\n│ │ ├── ...\n│ ├── written/\n│ │ ├── essay-09-a.xml\n│ │ ├── essay-09-b.xml\n│ │ ├── respo-09-a.xml\n│ │ ├── ...\n\n\n\nIn this example, we have a corpus folder with three sub-folders: documentation/, metadata/, and data/. The data/ folder contains two sub-folders: spoken/ and written/. Each folder contains the relevant data files.\nWhere a single file is easy to download from the web, a corpus with a more complex folder structure can be more difficult to access. For that reason, many corpus resources are packaged into and made into a single compressed file. File compression has two benefits: it preserves the folder structure in a format which is contained in a single file and it also reduces the overall storage size. Common file compression formats are .zip and .tar.gz. So a compressed corpus file for the example above may be named something like corpus.zip or corpus.tar.gz. To access the original data within a compressed file, one must use a decompression tool or software to extract the contents after downloading it.\nFile types\nIn our toy corpus example, you may have noticed that each of the filenames appear with either .md, .csv, .xml, or nothing appended. These are examples of file extensions. File extensions a short sequence of characters, usually preceded by a period (.) which are used to indicate the type or format of file. File extensions help both users and software programs to identify the content and purpose of a file.\n\n\n\n\n\n\n Warning\nIf you are working on your own desktop computer, you may not see the file extensions. This is because the file explorer is configured to hide them by default. To see the file extensions, you will need to change the settings in your file explorer. Use a search engine to find instructions for your operating system.\n\n\n\nIn addition to those listed above, other file extensions often encountered when working with data for text analysis include .txt, .pdf, .docx, .xlsx, .json, and .html. Common file extensions will often be associated with specific software programs on your computer, especially those which are directly associated with proprietary software such as .docx for Microsoft Word or .xlsx for Microsoft Excel. However, many file extensions are not directly associated with any specific software program and can be opened and edited with any text editor.\nIt is important to note that file extensions are helpful conventions, but they are not a guarantee of the file type or structure of the file content. Furthermore, corpus developers may create their own file extensions to signal the unique structure of their data. For example, the .utt file extension used in the Switchboard Dialogue Act Corpus (SWDA) or the .cha extension used for TalkBank resource transcripts. In either case, it is recommended to open the file in a text editor to inspect the structure of the file content before processing the data contained therein.\nFile content\nThe internal structure of the content of corpus data files is an important aspect of any corpus both in terms of what data is included and how to approach accessing and processing the data. A corpus may include various types of linguistic (e.g. part of speech, syntactic structure, named entities, etc.) or non-linguistic (e.g. source, dates, speaker information, etc.) attributes. These attributes are known as metadata, or data about data. As a general rule, files which include more metadata tend to be more internally structured. Internal file structure refers to the degree to which the content is easy to query and analyze by a computer. Let’s review characteristics of the three main types of file structure types and associate common file extensions that files in each have.\nUnstructured data is data which does not have a machine-readable internal structure. This is the case for plain text files (.txt), which are simply a sequence of characters. For example, in Example 2.2 we see a snippet of a plain text file from the the Manually Annotated Sub-Corpus of American English (MASC) (Ide et al. 2008):\n\nExample 2.2 MASC plain text\n\n\n&gt;Hotel California\n\nFact: Sound is a vibration. Sound travels as a mechanical wave through a\nmedium, and in space, there is no\nmedium. So when my shuttle malfunctioned and the airlocks didn't keep the\nair in, I heard nothing. After the\nfirst whoosh of the air being sucked away, there was lightning, but no\nthunder. Eyes bulging in\npanic, but no screams. Quiet and peaceful, right? Such a relief to never\nagain hear my crewmate Jesse natter\nabout his girl back on Earth and that all-expenses-paid vacation-for-two\nshe won last time he was on leave. I\nswore, if I ever had to see a photo of him in a skimpy bathing suit again,\ngiving the camera a cheesy thumbs-up\nfrom a lounge chair on one of those white sandy beaches, I'd kiss a monkey.\nMetaphorically, of course.\n\n\n\nOther examples of files which often contain unstructured data include .pdf and .docx files. While these file types may contain data which appears structured to the human eye, the structure is not designed to be machine-readable. As such the data would typically be read into R as a vector of character strings. It is possible to peform only the most rudimentary queries on this type of data, such as string matches. For anything more informative, it is necessary to further process this data.\nOn the other end of the spectrum, structured data is data which conforms to a tabular format in which elements in tables and relationships between tables are defined. This makes querying and analyzing easy and efficient. Relational databases (e.g. MySQL, PostgreSQL, etc.) are designed to store and query structured data. The data frame object in R is also a structured data format. In each case, the data is stored in a tabular format in which each row represents a single observation and each column represents a single attribute whose values are of the same type.\nIn Example 2.3 we see an example of an R data frame object which overlaps with the data in the plain text file above:\n\nExample 2.3 MASC data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntitle\ndate\nmodality\ndomain\nref_num\nword\nlemma\npos\n\n\n\nHotel California\n2008\nWriting\nGeneral Fiction\n0\n&gt;\n&gt;\nNN\n\n\nHotel California\n2008\nWriting\nGeneral Fiction\n1\nHotel\nhotel\nNNP\n\n\nHotel California\n2008\nWriting\nGeneral Fiction\n2\nCalifornia\ncalifornia\nNNP\n\n\nHotel California\n2008\nWriting\nGeneral Fiction\n3\nFact\nfact\nNNP\n\n\nHotel California\n2008\nWriting\nGeneral Fiction\n4\n:\n:\n:\n\n\nHotel California\n2008\nWriting\nGeneral Fiction\n5\nSound\nsound\nNNP\n\n\nHotel California\n2008\nWriting\nGeneral Fiction\n6\nis\nbe\nVBZ\n\n\nHotel California\n2008\nWriting\nGeneral Fiction\n7\na\na\nDT\n\n\nHotel California\n2008\nWriting\nGeneral Fiction\n8\nvibration\nvibration\nNN\n\n\nHotel California\n2008\nWriting\nGeneral Fiction\n9\n.\n.\n.\n\n\nHotel California\n2008\nWriting\nGeneral Fiction\n10\nSound\nsound\nNNP\n\n\n\n\n\n\nHere we see that the data is stored in a tabular format with each row representing a single observation (word) and each column representing a single attribute. Internally, R applies a schema to ensure the values in each column are of the same type (e.g. &lt;chr&gt;, &lt;dbl&gt;, &lt;fct&gt;, etc.). This structured format is designed to be easy to query and analyze and as such is the primary format for data analysis in R.\n\n\n\n\n\n\n Tip\nIt is conventional to work with column names for datasets in R using the same conventions that are used for naming objects. It is a matter of taste which convention is used, but I have adopted snake case as my personal preference. There are also alternatives. Regardless of the convention you choose, it is good practice to be consistent.\nIt is also of note that the column names should be balanced for meaningfulness and brevity. This brevity is of practical concern but can be somewhat opaque. For questions into the meaning of the column and is values consult the resource’s dataset documentation, consult Section 2.3.\n\n\n\nSemi-structured data falls between unstructured and structured data. This covers a wide range of file structuring approaches. For example, a otherwise plain text file with part-of-speech tags appended to each word is minimally structured (Example 2.4).\n\nExample 2.4 MASC plain text with part-of-speech tags\n\n\n&gt;/NN Hotel/NNP California/NNP Fact/NNP :/: Sound/NNP is/VBZ a/DT\nvibration/NN ./. Sound/NNP travels/VBZ as/IN a/DT mechanical/JJ wave/NN\nthrough/IN a/DT medium/NN ,/, and/CC in/IN space/NN ,/, there/EX is/VBZ\nno/DT medium/NN ./. So/RB when/WRB my/PRP$ shuttle/NN malfunctioned/JJ\nand/CC the/DT airlocks/NNS did/VBD n't/RB keep/VB the/DT air/NN in/IN ,/,\nI/PRP heard/VBD nothing/NN ./. After/IN the/DT\n\n\n\nTowards the more structured end, many file formats including .xml and .json contain highly structured, hierarchical data. For example, in Example 2.6 shows a snippet from a .xml file from the MASC corpus.\n\nExample 2.5 MASC XML\n\n\n&lt;a xml:id=\"penn-N65571\" label=\"tok\" ref=\"penn-n0\" as=\"anc\"&gt;\n&lt;fs&gt;\n&lt;f name=\"base\" value=\"&gt;\"/&gt;\n&lt;f name=\"msd\" value=\"NN\"/&gt;\n&lt;f name=\"string\" value=\"&gt;\"/&gt;\n&lt;/fs&gt;\n&lt;/a&gt;\n&lt;node xml:id=\"penn-n1\"&gt;\n&lt;link targets=\"seg-r1\"/&gt;\n&lt;/node&gt;\n&lt;a xml:id=\"penn-N65599\" label=\"tok\" ref=\"penn-n1\" as=\"anc\"&gt;\n&lt;fs&gt;\n&lt;f name=\"base\" value=\"hotel\"/&gt;\n&lt;f name=\"msd\" value=\"NNP\"/&gt;\n&lt;f name=\"string\" value=\"Hotel\"/&gt;\n&lt;/fs&gt;\n&lt;/a&gt;\n\n\n\nThe format of semi-structured data is often influenced by characteristics of the data or reflect an author’s individual preferences. It is sometimes the case that data will be semi-structured in a less-standard format. For example, the SWDA corpus includes a .utt file extension for files which contain utterances annotated with dialogue act tags.\n\nExample 2.6 SWDA .utt file\n\n\no A.1 utt1: Okay.  /\nqw A.1 utt2: {D So, }\n\nqy^d B.2 utt1: [ [ I guess, +\n\n+ A.3 utt1: What kind of experience [ do you, + do you ] have, then with\nchild care? /\n\n+ B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\n\nqy A.5 utt1: Does it say something? /\n\n\n\nWhether standard or not, semi-structured data is often designed to be machine-readable. As with unstructured data, the ultimate goal is to convert the data into a structured format and augment the data where necessary to prepare it for a particular research analysis.\nFile encoding\nThe last aspect to consider about corpus formats is file encoding. For a computer to display and process text characters, it must be encoded in a way that the computer can understand (i.e. 1’s and 0’s). Historically, character encoding schemes were developed to represent characters from specific character script sets (e.g. ASCII only includes characters from the English alphabet). However, as the need for a consistent and more inclusive way to encode characters from multiple languages and scripts became apparent, the Unicode standard, Unicode Transformation Format (UTF), was developed in the early 1990s. UTF encodings (UTF-8, UTF-16, and UTF-32) are now the most common way to encode text data and modern computers typically use them by default. Although other more script-specific encoding schemes can still be found in older data (e.g. ISO-8859, Windows-1252, Shift JIS).\nWhen working with corpus data, it is important to know if the encoding scheme used for the data is compatible with your computing environment’s default (most likely UTF). If it is not, you will need to convert the data to a compatible encoding scheme. Rest assured, there is support in R for converting between different encoding schemes if the need arises."
  },
  {
    "objectID": "understanding-data.html#information",
    "href": "understanding-data.html#information",
    "title": "2  Understanding data",
    "section": "\n2.2 Information",
    "text": "2.2 Information\nIdentifying an adequate corpus resource, in terms of content, licensing, and formatting, for the target research question is the first step in moving a quantitative text research project forward. The next step is to select the components or characteristics of this resource that are relevant for the research and then move to organize the attributes of this data into a more informative format. This is the process of converting corpus data into a dataset –a tabular representation of particular attributes of the data as the basis for generating information. Once the data represented as dataset, it is often manipulated and transformed adjusting and augmenting the data such that it better aligns with the research question and the analytical approach.\n\n2.2.1 Organization\nData alone is not informative. Only through explicit organization of the data in a way that makes relationships and meaning explicit does data become information. In this form, our data is called a dataset. This is a particularly salient hurdle in text analysis research. Many textual sources are unstructured or semi-structured, that is relationships that will be used in the analysis have yet to be purposefully drawn and organized from the data.\nTidy Data\nThe selection of the attributes from a corpus and the juxtaposition of these attributes in a relational format, or dataset, that converts data into information is known as data curation. The process of data curation minimally involves creating a base dataset, or curated dataset, which establishes the main informational associations according to philosophical approach outlined by Wickham (2014).\nIn this work, a tidy dataset refers both to the structural (physical) and informational (semantic) organization of the dataset. Physically, a tidy dataset is a tabular data structure, illustrated in Figure 2.2, where each row is an observation and each column is a variable that contains measures of a feature or attribute of each observation. Each cell where a given row-column intersect contains a value which is a particular attribute of a particular observation for the particular observation-feature pair also known as a data point.\n\n\n\n\nFigure 2.2: Visual summary of the tidy format.\n\n\n\nIn terms of semantics, columns and rows both contribute to the informational value of the dataset. Let’s start with columns. In a tidy dataset, each column is a variable, an attribute that can take on a number of values. Although variables vary in terms of values, they do not in type. A variable is of one and only one informational type. Statistically speaking, informational types are defined as levels of measurement, a classification system used to semantically distiguish between types of variables. There are four levels (or types) in this system: nominal, ordinal, interval, and ratio.\nIn practice, however, text analysis researchers often group these levels into three main informational types: categorical, ordinal, and numeric (Gries 2021). What do these informational types represent? Categorical data is for labeled data or classes that answer the question “what?” Ordinal data is categorical data with rank order that answers the question “what order?” Numeric data is ordinal data with equal intervals between values that answers the question “how much or how many?”\nLet’s look at an example of a tidy dataset. Using the criteria just described, let’s see if we can identify the informational values (categorical, ordinal, or numeric) of the variables that appear in a snippet from the MASC corpus in dataset form in Table 2.1.\n\n\n\n\nTable 2.1: MASC dataset variables.\n\n\n\n\n\n\n\n\n\n\ntitle\nmodality\ndate\nref_num\nword\npos\nnum_letters\n\n\n\nHotel California\nWriting\n2008\n0\n&gt;\nNN\n1\n\n\nHotel California\nWriting\n2008\n1\nHotel\nNNP\n5\n\n\nHotel California\nWriting\n2008\n2\nCalifornia\nNNP\n10\n\n\nHotel California\nWriting\n2008\n3\nFact\nNNP\n4\n\n\nHotel California\nWriting\n2008\n4\n:\n:\n1\n\n\nHotel California\nWriting\n2008\n5\nSound\nNNP\n5\n\n\nHotel California\nWriting\n2008\n6\nis\nVBZ\n2\n\n\nHotel California\nWriting\n2008\n7\na\nDT\n1\n\n\nHotel California\nWriting\n2008\n8\nvibration\nNN\n9\n\n\nHotel California\nWriting\n2008\n9\n.\n.\n1\n\n\n\n\n\n\nWe have seven variables listed as headers for each of the columns. We could go one-by-one left-to-right but let’s take another tack. Instead, let’s identify all those variables that cannot be numeric –these are all the non-numeral variables: title, modality, word, and pos. The question to ask of these variables is whether they represent an order or rank. Since titles, modalities, words, and parts-of-speech are not ordered values, they are all categorical.\nNow let’s come back to date, ref_num, and num_letters. All three are numerals, so they could be numeric. But they could also be numeral representations of ordinal data.\nBefore we can move forward, we need to make sure we understand what each variable means and how it is measured, or operationalized. The variable name and the values can be helpful in this respect. date is what it sounds like, a date, and is operationalized as a year in the Gregorian calendar. And num_letters seems quite descriptive as well, number of letters, appearing as a letter count. But in some cases is may be opaque as to what is being measured by the variable name alone, for example ref_num, and one will have to refer to the dataset documentation. In this case ref_num is a reference number operationalized as a unique identifier for each word per document in the corpus.\nWith this in mind, let’s return to the question of whether date, ref_num, and num_letters are numeric or ordinal. Starting with the trickiest one, date, we can ask the question to identify numeric data: “how much or how many?”. In the case of date, the answer is neither. A date is a point in time, not a quantity. So date is not numeric. But it does provide information about order. Hence, date is ordinal. ref_num is also ordinal because the question “what order?” can be asked of it. Finally, num_letters is numeric because it answers the question “how many?”.\nLet’s turn to the second semantic value of a tidy dataset. In a tidy dataset, each row is an observation. But an observation of what? This depends on what the unit of observation is. That sounds circular, but its not. The unit of observation is simply the primary entity that is being observed. Without context, it can it can be identified in a dataset by looking at the level of specificity of the variable values and asking what each variable describes. When one variable appears to be the most individualized and other variables appear to describe that variable, then the most individualized variable is likely the unit of observation of the dataset, i.e. the meaning of each observation.\nApplying these strategies to the Table in 2.1, we can see that each observation at its core is a word. We see that the values of each observation are the attributes of each word. word is the most individualized variable and the pos (part-of-speech), num_letters, and ref_num all describe the word.\nThe other variables title, modality, and date are not direct attributes of the word. Instead, they are attributes of the document in which the word appears. Together, however, they all provide information about the word.\n\n\n\n\n\n\n Consider this\nData can be organized in many ways. It is important to make clear that data in tabular format in itself does not constitute a dataset, in the tidy sense we will be using. Can you think of examples of tabular information that would not be in a tidy format? What would be the implications of this for data analysis?\n\n\n\nAs we round out this section on data organization, it is important to stress that the purpose of curation is to represent the corpus data in an informative, tidy format. In the subsequent section, we will turn from representing the data in a dataset to modifying the dataset, either row-wise or column-wise, to make it more amenable to the particular aims of an analysis.\n\n2.2.2 Transformation\nAt this point have introduced the first step creating a dataset ready for analysis, data curation. However, a curated dataset is rarely the final organizational step before proceeding to statistical analysis. Many times, if not always, the curated dataset requires data transformation to derive or generate new data for the dataset. This process may incur row-wise (observation) or column-wise (variable) level changes, as illustrated in Figure 2.3.\n\n\n\n\nFigure 2.3: Visualization of row-wise and column-wise transformation operations on a dataset.\n\n\n\nThe results build on and manipulate the curated dataset to produce a derived dataset. While there is typically one curated dataset that serves as the base organizational dataset, there may be multiple derived datasets, each aligning with the informational needs of specific analyses in the research project.\nIn what follows, we will discuss the most common types of data transformation: text normalization, text tokenization, variable recoding, variable generation, and observation/ variable merging. Note, however, that the order in which these transformations are applied in a given research project is not fixed and will vary depending on the dataset and the research question(s) to be addressed.\nText normalization\n\n\nThe process of text normalization aims to prepare and standardize text. It is often a preliminary step in data transformation processes which include variables with text. The aim is to convert the text into a uniform format to reduce unwanted variation and noise.\nLet’s take a toy dataset, in Table 2.2, as an example starting point. In this dataset, we have two variables, text_id and text. It only has one observation.\n\n\n\n\nTable 2.2: A toy dataset with two variables, text_id and text.\n\n\n\n\n\ntext_id\ntext\n\n\n1\nIt’s a beautiful day in the US, and our group decided to visit the famous Grand Canyon. As we reached the destination, Jane said, “I can’t believe we’re finally here!” The breathtaking view left us speechless; indeed, it was a sight to behold. During our trip, we encountered tourists from different countries, sharing stories and laughter. For all of us, this experience will be cherished forever.\n\n\n\n\n\nThe types of transformations we apply will depend on the specific needs of the project, but can include those found in Table 2.3.\n\n\nTable 2.3: Common text normalization tasks\n\n\n\n\n\n\nTask name\nRelevant example\nTypical purpose\n\n\n\nLowercasing\n\n\"Text\" to \"text\"\n\nMinimizing case sensitivity in subsequent analysis\n\n\nRemoval of Punctuation and Special Characters\n\n\"Hello, World!\" to \"Hello World\"\n\nRemoving non-alphanumeric characters that may not carry semantic value\n\n\nAdjustment of Forms\n\n\"colour\" to \"color\", \"it's\" to \"it is\", \"1\" to \"one\"\n\nStandardizing variations in spelling, contractions, and numeric forms to a common format\n\n\nStopword Removal\n\n\"This is a sentence\" to \"This sentence\"\n\nDiscarding common words that usually do not contain meaningful semantic information\n\n\n\n\nThese transformations are column-wise operations, meaning they preserve the number of rows in the dataset. They also preserve the number of columns, but do change the values of the variables. These tasks should be applied with an understanding of how the changes will impact the analysis. For example, lowercasing can be useful for reducing differences between words that are otherwise identical, yet differ in case due to word position in a sentence (“The” versus “the”). However, lowercasing can also be problematic if the case of the word carries semantic value, such as in the case of “US” (United States) and “us” (first person plural pronoun). The same can be said for removing or adjusting particular characters and discarding stopwords.\n\n\n\n\n\n\n Dive deeper\nStopwords are words that are so commonly used in a language that they tend not to contribute much to the meaning of a sentence. There are various predefined lists of stopwords for different languages available on the web and through R in the stopwords package (Benoit, Muhr, and Watanabe 2021). However, it is important to note the criteria used to determine which words are considered stopwords in a particular resource may not fit a researcher’s needs or the characteristics of the data. Learn more about stopwords in Kaur and Buttar (2018).\n\n\n\nLet’s be conservative and only apply lowercasing to our toy dataset as seen in Table 2.4.\n\n\n\n\nTable 2.4: A toy dataset with two variables, text_id and text, where the text has been lowercased.\n\n\n\n\n\ntext_id\ntext\n\n\n1\nit’s a beautiful day in the us, and our group decided to visit the famous grand canyon. as we reached the destination, jane said, “i can’t believe we’re finally here!” the breathtaking view left us speechless; indeed, it was a sight to behold. during our trip, we encountered tourists from different countries, sharing stories and laughter. for all of us, this experience will be cherished forever.\n\n\n\n\n\nWhen text normalization steps are motivated and applied with foresight they serve to enhance the quality of the data and improves the reliability of subsequent transformation steps.\nText tokenization\nAnother text-oriented transformation step is text tokenization. This process involves modifying the text such that it reflects the target linguistic unit that will be used in the analysis. This is a row-wise operation expanding the number of rows, if the linguistic unit is smaller than the original variable, and reducing the number of rows, if the linguistic unit is larger than the original variable. At its core, tokenization is the process which enables the quantitative analysis of text.\nText variables can be tokenized at any linguistic level. To illustrate, consider our toy dataset from Table 2.4. We can tokenize the text at the sentence level, in Table 2.5, by splitting the text at the period followed by a space. This results in a dataset with four observations, one for each sentence in the original text.\n\n\n\n\nTable 2.5: A toy dataset with two variables, text_id and sentence, where the text has been tokenized at the sentence level.\n\n\n\n\n\ntext_id\nsentence\n\n\n\n1\nit’s a beautiful day in the us, and our group decided to visit the famous grand canyon\n\n\n1\nas we reached the destination, jane said, “i can’t believe we’re finally here!” the breathtaking view left us speechless; indeed, it was a sight to behold\n\n\n1\nduring our trip, we encountered tourists from different countries, sharing stories and laughter\n\n\n1\nfor all of us, this experience will be cherished forever.\n\n\n\n\n\n\nIt is important to make explicit what the operationalization of our linguistic unit is as common terms such as sentence, word, etc. can be defined in different ways. For example, the sentence tokenization above is based on the assumption that sentences are separated by a period followed by a space. This is a suitable definition for this text, but likely will not be for other English text or for other languages/ writing scripts. For words, a very simple operationalization is to use whitespace separation (e.g. “I cannot believe it.” – [“I”, “cannot”, “believe”, “it.”]). However, this approach does not handle puntuation marks (e.g. [“it.”]) or contractions (e.g. [“can’t”]). A more sophisticated operationalization will be necessary for these, and possibly other, cases.\nAnother important token unit is the \\(n\\)-gram. Words or characters can be grouped into contiguous sequences with a moving window of a certain size \\(n\\). Single unit windows are referred to as unigrams, two units as bigrams, three units as trigrams, and so on. Let’s tokenize our toy dataset at the bigram level for words using a simple whitespace separation for words, as seen in Table 2.6.\n\n\n\n\nTable 2.6: A toy dataset with two variables, text_id and bigram, where the text has been tokenized at the bigram word level.\n\ntext_id\nword\n\n\n\n1\nit’s a\n\n\n1\na beautiful\n\n\n1\nbeautiful day\n\n\n1\nday in\n\n\n1\nin the\n\n\n1\nthe us\n\n\n1\nus and\n\n\n1\nand our\n\n\n1\nour group\n\n\n1\ngroup decided\n\n\n\n\n\n\nIn Table 2.6 we see that the first bigram is “it’s a” –the first two words (based on whitespace separation) in the text. The second bigram is “a toy” –the second and third words in the text. This continues to the end of the text. \\(N\\)-gram tokenization can be useful to capture context that would otherwise would be lost from tokenizing words or characters at the unigram level.\nUp to this point our tokens have been surface forms. That is, they are the actual words or characters as they appear in the text. However, we may want to reduce the tokens to their base form, removing their inflectional forms. This is known as lemmatization. For example, the word “run” is the lemma of the words “running”, “runs”, and “ran”. Let’s lemmatize the third sentence in our toy dataset. For comparison, word and lemma are shown side-by-side in Table 2.7.\n\n\n\n\nTable 2.7: A toy dataset with two variables, text_id and word, where the text has been tokenized at the unigram word level and lemmatized.\n\ntext_id\nword\nlemma\n\n\n\n1\nduring\nduring\n\n\n1\nour\nour\n\n\n1\ntrip\ntrip\n\n\n1\nwe\nwe\n\n\n1\nencountered\nencounter\n\n\n1\ntourists\ntourist\n\n\n1\nfrom\nfrom\n\n\n1\ndifferent\ndifferent\n\n\n1\ncountries\ncountry\n\n\n1\nsharing\nshare\n\n\n1\nstories\nstory\n\n\n1\nand\nand\n\n\n1\nlaughter\nlaughter\n\n\n\n\n\n\n\n\n\n\n\n\n Case study\nInflectional family size is the number of inflectional forms for a given word and can be calculated from a corpus by counting the number of surface forms for each lemma in the corpus (Kostić, Marković, and Baucal 2003). Baayen, Feldman, and Schreuder (2006) found that words with larger inflectional family size are associated with faster word recognition times in lexical processing tasks.\n\n\n\nTogether tokenization and lemmatization are powerful tools for transforming text. If our dataset contains more robust linguistic annotation or that annotation can be generated (see Section 2.2.2.4), this information can also be leveraged to tokenize language into a format that is easier to explore and quantify in an analysis.\nVariable recoding\nRecoding is the process of transforming the values of one or more variables into new values which are more amenable to analysis. The aim is to simplify complex variables, making it easier to identify patterns and trends relevant for the research question. This is a column-wise operation which can be applied to categorical or numeric variables.\nLet’s return to the MASC dataset and demonstrate recoding of categorical and numeric variables. In Table 2.1 the pos variable whose values represent the part-of-speech (POS) of each token in the text. The measure is a POS tag from the Penn Treebank tagset (Marcus, Santorini, and Marcinkiewicz 1993). This tagset makes twelve major and 45 minor grammatical class distinctions. In an analysis that aims to explore only major class distinctions, it would be useful to recode the pos variable into major classes only (i.e. noun, pronoun, adjective, verb, adverb, etc.) to facilitate queries, summaries, and visualizations.\n\n\n\n\nTable 2.8: A toy dataset with three variables, text_id, pos, major_pos, where the pos variable has been recoded into major grammatical classes major_pos.\n\n\n\n\n\n\n\n\n\n\n\ntitle\nmodality\ndate\nref_num\nword\npos\nmajor_pos\nnum_letters\n\n\n\nHotel California\nWriting\n2008\n0\n&gt;\nNN\nnoun\n1\n\n\nHotel California\nWriting\n2008\n1\nHotel\nNNP\nnoun\n5\n\n\nHotel California\nWriting\n2008\n2\nCalifornia\nNNP\nnoun\n10\n\n\nHotel California\nWriting\n2008\n3\nFact\nNNP\nnoun\n4\n\n\nHotel California\nWriting\n2008\n4\n:\n:\npunctuation\n1\n\n\nHotel California\nWriting\n2008\n5\nSound\nNNP\nnoun\n5\n\n\nHotel California\nWriting\n2008\n6\nis\nVBZ\nverb\n2\n\n\nHotel California\nWriting\n2008\n7\na\nDT\ndeterminer\n1\n\n\nHotel California\nWriting\n2008\n8\nvibration\nNN\nnoun\n9\n\n\nHotel California\nWriting\n2008\n9\n.\n.\npunctuation\n1\n\n\n\n\n\n\nIn Table 2.8, the pos variable has been recoded into major grammatical classes. The major_pos variable is a categorical variable with 12 levels, one for each major grammatical class in the Penn Treebank tagset. While the demonstration here demonstrates the simplification of a categorical variable, recoding can also be used to transliterate categorical variables. Continuing with the theme of POS tags, the pos variable could be recoded into a different tagset, such as the Universal Dependencies tagset (Nivre et al. 2016).\nNow, let’s look at recoding the numeric variable num_letters. This variable represents the number of letters in each token. In the MASC dataset, the num_letters variable is a numeric variable with a range of values from 1 to 21. In some analyses, it may be useful to recode this variable into discrete categories, or bins, such as short, medium, and long words.\n\n\n\n\nTable 2.9: The MASC dataset with the num_letters variable recoded into three categories: short, medium, and long words in word_length.\n\n\n\n\n\n\n\n\n\n\n\n\ntitle\nmodality\ndate\nref_num\nword\npos\nmajor_pos\nnum_letters\nword_length\n\n\n\nHotel California\nWriting\n2008\n0\n&gt;\nNN\nnoun\n1\nshort\n\n\nHotel California\nWriting\n2008\n1\nHotel\nNNP\nnoun\n5\nmedium\n\n\nHotel California\nWriting\n2008\n2\nCalifornia\nNNP\nnoun\n10\nlong\n\n\nHotel California\nWriting\n2008\n3\nFact\nNNP\nnoun\n4\nmedium\n\n\nHotel California\nWriting\n2008\n4\n:\n:\npunctuation\n1\nshort\n\n\nHotel California\nWriting\n2008\n5\nSound\nNNP\nnoun\n5\nmedium\n\n\nHotel California\nWriting\n2008\n6\nis\nVBZ\nverb\n2\nshort\n\n\nHotel California\nWriting\n2008\n7\na\nDT\ndeterminer\n1\nshort\n\n\nHotel California\nWriting\n2008\n8\nvibration\nNN\nnoun\n9\nlong\n\n\nHotel California\nWriting\n2008\n9\n.\n.\npunctuation\n1\nshort\n\n\n\n\n\n\nIn Table 2.9 the variable word_length appears with the values short, medium, and long. This is now a categorical variable of type ordinal. Of note, is that the operational definition of used to create these word length bins should be made explicit in the documentation of the dataset.\nIn sum, recoding is a useful data transformation technique that can be used to simplify complex variables, making it easier to identify patterns and trends relevant for the research question.\nVariable generation\nThe process of variable generation aims to augment existing variables or create new ones, and as such is a column-wise operation. Generation can include applying calculations or extracting relevant information from existing variables or enhancing text variables with linguistic annotation. Simplifying a bit, generation helps makes implicit attributes explicit. The results of this process enables direct access during analysis to features that were otherwise hidden or difficult to access.\nLet’s highlight a some common calculation and extraction examples that generate variables. First, let’s look at the calculation of measures. In text analysis, measures are often used to describe the properties of a document or linguistic unit. For example, the number of words in a corpus document, the lengths of sentences, the number of clauses in a sentence, etc.. In turn, these measures can be used to calculate other measures, such as lexical diversity or syntactic complexity measures.\nIn terms of extraction, the goal is to distill relevant information from existing variables. For example, extracting the year from a date variable, or extracting the first name from a full name variable. In text analysis, extraction is often used to extract information from text variables. Say we have a dataset with a variable containing conversation utterances. We may want to extract some characteristic from those utterances and capture their occurrence in a new variable.\nBut what if we want to extract linguistic information from a text variable that is not exicitly present in the text? This is where linguistic annotation comes in. Linguistic annotation is the process of enriching text with linguistic information, such as morphological features, part-of-speech tags, syntactic structure, or dependency relationships. This is often done using natural language processing (NLP) tools, many of which are available in R (see Chapter 7).\nTo illustrate the process of generating linguistic annotation, I will use the plain text version of the MASC. In Table 2.10, the text has been organized into a dataset and tokenized into sentences. The text_id variable is a unique identifier for each document, and the sentence_id variable is a unique identifier for each sentence.\n\n\n\n\nTable 2.10: A MASC sample document in dataset tokenized into sentences.\n\n\n\n\n\n\ntext_id\nsentence_id\nsentence\n\n\n\n1\n1\n&gt;Hotel California Fact: Sound is a vibration.\n\n\n1\n2\nSound travels as a mechanical wave through a medium, and in space, there is no medium.\n\n\n1\n3\nSo when my shuttle malfunctioned and the airlocks didn’t keep the air in, I heard nothing.\n\n\n1\n4\nAfter the first whoosh of the air being sucked away, there was lightning, but no thunder.\n\n\n1\n5\nEyes bulging in panic, but no screams.\n\n\n\n\n\n\nApplying a pre-trained model from the Universal Dependencies (UD)1 project, we can generate linguistic annotation for each token in the MASC.\n\n\n\n\nTable 2.11: Automatic linguistic annotation for grammatical category and syntactic structure for an example English sentence from the MASC.\n\n\n\n\n\n\n\n\n\n\ndoc_id\nsentence_id\ntoken_id\ntoken\nxpos\nfeatures\nsyntactic_relation\n\n\n\n1\n4\n1\nAfter\nIN\nNA\nmark\n\n\n1\n4\n2\nthe\nDT\nDefinite=Def|PronType=Art\ndet\n\n\n1\n4\n3\nfirst\nJJ\nDegree=Pos|NumType=Ord\namod\n\n\n1\n4\n4\nwhoosh\nNN\nNumber=Sing\nnsubj:pass\n\n\n1\n4\n5\nof\nIN\nNA\ncase\n\n\n1\n4\n6\nthe\nDT\nDefinite=Def|PronType=Art\ndet\n\n\n1\n4\n7\nair\nNN\nNumber=Sing\nnmod\n\n\n1\n4\n8\nbeing\nVBG\nVerbForm=Ger\naux:pass\n\n\n1\n4\n9\nsucked\nVBN\nTense=Past|VerbForm=Part|Voice=Pass\nadvcl\n\n\n1\n4\n10\naway\nRB\nNA\nadvmod\n\n\n1\n4\n11\n,\n,\nNA\npunct\n\n\n1\n4\n12\nthere\nEX\nNA\nexpl\n\n\n1\n4\n13\nwas\nVBD\nMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\nroot\n\n\n1\n4\n14\nlightning\nNN\nNumber=Sing\nnsubj\n\n\n1\n4\n15\n,\n,\nNA\npunct\n\n\n1\n4\n16\nbut\nCC\nNA\ncc\n\n\n1\n4\n17\nno\nDT\nNA\ndet\n\n\n1\n4\n18\nthunder\nNN\nNumber=Sing\nconj\n\n\n1\n4\n19\n.\n.\nNA\npunct\n\n\n\n\n\n\nThe annotated dataset now includes the key variables xpos (Penn treebank tags), features (morphological features), and syntactic_relation. The results of this process can then be further transformed as need be to fit the needs of the analysis.\nA word of caution: automated linguistic annotation offers rapid access to abundant and highly dependable linguistic data for numerous languages. However, linguistic annotation tools are not infallible. They are tools developed by training computational algorithms to identify patterns in manually annotated datasets, resulting in a language model. This model is then employed to predict linguistic annotations for new language data (as seen in Table 2.11). The accuracy of the linguistic annotation heavily relies on the congruence between the language sampling framework of the trained data and the language data set to be automatically annotated.\nObservation/ variable merging\n\n\n\nThe processing of merging datasets is a transformation step which can be row-wise or column-wise. Row-wise merging is the process of combining datasets by appending observations from one dataset to another. Column-wise merging is the process of combining datasets by appending variables from one dataset to another. In either case, merging provides a way to enrich a dataset by incorporating additional information.\nTo merge in row-wise manner the datasets involved in the process must have the same variables and variable types. This process is often referred to as concatenating datasets. It can be thought of as stacking datasets on top of each other to create a larger dataset. Remember, having the sample variables and variable types is not the same has having the same values.\nTake, for example, a case when a corpus resource contains data for two populations. In the course of curating and transforming the datasets it may make more sense to work with the datasets separately. However, when it comes time to analyze the data, it may be more convenient to work with the datasets as a single dataset. In this case, the datasets can be concatenated to create a single dataset.\nTo illustate, consider the toy datasets in Table 2.12 and Table 2.13.\n\n\n\n\nTable 2.12: Toy dataset of written text data.\n\n\n\n\n\n\n\nparticipant_id\ntext_id\nmodality\ntext\n\n\n\nP1\nT1\nWritten\nTechnology has revolutionized our lives in many ways. It has made communication easier, faster, and more efficient.\n\n\nP3\nT3\nWritten\nClimate change is a pressing issue that affects everyone on Earth. We must take immediate action to reduce our carbon footprint.\n\n\nP5\nT5\nWritten\nEducation is the key to personal and societal growth. Investing in quality education will lead to a brighter future for all.\n\n\n\n\n\n\n\n\n\n\nTable 2.13: Toy dataset of spoken text data.\n\n\n\n\n\n\n\nparticipant_id\ntext_id\nmodality\ntext\n\n\n\nP2\nT2\nSpoken\nHello, my name is X. I am a software engineer working at XYZ company.\n\n\nP4\nT4\nSpoken\nHi, I’m X, and I work as a project manager. My main responsibility is to ensure that projects are completed on time and within budget.\n\n\nP6\nT6\nSpoken\nHi, my name is X, and I’m a teacher. I teach English at a local high school.\n\n\n\n\n\n\nThese datasets contain the same variables and variable types, but different observations –one in which the sample contains written language and the other spoken.\nThe datasets can be concatenated to create a single dataset that contains all of the observations, as seen in Table 2.14.\n\n\n\n\nTable 2.14: Toy dataset of written and spoken text data concatenated.\n\n\n\n\n\n\n\nparticipant_id\ntext_id\nmodality\ntext\n\n\n\nP1\nT1\nWritten\nTechnology has revolutionized our lives in many ways. It has made communication easier, faster, and more efficient.\n\n\nP3\nT3\nWritten\nClimate change is a pressing issue that affects everyone on Earth. We must take immediate action to reduce our carbon footprint.\n\n\nP5\nT5\nWritten\nEducation is the key to personal and societal growth. Investing in quality education will lead to a brighter future for all.\n\n\nP2\nT2\nSpoken\nHello, my name is X. I am a software engineer working at XYZ company.\n\n\nP4\nT4\nSpoken\nHi, I’m X, and I work as a project manager. My main responsibility is to ensure that projects are completed on time and within budget.\n\n\nP6\nT6\nSpoken\nHi, my name is X, and I’m a teacher. I teach English at a local high school.\n\n\n\n\n\n\nMerging datasets can be performed in a column-wise manner as well. In this process, the datasets need not have the exact same variables and variable types, rather it is required that the datasets share a common variable of the same informational type that can be used to index the datasets. This process is often referred to as joining datasets.\nCorpus resources often include metadata in stand-off annotation format. That is, the metadata is not embedded in the corpus files, but rather is stored in a separate file. This file will share a common variable with the corpus files, which can be used to join the metadata with the corpus files.\nHere’s another toy dataset that shares the participant_id index with the previous dataset in Table 2.14 and includes the variables native_speaker_eng, age, and gender:\n\n\n\n\nTable 2.15: Toy dataset of participant data with a shared variable participant_id to index the datasets.\n\nparticipant_id\nnative_speaker_eng\nage\ngender\n\n\n\nP1\nYes\n28\nM\n\n\nP2\nNo\n35\nM\n\n\nP3\nYes\n42\nF\n\n\nP4\nNo\n26\nF\n\n\nP5\nYes\n31\nM\n\n\nP6\nNo\n39\nF\n\n\n\n\n\n\nThis dataset provides additional information about each participant, such as their English native speaker status, age, and gender.\nSince the two datasets share the participant_id variable, we can merge them to create a new dataset that combines the information from both datasets, as we see in Table 2.16.\n\n\n\n\nTable 2.16: Joining variables from two datasets based on a shared index variable.\n\n\n\n\n\n\n\n\n\n\nparticipant_id\ntext_id\nmodality\ntext\nnative_speaker_eng\nage\ngender\n\n\n\nP1\nT1\nWritten\nTechnology has revolutionized our lives in many ways. It has made communication easier, faster, and more efficient.\nYes\n28\nM\n\n\nP3\nT3\nWritten\nClimate change is a pressing issue that affects everyone on Earth. We must take immediate action to reduce our carbon footprint.\nYes\n42\nF\n\n\nP5\nT5\nWritten\nEducation is the key to personal and societal growth. Investing in quality education will lead to a brighter future for all.\nYes\n31\nM\n\n\nP2\nT2\nSpoken\nHello, my name is X. I am a software engineer working at XYZ company.\nNo\n35\nM\n\n\nP4\nT4\nSpoken\nHi, I’m X, and I work as a project manager. My main responsibility is to ensure that projects are completed on time and within budget.\nNo\n26\nF\n\n\nP6\nT6\nSpoken\nHi, my name is X, and I’m a teacher. I teach English at a local high school.\nNo\n39\nF\n\n\n\n\n\n\nAnother common case where joining datasets is useful is when there are external resources that can be used to enrich the dataset. For example, a dataset of text data may include a variable that identifies the language of each document. This variable can be used to join the dataset with a dataset of language metadata, such as the number of speakers of each language, as long as the language metadata dataset includes a variable that identifies the language of each document in the same format as the language variable in the text dataset.\n\nIn sum, the transformation steps described here collectively aim to produce higher quality datasets that are relevant in content and structure to submit to analysis. The process may include one or more of the previous transformations but is rarely linear and is most often iterative. It is typical to do some normalization then generation, then recoding, and then return to normalizing, and so forth. This process is highly idiosyncratic given the characteristics of the curated dataset and the ultimate goal for the derived dataset(s)."
  },
  {
    "objectID": "understanding-data.html#sec-ud-documentation",
    "href": "understanding-data.html#sec-ud-documentation",
    "title": "2  Understanding data",
    "section": "\n2.3 Documentation",
    "text": "2.3 Documentation\nAs we have seen in this chapter that acquiring corpus data and converting that data into information involves a number of conscious decisions and implementation steps. As a favor to ourselves as researchers and to the research community, it is crucial to document these decisions and steps. This makes it both possible to retrace our own steps and also provides a guide for future researchers that want to reproduce and/ or build on your research. A programmatic approach to quantitative research helps ensure that the implementation steps are documented and reproducible but it is also vital that the decisions that are made are documented as well. This includes data origin information for the acquired corpus data and data dictionaries for the curated and derived datasets.\n\n2.3.1 Data origin\nData acquired from corpus resources should be accompanied by information about the data origin. Table 2.17 provides a list of the types of information that should be included in the data origin information.\n\n\n\n\nTable 2.17: Data origin information.\n\n\n\n\n\nData origin information\nDescription\n\n\n\nResource name\nName of the corpus resource.\n\n\nData source\nURL, DOI, etc.\n\n\n\nData sampling frame\nLanguage, language variety, modality, genre, etc.\n\n\n\nDate of data collection\nThe date or date range of the data collection.\n\n\nData format\nPlain text, XML, HTML, etc.\n\n\n\nData schema\nRelationships between data elements: files, folders, etc.\n\n\n\nLicense and usage restrictions\nCC BY, CC BY-NC, etc.\n\n\n\nAttribution\nCitation information for the data source.\n\n\n\n\n\n\nFor many corpus resources, the corpus documentation will include all or most of this information as part of the resource download or documented online. If this information is not present in the corpus resource or you compile your own, it is important to document this information yourself. This information can be documented in file, such as a plain text file or spreadsheet, that is included with the corpus resource.\n\n2.3.2 Data dictionaries\nThe process of organizing the data into a dataset, curation, and modifications to the dataset in preparation for analysis, transformation, each include a number of project-specific decisions. These decisions should be documented.\nOn the one hand each dataset that is created should have a data dictionary file. A data dictionary is a document, usually in a spreadsheet format, that describes the variables in a dataset. The key information that should be included in a data dictionary is provided in Table 2.18.\n\n\n\n\nTable 2.18: Data dictionary information.\n\n\n\n\n\nData dictionary information\nDescription\n\n\n\nVariable name\nThe name of the variable as it appears in the dataset, e.g. participant_id, modality, etc.\n\n\n\nReadable variable name\nA human-readable name for the variable, e.g. ‘Participant ID’, ‘Language modality’, etc.\n\n\n\nVariable type\nThe type of information that the variable contains, e.g. ‘categorical’, ‘ordinal’, etc.\n\n\n\nVariable description\nA prose description expanding on the readable name and can include measurement units, allowed values, etc.\n\n\n\n\n\n\n\nOrganizing this information in a tabular format, such as a spreadsheet, can make it easy for others to read and understand your data dictionary.\nOn the other hand, the data curation and transformation steps should be documented in the code that is used to create the dataset. This is one of the valuable features of a programmatic approach to quantitative research. The transparency of this documentation is enhanced by using literate programming strategies to intermingling prose descriptions and code the steps in the same, reproducible document.\nBy providing a comprehensive data dictionary and using a programmatic approach to data curation and transformation, you ensure that others can easily understand and work with your dataset, facilitating collaboration and reproducibility."
  },
  {
    "objectID": "understanding-data.html#summary",
    "href": "understanding-data.html#summary",
    "title": "2  Understanding data",
    "section": "Summary",
    "text": "Summary\nIn this chapter we have focused on data and information –the first two components of DIKI Hierarchy. This process is visualized in Figure 2.4.\n\n\n\n\nFigure 2.4: Understanding data: visual summary\n\n\n\nFirst a distinction is made between populations and samples, the latter being a intentional and subjective selection of observations from the world which attempt to represent the population of interest. The result of this process is known as a corpus. Whether developing a corpus or selecting an existing a corpus it is important to vet the sampling frame for its applicability and viability as a resource for a given research project.\nOnce a viable corpus is identified, then that corpus is converted into a curated dataset which adopts the tidy dataset format where each column is a variable, each row is an observation, and the intersection of columns and rows contain values. This curated dataset serves to establish the base informational relationships from which your research will stem.\nThe curated dataset will most likely require transformations including normalization, tokenization, recoding, generation, and/ or merging to enhance the usefulness of the information to analysis. A derived dataset or datasets will the result from this process.\nFinally, documentation should be implemented at the acquisition, curation, and transformation stages of the analysis project process. The combination of data origin, data dictionary, and literate programming files establishes documentation of the data and implementation steps to ensure transparent and reproducible research."
  },
  {
    "objectID": "understanding-data.html#activities",
    "href": "understanding-data.html#activities",
    "title": "2  Understanding data",
    "section": "Activities",
    "text": "Activities\nIn the following activities you will learn how to read, inspect, and write data and datasets in R using reproducible strategies.\n\n\n\n\n\n\n Recipe\n\nWhat: Reading, inspecting, and writing dataHow: Read Recipe 2 and participate in the Hypothes.is online social annotation.Why: To use literate programming in Quarto to work with R coding strategies for reading, inspecting, and writing datasets.\n\n\n\n\n\n\n\n\n\n Lab\n\nWhat: Reading, inspecting, and writing dataHow: Clone, fork, and complete the steps in Lab 2.Why: To read datasets from packages and from plain-text files, inspect and report characteristics of datasets, and write datasets to plain-text files."
  },
  {
    "objectID": "understanding-data.html#questions",
    "href": "understanding-data.html#questions",
    "title": "2  Understanding data",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\n\n\nConceptual questions\n\nWhat is the difference between a population and a sample?\nWhy is it important to vet a corpus before using it in a research project?\nWhat is a curated dataset in the context of linguistic research?\nWhat is the difference between a variable, an observation, and a value?\nWhy is it important to identify the levels of measurement of variables in a dataset?\nWhat kinds of transformations may be performed on a curated dataset to enhance its usefulness for analysis?\nWhat is an analysis dataset and why is it important in linguistic research?\nWhy is documentation important in the process of conducting linguistic analysis?\nHow does a programmatic approach enhance documentation in linguistic research?\nHow does documenting the corpus data and the curated and derived datasets contribute to transparent and reproducible research in linguistics?\n\n\n\n\n\n\n\n\n\n\n\nTechnical questions\n\n\nCreating a sample corpus.\nWriting a corpus documentation.\nConverting a corpus to a derived dataset.\nWriting a data dictionary.\nTransforming a derived dataset.\nMerging datasets.\nWriting a dataset to disk.\nConsider (an example dataset) and its data dictionary, write a script to read the dataset, inspect it, and write it to disk.\nConsider a dataset and its data dictionary what appears to be the unit of analysis and the unit of observation?\n\n\n\n\n\n\n\n\n\n\nÄdel, Annelie. 2020. “Corpus Compilation.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Th. Gries, 3–24. Switzerland: Springer.\n\n\nBaayen, R. Harald, L. B. Feldman, and R. Schreuder. 2006. “Morphological Influences on the Recognition of Monosyllabic Monomorphemic Words.” Journal of Memory and Language 55: 290–313. https://doi.org/10.1016/j.jml.2006.03.008.\n\n\nBenoit, Kenneth, David Muhr, and Kohei Watanabe. 2021. Stopwords: Multilingual Stopword Lists. https://github.com/quanteda/stopwords.\n\n\nGries, Stefan Th. 2021. Statistics for Linguistics with r. De Gruyter Mouton.\n\n\nGrieve, Jack, Andrea Nini, and Diansheng Guo. 2018. “Mapping Lexical Innovation on American Social Media.” Journal of English Linguistics 46 (4): 293–319.\n\n\nIde, Nancy, Collin Baker, Christiane Fellbaum, Charles Fillmore, and Rebecca Passonneau. 2008. “MASC: The Manually Annotated Sub-Corpus of American English.” In 6th International Conference on Language Resources and Evaluation, LREC 2008, 2455–60. European Language Resources Association (ELRA).\n\n\nKaur, Jashanjot, and P. Kaur Buttar. 2018. “A Systematic Review on Stopword Removal Algorithms.” International Journal on Future Revolution in Computer Science & Communication Engineering 4 (4): 207–10.\n\n\nKostić, Aleksandar, Tanja Marković, and Aleksandar Baucal. 2003. “Inflectional Morphology and Word Meaning: Orthogonal or Co-Implicative Cognitive Domains?” In Morphological Structure in Language Processing, edited by R. Harald Baayen and Robert Schreuder, 1–44. De Gruyter Mouton. https://doi.org/10.1515/9783110910186.1.\n\n\nKucera, H, and W N Francis. 1967. Computational Analysis of Present Day American English. Brown University Press Providence.\n\n\nMagueresse, Alexandre, Vincent Carles, and Evan Heetderks. 2020. “Low-Resource Languages: A Review of Past Work and Future Challenges.” arXiv. https://arxiv.org/abs/2006.07264.\n\n\nMarcus, Mitchell P, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. “Building a Large Annotated Corpus of English: The Penn Treebank.” Computational Linguistics 19 (2): 313–30.\n\n\nNivre, Joakim, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajič, Christopher D Manning, Ryan McDonald, et al. 2016. “Universal Dependencies V1: A Multilingual Treebank Collection.” Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), 1659–66.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10."
  },
  {
    "objectID": "understanding-data.html#footnotes",
    "href": "understanding-data.html#footnotes",
    "title": "2  Understanding data",
    "section": "",
    "text": "The Universal Dependency project is an effort to develop cross-linguistically consistent treebank annotation for many languages. The project has developed a set of annotation guidelines and a set of tools for generating linguistic annotation. The project has also developed a set of pre-trained models for many languages.↩︎"
  },
  {
    "objectID": "framing-research.html#fr-keys",
    "href": "framing-research.html#fr-keys",
    "title": "4  Framing research",
    "section": "\n4.1 Keys to strong research",
    "text": "4.1 Keys to strong research\nTogether a research area, problem, aim and question and the research blueprint that forms the conceptual and practical scaffolding of the project ensure from the outset that the project is solidly grounded in the main characteristics of good research. These characteristics, summarized by Cross (2006), are found in Table 4.1.\n\n\n\n\nTable 4.1: Characteristics of research (Cross, 2006).\n\n\n\n\n\nCharacteristic\nDescription\n\n\n\nPurposive\nBased on identification of an issue or problem worthy and capable of investigation\n\n\nInquisitive\nSeeking to acquire new knowledge\n\n\nInformed\nConducted from an awareness of previous, related research\n\n\nMethodical\nPlanned and carried out in a disciplined manner\n\n\nCommunicable\nGenerating and reporting results which are feasible and accessible by others\n\n\n\n\n\n\nWith these characteristics in mind, let’s get started with the first component to address –connecting with the literature."
  },
  {
    "objectID": "framing-research.html#fr-connect",
    "href": "framing-research.html#fr-connect",
    "title": "4  Framing research",
    "section": "\n4.2 Connect",
    "text": "4.2 Connect\n\n4.2.1 Research area\nThe area of research is the first decision to make in terms of where to make a contribution to understanding. At this point, the aim is to identify a general area of interest where a researcher wants to derive insight. For those with an established research trajectory in language, the area of research to address through text analysis will likely be an extension of their prior work. For others, which include new researchers or researcher’s that want to explore new areas of language research or approach an area through a language-based lens, the choice of area may be less obvious. In either case, the choice of a research area should be guided by a desire to contribute something relevant to a theoretical, social, and/ or practical matter of personal interest. Personal relevance goes a long way to developing and carrying out purposive and inquisitive research.\nSo how do we get started? The first step is to reflect on your own areas of interest and knowledge, be it academic, professional, or personal. Language is at the heart of the human experience and therefore found in some fashion anywhere one seeks to find it. But it is a big world and more often than not the general question about what area to explore language use is sometimes the most difficult. To get the ball rolling, it is helpful to peruse disciplinary encyclopedias or handbooks of linguistics and language-related an academic fields (e.g. Encyclopedia of Language and Linguistics (Brown 2005), A Practical Guide to Electronic Resources in the Humanities (Dubnjakovic and Tomlin 2010), Routledge encyclopedia of translation technology (Chan 2014))\nA more personal, less academic, approach is to consult online forums, blogs, etc. that one already frequents or can be accessed via an online search. For example, Reddit has a wide variety of active subreddits (r/LanguageTechnology, r/Linguistics, r/corpuslinguistics, r/DigitalHumanities, etc.). Twitter and Facebook also have interesting posts on linguistics and language-related fields worth following. Through one of these social media site you may find particular people that maintain a blog worth browsing. For example, I follow Julia Silge, Rachel Tatman, and Ted Underwood, inter alia. Perusing these resources can help spark ideas and highlight the kinds of questions that interest you.\nRegardless of whether your inquiry stems from academic, professional, or personal interest, try to connect these findings to academic areas of research. Academic research is highly structured and well-documented and making associations with this network will aid in subsequent steps in developing a research project.\n\n4.2.2 Research problem\nOnce you’ve made a rough-cut decision about the area of research, it is now time to take a deeper dive into the subject area and jump into the literature. This is where the rich structure of disciplinary research will provide aid to traverse the vast world of academic knowledge and identify a research problem. A research problem highlights a particular topic of debate or uncertainty in existing knowledge which is worthy of study.\nSurveying the relevant literature is key to ensuring that your research is informed, that is, connected to previous work. Identifying relevant research to consult can be a bit of a ‘chicken or the egg’ problem –some knowledge of the area is necessary to find relevant topics, some knowledge of the topics is necessary to narrow the area of research. Many times the only way forward is to jump in conducting searches. These can be world-accessible resources (e.g. Google Scholar) or limited-access resources that are provided through an academic institution (e.g. Linguistics and Language Behavior Abstracts), ERIC, PsycINFO, etc.). Some organizations and academic institutions provide research guides to help researcher’s access the primary literature.\nAnother avenue to explore are journals dedicated to areas in which linguistics and language-related research is published. In the following tables I’ve listed a number of highly visible journals in linguistics, digital humanities, and computational linguistics.\n\n\n\n\nTable 4.2: A list of some linguistics journals.\n\n\n\n\n\nResource\nDescription\n\n\n\nCorpora\nAn international, peer-reviewed journal of corpus linguistics focusing on the many and varied uses of corpora both in linguistics and beyond.\n\n\nCorpus Linguistics and Linguistic Theory\nCorpus Linguistics and Linguistic Theory (CLLT) is a peer-reviewed journal publishing high-quality original corpus-based research focusing on theoretically relevant issues in all core areas of linguistic research, or other recognized topic areas.\n\n\nInternational Journal of Corpus Linguistics\nThe International Journal of Corpus Linguistics (IJCL) publishes original research covering methodological, applied and theoretical work in any area of corpus linguistics.\n\n\nInternational Journal of Language Studies\nIt is a refereed international journal publishing articles and reports dealing with theoretical as well as practical issues focusing on language, communication, society and culture.\n\n\nJournal of Child Language\nA key publication in the field, Journal of Child Language publishes articles on all aspects of the scientific study of language behaviour in children, the principles which underlie it, and the theories which may account for it.\n\n\nJournal of Linguistic Geography\nThe Journal of Linguistic Geography focuses on dialect geography and the spatial distribution of language relative to questions of variation and change.\n\n\nJournal of Quantitative Linguistics\nPublishes research on the quantitative characteristics of language and text in mathematical form, introducing methods of advanced scientific disciplines.\n\n\n\n\n\n\n\n\n\n\nTable 4.3: A list of some humanities journals.\n\n\n\n\n\nResource\nDescription\n\n\n\nDigital Humanities Quarterly\nDigital Humanities Quarterly (DHQ), an open-access, peer-reviewed, digital journal covering all aspects of digital media in the humanities.\n\n\nDigital Scholarship in the Humanities\nDSH or Digital Scholarship in the Humanities is an international, peer reviewed journal which publishes original contributions on all aspects of digital scholarship in the Humanities including, but not limited to, the field of what is currently called the Digital Humanities.\n\n\nJournal of Cultural Analytics\nCultural Analytics is an open-access journal dedicated to the computational study of culture. Its aim is to promote high quality scholarship that applies computational and quantitative methods to the study of cultural objects (sound, image, text), cultural processes (reading, listening, searching, sorting, hierarchizing) and cultural agents (artists, editors, producers, composers).\n\n\n\n\n\n\n\n\n\n\nTable 4.4: A list of some computational linguistics journals.\n\n\n\n\n\nResource\nDescription\n\n\n\nComputational Linguistics\nComputational Linguistics is the longest-running publication devoted exclusively to the computational and mathematical properties of language and the design and analysis of natural language processing systems.\n\n\nLREC Conferences\nThe International Conference on Language Resources and Evaluation is organised by ELRA biennially with the support of institutions and organisations involved in HLT.\n\n\nTransactions of the Association for Computational Linguistics\nTransactions of the Association for Computational Linguistics (TACL) is an ACL-sponsored journal published by MIT Press that publishes papers in all areas of computational linguistics and natural language processing.\n\n\n\n\n\n\nTo explore research related to text analysis it is helpful to start with the (sub)discipline name(s) you identified in when selecting your research area, more specific terms that occur to you or key terms from the literature, and terms such as ‘corpus study’ or ‘corpus-based’. The results from first searches may not turn out to be sources that end up figuring explicitly in your research, but it is important to skim these results and the publications themselves to mine information that can be useful to formulate better and more targeted searches. Relevant information for honing your searches can be found throughout an academic publication (article or book). However, pay particular attention to the abstract, in articles, and the table of contents, in books, and the cited references. Abstracts and tables of contents often include discipline-specific jargon that is commonly used in the field. In some articles there is even a short list of key terms listed below the abstract which can be extremely useful to seed better and more precise search results. The references section will contain relevant and influential research. Scan these references for publications which appear to narrowing in on topic of interest and treat it like a search in its own right.\nOnce your searches begin to show promising results it is time to keep track and organize these references. Whether you plan to collect thousands of references over a lifetime of academic research or your aim is centered around one project, software such as Zotero1, Mendeley, or BibDesk provide powerful, flexible, and easy-to-use tools to collect, organize, annotate, search, and export references. Citation management software is indispensable for modern research –and often free!\nAs your list of relevant references grows, you will want to start the investigation process in earnest. Begin skimming (not reading) the contents of each of these publications, starting with the most relevant first2. Annotate these publications using highlighting features of the citation management software to identify: (1) the stated goal(s) of the research, (2) the data source(s) used, (3) the information drawn from the data source(s), (4) the analysis approach employed, and (5) the main finding(s) of the research as they pertain to the stated goal(s). Next, in your own words, summarize these five key areas in prose adding your summary to the notes feature of the citation management software. This process will allow you to efficiently gather and document references with the relevant information to guide the identification of a research problem, to guide the formation of your problem statement, and ultimately, to support the literature review that will figure in your project write-up.\nFrom your preliminary annotated summaries you will undoubtedly start to recognize overlapping and contrasting aspects in the research literature. These aspects may be topical, theoretical, methodological, or appear along other lines. Note these aspects and continue to conduct more refine searches, annotate new references, and monitor for any emerging patterns of uncertainty or debate (gaps) which align with your research interest(s). When a promising pattern takes shape, it is time to engage with a more detailed reading of those references which appear most relevant highlighting the potential gap(s) in the literature. At this point you can focus energy on more nuanced aspects of a particular gap in the literature with the goal to formulate a problem statement. A problem statement directly acknowledges a gap in the literature and puts a finer point on the nature and relevance of this gap for understanding. This statement reflects your first deliberate attempt to establish a line of inquiry. It will be a targeted, but still somewhat general, statement framing the gap in the literature that will guide subsequent research design decisions."
  },
  {
    "objectID": "framing-research.html#findings",
    "href": "framing-research.html#findings",
    "title": "4  Framing research",
    "section": "\n4.3 Findings",
    "text": "4.3 Findings\n\n4.3.1 Research aim\nWith a problem statement in hand, it is now time to consider the goal(s) of the research. A research aim frames the type of inquiry to be conducted. Will the research aim to explore, examine, or explain? In other words, will the research seek to uncover novel relationships, assess the potential strength of a particular relationship, or test a particular relationship? As you can appreciate, the research aim is directly related to the analysis methods we touched upon in Chapter 3.\nTo gauge how to frame your research aim, reflect on the literature that led you to your problem statement and the nature of the problem statement itself. If the gap at the center of the problem statement is a lack of knowledge, your research aim may be exploratory. If the gap concerns a conjecture about a relationship, then your research may take a predictive approach. When the gap points to the validation of a relationship, then your research will likely be inferential in nature. Before selecting your research aim it is also helpful to consult the research aims of the primary literature that led you to your research statement. Consider how your research statement relates the previous literature. Do you aim to test a hypothesis based on previous exploratory analyses? Are you looking to generate new knowledge in an (apparently) uncharted area? Etc.\nIn general, a problem statement which addresses a smaller, nuanced gap will tend to adopt similar research aims as the previous literature while a larger, more divergent gap will tend to adopt a distinct research aim. This is not a hard rule, but more of a heuristic, however, and it is important to be familiar with both the previous literature, the nature of different types of analysis, and the goals of the research to ensure that the research is best-positioned to generate findings that will contribute to the existing body of understanding in a principled way.\n\n4.3.2 Research question\nThe next step in research design is to craft the research question. A research question is clearly defined statement which identifies an aspect of uncertainty and the particular relationships that this uncertainty concerns. The research question extends and narrows the line of inquiry established in the research statement and research aim. The research statement can be seen as the content and the research aim as the form.\nThe form of a research question will vary based on the analysis approach.\nFor inferential-based research, the research question will actually be a statement, not a question. This statement makes a testable claim about the nature of a particular relationship –i.e. asserts a hypothesis. For illustration, let’s return to one of the hypotheses we previously sketched out in Chapter 3, leaving aside the implicit null hypothesis.\nWomen use more questions than men in spontaneous conversations.\nFor predictive- and exploratory-based research, the research question is in fact a question. A reframing of the example hypothesis for a predictive-based research question might looks something like this.\nCan the number of questions used in spontaneous conversations predict if a speaker is male or female?\nAnd a similar exploratory-based research question would take this form.\nDo men and women differ in terms of the number of questions they use in spontaneous conversations?\nThe central research interest behind these hypothetical research questions is, admittedly, quite basic. But from these simplified examples, we are able to appreciate the similarities and differences between the forms of research statements that correspond to distinct research aims.\n\nIn terms of content, the research question will make reference to two key components. First, is the unit of analysis. The unit of analysis is the entity which the research aims to investigate. For our three example research aims, the unit of analysis is the same, namely men and women. Note, however, that the current unit of analysis is somewhat vague in the example research questions. A more precise unit of analysis would include more information about the population from which the men and women are drawn (i.e. English speakers, American English speakers, American English speakers of the Southeast, etc.).\nThe second key component is the unit of observation. The unit of observation is the primary element on which the insight into the unit of analysis is derived and in this way constitutes the essential organizational unit of the dataset to be analyzed. In our examples, the unit of observation, again, is unchanged and is spontaneous conversations. Note that while the unit of observation is key to identify as it forms the organizational backbone of the research, it is very common for the research to derive variables from this unit to provide evidence to investigate the research question. In the previous examples, we identified the number of conversations as part of the research question. But in other cases a researcher may seek to understand other aspects of questions in spontaneous conversations (i.e type of question, features of questions, etc.). The unit of observation, however, would remain the same."
  },
  {
    "objectID": "framing-research.html#blueprint",
    "href": "framing-research.html#blueprint",
    "title": "4  Framing research",
    "section": "\n4.4 Blueprint",
    "text": "4.4 Blueprint\nEfforts to craft a research question are a very important aspect of developing purposive, inquisitive, and informed research (returning to Cross’s characteristics of research). Moving beyond the research question in the project means developing and laying out the research design in a way such that the research is Methodical and Communicable. In this textbook, the method to achieve these goals is through the development of a research blueprint. The blueprint includes two components: (1) the process of identifying the data, information, and analysis methods to be used and (2) the creation of a plan to structure and document the project.\nAs Ignatow and Mihalcea (2017) point out:\n\nResearch design is essentially concerned with the basic architecture of research projects, with designing projects as systems that allow theory, data, and research methods to interface in such a way as to maximize a project’s ability to achieve its goals […]. Research design involves a sequence of decisions that have to be taken in a project’s early stages, when one oversight or poor decision can lead to results that are ultimately trivial or untrustworthy. Thus, it is critically important to think carefully and systematically about research design before committing time and resources to acquiring texts or mastering software packages or programming languages for your text mining project.\n\n\n4.4.1 Identify\nImportance of identifying and documenting the key aspects required to conduct the research cannot be understated. On the one hand this process links concept to implementation. In doing so, a researcher is better-positioned to conduct research with a clear view of what will be entailed. On the other hand, a promising research question, on paper, may present challenges that may require modification or reevaluation of the viability of the project. It is not uncommon to encounter roadblocks or even dead-ends for moving a well-founded research question forward when considering the available data, a researcher’s (current) technical and/ or research skills, and the given time frame for the project. In practice, the process of identifying the data, information, and methods of analysis are considered in tandem with the investigative work to develop a research aim and research question. In this subsection I will cover the main characteristics to consider when developing a research blueprint.\n\nThe first, and most important, part of establishing a research blueprint is to identify a viable data source. Regardless of how you find and access the data, it is essential to vet the corpus sample in light of the research question. In the case that research is inferential in nature, the sampling frame of the corpus is of primary importance as the goal is to generalize the findings to a target population. A corpus resource should align, to the extent feasible, with this target population. For predictive and exploratory research, the goal to generalize a claim is not central and for this reason the there is some freedom in terms of how representative a corpus sample is of a target population. Ideally a researcher will find and be able to model a language population of target interest. Since the goal, however, is not to test a hypothesis, but rather to explore particular or evaluate potential relationships, either in an exploratoy or predictive fashion, the research can often continue with the stipulation that the results are interpreted in the light of the characteristics of the available corpus sample.\n\n\nThe second step is to identify the key variables need to conduct the research are and then ensure that this information can be derived from the corpus data. The research question will reference the unit of analysis and the unit of observation, but it is important at this point to then pinpoint what the key variables will be. If the unit of observation is spontaneous conversations. The question as to what aspects of these conversations will be used in the analysis. In the research questions presented in this chapter, we will want to envision what needs to be done to derive a variable which measures the number of questions in each of the conversations. In other research, their may be features that need to be extracted, recoded, and/ or generated to address the research question. Other variables of importance may be non-linguistic in nature. In cases where there the meta-data is incomplete for the goals of the research, it is sometimes possible to merge meta-data from other sources.\n\n\nThe third step is to identify a method of analysis. The selection of the analysis approach that was part of the research aim and then the research question goes a long way to narrowing the methods that a researcher must consider. But there are a number of factors which will make some methods more appropriate than others. In inferential research, the number and information values of the variables to be analyzed will be of key importance (Gries 2013). The informational value of the dependent variable will again narrow the search for the appropriate method. The number of independent variables also plays an important role. For example, a study with a categorical dependent variable with a single categorical independent variable will lead the researcher to the Chi-squared test. A study with a continuous dependent variable with multiple independent variables will lead to linear regression. Another aspect of note for inference studies is the consideration of the distribution of continuous variables –a normal distribution will use a parametric test where a non-normal distribution will use a non-parametric test. These details need not be nailed down at this point, but it is helpful to have them on your radar to ensure that when the time comes to analyze the data, the appropriate steps are taken to test for normality and then apply the correct test.\nFor predictive-based research, the informational value of the target variable is key to deciding whether the prediction will be a classification task or a numeric prediction task. This has downstream effects when it comes time to evaluate and interpret the results. Although the feature engineering process in predictive analyses means that the features do not need to be specified from the outset and can be tweaked and changed as needed during an analysis, it is a good idea to start with a basic sense of what features most likely will be helpful in developing a robust predictive model. Furthermore, while the number and informational values of the features (predictor variables) are not as important to selecting a prediction method (algorithm) as they are in inferential analysis methods, it is important to recognize that algorithms have strengths and shortcomings when working large numbers and/ or types of features (Lantz 2013).\nExploratory research is the least restricted of the three types of analysis approaches. Although it may be the case that a research will not be able to specify from the outset of a project what the exact analysis methods will be, an attempt to consider what types of analysis methods will be most promising to provide results to address the research question goes a long way to steering a project in the right direction and grounding the research. As with the other analysis approaches, it is important to be aware of what the analysis methods available and what type of information they produce in light of the research question.\nIn sum, the identification of the data, information, and analysis methods that will be used in the proposed research are key to ensuring the research is viable. Be sure to document this process in prose and describe the strengths and potential shortcomings of (1) the corpus data selected, (2) the information to be extracted for analysis, and (3) the analysis method(s) that are appropriate for the research aim and what the evaluation method will be. Furthermore, not every eventuality can be foreseen. It is helpful to include a description of aspects of this process which may pose challenges and to include potential contingency plans as part of this prose description.\n\n\n4.4.2 Plan\nThe next step in creating a research blueprint is to consider how to physically implement your project. This includes how to organize files and directories in a fashion that both provides the researcher a logical and predictable structure to work with but also ensures that the research is Communicable. On the one hand, communicable research includes a strong write-up of the research, but, on the other hand, it is also important that the research is reproducible. Reproducibility strategies are a benefit to the researcher (in the moment and in the future) as it leads to better work habits and to better teamwork and it makes changes to the project easier. Reproducibility is also of benefit to the scientific community as shared reproducible research enhances replicability and encourages cumulative knowledge development (Gandrud 2015).\nPrinciples of reproducible projects\nThere are a set of guiding principles to accomplish these goals (Gentleman and Temple Lang 2007; Marwick, Boettiger, and Mullen 2018).\n\nAll files should be plain text which means they contain no formatting information other than whitespace.\nThere should be a clear separation between the data, method, and output of research. This should be apparent from the directory structure.\nA separation between original data and derived data should be made. Original data should be treated as ‘read-only’. Any changes to the original data should be justified, generated by the code, and documented (see point 6).\nEach analysis file (script) should represent a particular, well-defined step in the research process.\nEach analysis script should be modular –that is, each file should correspond to a specific goal in the analysis procedure with input and output only corresponding to this step.\nAll analysis scripts should be tied together by a ‘master’ script that is used to coordinate the execution of all the analysis steps.\nEverything should be documented. This includes data preprocessing, analysis steps, script code comments, data description in data dictionaries, information about the computing environment and packages used to conduct the analysis, and detailed instructions on how to reproduce the research.\n\nThese seven principles can be physically implemented in countless ways. In recent years, there has been a growing number of efforts to create R packages and templates to quickly generate the scaffolding and tools to facilitate reproducible research. Some notable R packages include workflowr and ProjectTemplate but there are many other resources for R included on the CRAN Task View for Reproducible Research. There are many advantages to working with pre-existing frameworks for the savvy R programmer.\n\nProject template\n\nIn this textbook, however, I have developed a project template (available on GitHub) which I believe simplifies and makes the process more transparent for beginning and intermediate R programmers, the directory structure is provided below.\n\n\n&gt; ../project_template/\n&gt; ├── README.md\n&gt; ├── _pipeline.R\n&gt; ├── analysis\n&gt; │   ├── 1_acquire_data.Rmd\n&gt; │   ├── 2_curate_dataset.Rmd\n&gt; │   ├── 3_transform_dataset.Rmd\n&gt; │   ├── 4_analyze_dataset.Rmd\n&gt; │   ├── 5_generate_article.Rmd\n&gt; │   ├── _session-info.Rmd\n&gt; │   ├── _site.yml\n&gt; │   ├── index.Rmd\n&gt; │   └── references.bib\n&gt; ├── data\n&gt; │   ├── derived\n&gt; │   └── original\n&gt; └── output\n&gt;     ├── figures\n&gt;     └── results\n\n\nLet me now describe how this template structure aligns with the seven principles of quality reproducible research.\n\nAll files are plain text (e.g. .R, .Rmd, .csv, .txt, etc.).\nThere are three main directories analysis/, data/, and ouput/.\nThe data/ directory contains sub-directories for original (‘read-only’) data and derived data.\nThe analysis/ directory contains five scripts which are numbered to correspond with their sequential role in the research process.\nEach of these analysis scripts are designed to be modular; input and output must be explicit and no intermediate objects are carried over to other analysis scripts. Dataset output should be written to and read from the data/derived/ directory. Figures and statistical results should be written to and read from output/figures/ and output/results respectively.\nAll of the analysis scripts, and therefore the entire project, are tied to the _pipeline.R script. To reproduce the entire project only this script need be run.\nDocumentation takes place at many levels. The README.md file is the first file that a researcher will consult. It contains a brief description of the project goals and how to reproduce the analysis. Analysis scripts use the Rmarkdown format (.Rmd). This format allows researchers to interleave prose description and executable code in the same script. This ensures that the rationale for the steps taken are described in prose, the code is made available to consult, and that code comments can be added to every line. The _sesssion-info.Rmd script is merged with each analysis script to provide information about the computing environment and packages used to conduct each step analysis. As this is a template, no data or datasets appear. However, once data is acquired and that data is curated and transformed, documentation for these resources should be documented for each resource in a data dictionary along side the data(set) itself.\n\nThe aspects of the project template described in points 1-7 together form the backbone for reproducible research. This template, however, includes additional functionality to enhance efficient and communicable research. The _pipeline.R script executes the analysis scripts in the analysis directory, but as a side effect also produces a working website and a journal-ready article for publishing your analysis, results, and findings to the web in HTML and PDF format. The index.Rmd file is the splash page for the website and is a good place to house your pre-analysis investigative work including your research area, problem, aim, and question and to document your research blueprint including the identification of viable data resource(s), the key variables for the analysis, the analysis method, and the method of assessment. All Rmarkdown files provide functionality for citing and organizing references. The references.bib file is where references are stored and can be used to include citations that support your research throughout your project.\n\n4.4.3 Scaffold\nThis template will allow you to organize your research design and align it with implementation steps to conduct quality reproducible research. To set the structure for you to conduct your analysis, you will need to download or fork and clone this template from the GitHub repository and then make some adjustments to personalize this template for your research.\nTo create a local copy of this project template either:\n\nDownload and decompress the .zip file\n\nIf you have git installed on your machine and a GitHub account, fork the repository to your own GitHub account. Then open a terminal in the desired location and clone the repository. If you are using RStudio, you can setup a new RStudio Project with the clone using the ‘New Project…’ dialog, choosing ‘Version Control’, and following the steps.\n\nBefore you begin configuring and adding your project-specific details to this template. Reproduce this project ‘as-is’ to confirm that it builds on your local machine.\nIn RStudio or in R session in a Terminal application, open the console in the root directory of the project. Then run:\nsource(\"_pipeline.R\")\nIt will take some time to complete, when it does the prompt (&gt;) in the console will return. Then navigate to and open docs/index.html in a browser.\nOnce you have confirmed that the project template builds, then you can begin to configure the template to reflect your project. There a few files to consider first. These files are places where the title of your project should appear.\n\nREADME.md\n_pipeline.R\nanalysis/index.Rmd\n\nAfter updating these files, build the project again and make sure that the new changes appear as you would like them. You are now ready to start your research project!"
  },
  {
    "objectID": "framing-research.html#summary",
    "href": "framing-research.html#summary",
    "title": "4  Framing research",
    "section": "Summary",
    "text": "Summary\nThe aim of this chapter is to provide the key conceptual and practical points to guide the development of a viable research project. Good research is purposive, inquisitive, informed, methodological, and communicable. It is not, however, always a linear process. Exploring your area(s) of interest and connecting with existing work will help couch and refine your research. But practical considerations, such as the existence of viable data, technical skills, and/ or time constrains, sometimes pose challenges and require a researcher to rethink and/ or redirect the research in sometimes small and other times more significant ways. The process of formulating a research question and developing a viable research plan is key to supporting viable, successful, and insightful research. To ensure that the effort to derive insight from data is of most value to the researcher and the research community, the research should strive to be methodological and communicable adopting best practices for reproducible research.\nThis chapter concludes the Orientation section of this textbook. At this point the fundamental characteristics of research are in place to move a project towards implementation. The next section, Preparation, aims to cover the acquisition, curation, and transformation of data in preparation for analysis. These are the first steps in putting a research blueprint into action and by no coincidence the first components in the Data to Insight Hierarchy. Following the Preparation section our attention will turn to the implementation of the three analysis approaches we have covered: inference, prediction, and exploration. Throughout these next sections we will maintain our aim to develop methodological and communicable research by connecting our implementation process to reproducible programming strategies.\n\n\n\n\nFigure 4.1: Framing research: visual summary"
  },
  {
    "objectID": "framing-research.html#activities",
    "href": "framing-research.html#activities",
    "title": "4  Framing research",
    "section": "Activities",
    "text": "Activities\n\n\n\n\n\n\nRecipe\n\n\n\n\nWhat: Project management with Git, GitHub, and RStudio CloudHow: Read Recipe 5 and participate in the Hypothes.is online social annotation.Why: To learn how to use Git, GitHub, and RStudio to manage, store, and publish reproducible research projects.\n\n\n\n\n\n\n\n\nLab\n\n\n\n\nWhat: Project management with Git, GitHub, and RStudio CloudHow: Clone, fork, and complete the steps in Lab 5.Why: To set up a GitHub account, fork and copy a GitHub repository to RStudio Cloud, and use R, Git, and GitHub to manage, store, and publish changes to a reproducible research project."
  },
  {
    "objectID": "framing-research.html#questions",
    "href": "framing-research.html#questions",
    "title": "4  Framing research",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\n\nConceptual questions\n\n\n\n\nWhat is the difference between a research question and a research hypothesis?\nWhat is the difference between a research design and a research plan?\n\n\n\n\n\n\n\n\n\n\nTechnical exercises\n\n\n\n\nMatching research questions with data sources\nMatching research questions with research designs\nPreregistering a research project (?)\nPropose a quantitative research topic (or question if possible). Support your topic with supporting literature. (?)\n\n\n\n\n\n\n\n\nBrown, Keith. 2005. Encyclopedia of Language and Linguistics. Vol. 1. Elsevier.\n\n\nChan, Sin-wai. 2014. Routledge Encyclopedia of Translation Technology. Routledge.\n\n\nCross, Nigel. 2006. “Design as a Discipline.” Designerly Ways of Knowing, 95–103.\n\n\nDubnjakovic, Ana, and Patrick Tomlin. 2010. A Practical Guide to Electronic Resources in the Humanities. Elsevier.\n\n\nGandrud, Christopher. 2015. Reproducible Research with r and r Studio. Second edition. CRC Press.\n\n\nGentleman, Robert, and Duncan Temple Lang. 2007. “Statistical Analyses and Reproducible Research.” Journal of Computational and Graphical Statistics 16 (1): 1–23.\n\n\nGries, Stefan Th. 2013. Statistics for Linguistics with r. A Practical Introduction. 2nd revise.\n\n\nIgnatow, Gabe, and Rada Mihalcea. 2017. An Introduction to Text Mining: Research Design, Data Collection, and Analysis. Sage Publications.\n\n\nLantz, Brett. 2013. Machine Learning with r. Birmingham: Packt Publishing.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging Data Analytical Work Reproducibly Using r (and Friends).” The American Statistician 72 (1): 80–88."
  },
  {
    "objectID": "framing-research.html#footnotes",
    "href": "framing-research.html#footnotes",
    "title": "4  Framing research",
    "section": "",
    "text": "Zotero Guide↩︎\nOr what appears to be most relevant. This may change as you start to take a closer look.↩︎"
  },
  {
    "objectID": "preparation.html",
    "href": "preparation.html",
    "title": "Preparation",
    "section": "",
    "text": "At this point we will turn our attention to implementing the specifics outlined in our research blueprint. This section will group the components which concern the acquisition, curation, and transformation of data into a dataset which is prepared to be submitted to analysis. In each of these three chapters I will outline some of the main characteristics to consider in each of these research steps and provide authentic examples of working with R to implement these steps. In Chapter 5 this includes downloads, working with APIs, and webscraping. In Chapter 6 we turn to organize data into rectangular, or ‘tidy’, format. Depending on the data or dataset acquired for the research project, the steps necessary to shape our data into a base dataset will vary, as we will see. In Chapter 7 we will work to manipulate curated datasets to create datasets which are aligned with the research aim and research question. This often includes normalizing values, recoding variables, and generating new variables as well as and sourcing and merging information from other datasets with the dataset to be submitted for analysis."
  },
  {
    "objectID": "acquire-data.html#downloads",
    "href": "acquire-data.html#downloads",
    "title": "5  Acquire data",
    "section": "\n5.1 Downloads",
    "text": "5.1 Downloads\n\n5.1.1 Manual\nThe first acquisition method I will cover here is inherently non-reproducible from the standpoint that the programming implementation cannot acquire the data based solely on running the project code itself. In other words, it requires manual intervention. Manual downloads are typical for data resources which are not openly accessible on the public facing web. These can be resources that require institutional or private licensing (Language Data Consortium, International Corpus of English, BYU Corpora, etc.), require authorization/ registration (The Language Archive, COW Corpora, etc.), and/ or are only accessible via resource search interfaces (Corpus of Spanish in Southern Arizona, Corpus Escrito del Español como L2 (CEDEL2), etc.).\nLet’s work with the CEDEL2 corpus (Lozano 2009) which provides a search interface and open access to the data through the search interface. The homepage can be seen in Figure 5.1.\n\n\n\n\nFigure 5.1: CEDEL2 Corpus homepage\n\n\n\nFollowing the search/ download link you can find a search interface that allows the user to select the sub-corpus of interest. I’ve selected the subcorpus “Learners of L2 Spanish” and specified the L1 as English.\n\n\n\n\nFigure 5.2: Search and download interface for the CEDEL2 Corpus\n\n\n\nThe ‘Download’ link now appears for this search criteria. Following this link will provide the user a form to fill out. This particular resource allows for access to different formats to download (Texts only, Texts with metadata, CSV (Excel), CSV (Others)). I will select the ‘CSV (Others)’ option so that the data is structured for easier processing downstream when we work to curate the data in our next processing step. Then I will choose to save the CSV in the data/original/ directory of my project and create a sub-directory called cedel2/.\ndata/\n├── derived\n└── original\n    └── cedel2\n       └── texts.csv\nOther resources will inevitably include unique processes to obtaining the data, but in the end the data should be archived in the research structure in the data/original/ directory and be treated as ‘read-only’.\n\n5.1.2 Programmatic\nThere are many resources that provide corpus data is directly accessible for which programmatic approaches can be applied. Let’s take a look at how this works starting with the a sample from the Switchboard Corpus, a corpus of 2,400 telephone conversations by 543 speakers. First we navigate to the site with a browser and download the file that we are looking for. In this case I found the Switchboard Corpus on the NLTK data repository site. More often than not this file will be some type of compressed archive file with an extension such as .zip or .tz, which is the case here. Archive files make downloading large single files or multiple files easy by grouping files and directories into one file. In R we can used the download.file() function from the base R library1. There are a number of arguments that a function may require or provide optionally. The download.file() function minimally requires two: url and destfile. That is the file to download and the location where it is to be saved to disk.\n\n# Download .zip file and write to disk\ndownload.file(url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip\",\n    destfile = \"../data/original/switchboard.zip\")\n\nAs we can see looking at the directory structure for data/ the switchboard.zip file has been downloaded.\ndata\n├── derived\n└── original\n    └── switchboard.zip\nOnce an archive file is downloaded, however, the file needs to be ‘decompressed’ to reveal the file structure. To decompress this file we use the unzip() function with the arguments zipfile pointing to the .zip file and exdir specifying the directory where we want the files to be extracted to.\n\n# Decompress .zip file and extract to our target directory\nunzip(zipfile = \"../data/original/switchboard.zip\", exdir = \"../data/original/\")\n\nThe directory structure of data/ now should look like this:\ndata\n├── derived\n└── original\n    ├── switchboard\n    │   ├── README\n    │   ├── discourse\n    │   ├── disfluency\n    │   ├── tagged\n    │   ├── timed-transcript\n    │   └── transcript\n    └── switchboard.zip\nAt this point we have acquired the data programmatically and with this code as part of our workflow anyone could run this code and reproduce the same results. The code as it is, however, is not ideally efficient. Firstly the switchboard.zip file is not strictly needed after we decompress it and it occupies disk space if we keep it. And second, each time we run this code the file will be downloaded from the remote serve leading to unnecessary data transfer and server traffic. Let’s tackle each of these issues in turn.\nTo avoid writing the switchboard.zip file to disk (long-term) we can use the tempfile() function to open a temporary holding space for the file. This space can then be used to store the file, unzip it, and then the temporary file will be destroyed. We assign the temporary space to an R object we will name temp with the tempfile() function. This object can now be used as the value of the argument destfile in the download.file() function. Let’s also assign the web address to another object url which we will use as the value of the url argument.\n\n# Create a temporary file space for our .zip file\ntemp &lt;- tempfile()\n# Assign our web address to `url`\nurl &lt;- \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip\"\n# Download .zip file and write to disk\ndownload.file(url, temp)\n\n\n\n\n\n\n\nTip\n\n\n\nIn the previous code I’ve used the values stored in the objects url and temp in the download.file() function without specifying the argument names –only providing the names of the objects. R will assume that values of a function map to the ordering of the arguments. If your values do not map to ordering of the arguments you are required to specify the argument name and the value. To view the ordering of objects hit TAB after entering the function name or consult the function documentation by prefixing the function name with ? and hitting ENTER.\n\n\nAt this point our downloaded file is stored temporarily on disk and can be accessed and decompressed to our target directory using temp as the value for the argument zipfile from the unzip() function. I’ve assigned our target directory path to target_dir and used it as the value for the argument exdir to prepare us for the next tweak on our approach.\n\n# Assign our target directory to `target_dir`\ntarget_dir &lt;- \"../data/original/\"\n# Decompress .zip file and extract to our target directory\nunzip(zipfile = temp, exdir = target_dir)\n\nOur directory structure now looks like this:\ndata\n├── derived\n└── original\n    └── switchboard\n        ├── README\n        ├── discourse\n        ├── disfluency\n        ├── tagged\n        ├── timed-transcript\n        └── transcript\nThe second issue I raised concerns the fact that running this code as part of our project will repeat the download each time. Since we would like to be good citizens and avoid unnecessary traffic on the web it would be nice if our code checked to see if we already have the data on disk and if it exists, then skip the download, if not then download it.\nTo achieve this we need to introduce two new functions if() and dir.exists(). dir.exists() takes a path to a directory as an argument and returns the logical value, TRUE, if that directory exists, and FALSE if it does not. if() evaluates logical statements and processes subsequent code based on the logical value it is passed as an argument. Let’s look at a toy example.\n\nnum &lt;- 1\nif (num == 1) {\n    cat(num, \"is 1\")\n} else {\n    cat(num, \"is not 1\")\n}\n\n&gt; 1 is 1\n\n\nI assigned num to the value 1 and created a logical evaluation num == whose result is passed as the argument to if(). If the statement returns TRUE then the code withing the first set of curly braces {...} is run. If num == 1 is false, like in the code below, the code withing the braces following the else will be run.\n\nnum &lt;- 2\nif (num == 1) {\n    cat(num, \"is 1\")\n} else {\n    cat(num, \"is not 1\")\n}\n\n&gt; 2 is not 1\n\n\nThe function if() is one of various functions that are called control statements. Theses functions provide a lot of power to make dynamic choices as code is run.\nBefore we get back to our key objective to avoid downloading resources that we already have on disk, let me introduce another strategy to making code more powerful and ultimately more efficient and as well as more legible –the custom function. Custom functions are functions that the user writes to create a set of procedures that can be run in similar contexts. I’ve created a custom function named eval_num() below.\n\neval_num &lt;- function(num) {\n    if (num == 1) {\n        cat(num, \"is 1\")\n    } else {\n        cat(num, \"is not 1\")\n    }\n}\n\nLet’s take a closer look at what’s going on here. The function function() creates a function in which the user decides what arguments are necessary for the code to perform its task. In this case the only necessary argument is the object to store a numeric value to be evaluated. I’ve called it num because it reflects the name of the object in our toy example, but there is nothing special about this name. It’s only important that the object names be consistently used. I’ve included our previous code (except for the hard-coded assignment of num) inside the curly braces and assigned the entire code chunk to eval_num.\nWe can now use the function eval_num() to perform the task of evaluating whether a value of num is or is not equal to 1.\n\neval_num(num = 1)\n\n&gt; 1 is 1\n\neval_num(num = 2)\n\n&gt; 2 is not 1\n\neval_num(num = 3)\n\n&gt; 3 is not 1\n\n\nI’ve put these coding strategies together with our previous code in a custom function I named get_zip_data(). There is a lot going on here. Take a look first and see if you can follow the logic involved given what you now know.\n\nget_zip_data &lt;- function(url, target_dir) {\n    # Function: to download and decompress a .zip file to a target directory\n\n    # Check to see if the data already exists if data does not exist, download/\n    # decompress\n    if (!dir.exists(target_dir)) {\n        cat(\"Creating target data directory \\n\")  # print status message\n        dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE)  # create target data directory\n        cat(\"Downloading data... \\n\")  # print status message\n        temp &lt;- tempfile()  # create a temporary space for the file to be written to\n        download.file(url = url, destfile = temp)  # download the data to the temp file\n        unzip(zipfile = temp, exdir = target_dir, junkpaths = TRUE)  # decompress the temp file in the target directory\n        cat(\"Data downloaded! \\n\")  # print status message\n    } else {\n        # if data exists, don't download it again\n        cat(\"Data already exists \\n\")  # print status message\n    }\n}\n\nOK. You should have recognized the general steps in this function: the argument url and target_dir specify where to get the data and where to write the decompressed files, the if() statement evaluates whether the data already exists, if not (!dir.exists(target_dir)) then the data is downloaded and decompressed, if it does exist (else) then it is not downloaded.\n\n\n\n\n\n\nTip\n\n\n\nThe prefixed ! in the logical expression dir.exists(target_dir) returns the opposite logical value. This is needed in this case so when the target directory exists, the expression will return FALSE, not TRUE, and therefore not proceed in downloading the resource.\n\n\nThere are a couple key tweaks I’ve added that provide some additional functionality. For one I’ve included the function dir.create() to create the target directory where the data will be written. I’ve also added an additional argument to the unzip() function, junkpaths = TRUE. Together these additions allow the user to create an arbitrary directory path where the files, and only the files, will be extracted to on our disk. This will discard the containing directory of the .zip file which can be helpful when we want to add multiple .zip files to the same target directory.\nA practical scenario where this applies is when we want to download data from a corpus that is contained in multiple .zip files but still maintain these files in a single primary data directory. Take for example the Santa Barbara Corpus. This corpus resource includes a series of interviews in which there is one .zip file, SBCorpus.zip which contains the transcribed interviews and another .zip file, metadata.zip which organizes the meta-data associated with each speaker. Applying our initial strategy to download and decompress the data will lead to the following directory structure:\ndata\n├── derived\n└── original\n    ├── SBCorpus\n    │   ├── TRN\n    │   └── __MACOSX\n    │       └── TRN\n    └── metadata\n        └── __MACOSX\nBy applying our new custom function get_zip_data() to the transcriptions and then the meta-data we can better organize the data.\n\n# Download corpus transcriptions\nget_zip_data(url = \"http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip\",\n    target_dir = \"../data/original/sbc/transcriptions/\")\n\n# Download corpus meta-data\nget_zip_data(url = \"http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip\",\n    target_dir = \"../data/original/sbc/meta-data/\")\n\ndata\n├── derived\n└── original\n    └── sbc\n        ├── meta-data\n        └── transcriptions\nIf we add data from other sources we can keep them logical separate and allow our data collection to scale without creating unnecessary complexity. Let’s add the Switchboard Corpus sample using our get_zip_data() function to see this in action.\n\n# Download corpus\nget_zip_data(url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip\",\n    target_dir = \"../data/original/scs/\")\n\ndata\n├── derived\n└── original\n    ├── sbc\n    │   ├── meta-data\n    │   └── transcriptions\n    └── scs\n        ├── README\n        ├── discourse\n        ├── disfluency\n        ├── tagged\n        ├── timed-transcript\n        └── transcript\nAt this point we have what we need to continue to the next step in our data analysis project. But before we go, we should do some housekeeping to document and organize this process to make our work reproducible. We will take advantage of the project-template directory structure, seen below.\n├── README.md\n├── _pipeline.R\n├── analysis\n│   ├── 1_acquire_data.Rmd\n│   ├── 2_curate_dataset.Rmd\n│   ├── 3_transform_dataset.Rmd\n│   ├── 4_analyze_dataset.Rmd\n│   ├── 5_generate_article.Rmd\n│   ├── _session-info.Rmd\n│   ├── _site.yml\n│   ├── index.Rmd\n│   └── references.bib\n├── data\n│   ├── derived\n│   └── original\n│       ├── sbc\n│       └── scs\n├── functions\n└── output\n    ├── figures\n    └── results\nFirst it is good practice to separate custom functions from our processing scripts. We can create a file in our functions/ directory named acquire_functions.R and add our custom function get_zip_data() there.\n\n\n\n\n\n\nTip\n\n\n\nNote that that the acquire_functions.R file is an R script, not an Rmarkdown document. Therefore code chunks that are used in .Rmd files are not used, only the R code itself.\n\n\nWe then use the source() function to read that function into our current script to make it available to use as needed. It is good practice to source your functions in the SETUP section of your script.\n\n# Load custom functions for this project\nsource(file = \"../functions/acquire_functions.R\")\n\nIn this section, to sum up, we’ve covered how to access, download, and organize data contained in .zip files; the most common format for language data found on repositories and individual sites. This included an introduction to a few key R programming concepts and strategies including using functions, writing custom functions, and controlling program flow with control statements. Our approach was to gather data while also keeping in mind the reproducibility of the code. To this end I introduced programming strategies for avoiding unnecessary web traffic (downloads), scalable directory creation, and data documentation.\n\n\n\n\n\n\nTip\n\n\n\nThe custom function get_zip_data() works with .zip files. There are many other compressed file formats (e.g. .gz, .tar, .tgz), however. In the R package tadr that accompanies this coursebook, a modified version of the get_zip_data() function, get_compressed_data(), extends the same logic to deal with a wider range of compressed file formats, including .zip files.\nExplore this function’s documentation (?tadr::get_compressed_data()) and/ or view the code (tadr::get_compressed_data) to better understand this function."
  },
  {
    "objectID": "acquire-data.html#apis",
    "href": "acquire-data.html#apis",
    "title": "5  Acquire data",
    "section": "\n5.2 APIs",
    "text": "5.2 APIs\nA convenient alternative method for acquiring data in R is through package interfaces to web services. These interfaces are built using R code to make connections with resources on the web through Application Programming Interfaces (APIs). Websites such as Project Gutenberg, Twitter, Facebook, and many others provide APIs to allow access to their data under certain conditions, some more limiting for data collection than others. Programmers (like you!) in the R community take up the task of wrapping calls to an API with R code to make accessing that data from R possible. For example, gutenbergr provides access to Project Gutenberg, rtweet to Twitter, and Rfacebook to Facebook.2\n\n\n5.2.1 Open access\nUsing R package interfaces, however, often requires some more knowledge about R objects and functions. Let’s take a look at how to access data from Project Gutenberg through the gutenbergr package. Along the way we will touch upon various functions and concepts that are key to working with the R data types vectors and data frames including filtering and writing tabular data to disk in plain-text format.\nTo get started let’s install and/ or load the gutenbergr package. If a package is not part of the R base library, we cannot assume that the user will have the package in their library. The standard approach for installing and then loading a package is by using the install.packages() function and then calling library().\n\ninstall.packages(\"gutenbergr\")  # install `gutenbergr` package\nlibrary(gutenbergr)  # load the `gutenbergr` package\n\nThis approach works just fine, but luck has it that there is an R package for installing and loading packages! The pacman package includes a set of functions for managing packages. A very useful one is p_load() which will look for a package on a system, load it if it is found, and install and then load it if it is not found. This helps potentially avoid using unnecessary bandwidth to install packages that may already exist on a user’s system. But, to use pacman we need to include the code to install and load it with the functions install.packages() and library(). I’ve included some code that will mimic the behavior of p_load() for installing pacman itself, but as you can see it is not elegant, luckily it’s only used once as we add it to the SETUP section of our master file, _pipeline.R.\n\n# Load `pacman`. If not installed, install then load.\nif (!require(\"pacman\", character.only = TRUE)) {\n    install.packages(\"pacman\")\n    library(\"pacman\", character.only = TRUE)\n}\n\nNow that we have pacman installed and loaded into our R session, let’s use the p_load() function to make sure to install/ load the two packages we will need for the upcoming tasks. If you are following along with the project_template, add this code within the SETUP section of the 1_acquire_data.Rmd file.\n\n# Script-specific options or packages\npacman::p_load(tidyverse, gutenbergr)\n\n\n\n\n\n\n\nTip\n\n\n\nNote that the arguments tidyverse and gutenbergr are comma-separated but not quoted when using p_load(). When using install.packages() to install, package names need to be quoted (character strings). library() can take quotes or no quotes, but only one package at a time.\n\n\nProject Gutenberg provides access to thousands of texts in the public domain. The gutenbergr package contains a set of tables, or data frames in R speak, that index the meta-data for these texts broken down by text (gutenberg_metadata), author (gutenberg_authors), and subject (gutenberg_subjects). I’ll use the glimpse() function loaded in the tidyverse package 3 to summarize the structure of these data frames.\n\nglimpse(gutenberg_metadata)  # summarize text meta-data\n\n&gt; Rows: 69,199\n&gt; Columns: 8\n&gt; $ gutenberg_id        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n&gt; $ title               &lt;chr&gt; \"The Declaration of Independence of the United Sta…\n&gt; $ author              &lt;chr&gt; \"Jefferson, Thomas\", \"United States\", \"Kennedy, Jo…\n&gt; $ gutenberg_author_id &lt;int&gt; 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7, 8, …\n&gt; $ language            &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n&gt; $ gutenberg_bookshelf &lt;chr&gt; \"Politics/American Revolutionary War/United States…\n&gt; $ rights              &lt;chr&gt; \"Public domain in the USA.\", \"Public domain in the…\n&gt; $ has_text            &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\nglimpse(gutenberg_authors)  # summarize authors meta-data\n\n&gt; Rows: 21,323\n&gt; Columns: 7\n&gt; $ gutenberg_author_id &lt;int&gt; 1, 3, 4, 5, 7, 8, 9, 10, 12, 14, 16, 17, 18, 20, 2…\n&gt; $ author              &lt;chr&gt; \"United States\", \"Lincoln, Abraham\", \"Henry, Patri…\n&gt; $ alias               &lt;chr&gt; \"U.S.A.\", NA, NA, NA, \"Dodgson, Charles Lutwidge\",…\n&gt; $ birthdate           &lt;int&gt; NA, 1809, 1736, 1849, 1832, NA, 1819, 1860, 1805, …\n&gt; $ deathdate           &lt;int&gt; NA, 1865, 1799, 1931, 1898, NA, 1891, 1937, 1844, …\n&gt; $ wikipedia           &lt;chr&gt; \"https://en.wikipedia.org/wiki/United_States\", \"ht…\n&gt; $ aliases             &lt;chr&gt; NA, \"United States President (1861-1865)/Lincoln, …\n\nglimpse(gutenberg_subjects)  # summarize subjects meta-data\n\n&gt; Rows: 230,993\n&gt; Columns: 3\n&gt; $ gutenberg_id &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, …\n&gt; $ subject_type &lt;chr&gt; \"lcsh\", \"lcsh\", \"lcc\", \"lcc\", \"lcsh\", \"lcsh\", \"lcc\", \"lcc…\n&gt; $ subject      &lt;chr&gt; \"United States -- History -- Revolution, 1775-1783 -- Sou…\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe gutenberg_metadata, gutenberg_authors, and gutenberg_subjects are periodically updated. To check to see when each data frame was last updated run:\nattr(gutenberg_metadata, \"date_updated\")\n\n\nTo download the text itself we use the gutenberg_download() function which takes one required argument, gutenberg_id. The gutenberg_download() function is what is known as ‘vectorized’, that is, it can take a single value or multiple values for the argument gutenberg_id. Vectorization refers to the process of applying a function to each of the elements stored in a vector –a primary object type in R. A vector is a grouping of values of one of various types including character (chr), integer (int), double (dbl), and logical (lgl) and a data frame is a grouping of vectors. The gutenberg_download() function takes an integer vector which can be manually added or selected from the gutenberg_metadata or gutenberg_subjects data frames using the $ operator (e.g. gutenberg_metadata$gutenberg_id).\nLet’s first add them manually here as a toy example by generating a vector of integers from 1 to 5 assigned to the variable name ids.\n\nids &lt;- 1:5  # integer vector of values 1 to 5\nids\n\n&gt; [1] 1 2 3 4 5\n\n\nTo download the works from Project Gutenberg corresponding to the gutenberg_ids 1 to 5, we pass the ids object to the gutenberg_download() function.\n\nworks_sample &lt;- gutenberg_download(gutenberg_id = ids)  # download works with `gutenberg_id` 1-5\nglimpse(works_sample)  # summarize `works` dataset\n\n&gt; Rows: 2,959\n&gt; Columns: 2\n&gt; $ gutenberg_id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n&gt; $ text         &lt;chr&gt; \"December, 1971  [Etext #1]\", \"\", \"\", \"The Project Gutenb…\n\n\nTwo attributes are returned: gutenberg_id and text. The text column contains values for each line of text (delimited by a carriage return) for each of the 5 works we downloaded. There are many more attributes available from the Project Gutenberg API that can be accessed by passing a character vector of the attribute names to the argument meta_fields. The column names of the gutenberg_metadata data frame contains the available attributes.\n\nnames(gutenberg_metadata)  # print the column names of the `gutenberg_metadata` data frame\n\n&gt; [1] \"gutenberg_id\"        \"title\"               \"author\"             \n&gt; [4] \"gutenberg_author_id\" \"language\"            \"gutenberg_bookshelf\"\n&gt; [7] \"rights\"              \"has_text\"\n\n\nLet’s augment our previous download with the title and author of each of the works. To create a character vector we use the c() function, then, quote and delimit the individual elements of the vector with a comma.\n\n# download works with `gutenberg_id` 1-5 including `title` and `author` as\n# attributes\nworks_sample &lt;- gutenberg_download(gutenberg_id = ids, meta_fields = c(\"title\", \"author\"))  #\n\nglimpse(works_sample)  # summarize dataset\n\n&gt; Rows: 2,959\n&gt; Columns: 4\n&gt; $ gutenberg_id &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n&gt; $ text         &lt;chr&gt; \"December, 1971  [Etext #1]\", \"\", \"\", \"The Project Gutenb…\n&gt; $ title        &lt;chr&gt; \"The Declaration of Independence of the United States of …\n&gt; $ author       &lt;chr&gt; \"Jefferson, Thomas\", \"Jefferson, Thomas\", \"Jefferson, Tho…\n\n\nNow, in a more practical scenario we would like to select the values of gutenberg_id by some principled query such as works from a specific author, language, or subject. To do this we first query either the gutenberg_metadata data frame or the gutenberg_subjects data frame. Let’s say we want to download a random sample of 10 works from English Literature (Library of Congress Classification, “PR”). Using the dplyr::filter() function (dplyr is part of the tidyverse package set) we first extract all the Gutenberg ids from gutenberg_subjects where subject_type == \"lcc\" and subject == \"PR\" assigning the result to ids.4\n\n# filter for only English literature\nids &lt;- filter(gutenberg_subjects, subject_type == \"lcc\", subject == \"PR\")\nglimpse(ids)\n\n&gt; Rows: 9,900\n&gt; Columns: 3\n&gt; $ gutenberg_id &lt;int&gt; 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43, 46, 58, 60, 8…\n&gt; $ subject_type &lt;chr&gt; \"lcc\", \"lcc\", \"lcc\", \"lcc\", \"lcc\", \"lcc\", \"lcc\", \"lcc\", \"…\n&gt; $ subject      &lt;chr&gt; \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR\", \"PR…\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe operators = and == are not equivalents. == is used for logical evaluation and = is an alternate notation for variable assignment (&lt;-).\n\n\nThe gutenberg_subjects data frame does not contain information as to whether a gutenberg_id is associated with a plain-text version. To limit our query to only those English Literature works with text, we filter the gutenberg_metadata data frame by the ids we have selected in ids and the attribute has_text in the gutenberg_metadata data frame.\n\n# Filter for only those works that have text\nids_has_text &lt;- \n  filter(gutenberg_metadata, \n         gutenberg_id %in% ids$gutenberg_id, \n         has_text == TRUE)\nglimpse(ids_has_text)\n\n&gt; Rows: 9,548\n&gt; Columns: 8\n&gt; $ gutenberg_id        &lt;int&gt; 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43, 46, 58…\n&gt; $ title               &lt;chr&gt; \"Alice's Adventures in Wonderland\", \"Through the L…\n&gt; $ author              &lt;chr&gt; \"Carroll, Lewis\", \"Carroll, Lewis\", \"Carroll, Lewi…\n&gt; $ gutenberg_author_id &lt;int&gt; 7, 7, 7, 10, 17, 17, 23, 30, 30, 35, 35, 37, 17, 4…\n&gt; $ language            &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"en\", \"e…\n&gt; $ gutenberg_bookshelf &lt;chr&gt; \"Children's Literature\", \"Best Books Ever Listings…\n&gt; $ rights              &lt;chr&gt; \"Public domain in the USA.\", \"Public domain in the…\n&gt; $ has_text            &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\n\n\n\n\n\n\n\nTip\n\n\n\nA couple R programming notes on the code phrase gutenberg_id %in% ids$gutenberg_id. First, the $ symbol in ids$gutenberg_id is the programmatic way to target a particular column in an R data frame. In this example we select the ids data frame and the column gutenberg_id, which is a integer vector. The gutenberg_id variable that precedes the %in% operator does not need an explicit reference to a data frame because the primary argument of the filter() function is this data frame (gutenberg_metadata). Second, the %in% operator logically evaluates whether the vector elements in gutenberg_metadata$gutenberg_ids are also found in the vector ids$gutenberg_id returning TRUE and FALSE accordingly. This effectively filters those ids which are not in both vectors.\n\n\nAs we can see the number of works with text is fewer than the number of works listed, 9900 versus 9548. Now we can safely do our random selection of 10 works, with the function slice_sample() and be confident that the ids we select will contain text when we take the next step by downloading the data.\n\nset.seed(123)  # make the sampling reproducible\nids_sample &lt;- slice_sample(ids_has_text, n = 10)  # sample 10 works\nglimpse(ids_sample)  # summarize the dataset\n\n&gt; Rows: 10\n&gt; Columns: 8\n&gt; $ gutenberg_id        &lt;int&gt; 10542, 10734, 60253, 13776, 7532, 67002, 16604, 28…\n&gt; $ title               &lt;chr&gt; \"The Boats of the \\\"Glen Carrig\\\"\\r\\nBeing an acco…\n&gt; $ author              &lt;chr&gt; \"Hodgson, William Hope\", NA, \"Orczy, Emmuska Orczy…\n&gt; $ gutenberg_author_id &lt;int&gt; 3260, NA, 45, NA, NA, 30, 3579, 6137, 797, 1865\n&gt; $ language            &lt;chr&gt; \"en\", \"en\", \"en\", \"en\", \"en\", \"hu\", \"en\", \"en\", \"e…\n&gt; $ gutenberg_bookshelf &lt;chr&gt; \"Horror\", NA, NA, NA, NA, NA, NA, \"Humor\", NA, NA\n&gt; $ rights              &lt;chr&gt; \"Public domain in the USA.\", \"Public domain in the…\n&gt; $ has_text            &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\n\n\nworks_pr &lt;- gutenberg_download(gutenberg_id = ids_sample$gutenberg_id, meta_fields = c(\"author\",\n    \"title\"))\nglimpse(works_pr)  # summarize the dataset\n\n\nworks_pr &lt;- gutenberg_download(gutenberg_id = ids_sample$gutenberg_id, meta_fields = c(\"author\",\n    \"title\"))\nwrite_rds(works_pr, file = \"data/acquire-data/gutenberg_works_pr.rds\")\n\n\n\n&gt; Rows: 86,086\n&gt; Columns: 4\n&gt; $ gutenberg_id &lt;int&gt; 7532, 7532, 7532, 7532, 7532, 7532, 7532, 7532, 7532, 753…\n&gt; $ text         &lt;chr&gt; \"A BOOK OF OLD BALLADS\", \"\", \"Selected and with an Introd…\n&gt; $ author       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n&gt; $ title        &lt;chr&gt; \"A Book of Old Ballads — Volume 2\", \"A Book of Old Ballad…\n\n\nAt this point we have data and could move on to processing this dataset in preparation for analysis. However, we are aiming for a reproducible workflow and this code does not conform to our principle of modularity: each subsequent step in our analysis will depend on running this code first. Furthermore, running this code as it is creates issues with bandwidth, as in our previous examples from direct downloads. To address modularity we will write the dataset to disk in plain-text format. In this way each subsequent step in our analysis can access the dataset locally. To address bandwidth concerns, we will devise a method for checking to see if the dataset is already downloaded and skip the download, if possible, to avoid accessing the Project Gutenberg server unnecessarily.\nTo write our data frame to disk we will export it into a standard plain-text format for two-dimensional datasets: a CSV file (comma-separated value). The CSV structure for this dataset will look like this:\n\nworks_pr |&gt;\n    head() |&gt;\n    format_csv() |&gt;\n    cat()\n\n&gt; gutenberg_id,text,author,title\n&gt; 7532,A BOOK OF OLD BALLADS,NA,A Book of Old Ballads — Volume 2\n&gt; 7532,,NA,A Book of Old Ballads — Volume 2\n&gt; 7532,Selected and with an Introduction,NA,A Book of Old Ballads — Volume 2\n&gt; 7532,,NA,A Book of Old Ballads — Volume 2\n&gt; 7532,by,NA,A Book of Old Ballads — Volume 2\n&gt; 7532,,NA,A Book of Old Ballads — Volume 2\n\n\nThe first line contains the names of the columns and subsequent lines the observations. Data points that contain commas themselves (e.g. “Shaw, Bernard”) are quoted to avoid misinterpreting these commas a deliminators in our data. To write this dataset to disk we will use the reader::write_csv() function.\n\nwrite_csv(works_pr, file = \"../data/original/gutenberg_works_pr.csv\")\n\nTo avoid downloading dataset that already resides on disk, let’s implement a similar strategy to the one used for direct downloads (get_zip_data()). I’ve incorporated the code for sampling and downloading data for a particular subject from Project Gutenberg with a control statement to check if the dataset file already exists into a function I named get_gutenberg_subject(). Take a look at this function below.\n\nget_gutenberg_subject &lt;- function(subject, target_file, sample_size = 10) {\n  # Function: to download texts from Project Gutenberg with \n  # a specific LCC subject and write the data to disk.\n  \n  pacman::p_load(tidyverse, gutenbergr) # install/load necessary packages\n  \n  # Check to see if the data already exists\n  if(!file.exists(target_file)) { # if data does not exist, download and write\n    target_dir &lt;- dirname(target_file) # generate target directory for the .csv file\n    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE) # create target data directory\n    cat(\"Downloading data... \\n\") # print status message\n    # Select all records with a particular LCC subject\n    ids &lt;- \n      filter(gutenberg_subjects, \n             subject_type == \"lcc\", subject == subject) # select subject\n    # Select only those records with plain text available\n    set.seed(123) # make the sampling reproducible\n    ids_sample &lt;- \n      filter(gutenberg_metadata, \n             gutenberg_id %in% ids$gutenberg_id, # select ids in both data frames \n             has_text == TRUE) |&gt; # select those ids that have text\n      slice_sample(n = sample_size) # sample N works \n    # Download sample with associated `author` and `title` metadata\n    works_sample &lt;- \n      gutenberg_download(gutenberg_id = ids_sample$gutenberg_id, \n                         meta_fields = c(\"author\", \"title\"))\n    # Write the dataset to disk in .csv format\n    write_csv(works_sample, file = target_file)\n    cat(\"Data downloaded! \\n\") # print status message\n  } else { # if data exists, don't download it again\n    cat(\"Data already exists \\n\") # print status message\n  }\n}\n\nAdding this function to our function script functions/acquire_functions.R, we can now source this function in our analysis/1_acquire_data.Rmd script to download multiple subjects and store them in on disk in their own file.\nLet’s download American Literature now (LCC code “PQ”).\n\n# Download Project Gutenberg text for subject 'PQ' (American Literature)\n# and then write this dataset to disk in .rds format\n\n# Select all records with a particular LCC subject\nids &lt;- \n  filter(gutenberg_subjects, \n         subject_type == \"lcc\", subject == \"PQ\") # select subject\n# Select only those records with plain text available\nset.seed(123) # make the sampling reproducible\nids_sample &lt;- \n  filter(gutenberg_metadata, \n         gutenberg_id %in% ids$gutenberg_id, # select ids in both data frames \n         has_text == TRUE) |&gt; # select those ids that have text\n  slice_sample(n = sample_size) # sample N works \n# Download sample with associated `author` and `title` metadata\nworks_pq &lt;- \n  gutenberg_download(gutenberg_id = ids_sample$gutenberg_id, \n                     meta_fields = c(\"author\", \"title\"))\n\nwrite_rds(x = works_pq, file = \"data/acquire-data/gutenberg_works_pq.rds\")\n\n\n# Download Project Gutenberg text for subject 'PQ' (American Literature) and\n# then write this dataset to disk in .csv format\nget_gutenberg_subject(subject = \"PQ\", target_file = \"../data/original/gutenberg/works_pq.csv\")\n\nApplying this function to both the English and American Literature datasets, our data directory structure now looks like this:\ndata\n├── derived\n└── original\n    ├── gutenberg\n    │   ├── works_pq.csv\n    │   └── works_pr.csv\n    ├── sbc\n    │   ├── meta-data\n    │   └── transcriptions\n    └── scs\n        ├── README\n        ├── discourse\n        ├── disfluency\n        ├── documentation\n        ├── tagged\n        ├── timed-transcript\n        └── transcript\n\n5.2.2 Authentication\n\nSome APIs and the R interfaces that provide access to them require authentication. This may either be through an interactive process that is mediated between R and the web service and/ or by visiting the developer website of the particular API. In either case, there is an extra step that is necessary to make the connect to the API to access the data.\nLet’s take a look at the popular micro-blogging platform Twitter. The rtweet package (Kearney, Revilla Sancho, and Wickham 2023) provides access to tweets in various ways. To get started install and/or load the rtweet package.\n\npacman::p_load(rtweet)  # install/load rtweet package\n\nNow before a researcher can access data from Twitter with rtweet, an authentication token must be setup and made accessible. After following the steps for setting up an authentication token and saving it, that token can be accessed with the auth_as() function.\n\nauth_as(twitter_auth)  # load the saved `twitter_auth` token\n\nNow that we the R session is authenticated, we can explore a popular method for querying the Twitter API which searchs tweets (search_tweets) posted in the recent past (6-9 days).\nLet’s look at a typical query using the search_tweets() function.\n\nrt_latinx &lt;- \n  search_tweets(q = \"latinx\", # query term\n                n = 100, # number of tweets desired\n                type = \"mixed\", # a mix of `recent` and `popular` tweets\n                include_rts = FALSE) # do not include RTs\n\nLooking at the arguments in this function, we see I’ve specified the query term to be ‘latinx’. This is a single word query but if the query included multiple words, the spaces between would be interpreted as the logical AND (only match tweets with all the individual terms). If one would like to include multi-word expressions, the expressions should be enclosed by single quotes (i.e. q = \"'spanish speakers' AND latinx\"). Another approach would be to include the logical OR (match tweets with either of the terms). Multi-word expressions can be included as in the previous case. Of note, hashtags are acceptable terms, so q = \"#latinx\" would match tweets with this hashtag.\nThe number of results has been set at ‘100’, but this is the default, so I could have left it out. But you can increase the number of desired tweets. There are rate limits which cap the number of tweets you can access in a given 15-minute time period.\nAnother argument of importance is the type argument. This argument has three possible attributes popular, recent, and mixed. When the popular attribute he Twitter API will tend to return fewer tweets than specified by n. With recent or mixed you will most likely get the n you specified (note that mixed is a mix of popular and recent).\nA final argument to note is the include_rts whose attribute is logical. If FALSE no retweets will be included in the results. This is often what a language researcher will want.\nNow, once the search_tweets query has been run, there a a large number of variables that are included in the resulting data frame. Here’s an overview of the names of the variables and the vector types for each variable.\n\n\n\n\nTable 5.1: Variables and variable types returned from Twitter API via rtweet’s search_tweets() function.\n\n\ncreated_at\ncharacter\n\n\nid\ndouble\n\n\nid_str\ncharacter\n\n\nfull_text\ncharacter\n\n\ntruncated\nlogical\n\n\ndisplay_text_range\ndouble\n\n\nentities\nlist\n\n\nmetadata\nlist\n\n\nsource\ncharacter\n\n\nin_reply_to_status_id\ndouble\n\n\nin_reply_to_status_id_str\ncharacter\n\n\nin_reply_to_user_id\ndouble\n\n\nin_reply_to_user_id_str\ncharacter\n\n\nin_reply_to_screen_name\ncharacter\n\n\ngeo\nlogical\n\n\ncoordinates\nlist\n\n\nplace\nlist\n\n\ncontributors\nlogical\n\n\nis_quote_status\nlogical\n\n\nretweet_count\ninteger\n\n\nfavorite_count\ninteger\n\n\nfavorited\nlogical\n\n\nretweeted\nlogical\n\n\nlang\ncharacter\n\n\npossibly_sensitive\nlogical\n\n\nquoted_status_id\ndouble\n\n\nquoted_status_id_str\ncharacter\n\n\nquoted_status\nlist\n\n\ntext\ncharacter\n\n\nfavorited_by\nlogical\n\n\ndisplay_text_width\nlogical\n\n\nretweeted_status\nlogical\n\n\nquoted_status_permalink\nlogical\n\n\nquery\nlogical\n\n\npossibly_sensitive_appealable\nlogical\n\n\n\n\n\n\nThe Twitter API documentation for the standard Search Tweets call, which is what search_tweets() interfaces with has quite a few variables (35 to be exact). For many purposes it is not necessary to keep all the variables. Furthermore, since we will want to write a plain-text file to disk as part of our project, we will need to either convert or eliminate any of the variables that are marked as type list. The most common variable to convert is the coordinates variable, as it will contain the geolocation codes for those Twitter users’ tweets captured in the query that have geolocation enabled on their device. It is of note, however, that using search_tweets() without specifying that only tweets with geocodes should be captured (geocode =) will tend to return very few, if any, tweets with geolocation information as the majority of Twitter users do not have geolocation enabled.\nLet’s assume that we want to keep all the variables that are not of type list. One option is to use select() and name each variable we want to keep. On the other hand we can use a combination of select() and negated !where() to select all the variables that are not lists (is_list). Let’s do the later approach.\n\nrt_latinx_subset &lt;- \n  rt_latinx |&gt; # dataset\n  select(!where(is_list))  # select all variables that are NOT lists\n\nrt_latinx_subset |&gt; # subsetted dataset\n  glimpse() # overview\n\n&gt; Rows: 100\n&gt; Columns: 30\n&gt; $ created_at                    &lt;chr&gt; \"Sun Sep 26 17:38:06 +0000 2021\", \"Sun S…\n&gt; $ id                            &lt;dbl&gt; 1.44e+18, 1.44e+18, 1.44e+18, 1.44e+18, …\n&gt; $ id_str                        &lt;chr&gt; \"1442181701967302659\", \"1442196629801488…\n&gt; $ full_text                     &lt;chr&gt; \"If we call it Latinx Mass they can't ca…\n&gt; $ truncated                     &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n&gt; $ display_text_range            &lt;dbl&gt; 57, 177, 166, 23, 261, 153, 202, 211, 57…\n&gt; $ source                        &lt;chr&gt; \"&lt;a href=\\\"https://mobile.twitter.com\\\" …\n&gt; $ in_reply_to_status_id         &lt;dbl&gt; NA, NA, NA, 1.44e+18, NA, NA, NA, NA, 1.…\n&gt; $ in_reply_to_status_id_str     &lt;chr&gt; NA, NA, NA, \"1437436224042635269\", NA, N…\n&gt; $ in_reply_to_user_id           &lt;dbl&gt; NA, NA, NA, 4.26e+08, NA, NA, NA, NA, 2.…\n&gt; $ in_reply_to_user_id_str       &lt;chr&gt; NA, NA, NA, \"426159377\", NA, NA, NA, NA,…\n&gt; $ in_reply_to_screen_name       &lt;chr&gt; NA, NA, NA, \"MorganStanley\", NA, NA, NA,…\n&gt; $ geo                           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n&gt; $ contributors                  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n&gt; $ is_quote_status               &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n&gt; $ retweet_count                 &lt;int&gt; 351, 124, 62, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n&gt; $ favorite_count                &lt;int&gt; 3902, 898, 280, 0, 0, 0, 0, 0, 0, 7, 0, …\n&gt; $ favorited                     &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n&gt; $ retweeted                     &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n&gt; $ lang                          &lt;chr&gt; \"en\", \"en\", \"es\", \"en\", \"en\", \"en\", \"en\"…\n&gt; $ possibly_sensitive            &lt;lgl&gt; NA, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n&gt; $ quoted_status_id              &lt;dbl&gt; NA, NA, NA, NA, 1.44e+18, NA, NA, NA, NA…\n&gt; $ quoted_status_id_str          &lt;chr&gt; NA, NA, NA, NA, \"1442475408058830856\", N…\n&gt; $ text                          &lt;chr&gt; \"If we call it Latinx Mass they can't ca…\n&gt; $ favorited_by                  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n&gt; $ display_text_width            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n&gt; $ retweeted_status              &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n&gt; $ quoted_status_permalink       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n&gt; $ query                         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n&gt; $ possibly_sensitive_appealable &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\nNow we have the 30 variables which can be written to disk as a plain-text file. Let’s go ahead a do this, but wrap it in a function that does all the work we’ve just laid out in one function. In addition we will check to see if the same query has been run, and skip running the query if the dataset is on disk.\n\nwrite_search_tweets &lt;- \n  function(query, path, n = 100, type = \"mixed\", include_rts = FALSE) {\n    # Function\n    # Conduct a Twitter search query and write the results to a csv file\n    \n    if(!file.exists(path)) { # check to see if the file already exists\n      cat(\"File does not exist \\n\") # message\n      \n      library(rtweet) # to use Twitter API\n      library(tidyverse) # to manipulate data\n      \n      auth_get() # get authentication token\n      \n      results &lt;- # query results\n        search_tweets(q = query, # query term\n                      n = n, # number of tweets desired (default 100)\n                      type = type, # type of query\n                      include_rts = include_rts) |&gt;  # to include RTs\n        select(!where(is_list))  # remove list variables\n      \n      if(!dir.exists(dirname(path))) { # isolate directory and check if exists\n        cat(\"Creating directory \\n\") # message\n        \n        dir.create(path = dirname(path), # isolate and create directory (remove file name)\n                   recursive = TRUE, # create embedded directories if necessary\n                   showWarnings = FALSE) # do not report warnings\n      }\n      \n      write_csv(x = results, file = path) # write results to csv file \n      cat(\"Twitter search results written to disk \\n\") # message\n      \n    } else {\n      cat(\"File already exists! \\n\") # message\n    }\n  }\n\nLet’s run this function with the same query as above.\n\nwrite_search_tweets(query = \"latinx\", path = \"../data/original/twitter/rt_latinx.csv\")\n\nAnd the appropriate directory structure and file have been written to disk.\ndata/original/twitter/\n└── rt_latinx.csv\nIn sum, this subsection provided an overview to acquiring data from web service APIs through R packages. We took at closer look at the gutenbergr package which provides programmatic access to works available on Projec t Gutenberg and the rtweet package which provides authenticated access to Twitter. Working with package interfaces requires more knowledge of R including loading/ installing packages, working with vectors and data frames, and exporting data from an R session. We touched on these programming concepts and also outlined a method to create a reproducible workflow."
  },
  {
    "objectID": "acquire-data.html#web-scraping",
    "href": "acquire-data.html#web-scraping",
    "title": "5  Acquire data",
    "section": "\n5.3 Web scraping",
    "text": "5.3 Web scraping\nThere are many resources available through manual and direct downloads from repositories and individual sites and R package interfaces to web resources with APIs, but these resources are relatively limited to the amount of public-facing textual data recorded on the web. In the case that you want to acquire data from webpages, R can be used to access the web programmatically through a process known as web scraping. The complexity of web scrapes can vary but in general it requires more advanced knowledge of R as well as the structure of the language of the web: HTML (Hypertext Markup Language).\n\n5.3.1 A toy example\nHTML is a cousin of XML (eXtensible Markup Language) and as such organizes web documents in a hierarchical format that is read by your browser as you navigate the web. Take for example the toy webpage I created as a demonstration in Figure 5.3.\n\n\n\n\nFigure 5.3: Example web page.\n\n\n\nThe file accessed by my browser to render this webpage is test.html and in plain-text format looks like this:\n\n\n\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;My website&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div class=\"intro\"&gt;\n      &lt;p&gt;Welcome!&lt;/p&gt;\n      &lt;p&gt;This is my first website. &lt;/p&gt;\n    &lt;/div&gt;\n    &lt;table&gt;\n      &lt;tr&gt;\n        &lt;td&gt;Contact me:&lt;/td&gt;\n        &lt;td&gt;\n          &lt;a href=\"mailto:francojc@wfu.edu\"&gt;francojc@wfu.edu&lt;/a&gt;\n        &lt;/td&gt;\n      &lt;/tr&gt;\n    &lt;/table&gt;\n    &lt;div class=\"conc\"&gt;\n      &lt;p&gt;Good-bye!&lt;/p&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\n\nEach element in this file is delineated by an opening and closing tag, &lt;head&gt;&lt;/head&gt;. Tags are nested within other tags to create the structural hierarchy. Tags can take class and id labels to distinguish them from other tags and often contain other attributes that dictate how the tag is to behave when rendered visually by a browser. For example, there are two &lt;div&gt; tags in our toy example: one has the label class = \"intro\" and the other class = \"conc\". &lt;div&gt; tags are often used to separate sections of a webpage that may require special visual formatting. The &lt;a&gt; tag, on the other hand, creates a web link. As part of this tag’s function, it requires the attribute href= and a web protocol –in this case it is a link to an email address mailto:francojc@wfu.edu. More often than not, however, the href= contains a URL (Uniform Resource Locator). A working example might look like this: &lt;a href=\"https://francojc.github.io/\"&gt;My homepage&lt;/a&gt;.\nThe aim of a web scrape is to download the HTML file, parse the document structure, and extract the elements containing the relevant information we wish to capture. Let’s attempt to extract some information from our toy example. To do this we will need the rvest(Wickham 2022) package. First, install/load the package, then, read and parse the HTML from the character vector named web_file assigning the result to html.\n\npacman::p_load(rvest)  # install/ load `rvest`\n\nhtml &lt;- read_html(web_file)  # read raw html and parse to xml\nhtml\n\n&gt; {html_document}\n&gt; &lt;html&gt;\n&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n&gt; [2] &lt;body&gt;\\n    &lt;div class=\"intro\"&gt;\\n      &lt;p&gt;Welcome!&lt;/p&gt;\\n      &lt;p&gt;This is  ...\n\n\nread_html() parses the raw HTML into an object of class xml_document. The summary output above shows that tags the HTML structure have been parsed into ‘elements’. The tag elements can be accessed by using the html_elements() function by specifying the tag to isolate.\n\nhtml |&gt;\n    html_elements(\"div\")\n\n&gt; {xml_nodeset (2)}\n&gt; [1] &lt;div class=\"intro\"&gt;\\n      &lt;p&gt;Welcome!&lt;/p&gt;\\n      &lt;p&gt;This is my first web ...\n&gt; [2] &lt;div class=\"conc\"&gt;\\n      &lt;p&gt;Good-bye!&lt;/p&gt;\\n    &lt;/div&gt;\n\n\nNotice that html_elements(\"div\") has returned both div tags. To isolate one of tags by its class, we add the class name to the tag separating it with a ..\n\nhtml |&gt;\n    html_elements(\"div.intro\")\n\n&gt; {xml_nodeset (1)}\n&gt; [1] &lt;div class=\"intro\"&gt;\\n      &lt;p&gt;Welcome!&lt;/p&gt;\\n      &lt;p&gt;This is my first web ...\n\n\nGreat. Now say we want to drill down and isolate the subordinate &lt;p&gt; nodes. We can add p to our node filter.\n\nhtml |&gt;\n    html_elements(\"div.intro p\")\n\n&gt; {xml_nodeset (2)}\n&gt; [1] &lt;p&gt;Welcome!&lt;/p&gt;\n&gt; [2] &lt;p&gt;This is my first website. &lt;/p&gt;\n\n\nTo extract the text contained within a node we use the html_text() function.\n\nhtml |&gt;\n    html_elements(\"div.intro p\") |&gt;\n    html_text()\n\n&gt; [1] \"Welcome!\"                   \"This is my first website. \"\n\n\nThe result is a character vector with two elements corresponding to the text contained in each &lt;p&gt; tag. If you were paying close attention you might have noticed that the second element in our vector includes extra whitespace after the period. To trim leading and trailing whitespace from text we can add the trim = TRUE argument to html_text().\n\nhtml |&gt;\n    html_elements(\"div.intro p\") |&gt;\n    html_text(trim = TRUE)\n\n&gt; [1] \"Welcome!\"                  \"This is my first website.\"\n\n\nFrom here we would then work to organize the text into a format we want to store it in and write the results to disk. Let’s leave writing data to disk for later in the chapter. For now keep our focus on working with rvest to acquire data from html documents working with a more practical example.\n\n5.3.2 A practical example\n\n\nWith some basic understanding of HTML and how to use the rvest package, let’s turn to a realistic example. Say we want to acquire lyrics from the online music website and database last.fm. The first step in any web scrape is to investigate the site and page(s) we want to scrape to ascertain if there any licensing restrictions. Many, but not all websites, will include a plain text file robots.txt at the root of the main URL. This file is declares which webpages a ‘robot’ (including web scraping scripts) can and cannot access. We can use the robotstxt package to find out which URLs are accessible 5.\n\n\npacman::p_load(robotstxt)  # load/ install `robotstxt`\n\npaths_allowed(paths = \"https://www.last.fm/\")  # check permissions\n\n&gt; [1] TRUE\n\n\n\nThe next step includes identifying the URL we want to target and exploring the structure of the HTML document. Take the following webpage I have identified, seen in Figure 5.4.\n\n\n\n\nFigure 5.4: Lyrics page from last.fm\n\n\n\nAs in our toy example, first we want to feed the HTML web address to the read_html() function to parse the tags into elements. We will then assign the result to html.\n\n\n\n# read and parse html as an xml object\nlyrics_url &lt;- \"https://www.last.fm/music/Radiohead/_/Karma+Police/+lyrics\"\nhtml &lt;- read_html(lyrics_url)  # read raw html and parse to xml\nhtml\n\n\n\n\n&gt; {html_document}\n&gt; &lt;html lang=\"en\" class=\"\n&gt;         no-js\n&gt;         playbar-masthead-release-shim\n&gt;         youtube-provider-not-ready\n&gt;     \"&gt;\n&gt; [1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n&gt; [2] &lt;body&gt;\\n&lt;div id=\"initial-tealium-data\" data-require=\"tracking/tealium-uta ...\n\n\nAt this point we have captured and parsed the raw HTML assigning it to the object named html. The next step is to identify the html elements that contain the information we want to extract from the page. To do this it is helpful to use a browser to inspect specific elements of the webpage. Your browser will be equipped with a command that you can enable by hovering your mouse over the element of the page you want to target and using a right click to select “Inspect” (Chrome) or “Inspect Element” (Safari, Brave). This will split your browser window vertical or horizontally showing you the raw HTML underlying the webpage.\n\n\n\n\nFigure 5.5: Using the “Inspect Element” command to explore raw html.\n\n\n\n\nFrom Figure 5.5 we see that the element we want to target is contained within an &lt;a&gt;&lt;/a&gt; tag. Now this tag is common and we don’t want to extract every a so we use the class header-new-crumb to specify we only want the artist name. Using the convention described in our toy example, we can isolate the artist of the lyrics page.\n\n\nhtml |&gt;\n    html_element(\"a.header-new-crumb\")\n\n&gt; {html_node}\n&gt; &lt;a class=\"header-new-crumb\" itemprop=\"url\" href=\"/music/Radiohead\"&gt;\n&gt; [1] &lt;span itemprop=\"name\"&gt;Radiohead&lt;/span&gt;\n\n\nWe can then extract the text with html_text().\n\nartist &lt;- html |&gt;\n    html_element(\"a.header-new-crumb\") |&gt;\n    html_text()\nartist\n\n&gt; [1] \"Radiohead\"\n\n\nLet’s extract the song title in the same way.\n\n\nsong &lt;- html |&gt;\n    html_element(\"h1.header-new-title\") |&gt;\n    html_text()\nsong\n\n&gt; [1] \"Karma Police\"\n\n\nNow if we inspect the HTML of the lyrics page, we will notice that the lyrics are contained in &lt;p&gt;&lt;/p&gt; tags with the class lyrics-paragraph.\n\n\n\n\nFigure 5.6: Using the “Inspect Element” command to explore raw html.\n\n\n\nSince there are multiple elements that we want to extract, we will need to use the html_elements() function instead of the html_element() which only targets one element.\n\nlyrics &lt;- html |&gt;\n    html_elements(\"p.lyrics-paragraph\") |&gt;\n    html_text()\nlyrics\n\n&gt; [1] \"Karma policeArrest this manHe talks in mathsHe buzzes like a fridgeHe's like a detuned radio\"      \n&gt; [2] \"Karma policeArrest this girlHer Hitler hairdoIs making me feel illAnd we have crashed her party\"   \n&gt; [3] \"This is what you'll getThis is what you'll getThis is what you'll getWhen you mess with us\"        \n&gt; [4] \"Karma policeI've given all I canIt's not enoughI've given all I canBut we're still on the payroll\" \n&gt; [5] \"This is what you'll getThis is what you'll getThis is what you'll getWhen you mess with us\"        \n&gt; [6] \"For a minute thereI lost myself, I lost myselfPhew, for a minute thereI lost myself, I lost myself\"\n&gt; [7] \"For a minute thereI lost myself, I lost myselfPhew, for a minute thereI lost myself, I lost myself\"\n\n\nAt this point, we have isolated and extracted the artist, song, and lyrics from the webpage. Each of these elements are stored in character vectors in our R session. To complete our task we need to write this data to disk as plain text. With an eye towards a tidy dataset, an ideal format to store the data is in a CSV file where each column corresponds to one of the elements from our scrape and each row an observation. A CSV file is a tabular format and so before we can write the data to disk let’s coerce the data that we have into tabular format. We will use the tibble() function here to streamline our data frame creation. 6 Feeding each of the vectors artist, song, and lyrics as arguments to tibble() creates the tabular format we are looking for.\n\ntibble(artist, song, lyrics) |&gt;\n    glimpse()\n\n&gt; Rows: 7\n&gt; Columns: 3\n&gt; $ artist &lt;chr&gt; \"Radiohead\", \"Radiohead\", \"Radiohead\", \"Radiohead\", \"Radiohead\"…\n&gt; $ song   &lt;chr&gt; \"Karma Police\", \"Karma Police\", \"Karma Police\", \"Karma Police\",…\n&gt; $ lyrics &lt;chr&gt; \"Karma policeArrest this manHe talks in mathsHe buzzes like a f…\n\n\nNotice that there are seven rows in this data frame, one corresponding to each paragraph in lyrics. R has a bias towards working with vectors of the same length. As such each of the other vectors (artist, and song) are replicated, or recycled, until they are the same length as the longest vector lyrics, which a length of seven.\nFor good documentation let’s add our object lyrics_url to the data frame, which contains the actual web link to this page, and assign the result to song_lyrics.\n\nsong_lyrics &lt;- tibble(artist, song, lyrics, lyrics_url)\n\nThe final step is to write this data to disk. To do this we will use the write_csv() function.\n\n\nwrite_csv(x = song_lyrics, path = \"../data/original/lyrics.csv\")\n\n\n5.3.3 Scaling up\nAt this point you may be think, ‘Great, I can download data from a single page, but what about downloading multiple pages?’ Good question. That’s really where the strength of a programming approach takes hold. Extracting information from multiple pages is not fundamentally different than working with a single page. However, it does require more sophisticated understanding of the web and R coding strategies, in particular iteration.\nBefore we get to iteration, let’s first create a couple functions to make it possible to efficiently reuse the code we have developed so far:\n\nthe get_lyrics function wraps the code for scraping a single lyrics webpage from last.fm.\n\n\nget_lyrics &lt;- function(lyrics_url) {\n    # Function: Scrape last.fm lyrics page for: artist, song, and lyrics from a\n    # provided content link.  Return as a tibble/data.frame\n\n    cat(\"Scraping song lyrics from:\", lyrics_url, \"\\n\")\n\n    pacman::p_load(tidyverse, rvest)  # install/ load package(s)\n\n    url &lt;- url(lyrics_url, \"rb\")  # open url connection \n    html &lt;- read_html(url)  # read and parse html as an xml object\n    close(url)  # close url connection\n\n    artist &lt;- html |&gt;\n        html_element(\"a.header-new-crumb\") |&gt;\n        html_text()\n\n    song &lt;- html |&gt;\n        html_element(\"h1.header-new-title\") |&gt;\n        html_text()\n\n    lyrics &lt;- html |&gt;\n        html_elements(\"p.lyrics-paragraph\") |&gt;\n        html_text()\n\n    cat(\"...one moment \")\n\n    Sys.sleep(1)  # sleep for 1 second to reduce server load\n\n    song_lyrics &lt;- tibble(artist, song, lyrics, lyrics_url)\n\n    cat(\"... done! \\n\")\n\n    return(song_lyrics)\n}\n\n\n\n\n\n\n\nTip\n\n\n\nThe get_lyrics function includes all of the code developed previously, but also includes: (1) output messages (cat()), (2) a processing pause (Sys.sleep()), and (3) code to manage opening and closing web connections (url() and close()).\nPoints (1) and (2) will be useful when we iterate over this function to provide status messages and to reduce server load when processing multiple webpages from a web domain. (3) will be necessary to manage webpages that are non-existent. As we will see, we will generate url link to multiple song lyrics some of which will not be valid. To avoid errors that will stop the processing these steps have been incorporated here.\n\n\n\nthe write_content writes the webscraped data to our local machine, including functionality to create the necessary directory structure of the target file path we choose.\n\n\nwrite_content &lt;- function(content, target_file) {\n    # Function: Write the tibble content to disk. Create the directory if it\n    # does not already exist.\n\n    pacman::p_load(tidyverse)  # install/ load packages\n\n    target_dir &lt;- dirname(target_file)  # identify target file directory structure\n    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE)  # create directory\n    write_csv(content, target_file)  # write csv file to target location\n\n    cat(\"Content written to disk!\\n\")\n}\n\nWith just these two functions, we can take a lyrics URL from last.fm and scrape and write the data to disk like this.\n\nlyrics_url &lt;- \"https://www.last.fm/music/Pixies/_/Where+Is+My+Mind%3F/+lyrics\"\n\nlyrics_url |&gt;\n    get_lyrics() |&gt;\n    write_content(target_file = \"../data/original/lastfm/lyrics.csv\")\n\ndata/original/lastfm/\n└── lyrics.csv\nNow we could manually search and copy URLs and run this function pipeline. This would be fine if we had just a few particular URLs that we wanted to scrape. But if we want to, say, scrape a set of lyrics grouped by genre. We would probably want a more programmatic approach. The good news is we can leverage our understanding of webscraping to scrape last.fm to harvest the information needed to create and store links to songs by genre. We can then pass these links to a pipeline, similar to the previous one, to scrape lyrics for many songs and store the results in files grouped by genre.\nLast.fm provides a genres page where some of the top genres are listed and can be further explored.\n\n\n\n\nFigure 5.7: Genre page on last.fm\n\n\n\nDiving into a a particular genre, ‘rock’ for example, you will get a listing of the top tracks in that genre.\n\n\n\n\nFigure 5.8: Tracks by genre list page on last.fm\n\n\n\nIf we inspect the HTML elements for the track names in Figure 5.8, we can see that a relative URL is found for the track. In this case, I have ‘Smells Like Teen Spirit’ by Nirvana highlighted in the inspector. If we follow this link to the track page and then to the lyrics for the track, you will notice that the relative URL on the track listings page has all the unique information. Only the web domain https://www.last.fm and the post-pended /+lyrics is missing.\nSo with this we can put together a function which gets the track listing for a last.fm genre, scrapes the relative URLs for each of the tracks, and creates a full absolute URL to the lyrics page.\n\nget_genre_lyrics_urls &lt;- function(last_fm_genre) {\n  # Function: Scrapes a given last.fm genre title for top tracks in\n  # that genre and then creates links to the lyrics pages for these tracks\n  \n  cat(\"Scraping top songs from:\", last_fm_genre, \"genre: \\n\")\n  \n  pacman::p_load(tidyverse, rvest) # install/ load packages\n  \n  # create web url for the genre listing page\n  genre_listing_url &lt;- \n    paste0(\"https://www.last.fm/tag/\", last_fm_genre, \"/tracks\") \n  \n  genre_lyrics_urls &lt;- \n    read_html(genre_listing_url) |&gt; # read raw html and parse to xml\n    html_elements(\"td.chartlist-name a\") |&gt; # isolate the track elements\n    html_attr(\"href\") |&gt; # extract the href attribute\n    paste0(\"https://www.last.fm\", ., \"/+lyrics\") # join the domain, relative artist path, and the post-pended /+lyrics to create an absolute URL\n  \n  return(genre_lyrics_urls)\n}\n\nWith this function, all we need is to identify the verbatim way last.fm lists the genres. For Rock, it is rock but for Hip Hop, it is hip+hop.\n\nget_genre_lyrics_urls(\"hip+hop\") |&gt;  # get urls for top hip hop tracks\n  head(n = 10) # only display 10 tracks\n\n\n\n&gt; Scraping top songs from: hip+hop genre:\n\n\n&gt;  [1] \"https://www.last.fm/music/Juzhin/_/Charlie+Conscience+(feat.+MMAIO)/+lyrics\"\n&gt;  [2] \"https://www.last.fm/music/Juzhin/_/Railways/+lyrics\"                        \n&gt;  [3] \"https://www.last.fm/music/Juzhin/_/Coming+Down/+lyrics\"                     \n&gt;  [4] \"https://www.last.fm/music/Juzhin/_/Tupona/+lyrics\"                          \n&gt;  [5] \"https://www.last.fm/music/Juzhin/_/Sakhalin/+lyrics\"                        \n&gt;  [6] \"https://www.last.fm/music/Juzhin/_/3+Simple+Minutes/+lyrics\"                \n&gt;  [7] \"https://www.last.fm/music/Juzhin/_/Lost+Sense/+lyrics\"                      \n&gt;  [8] \"https://www.last.fm/music/Juzhin/_/Wonderful/+lyrics\"                       \n&gt;  [9] \"https://www.last.fm/music/Gina+Moryson/_/Vanilla+Smoothy+(Live)/+lyrics\"    \n&gt; [10] \"https://www.last.fm/music/Juzhin/_/Flunk-Down+(Juzhin+Remix)/+lyrics\"\n\n\nSo now we have a method to scrape URLs by genre and list them in a vector. Our approach, then, could be to pass these lyrics URLs to our existing pipeline which downloads the lyrics (get_lyrics()) and then writes them to disk (write_content()).\n\n# Note: will not run\nget_genre_lyrics_urls(\"hip+hop\") |&gt; # get lyrics urls for specific genre\n  get_lyrics() |&gt; # scrape lyrics url\n  write_content(target_file = \"../data/original/lastfm/hip_hop.csv\") # write to disk\n\nThis approach, however, has a couple of problems. (1) our get_lyrics() function only takes one URL at a time, but the result of get_genre_lyrics_urls() will produce many URLs. We will be able to solve this with iteration using the purrr package, specifically the map() function which will iteratively map each URL output from get_genre_lyrics_urls() to get_lyrics() in turn. (2) the output from our iterative application of get_lyrics() will produce a tibble for each URL, which then sets up a problem with writing the tibbles to disk with the write_content() function. To avoid this we will want to combine the tibbles into one single tibble and then send it to be written to disk. The bind_rows() function will do just this.\n\n# Note: will run, but with occasional errors\nget_genre_lyrics_urls(\"hip+hop\") |&gt; # get lyrics urls for specific genre\n  map(get_lyrics) |&gt;  # scrape lyrics url\n  bind_rows() |&gt; # combine tibbles into one\n  write_content(target_file = \"../data/original/lastfm/hip_hop.csv\") # write to disk\n\nThis preceding pipeline conceptually will work. However, on my testing, it turns out that some of the URLs that are generated in the get_genre_lyrics_urls() do not exist on the site. That is, the song is listed but no lyrics have been added to the song site. This will mean that when the URL is sent to the get_lyrics() function, there will be an error when attempting to download and parse the page with read_html() which will halt the entire process. To avoid this error, we can wrap the get_lyrics() function in a function designed to attempt to download and parse the URL (tryCatch()), but if there is an error, it will skip it and move on to the next URL without stopping the processing. This approach is reflected in the get_lyrics_catch() function below.\n\n# Wrap the `get_lyrics()` function with `tryCatch()` to skip URLs that have no\n# lyrics\n\nget_lyrics_catch &lt;- function(lyrics_url) {\n    tryCatch(get_lyrics(lyrics_url), error = function(e) return(NULL))  # no, URL, return(NULL)/ skip\n}\n\nUpdating the pipeline with the get_lyrics_catch() function would look like this:\n\n# Note: will run, but we can do better\nget_genre_lyrics_urls(\"hip+hop\") |&gt; # get lyrics urls for specific genre\n  map(get_lyrics_catch) |&gt;  # scrape lyrics url\n  bind_rows() |&gt; # combine tibbles into one\n  write_content(target_file = \"../data/original/lastfm/hip_hop.csv\") # write to disk\n\nThis will work, but as we have discussed before one of this goals we have we acquiring data for a reproducible research project is to make sure that we are developing efficient code that will not burden site’s server we are scraping from. In this case, we would like to check to see if the data is already downloaded. If not, then the script should run. If so, then the script does not run. Of course this is a perfect use of a conditional statement. To make this a single function we can call, I’ve wrapped the functions we created for getting lyric URLs from last.fm, scraping the URLs, and writing the results to disk in the download_lastfm_lyrics() function below. I also added a line to add a last_fm_genre column to the combined tibble to store the name of the genre we scraped (i.e. mutate(genre = last_fm_genre).\n\ndownload_lastfm_lyrics &lt;- function(last_fm_genre, target_file) {\n    # Function: get last.fm lyric urls by genre and write them to disk\n\n    if (!file.exists(target_file)) {\n\n        cat(\"Downloading data.\\n\")\n\n        get_genre_lyrics_urls(last_fm_genre) |&gt;\n            map(get_lyrics_catch) |&gt;\n            bind_rows() |&gt;\n            mutate(genre = last_fm_genre) |&gt;\n            write_content(target_file)\n\n    } else {\n        cat(\"Data already downloaded!\\n\")\n    }\n}\n\nNow we can call this function on any genre on the last.fm site and download the top 50 song lyrics for that genre (provided they all have lyrics pages).\n\n# Scrape lyrics for 'pop'\ndownload_lastfm_lyrics(last_fm_genre = \"pop\", target_file = \"../data/original/lastfm/pop.csv\")\n\n# Scrape lyrics for 'rock'\ndownload_lastfm_lyrics(last_fm_genre = \"rock\", target_file = \"../data/original/lastfm/rock.csv\")\n\n# Scrape lyrics for 'hip hop'\ndownload_lastfm_lyrics(last_fm_genre = \"hip+hop\", target_file = \"../data/original/lastfm/hip_hop.csv\")\n\n# Scrape lyrics for 'metal'\ndownload_lastfm_lyrics(last_fm_genre = \"metal\", target_file = \"../data/original/lastfm/metal.csv\")\n\nNow we can see that our web scrape data is organized in a similar fashion to the other data we acquired in this chapter.\ndata/\n├── derived/\n└── original/\n    ├── cedel2/\n    │   └── texts.csv\n    ├── gutenberg/\n    │   ├── works_pq.csv\n    │   └── works_pr.csv\n    ├── lastfm/\n    │   ├── country.csv\n    │   ├── hip_hop.csv\n    │   ├── lyrics.csv\n    │   ├── metal.csv\n    │   ├── pop.csv\n    │   └── rock.csv\n    ├── sbc/\n    │   ├── meta-data/\n    │   └── transcriptions/\n    ├── scs/\n    │   ├── README\n    │   ├── discourse\n    │   ├── disfluency\n    │   ├── documentation/\n    │   ├── tagged\n    │   ├── timed-transcript\n    │   └── transcript\n    └── twitter/\n        └── rt_latinx.csv\nAgain, it is important to add these custom functions to our acquire_functions.R script in the functions/ directory so we can access them in our scripts more efficiently and make our analysis steps more succinct and legible.\nIn this section we covered scraping language data from the web. The rvest package provides a host of functions for downloading and parsing HTML. We first looked at a toy example to get a basic understanding of how HTML works and then moved to applying this knowledge to a practical example. To maintain a reproducible workflow, the code developed in this example was grouped into task-oriented functions which were in turn joined and wrapped into a function that provided convenient access to our workflow and avoided unnecessary downloads (in the case the data already exists on disk).\nHere we have built on previously introduced R coding concepts and demonstrated various others. Web scraping often requires more knowledge of and familiarity with R as well as other web technologies. Rest assured, however, practice will increase confidence in your abilities. I encourage you to practice on your own with other websites. You will encounter problems. Consult the R documentation in RStudio or online and lean on the R community on the web at sites such as Stack Overflow inter alia."
  },
  {
    "objectID": "acquire-data.html#documentation",
    "href": "acquire-data.html#documentation",
    "title": "5  Acquire data",
    "section": "\n5.4 Documentation",
    "text": "5.4 Documentation\nAs part of the data acquisition process it is important include documentation that describes the data resource(s) that will serve as the base for a research project. For all resources the data should include as much information possible that outlines the sampling frame of the data (Ädel 2020). For a corpus sample acquired from a repository will often include documentation which will outline the sampling frame –this most likely will be the very information which leads a researcher to select this resource for the project at hand. It is important to include this documentation (HTML or PDF file) or reference to the documentation (article citation or link7) within the reproducible project’s directory structure.\nIn other cases where the data acquisition process is formulated and conducted by the researcher for the specific aims of the research (i.e. API and web scraping approaches), the researcher should make an effort to document those aspects which are key for the study, but that may also be of interest to other researchers for similar research questions. This will may include language characteristics such as modality, register, genre, etc., speaker/ writer characteristics such as demographics, time period(s), context of the linguistic communication, etc. and process characteristics such as the source of the data, the process of acquisition, date of acquisition, etc. However, it is important to recognize that each language sample and the resource from which it is drawn is unique. As a general rule of thumb, a researcher should document the resource as if this were a resource they were to encounter for the first time. To archive this information, it is standard practice to include a README file in the relevant directory where the data is stored.\ndata/\n├── derived/\n└── original/\n    ├── cedel2/\n    │   ├── documentation/\n    │   └── texts.csv\n    ├── gutenberg/\n    │   ├── README.md\n    │   ├── works_pq.csv\n    │   └── works_pr.csv\n    ├── lastfm/\n    │   ├── README.md\n    │   ├── country.csv\n    │   ├── hip_hop.csv\n    │   ├── lyrics.csv\n    │   ├── metal.csv\n    │   ├── pop.csv\n    │   └── rock.csv\n    ├── sbc/\n    │   ├── meta-data/\n    │   └── transcriptions/\n    ├── scs/\n    │   ├── README\n    │   ├── discourse\n    │   ├── disfluency\n    │   ├── documentation/\n    │   ├── tagged\n    │   ├── timed-transcript\n    │   └── transcript\n    └── twitter/\n        ├── README.md\n        └── rt_latinx.csv\nFor both existing corpora and data samples acquired by the researcher it is also important to signal if there are conditions and/ or licensing restrictions that one should heed when using and potentially sharing the data. In some cases existing corpus data come with restrictions on data sharing. These can be quite restrictive and ultimately require that the corpus data not be included in publically available reproducible project or data can only be shared in a derived format. If this the case, it is important to document the steps to legally acquire the data so that a researcher can acquire their own license and take full advantage of your reproducible project.\nIn the case of data from APIs or web scraping, there too may be stipulations on sharing data. A growing number of data sources apply one of the available Creative Common Licenses. Check the source of your data for more information and if you are a member of a research institution you will likely have a specialist on Copyright and Fair Use."
  },
  {
    "objectID": "acquire-data.html#summary",
    "href": "acquire-data.html#summary",
    "title": "5  Acquire data",
    "section": "Summary",
    "text": "Summary\nIn this chapter we have covered a lot of ground. On the surface we have discussed three methods for acquiring corpus data for use in text analysis. In the process we have delved into various aspects of the R programming language. Some key concepts include writing custom functions and working with those function in an iterative manner. We have also considered topics that are more general in nature and concern interacting with data found on the internet.\nEach of these methods should be approached in a way that is transparent to the researcher and to would-be collaborators and the general research community. For this reason the documentation of the steps taken to acquire data are key both in the code and in human-facing documentation.\nAt this point you have both a bird’s eye view of the data available on the web and strategies on how to access a great majority of it. It is now time to turn to the next step in our data analysis project: data curation. In the next posts I will cover how to wrangle your raw data into a tidy dataset. This will include working with and incorporating meta-data as well as augmenting a dataset with linguistic annotations."
  },
  {
    "objectID": "acquire-data.html#activities",
    "href": "acquire-data.html#activities",
    "title": "5  Acquire data",
    "section": "Activities",
    "text": "Activities\n\n\n\n\n\n\nRecipe\n\n\n\nWhat: Control statements, custom functions, and iterationHow: Read Recipe 6 and participate in the Hypothes.is online social annotation.Why: To increase your ability to produce effective, concise, and reproducible code. The three main areas we will cover are working with control statements, writing custom functions, and leveraging iteration. These programming strategies are often useful for acquiring data but, as we will see, they are powerful concepts that can be used throughout a reproducible research project.\n\n\n\n\n\n\n\n\nLab\n\n\n\nWhat: Control statements, custom functions, and iterationHow: Clone, fork, and complete the steps in Lab 6.Why: To gain experience working with coding strategies such as control statements, custom functions, and iteration, practice working with direct downloads and API interfaces to acquire data, and implement organizational strategies for organizing data in reproducible fashion."
  },
  {
    "objectID": "acquire-data.html#questions",
    "href": "acquire-data.html#questions",
    "title": "5  Acquire data",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\nConceptual questions\n\n\n\n\n…\n…\n\n\n\n\n\n\n\n\n\nTechnical exercises\n\n\n\n\n…\n…\n\n\n\n\n\n\n\nÄdel, Annelie. 2020. “Corpus Compilation.” In A Practical Handbook of Corpus Linguistics, edited by Magali Paquot and Stefan Th. Gries, 3–24. Switzerland: Springer.\n\n\nKearney, Michael W., Lluís Revilla Sancho, and Hadley Wickham. 2023. Rtweet: Collecting Twitter Data. https://CRAN.R-project.org/package=rtweet.\n\n\nLozano, Cristóbal. 2009. “CEDEL2: Corpus Escrito Del Español L2.” Applied Linguistics Now: Understanding Language and Mind/La Lingüística Aplicada Hoy: Comprendiendo El Lenguaje y La Mente. Almería: Universidad de Almería, 197–212.\n\n\nWickham, Hadley. 2022. Rvest: Easily Harvest (Scrape) Web Pages. https://CRAN.R-project.org/package=rvest."
  },
  {
    "objectID": "acquire-data.html#footnotes",
    "href": "acquire-data.html#footnotes",
    "title": "5  Acquire data",
    "section": "",
    "text": "Remember base R packages are installed by default with R and are loaded and accessible by default in each R session.↩︎\nSee Section @ref(sources) for a list of some other API packages.↩︎\ntidyverse is not a typical package. It is a set of packages: ggplot2, dplyr, tidyr, readr, purrr, and tibble. These packages are all installed/ loaded with tidyverse and form the backbone for the type of work you will typically do in most analyses.↩︎\nSee Library of Congress Classification documentation for a complete list of subject codes.↩︎\nIt is important to check the paths of sub-domains as some website allow access in some areas and not in others↩︎\ntibble objects are data.frame objects with some added extra bells and whistles that we won’t get into here.↩︎\nNote that web links can change and it is often best to safeguard the documentation by downloading the HTML documentation page instead of linking↩︎"
  },
  {
    "objectID": "curate-datasets.html#unstructured",
    "href": "curate-datasets.html#unstructured",
    "title": "6  Curate data(sets)",
    "section": "\n6.1 Unstructured",
    "text": "6.1 Unstructured\nThe bulk of text that is available in the wild is of the unstructured variety. Unstructured data is data that has not been organized to make the information contained within explicit. Explicit information that is included with data is called metadata. Metadata can be linguistic or non-linguistic in nature. So for unstructured data there is little to no metadata directly associated with the data. This information needs to be added or derived for the purposes of the research, either through manual inspection or (semi-)automatic processes. For now, however, our job is just to get the unstructured data into a structured format with a minimal set of metadata that we can derive from the resource.\nAs an example of an unstructured source of text data, let’s take a look at the Europarle Parallel Corpus, as introduced in Chapter 2 “Understanding data”. This data contains parallel texts (source and translated documents) from the European Parliamentary proceedings for some 21 European languages. Here we will focus in on the translation from Spanish to English (Spanish-English).\n\n6.1.1 Orientation\nWith the data downloaded into the data/original/europarle/ directory we see that there are two files. One corresponding to the source language (Spanish) and one for the target language (English).\ndata/original/europarle/\n├── europarl-v7.es-en.en\n└── europarl-v7.es-en.es\nLooking at the first 10 lines of the first file, we can see that this is running text.\n\n\n&gt; Resumption of the session\n&gt; I declare resumed the session of the European Parliament adjourned on\nFriday 17 December 1999, and I would like once again to wish you a\nhappy new year in the hope that you enjoyed a pleasant festive period.\n&gt; Although, as you will have seen, the dreaded 'millennium bug' failed\nto materialise, still the people in a number of countries suffered a\nseries of natural disasters that truly were dreadful.\n&gt; You have requested a debate on this subject in the course of the next\nfew days, during this part-session.\n&gt; In the meantime, I should like to observe a minute' s silence, as a\nnumber of Members have requested, on behalf of all the victims\nconcerned, particularly those of the terrible storms, in the various\ncountries of the European Union.\n&gt; Please rise, then, for this minute' s silence.\n&gt; (The House rose and observed a minute' s silence)\n&gt; Madam President, on a point of order.\n&gt; You will be aware from the press and television that there have been\na number of bomb explosions and killings in Sri Lanka.\n&gt; One of the people assassinated very recently in Sri Lanka was Mr\nKumar Ponnambalam, who had visited the European Parliament just a few\nmonths ago.\n\n\nThe only meta information that we can surmise from these files is the fact that we know one is the source language and one is the target language and that each sentence is aligned (parallel) with the lines in the other file.\nSo with what we have we’d like to create a data frame that has the seen in Table 6.1.\n\n\n\n\nTable 6.1: Idealized structure for the Europarle Corpus dataset.\n\ntype\nsentence_id\nsentence\n\n\n\nSource\n1\n…sentence from source language\n\n\nTarget\n1\n…sentence from target language\n\n\n\n\n\n\n\n6.1.2 Tidy the data\nTo create this dataset structure lets’s read the files with the readtext() function from readtext package and assign them to a meaningful variable.\n\n# Read the Europarle files\neuroparle_en &lt;-  # English target text\n  readtext::readtext(\"../data/original/europarle/europarl-v7.es-en.en\", # path to the data\n                     verbosity = 0) # don't show warnings\n\neuroparle_es &lt;- # Spanish source text\n  readtext::readtext(\"../data/original/europarle/europarl-v7.es-en.es\", # path to the data\n                     verbosity = 0) # don't show warnings\n\n\n\n\n\n\n\nTip\n\n\n\nThe readtext() function can read many different types of file formats, from structured to unstructured. However, it depends in large part on the extension of the file to recognize what algorithm to use when reading a file. In this particular case the Europarle files do not have a typical extension (they have .en and .es). The readtext() function will treat them as plain text (.txt), but it will throw a warning message. To suppress the warning message you can add the verbosity = 0 argument.\n\n\nNow there are a couple things to note about thbe europarle_en and europarle_es objects. If we inspect their structure, we will find that the dimensions of the data frame that is created is one row by two columns.\n\nstr(europarle_en) # inspect the structure of the object\n\n#&gt; Classes 'readtext' and 'data.frame': 1 obs. of 2 variables:\n#&gt; $ doc_id: chr \"europarl-v7.es-en.en\"\n#&gt; $ text : chr \"Resumption of the session\\nI declare resumed the\nsession of the European Parliament adjourned on Friday 17 Dece\"|\n__truncated__\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that the str() function from base R is similar to glimpse(). However, glimpse() will attempt to show you as much data as possible. In this case since our column text is a very long character vector it will take a long time to render. I’ve chosen the str() function as it will automatically truncate the data.\n\n\nThe columns are doc_id and text. doc_id is created by readtext to index each file that is read in. The text column is where the text appears. The fact that we only have one row means that all the text in the entire file is contained in one cell! We will want to break this cell up into rows for each sentence, but for now let’s work with getting the columns to line up with our idealized dataset structure.\nFirst let’s change the type of data frame that we are working with to a tibble. This will make sure we don’t accidentally print hundreds of lines to our R Markdown output and/ or the R Console. Then we will rename the doc_id column to type and change the value of that column to “Target” (for English) and “Source” (for Spanish).\n\neuroparle_target &lt;- \n  europarle_en |&gt; # readtext data frame\n  as_tibble() |&gt; # convert to tibble\n  rename(type = doc_id) |&gt; # rename doc_id to type\n  mutate(type = \"Target\") # change type value to 'Target'\n\neuroparle_source &lt;- \n  europarle_es |&gt; # readtext data frame\n  as_tibble() |&gt; # convert to tibble\n  rename(type = doc_id) |&gt; # rename doc_id to type\n  mutate(type = \"Source\") # change type value to 'Source'\n\nWe have two objects now, one corresponding to the ‘Source’ and the other the ‘Target’ parallel texts. Let’s now join these two datasets, one on top of the other –that is, by rows. We wil use the bind_rows() function for this.\n\neuroparle &lt;- \n  bind_rows(europarle_target, europarle_source)\n\nstr(europarle) # inspect the structure of the object\n\n#&gt; tibble [2 × 2] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ type: chr [1:2] \"Target\" \"Source\"\n#&gt;  $ text: chr [1:2] \"Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Friday 17 Dece\"| __truncated__ \"Reanudación del período de sesiones\\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrump\"| __truncated__\n\n\nThe europarle dataset now has 2 columns, as before, and 2 rows –each corresponding to the distinct language types (Source/ Target).\nRemember our goal is to create a dataset structure with three columns type, sentence_id, and sentence. At the moment we have type and text –where text has all of the sentences in for each type in a cell. So we are going to want to break up the text column into sentences, group the sentences that are created by type, and then number these sentences so that they are aligned between the distinct types.\nTo break up the text into sentences we are going to turn to the tidytext package. This package has a extremely useful function unnest_tokens() which provides an effective way to break text into various units (see ?tidytext::unnest_tokens for a full list of token types). Since I know from looking at the raw text that each sentence is on its own line, the best strategy to break the text into sentence units is to find a way to break each line into a new row in our dataset. To do this we need to use the token = \"regex\" (for Regular Expression) and use the pattern = \"\\\\n\" which tells R to look for carriage returns to use as the breaking criterion.\n\neuroparle_sentences &lt;- \n  europarle |&gt; \n  tidytext::unnest_tokens(output = sentence, # new column\n                          input = text, # column to find text\n                          token = \"regex\", # use a regular expression to break up the text\n                          pattern = \"\\\\n\", # break text by carriage returns (returns after lines)\n                          to_lower = FALSE) # do not lowercase the text\n\nglimpse(europarle_sentences) # preview the structure\n\n#&gt; Rows: 3,926,375\n#&gt; Columns: 2\n#&gt; $ type     &lt;chr&gt; \"Target\", \"Target\", \"Target\", \"Target\", \"Target\", \"Target\", \"…\n#&gt; $ sentence &lt;chr&gt; \"Resumption of the session\", \"I declare resumed the session o…\n\n\n\n\n\n\n\n\nTip\n\n\n\nRegular Expressions are a powerful pattern matching syntax. They are used extensively in text manipulation and we will see them again and again. A good website to practice Regular Expressions is RegEx101. You can also install the regexplain package in R to get access to a useful RStudio Addin.\n\n\nOur new europarle_sentences object is a data frame with almost 4 million rows! The final step to get to our envisioned dataset structure is to add the sentence_id column which will be calculated by grouping the data by type and then assigning a row number to each of the sentences in each group.\n\neuroparle_sentences_id &lt;- \n  europarle_sentences |&gt; # dataset\n  group_by(type) |&gt; # group by type\n  mutate(sentence_id = row_number()) |&gt; # add a row number for each sentence for each level of type\n  select(type, sentence_id, sentence) |&gt; # select the relevant columns to keep\n  ungroup() |&gt;  # ungroup by type\n  arrange(sentence_id, type) # arrange the dataset\n\neuroparle_sentences_id |&gt; \n  slice_head(n = 10) |&gt; \n  knitr::kable(booktabs = TRUE)\n\n\n\nTable 6.2: First ten sentences in the Europarle Corpus curated dataset.\n\n\n\n\n\n\ntype\nsentence_id\nsentence\n\n\n\nSource\n1\nReanudación del período de sesiones\n\n\nTarget\n1\nResumption of the session\n\n\nSource\n2\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\n\n\nTarget\n2\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n\n\nSource\n3\nComo todos han podido comprobar, el gran “efecto del año 2000” no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\n\n\nTarget\n3\nAlthough, as you will have seen, the dreaded ‘millennium bug’ failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n\n\nSource\n4\nSus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\n\n\nTarget\n4\nYou have requested a debate on this subject in the course of the next few days, during this part-session.\n\n\nSource\n5\nA la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\n\n\nTarget\n5\nIn the meantime, I should like to observe a minute’ s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n\n\n\n\n\n\n\n6.1.3 Write dataset\nAt this point we have the curated dataset (europarle_sentences_id) in a tidy format. This dataset, however, is only in the current R session. We will want to write this dataset to disk so that in the next step of the text analysis workflow (transform data) we will be able to start work on this dataset and make changes as needed to fit our analysis needs.\nWe will leverage the project directory structure which has distinct directories for original/ and derived/ data(sets).\ndata/\n├── derived\n└── original\nSince this dataset is derived by our work, we will added it to the derived/ directory. I’ll create a europarle/ directory just to keep things organized.\n\n# Write the curated dataset to disk\nfs::dir_create(path = \"../data/derived/europarle/\") # create the europarle directory\nwrite_csv(x = europarle_sentences_id, # object to write\n          file = \"../data/derived/europarle/europarle_curated.csv\") # target file location/ name\n\nThis is how the directory structure under the derived/ directory looks now.\ndata/\n├── derived\n│   └── europarle\n│       └── europarle_curated.csv\n└── original\n    └──europarle\n        ├── europarl-v7.es-en.en\n        └── europarl-v7.es-en.es\n\n6.1.4 Summary\nIn this section we worked with unstructured data and looked at how to read the data into an R session and manipulate the data to form a tidy dataset with a few columns that we could derive based on the information we have about the corpus.\nIn our discussion we worked step by step to curate the Europarle Corpus, adding in intermediate steps for illustration purposes. However, in a more realistic case the code would most likely make more extensive use of piping (|&gt;) to reduce the number of intermediate objects and make the code more legible. Below I’ve included a sample of what that code might look like.\n\n# Read data and set up `type` column\neuroparle_en &lt;-  \n  readtext::readtext(\"../data/original/europarle/europarl-v7.es-en.en\", # path to the data\n                     verbosity = 0) |&gt; # don't show warnings\n  as_tibble() |&gt; # covert to tibble\n  rename(type = doc_id) |&gt; # rename doc_id to type\n  mutate(type = \"Target\") # change type value to 'Target'\n\neuroparle_es &lt;- \n  readtext::readtext(\"../data/original/europarle/europarl-v7.es-en.en\", # path to the data\n                     verbosity = 0) |&gt;  # don't show warnings\n  as_tibble() |&gt; # covert to tibble\n  rename(type = doc_id) |&gt; # rename doc_id to type\n  mutate(type = \"Source\") # change type value to 'Source'\n\n# Join the datasets by rows\neuroparle &lt;- \n  bind_rows(europarle_en, europarle_es)\n\n# Segment the `text` column into `sentence` units\neuroparle &lt;- \n  europarle |&gt; # dataset\n  tidytext::unnest_tokens(output = sentence, # new column\n                          input = text, # column to find text\n                          token = \"regex\", # use a regular expression to break up the text\n                          pattern = \"\\\\n\", # break text by carriage returns (returns after lines)\n                          to_lower = FALSE) # do not lowercase the text\n\n# Add `sentence_id` to each `type`\neuroparle &lt;- \n  europarle |&gt; # dataset\n  group_by(type) |&gt; # group by type\n  mutate(sentence_id = row_number()) |&gt; # add a row number for each sentence for each level of type\n  select(type, sentence_id, sentence) |&gt; # select the relevant columns to keep\n  ungroup() |&gt;  # ungroup by type\n  arrange(sentence_id, type) # arrange the dataset\n\n# Write the curated dataset to disk\nfs::dir_create(path = \"../data/derived/europarle/\") # create the europarle directory\nwrite_csv(x = europarle_sentences_id, # object to write\n          file = \"../data/derived/europarle/europarle_curated.csv\") # target file location/ name"
  },
  {
    "objectID": "curate-datasets.html#structured",
    "href": "curate-datasets.html#structured",
    "title": "6  Curate data(sets)",
    "section": "\n6.2 Structured",
    "text": "6.2 Structured\nOn the opposite side of the spectrum from unstructured data, structured data includes more metadata information –often much more. The association of metadata with the language to be analyzed means that the data has already be curated to some degree, therefore it is more apt to discuss structured data as a dataset. There are two questions, however, that need to be taken into account. One, logistical question, is what file format the dataset is in and how to we read it into R. And the second, more research-based, is whether the data is curated in a fashion that makes sense for the current research. Let’s look at each of these questions briefly and then get to a practical example.\nThere are file formats which are purposely designed for storing structured datasets. Some very common file types are .csv, .xml, .json, etc. The data within these files is explicitly organized. For example, in a .csv file, the dataset structure is represented by delimiting the columns and rows by commas.\ncolumn_1,column_2,column_3\nrow 1 value 1,row 1 value 2,row 1 value 3\nrow 2 value 1,row 2 value 2,row 2 value 3\nWhen read into R, the .csv file format is converted to a data frame with the appropriate structure.\n\n\n\n\nTable 6.3: Example .csv file in R\n\ncolumn_1\ncolumn_2\ncolumn_3\n\n\n\nrow 1 value 1\nrow 1 value 2\nrow 1 value 3\n\n\nrow 2 value 1\nrow 2 value 2\nrow 2 value 3\n\n\n\n\n\n\n\nWith an understanding of how the information is encoding into a file, we can now turn to considerations about how the original dataset is structure and how that structure is to be used for a given research project. The curation process that is reflected in a structured dataset may or may not initially align with the goals of our research either in terms of the type(s) of information or the unit of analysis of the structured dataset. The aim, then, is to take advantage of the information and curate it such that it does align.\nAs an example case of curating structured datasets, we will look at the song lyric datasets acquired from Last.fm in the previous chapter.\n\n6.2.1 Orientation\nThe individual datasets from the Last.fm webscrape are found inside the data/original/lastfm/ directory, and includes the README.md documentation file.\ndata/\n├── derived/\n└── original/\n    └── lastfm/\n        ├── README.md\n        ├── country.csv\n        ├── hip_hop.csv\n        ├── lyrics.csv\n        ├── metal.csv\n        ├── pop.csv\n        └── rock.csv\nLet’s take a look at the structure of one of genres from these set of lyrics to familiarize ourselves with the structure.\n\nlf_country &lt;- read_csv(file = \"../data/original/lastfm/country.csv\") # read the csv file\nlf_country |&gt; # dataset\n  slice_head(n = 10) |&gt; # first 10 observations \nknitr::kable(booktabs = TRUE) # print pretty table\n\n\n?(caption)\n\n\n\n\n\n\n\nTable 6.4: Example file from the Last.fm dataset of song lyrics.\n\n\n\n\n\n\n\n\nartist\nsong\nlyrics\nlyrics_url\ngenre\n\n\n\nJohnny Cash\nHurt\nI hurt myself todayTo see if I still feelI focus on the painThe only thing that’s realThe needle tears a holeThe old familiar stingTry to kill it all awayBut I remember everything\nhttps://www.last.fm/music/Johnny+Cash/_/Hurt/+lyrics\ncountry\n\n\nJohnny Cash\nHurt\nWhat have I becomeMy sweetest friendEveryone I know goes awayIn the endAnd you could have it allMy empire of dirtI will let you downI will make you hurt\nhttps://www.last.fm/music/Johnny+Cash/_/Hurt/+lyrics\ncountry\n\n\nJohnny Cash\nHurt\nI wear this crown of thornsUpon my liar’s chairFull of broken thoughtsI cannot repairBeneath the stains of timeThe feelings disappearYou are someone elseI am still right here\nhttps://www.last.fm/music/Johnny+Cash/_/Hurt/+lyrics\ncountry\n\n\nJohnny Cash\nHurt\nWhat have I becomeMy sweetest friendEveryone I know goes awayIn the endAnd you could have it allMy empire of dirtI will let you downI will make you hurt\nhttps://www.last.fm/music/Johnny+Cash/_/Hurt/+lyrics\ncountry\n\n\nJohnny Cash\nHurt\nIf I could start againA million miles awayI would keep myselfI would find a way\nhttps://www.last.fm/music/Johnny+Cash/_/Hurt/+lyrics\ncountry\n\n\nJohnny Cash\nRing of Fire\nLove is a burning thingAnd it makes a fiery ringBound by wild desireI fell into a ring of fire\nhttps://www.last.fm/music/Johnny+Cash/_/Ring+of+Fire/+lyrics\ncountry\n\n\nJohnny Cash\nRing of Fire\nI fell into a burning ring of fireI went down, down, downAnd the flames went higherAnd it burns, burns, burnsThe ring of fire, the ring of fire\nhttps://www.last.fm/music/Johnny+Cash/_/Ring+of+Fire/+lyrics\ncountry\n\n\nJohnny Cash\nRing of Fire\nI fell into a burning ring of fireI went down, down, downAnd the flames went higherAnd it burns, burns, burnsThe ring of fire, the ring of fire\nhttps://www.last.fm/music/Johnny+Cash/_/Ring+of+Fire/+lyrics\ncountry\n\n\nJohnny Cash\nRing of Fire\nThe taste of love is sweetWhen hearts like ours meetI fell for you like a child\nhttps://www.last.fm/music/Johnny+Cash/_/Ring+of+Fire/+lyrics\ncountry\n\n\nJohnny Cash\nRing of Fire\nOh, but the fire went wild\nhttps://www.last.fm/music/Johnny+Cash/_/Ring+of+Fire/+lyrics\ncountry\n\n\n\n\n\n\nWe can see a couple important characteristics from this preview of the dataset. First, we see the columns include artist, song, lyrics, lyrics_url, and genre. Second, we see that for each son the lyrics are segmented across multiple rows.\n\n\n\n\n\n\nTip\n\n\n\nYou may notice that in addition to the lyrics being separated by line, there appears to be an artifact from the original webscrape of this data which has individual lyric lines run in to the next. An example is the the lyrics “… hurt myself todayTosee if I still feelI focus…”. We will address this issue when it comes time to normalize the dataset in the transform process.\n\n\nGiven the fact that each of these files will include a genre label, that means that we will be able to read in each of these files in one operation and the distinction between genres will be recoverable. The next thing to think about is how we want to curate the dataset for our purposes. That is, what should the base structure of our curated dataset look like?\nLet’s make the assumption that we want to have the columns artist, song, lyrics, and genre. The lyrics_url could be useful for documentation purposes, but for our text analysis it does not appear to be very relevant –so we will drop it. The second aspect concerns the observations. As it stands, the dataset the observations reflect the formatting of the website from which the lyrics were drawn. A potentially better organization would to have each observation correspond to all the lyrics for a single song. In this case we will want to collapse the current lyrics column’s values into lyrics for the entire song –maintaining the other measure for each of the other columns.\nWith this structure in mind, we are shooting for an idealized structure such as the one below.\n\n\n\n\nTable 6.5: Idealized structure for the Last.fm dataset.\n\nartist\nsong\nlyrics\ngenre\n\n\n\nJohnny Cash\nHurt\n…lyrics for the song…\ncountry\n\n\nJohnny Cash\nRing of Fire\n…lyrics for the song…\ncountry\n\n\n\n\n\n\n\n6.2.2 Tidy the datasets\nSo our objectives are set, let’s first read in all the files. To do this we will again use the readtext() function. But instead of reading one file at a time we will read all the files of interest (those with the .csv extension) in one go. The readtext() function allows for the use of ‘wildcard’ notation (*) in the file(s) path to enable pattern matching.\nSo the files in the data/original/lastfm/ directory look like this.\n../data/original/lastfm/README.md\n../data/original/lastfm/country.csv\n../data/original/lastfm/hip_hop.csv\n../data/original/lastfm/lyrics.csv\n../data/original/lastfm/metal.csv\n../data/original/lastfm/pop.csv\n../data/original/lastfm/rock.csv\nWe want all the files, except the REAME.md file. To do this we want our path to look like this:\n../data/original/lastfm/*.csv\nThe wildcard * replaces the genre names and this effectively only matches files ending in .csv.\nGreat, that will capture the files we are looking for but when working with readtext() we will need to set the text_field argument to the column that corresponds to the text in our dataset. That is the lyrics column. Let’s go ahead and do this and convert the result to a tibble.\n\nlastfm &lt;- \n  readtext(file = \"../data/original/lastfm/*.csv\", # files to match using *.csv\n           text_field = \"lyrics\") |&gt; # text column from the datasets\n  as_tibble() # convert to a tibble\n\nglimpse(lastfm) # preview\n\n\n\n#&gt; Rows: 2,172\n#&gt; Columns: 6\n#&gt; $ doc_id     &lt;chr&gt; \"country.csv.1\", \"country.csv.2\", \"country.csv.3\", \"country…\n#&gt; $ text       &lt;chr&gt; \"I hurt myself todayTo see if I still feelI focus on the pa…\n#&gt; $ artist     &lt;chr&gt; \"Johnny Cash\", \"Johnny Cash\", \"Johnny Cash\", \"Johnny Cash\",…\n#&gt; $ song       &lt;chr&gt; \"Hurt\", \"Hurt\", \"Hurt\", \"Hurt\", \"Hurt\", \"Ring of Fire\", \"Ri…\n#&gt; $ lyrics_url &lt;chr&gt; \"https://www.last.fm/music/Johnny+Cash/_/Hurt/+lyrics\", \"ht…\n#&gt; $ genre      &lt;chr&gt; \"country\", \"country\", \"country\", \"country\", \"country\", \"cou…\n\n\nLooking at the preview of the data frame we now have in lastfm there are a couple things to note. First, we see that a column doc_id has been added. This column is used by readtext() to index the file from which the data was read. In our case since we already have sufficient information to index our dataset, we can drop this column. Next we see that the lyrics column has been renamed to text. This is because we set this as the text_field when we read in the files. We can easily rename this column, but we’ll leave that for later.\nLet’s go ahead and drop the columns that we have decided will not figure in our curated dataset. We can use the select() function to either select those columns we want to keep or by using the - operator, identify the columns we want to drop. The decision of ‘selecting’ or ‘deselecting’ is usually one of personal choice and code succinctness. In this case, we are dropping two columns and keeping four, so let’s deselect. I will assign the result to the same name as our current dataset, effectively overwriting that dataset.\n\nlastfm &lt;- # new dataset\n  select(lastfm, # original dataset\n         -doc_id, -lyrics_url) # drop these columns\n\nglimpse(lastfm) # preview\n\n#&gt; Rows: 2,172\n#&gt; Columns: 4\n#&gt; $ text   &lt;chr&gt; \"I hurt myself todayTo see if I still feelI focus on the painTh…\n#&gt; $ artist &lt;chr&gt; \"Johnny Cash\", \"Johnny Cash\", \"Johnny Cash\", \"Johnny Cash\", \"Jo…\n#&gt; $ song   &lt;chr&gt; \"Hurt\", \"Hurt\", \"Hurt\", \"Hurt\", \"Hurt\", \"Ring of Fire\", \"Ring o…\n#&gt; $ genre  &lt;chr&gt; \"country\", \"country\", \"country\", \"country\", \"country\", \"country…\n\n\nNow let’s work to collapse the lyrics in the text column by each distinct artist, song, and genre combination. We will use the group_by() function to create artist song genre groupings and then use summarize() to create a new column in which the text field is collapsed into all the song lyrics for this grouping. Inside the summarize() function we use str_flatten() with the argument collapse = \" \" to collapse each observation in text leaving a single whitespace between the observations (otherwise each line would then be joined contiguously to the next).\n\nlastfm &lt;- \n  lastfm |&gt; # dataset\n  group_by(artist, song, genre) |&gt; # grouping\n  summarise(lyrics = str_flatten(text, collapse = \" \")) |&gt;  # collapse text into the new column `lyrics` (dropping `text`)\n  ungroup() # unset the groupings\n\nglimpse(lastfm) # preview\n\n#&gt; Rows: 199\n#&gt; Columns: 4\n#&gt; $ artist &lt;chr&gt; \"3 Doors Down\", \"3 Doors Down\", \"50 Cent\", \"a-ha\", \"ABBA\", \"Aer…\n#&gt; $ song   &lt;chr&gt; \"Here Without You\", \"Kryptonite\", \"In Da Club\", \"Take On Me\", \"…\n#&gt; $ genre  &lt;chr&gt; \"rock\", \"rock\", \"hip-hop\", \"pop\", \"pop\", \"rock\", \"country\", \"co…\n#&gt; $ lyrics &lt;chr&gt; \"A hundred days have made me olderSince the last time that I sa…\n\n\nLet’s take a look at the first 5 observations from this collapsed dataset.\n\nlastfm |&gt; # dataset\n  slice_head(n = 5) |&gt; # first 5 observations\n  knitr::kable(booktabs = TRUE) # print pretty table\n\n\n\nTable 6.6: Sample lyrics from Last.fm dataset collapsed by artist, song, and genre.\n\n\n\n\n\n\n\nartist\nsong\ngenre\nlyrics\n\n\n\n3 Doors Down\nHere Without You\nrock\nA hundred days have made me olderSince the last time that I saw your pretty faceA thousand lies have made me colderAnd I don’t think I can look at this the same But all the miles that separateDisappear now when I’m dreaming of your face I’m here without you, babyBut you’re still on my lonely mindI think about you, babyAnd I dream about you all the time I’m here without you, babyBut you’re still with me in my dreamsAnd tonight it’s only you and me, yeah The miles just keep rollin’As the people leave their way to say helloI’ve heard this life is overratedBut I hope that it gets better as we go, oh yeah, yeah I’m here without you, babyBut you’re still on my lonely mindI think about you, babyAnd I dream about you all the time I’m here without you, babyBut you’re still with me in my dreamsAnd tonight, girl, it’s only you and me Everything I know, and anywhere I go (Oh whoa)It gets hard but it won’t take away my love (Oh whoa)And when the last one fallsWhen it’s all said and doneIt gets hard but it won’t take away my love, whoa, oh, oh I’m here without you, babyBut you’re still on my lonely mindI think about you, babyAnd I dream about you all the time I’m here without you, babyBut you’re still with me in my dreamsAnd tonight, girl, it’s only you and me, yeahOh girl, oh oh\n\n\n3 Doors Down\nKryptonite\nrock\nWell I took a walk around the world to ease my troubled mindI left my body lying somewhere in the sands of timeWell I watched the world float to the dark side of the moonI feel there’s nothing I can do,yeah I watched the world float to the dark side of the moonAfter all I knew it had to be something to do with youI really don’t mind what happens now and thenAs long as you’ll be my friend at the end If I go crazy then will you still call me Superman?If I’m alive and well, will you be there a-holding my hand?I’ll keep you by my side with my superhuman mightKryptonite You called me strong, you called me weakBut still your secrets I will keepYou took for granted all the times I never let you downYou stumbled in and bumped your headIf not for me then you’d be deadI picked you up and put you back on solid ground If I go crazy then will you still call me Superman?If I’m alive and well, will you be there a-holding my hand?I’ll keep you by my side with my superhuman mightKryptonite If I go crazy then will you still call me Superman?If I’m alive and well, will you be there holding my hand?I’ll keep you by my side with my superhuman mightKryptoniteYeah! If I go crazy then will you still call me Superman?If I’m alive and well, will you be there a-holding my hand?I’ll keep you by my side with my superhuman mightKryptonite Oh, whoa, whoaOh, whoa, whoaOh, whoa, whoa\n\n\n50 Cent\nIn Da Club\nhip-hop\nGo, go, go, go, go, goGo, shortyIt’s your birthdayWe gon’ party like it’s your birthdayWe gon’ sip Bacardi like it’s your birthdayAnd you know we don’t give a fuck it’s not your birthday You can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin’ drugsI’m into havin’ sex, I ain’t into makin’ loveSo come give me a hug, if you into getting rubbed You can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin’ drugsI’m into havin’ sex, I ain’t into makin’ loveSo come give me a hug, if you into getting rubbed When I pull out up front, you see the Benz on dubsWhen I roll 20 deep, it’s 20 knives in the clubNiggas heard I fuck with Dre, now they wanna show me loveWhen you sell like Eminem, and the hoes they wanna fuckBut, homie, ain’t nothing change hoes down, G’s upI see Xzibit in the Cut, that nigga roll that weed upIf you watch how I move, you’ll mistake me for a playa or pimpBeen hit wit’ a few shells, but I don’t walk wit’ a limp (I’m ight)In the hood, in L.A, they saying ““50 you hot”“They like me, I want them to love me like they love ‘PacBut holla, in New York them niggas’ll tell ya I’m locoAnd the plan is to put the rap game in a choke holdI’m full of focused man, my money on my mindI got a mill out the deal and I’m still on the grindNow shorty said she feeling my style, she feeling my flowHer girlfriend wanna get bi and they ready to go You can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin’ drugsI’m into havin’ sex, I ain’t into makin’ loveSo come give me a hug, if you into getting rubbed You can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin’ drugsI’m into havin’ sex I, ain’t into makin’ loveSo come give me a hug, if you into getting rubbed My flow, my show brought me the doeThat bought me all my fancy thingsMy crib, my cars, my clothes, my jewelsLook, nigga, I done came up and I ain’t changeAnd you should love it, way more then you hate itNigga, you mad? I thought that you’d be happy I made itI’m that cat by the bar toasting to the good lifeYou that faggot ass nigga trying to pull me back right?When my jaws get to bumpin’ in the club it’s onI wink my eye at you, bitch, if she smiles she goneIf the roof on fire, let the motherfucker burnIf you talking ‘bout money, homie, I ain’t concernedI’m a tell you what Banks told me ’cuz go ’head switch the style upIf the niggas hate then let ’em hate and watch the money pile upOr we go upside they head wit’ a bottle of bubThey know where we fuckin’ be You can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin’ drugsI’m into havin’ sex, I ain’t into makin’ love So come give me a hug, if you into getting rubbedYou can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin’ drugsI’m into havin’ sex, I ain’t into makin’ loveSo come give me a hug, if you into getting rubbed Don’t try to act like you ain’t know where we been either, niggaWe in the club all the time, nigga, it’s about to pop off, niggaG-Unit\n\n\na-ha\nTake On Me\npop\nTalking awayI don’t know whatWhat to sayI’ll say it anywayToday is another day to find youShying awayOh, I’ll be coming for your love, okay? Take On Me (Take On Me)Take me on (Take On Me)I’ll be goneIn a day or two So needless to sayI’m odds and endsBut I’ll beStumbling awaySlowly learning that life is okaySay after meOh, it’s no better to be safe than sorry Take On Me (Take On Me)Take me on (Take On Me)I’ll be goneIn a day or two Oh, the things that you sayIs it life or just to playMy worries away?You’re all the things I’ve got to rememberYou’re shying awayI’ll be coming for you anyway Take On Me (Take On Me)Take me on (Take On Me)I’ll be goneIn a dayTake On Me (Take On Me)Take me on (Take On Me)I’ll be gone (Take On Me)In a day (Take me on, Take On Me)Take On Me (Take On Me)Take me on (Take On Me)\n\n\nABBA\nDancing Queen\npop\nYou can dance, you can jiveHaving the time of your life, oohSee that girl, watch that sceneDig in the Dancing Queen Friday night and the lights are lowLooking out for a place to goWhere they play the right music, getting in the swingYou come to look for a king Anybody could be that guyNight is young and the music’s highWith a bit of rock music, everything is fineYou’re in the mood for a dance And when you get the chance You are the Dancing QueenYoung and sweet, only seventeenDancing QueenFeel the beat from the tambourine, oh yeah You can dance, you can jiveHaving the time of your life, oohSee that girl, watch that sceneDig in the Dancing Queen You’re a teaser, you turn ’em onLeave them burning and then you’re goneLooking out for another, anyone will doYou’re in the mood for a dance And when you get the chance You are the Dancing QueenYoung and sweet, only seventeenDancing QueenFeel the beat from the tambourine, oh yeah You can dance, you can jiveHaving the time of your life, ohSee that girl, watch that sceneDig in the Dancing Queen Dig in the Dancing Queen\n\n\n\n\n\n\nAt this point, the only thing left to do to get this dataset to align with our idealized dataset structure is to organize the column ordering (using select()). I will also arrange the dataset alphabetically by genre and artist (using arrange()).\n\nlastfm &lt;- \n  lastfm |&gt; # original dataset\n  select(artist, song, lyrics, genre) |&gt; # order columns (and rename `text` to `lyrics`)\n  arrange(genre, artist) # arrange rows by `genre` and `artist`\n\nlastfm |&gt; # curated dataset\n  slice_head(n = 5) |&gt; # first 5 observations\n  knitr::kable(booktabs = TRUE) # print pretty table\n\n\n\nTable 6.7: Sample lyrics from curated Last.fm dataset.\n\n\n\n\n\n\n\nartist\nsong\nlyrics\ngenre\n\n\n\nAlan Jackson\nLittle Bitty\nHave a little love on a little honeymoonYou got a little dish and you got a little spoonA little bitty house and a little bitty yardA little bitty dog and a little bitty car Well, it’s alright to be little bittyA little hometown or a big old cityMight as well share, might as well smileLife goes on for a little bitty while A little bitty baby in a little bitty gownIt’ll grow up in a little bitty townA big yellow bus and little bitty booksIt all started with a little bitty look Well, it’s alright to be little bittyA little hometown or a big old cityMight as well share, might as well smileLife goes on for a little bitty while Heey You know you got a job and a little bitty checkA six pack of beer and a television setLittle bitty world goes around and aroundLittle bit of silence and a little bit of sound A good ol’ boy and a pretty little girlStart all over in a little bitty worldLittle bitty plan and a little bitty dreamIt’s all part of a little bitty schemeIt’s alright to be little bitty A little hometown or a big old cityMight as well share, might as well smileLife goes on for a little bitty whileIt’s alright to be little bittyA little hometown or a big old cityMight as well share, might as well smileLife goes on for a little bitty while Whoo\ncountry\n\n\nAlan Jackson\nRemember When\nRemember when I was young and so were youAnd time stood still and love was all we knewYou were the first so was IWe made love and then you criedRemember when Remember when we vowed the vowsAnd walk the walkGave our hearts, made the start and it was hardWe lived and learned, life threw curvesThere was joy, there was hurtRemember when Remember when old ones died and new were bornAnd life was changed, dissassembled, rearrangedWe came together, fell apartAnd broke each other’s heartsRemember when Remember when the sound of little feetWas the music we danced to week to weekBrought back the love, we found trustVowed we’d never give it upRemember when Remember when thirty seemed so oldNow,lookin’ back, it’s just a steppin’ stoneTo where we are, where we’ve beenSaid we’d do it all againRemember when Remember when we said when we turned grayWhen the children grow up and move awayWe won’t be sad, we’ll be gladFor all the life we’ve hadAnd we’ll remember when\ncountry\n\n\nBrad Paisley\nMud on the Tires\nI’ve got some big newsThe bank finally came throughAnd I’m holdin’ the keys to a brand new ChevroletHave you been outside, it sure is a nice nightHow about a little test driveDown by the lake? There’s a place I know about where the dirt road runs outAnd we can try out the four-wheel driveCome on now what do you sayGirl, I can hardly wait to get a little mud on the tires ‘Cause it’s a good nightTo be out there soakin’ up the moonlightStake out a little piece of shore lineI’ve got the perfect place in mindIt’s in the middle of nowhere only one way to get thereYou got to get a little mud on the tires Moonlight on a duck blindCatfish on a trout lineSun sets about nine this time of yearWe can throw a blanket downCrickets singin’ in the backgroundAnd more stars that you can count on a night this clear I tell you what we need to doIs grab a sleepin’ bag or twoAnd build us a little campfireAnd then with a little luck we might just get stuckLet’s get a little mud on the tires ‘Cause it’s a good nightTo be out there soakin’ up the moonlightStake out a little piece of shore lineI’ve got the perfect place in mind It’s in the middle of nowhere only one way to get thereYou got to get a little mud on the tiresAnd then with a little luck we might just get stuckLet’s get a little mud on the tires\ncountry\n\n\nCarrie Underwood\nBefore He Cheats\nRight now, he’s probably slow dancin’With a bleached-blond tramp and she’s probably gettin’ friskyRight now, he’s probably buyin’ her some fruity little drink’Cause she can’t shoot whiskeyRight now, he’s probably up behind her with a pool-stickShowin’ her how to shoot a comboAnd he don’t know I dug my key into the side of his pretty little souped-up four-wheel driveCarved my name into his leather seatsI took a Louisville slugger to both headlightsI slashed a hole in all four tiresMaybe next time he’ll think before he cheats Right now, she’s probably up singing someWhite-trash version of Shania karaokeRight now, she’s probably sayin’ ““I’m drunk”“And he’s a-thinkin’ that he’s gonna get luckyRight now, he’s probably dabbin’ onThree dollars worth of that bathroom PoloOh, and he don’t know That I dug my key into the side of his pretty little souped-up four-wheel driveCarved my name into his leather seatsI took a Louisville slugger to both headlightsI slashed a hole in all four tiresMaybe next time he’ll think before he cheats I might have saved a little trouble for the next girlA-’cause the next time that he cheatsOh, you know it won’t be on meNo, not on me ’Cause I dug my key into the side of his pretty little souped-up four-wheel driveCarved my name into his leather seatsI took a Louisville slugger to both headlightsI slashed a hole in all four tiresMaybe next time he’ll think before he cheats Oh, maybe next time he’ll think before he cheatsOh, before he cheatsOh-oh\ncountry\n\n\nDierks Bentley\nWhat Was I Thinkin’\nBecky was a beauty from south AlabamaHer daddy had a heart like a nine pound hammerThink he even did a little time in the slammerWhat was I thinkin’? She snuck out one night and met me by the front gateHer daddy came out wavin’ that 12-gaugeWe tore out the drive, he peppered my tailgateWhat was I thinkin’? Oh, I knew there’d be hell to payBut that crossed my mind a little too late ‘Cause I was thinkin’ ‘bout a little white tank topSittin’ right there in the middle by meI was thinkin’ ‘bout a long kissMan, just gotta get goin’ where the night might lead I know what I was feelin’But what was I thinkin’?What was I thinkin’? By the county line, the cops were nippin’ on our heelsPulled off the road and kicked it in four-wheelShut off the lights and tore through a cornfieldWhat was I thinkin’? Out the other side, she was hollerin’, ““Faster”“Took a dirt road, had the radio blastin’Hit the honky-tonk for a little close dancin’What was I thinkin’? Oh, I knew there’d be hell to payBut that crossed my mind a little too late ‘Cause I was thinkin’ ‘bout a little white tank topSittin’ right there in the middle by meI was thinkin’ ‘bout a long kissMan, just gotta get goin’ where the night might lead I know what I was feelin’But what was I thinkin’? When a mountain of a man with a”“Born to kill”” tattooTried to cut in, I knocked out his front toothWe ran outside, hood slidin’ like Bo DukeWhat was I thinkin’? I finally got her home at a half past two, laterDaddy’s in a lawn chair sittin’ on the drivewayPut it in park as he started my wayWhat was I thinkin’? Oh, what was I thinkin’?Oh, what was I thinkin’? Then she gave a ““Come and get me”” grinAnd like a bullet, we were gone again ‘Cause I was thinkin’ ‘bout a little white tank topSittin’ right there in the middle by meI was thinkin’ ‘bout a long kissMan, just gotta get goin’ where the night might leadI know what I was feelin’Yeah, I know what was I feelin’But what was I thinkin’? What was I thinkin’?I know what I was feelin’What was I thinkin’?Guess I was thinkin’ ’bout that tank topThose cutoffs\ncountry\n\n\n\n\n\n\n\n6.2.3 Write dataset\nWe now have a curated dataset that we can write to disk. Again, as with the Europarle Corpus dataset we curated before, we will write this dataset to the data/derived/ directory –effectively ensuring that it is clear that this dataset was created by our project work.\n\nfs::dir_create(path = \"../data/derived/lastfm/\") # create lastfm subdirectory\nwrite_csv(lastfm, \n          file = \"../data/derived/lastfm/lastfm_curated.csv\") # write lastfm to disk and label as the curated dataset\n\nAnd here’s an overview of our new directory structure.\ndata/\n├── derived/\n│   └── lastfm/\n│       └── lastfm_curated.csv\n└── original/\n    └── lastfm/\n        ├── README.md\n        ├── country.csv\n        ├── hip_hop.csv\n        ├── lyrics.csv\n        ├── metal.csv\n        ├── pop.csv\n        └── rock.csv\n\n6.2.4 Summary\nAgain, to summarize, here is the code that will accomplish the steps we covered in this section on curating structured datasets.\n\n# Read Last.fm lyrics and subset relevant columns\nlastfm &lt;- \n  readtext(file = \"../data/original/lastfm/*.csv\", # files to match using *.csv\n           text_field = \"lyrics\") |&gt; # text column from the datasets\n  as_tibble() |&gt; # convert to a tibble\n  select(-doc_id, -lyrics_url) # drop these columns\n\n# Collapse text by artist, song, and genre grouping\nlastfm &lt;- \n  lastfm |&gt; # dataset\n  group_by(artist, song, genre) |&gt; # grouping\n  summarise(lyrics = str_flatten(text, collapse = \" \")) |&gt;  # collapse text into the new column `lyrics` (dropping `text`)\n  ungroup() # unset the groupings\n\n# Order columns and arrange rows\nlastfm &lt;- \n  lastfm |&gt; # original dataset\n  select(artist, song, lyrics, genre) |&gt; # order columns (and rename `text` to `lyrics`)\n  arrange(genre, artist) # arrange rows by `genre` and `artist`\n\n# Write curated dataset to disk\nfs::dir_create(path = \"../data/derived/lastfm/\") # create lastfm subdirectory\nwrite_csv(lastfm, \n          file = \"../data/derived/lastfm/lastfm_curated.csv\") # write lastfm to disk and label as the curated dataset"
  },
  {
    "objectID": "curate-datasets.html#semi-structured",
    "href": "curate-datasets.html#semi-structured",
    "title": "6  Curate data(sets)",
    "section": "\n6.3 Semi-structured",
    "text": "6.3 Semi-structured\nAt this point we have discussed curating unstructured data and structured datasets. Between these two extremes falls semi-structured data. And as the name suggests, it is a hybrid between unstructured and structured data. This means that there will be important structured metadata included with unstructured elements. The file formats and approaches to encoding the structured aspects of the data vary widely from resource to resource and therefore often requires more detailed attention to the structure of the data and often includes more sophisticated programming strategies to curate the data to produce a tidy dataset.\nAs an example we will work with the The Switchboard Dialog Act Corpus (SDAC) which extends the Switchboard Corpus with speech act annotation. (ADD CITATION)\n\n\n\n\n\n\nTip\n\n\n\nThe SDAC dialogues (swb1_dialogact_annot.tar.gz) are available as a free download from the LDC. To download, decompress, and organize this resource, follow the strategies discussed in “Acquire data” for Direct Downloads. The tadr package provides the tadr::get_compressed_data() function to accomplish this step.\n\n\n\n6.3.1 Orientation\nThe main directory structure of the SDAC data looks like this:\ndata/\n├── derived/\n└── original/\n    └── sdac/\n        ├── README\n        ├── doc/\n        ├── sw00utt/\n        ├── sw01utt/\n        ├── sw02utt/\n        ├── sw03utt/\n        ├── sw04utt/\n        ├── sw05utt/\n        ├── sw06utt/\n        ├── sw07utt/\n        ├── sw08utt/\n        ├── sw09utt/\n        ├── sw10utt/\n        ├── sw11utt/\n        ├── sw12utt/\n        └── sw13utt/\nThe README file contains basic information about the resource, the doc/ directory contains more detailed information about the dialog annotations, and each of the following directories prefixed with sw... contain individual conversation files. Here’s a peek at internal structure of the first couple directories.\n├── README\n├── doc\n│   └── manual.august1.html\n├── sw00utt\n│   ├── sw_0001_4325.utt\n│   ├── sw_0002_4330.utt\n│   ├── sw_0003_4103.utt\n│   ├── sw_0004_4327.utt\n│   ├── sw_0005_4646.utt\nLet’s take a look at the first conversation file (sw_0001_4325.utt) to see how it is structured.\n\n\n&gt;\n*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n&gt;\n*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n&gt; *x* *x*\n&gt; *x* Copyright (C) 1995 University of Pennsylvania *x*\n&gt; *x* *x*\n&gt; *x* The data in this file are part of a preliminary version of the\n*x*\n&gt; *x* Penn Treebank Corpus and should not be redistributed.  Any *x*\n&gt; *x* research using this corpus or based on it should acknowledge *x*\n&gt; *x* that fact, as well as the preliminary nature of the corpus.  *x*\n&gt; *x* *x*\n&gt;\n*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n&gt;\n*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n&gt;\n&gt;\n&gt; FILENAME: 4325_1632_1519\n&gt; TOPIC#: 323\n&gt; DATE: 920323\n&gt; TRANSCRIBER: glp\n&gt; UTT_CODER: tc\n&gt; DIFFICULTY: 1\n&gt; TOPICALITY: 3\n&gt; NATURALNESS: 2\n&gt; ECHO_FROM_B: 1\n&gt; ECHO_FROM_A: 4\n&gt; STATIC_ON_A: 1\n&gt; STATIC_ON_B: 1\n&gt; BACKGROUND_A: 1\n&gt; BACKGROUND_B: 2\n&gt; REMARKS: None.\n&gt;\n&gt;\n=========================================================================\n&gt;\n&gt;\n&gt; o A.1 utt1: Okay.  /\n&gt; qw A.1 utt2: {D So, }\n&gt;\n&gt; qy^d B.2 utt1: [ [ I guess, +\n&gt;\n&gt; + A.3 utt1: What kind of experience [ do you, + do you ] have, then\nwith child care? /\n\n\nThere are few things to take note of here. First we see that the conversation files have a meta-data header offset from the conversation text by a line of = characters. Second the header contains meta-information of various types. Third, the text is interleaved with an annotation scheme.\nSome of the information may be readily understandable, such as the various pieces of meta-data in the header, but to get a better understanding of what information is encoded here let’s take a look at the README file. In this file we get a birds eye view of what is going on. In short, the data includes 1155 telephone conversations between two people annotated with 42 ‘DAMSL’ dialog act labels. The README file refers us to the doc/manual.august1.html file for more information on this scheme.\nAt this point we open the the doc/manual.august1.html file in a browser and do some investigation. We find out that ‘DAMSL’ stands for ‘Discourse Annotation and Markup System of Labeling’ and that the first characters of each line of the conversation text correspond to one or a combination of labels for each utterance. So for our first utterances we have:\no = \"Other\"\nqw = \"Wh-Question\"\nqy^d = \"Declarative Yes-No-Question\"\n+ = \"Segment (multi-utterance)\"\nEach utterance is also labeled for speaker (‘A’ or ‘B’), speaker turn (‘1’, ‘2’, ‘3’, etc.), and each utterance within that turn (‘utt1’, ‘utt2’, etc.). There is other annotation provided withing each utterance, but this should be enough to get us started on the conversations.\nNow let’s turn to the meta-data in the header. We see here that there is information about the creation of the file: ‘FILENAME’, ‘TOPIC’, ‘DATE’, etc. The doc/manual.august1.html file doesn’t have much to say about this information so I returned to the LDC Documentation and found more information in the Online Documentation section. After some poking around in this documentation I discovered that that meta-data for each speaker in the corpus is found in the caller_tab.csv file. This tabular file does not contain column names, but the caller_doc.txt does. After inspecting these files manually and comparing them with the information in the conversation file I noticed that the ‘FILENAME’ information contained three pieces of useful information delimited by underscores _.\n*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*\n\n\nFILENAME:   4325_1632_1519\nTOPIC#:     323\nDATE:       920323\nTRANSCRIBER:    glp\nThe first information is the document id (4325), the second and third correspond to the speaker number: the first being speaker A (1632) and the second speaker B (1519).\nIn sum, we have 1155 conversation files. Each file has two parts, a header and text section, separated by a line of = characters. The header section contains a ‘FILENAME’ line which has the document id, and ids for speaker A and speaker B. The text section is annotated with DAMSL tags beginning each line, followed by speaker, turn number, utterance number, and the utterance text. With this knowledge in hand, let’s set out to create a tidy dataset with the following column structure:\n\n\n\n\nTable 6.8: Idealized structure for the SDAC dataset.\n\n\n\n\n\n\n\n\n\n\ndoc_id\ndamsl_tag\nspeaker\nturn_num\nutterance_num\nutterance_text\nspeaker_id\n\n\n\n4325\no\nA\n1\n1\nOkay. /\n1632\n\n\n4325\nqw\nA\n1\n2\n{D So, }\n1632\n\n\n4325\nqy^d\nB\n2\n1\n[ [ I guess, +\n1519\n\n\n\n\n\n\n\n6.3.2 Tidy the data\nLet’s begin by reading one of the conversation files into R as a character vector using the read_lines() function from the readr package.\n\ndoc &lt;- \n  read_lines(file = \"../data/original/sdac/sw00utt/sw_0001_4325.utt\") # read a single file as character vector\n\nTo isolate the vector element that contains the document and speaker ids, we use str_detect() from the stringr package. This function takes two arguments, a string and a pattern, and returns a logical value, TRUE if the pattern is matched or FALSE if not. We can use the output of this function, then, to subset the doc character vector and only return the vector element (line) that contains digits_digits_digits with a regular expression. The expression combines the digit matching operator \\\\d with the + operator to match 1 or more contiguous digits. We then separate three groups of \\\\d+ with underscores _. The result is \\\\d+_\\\\d+_\\\\d+.\n\ndoc[str_detect(doc, pattern = \"\\\\d+_\\\\d+_\\\\d+\")] # isolate pattern\n\n#&gt; [1] \"FILENAME:\\t4325_1632_1519\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe stringr package has a handy function str_view() and str_view_all() which allow for interactive pattern matching. There is also an RStudio Addin with the regexplain package which also can be very helpful for developing regular expression syntax.\n\n\nThe next step is to extract the three digit sequences that correspond to the doc_id, speaker_a_id, and speaker_b_id. First we extract the pattern that we have identified with str_extract() and then we can break up the single character vector into multiple parts based on the underscore _. The str_split() function takes a string and then a pattern to use to split a character vector. It will return a list of character vectors.\n\ndoc[str_detect(doc, \"\\\\d+_\\\\d+_\\\\d+\")] |&gt; # isolate pattern\n  str_extract(pattern = \"\\\\d+_\\\\d+_\\\\d+\") |&gt; # extract the pattern\n  str_split(pattern = \"_\") # split the character vector\n\n#&gt; [[1]]\n#&gt; [1] \"4325\" \"1632\" \"1519\"\n\n\nA list is a special object type in R. It is an unordered collection of objects whose lengths can differ (contrast this with a data frame which is a collection of objects whose lengths are the same –hence the tabular format). In this case we have a list of length 1, whose sole element is a character vector of length 3 –one element per segment returned from our split. This is a desired result in most cases as if we were to pass multiple character vectors to our str_split() function we don’t want the results to be conflated as a single character vector blurring the distinction between the individual character vectors. If we would like to conflate, or flatten a list, we can use the unlist() function.\n\ndoc[str_detect(doc, \"\\\\d+_\\\\d+_\\\\d+\")] |&gt; # isolate pattern\n  str_extract(pattern = \"\\\\d+_\\\\d+_\\\\d+\") |&gt; # extract the pattern\n  str_split(pattern = \"_\") |&gt; # split the character vector\n  unlist() # flatten the list to a character vector\n\n#&gt; [1] \"4325\" \"1632\" \"1519\"\n\n\nLet’s flatten the list in this case, as we have a single character vector, and assign this result to doc_speaker_info.\n\ndoc_speaker_info &lt;- \n  doc[str_detect(doc, \"\\\\d+_\\\\d+_\\\\d+\")] |&gt; # isolate pattern\n  str_extract(pattern = \"\\\\d+_\\\\d+_\\\\d+\") |&gt; # extract the pattern\n  str_split(pattern = \"_\") |&gt;  # split the character vector\n  unlist() # flatten the list to a character vector\n\ndoc_speaker_info is now a character vector of length three. Let’s subset each of the elements and assign them to meaningful variable names so we can conveniently use them later on in the tidying process.\n\ndoc_id &lt;- doc_speaker_info[1] # extract by index\nspeaker_a_id &lt;- doc_speaker_info[2] # extract by index\nspeaker_b_id &lt;- doc_speaker_info[3] # extract by index\n\nThe next step is to isolate the text section extracting it from rest of the document. As noted previously, a sequence of = separates the header section from the text section. What we need to do is to index the point in our character vector doc where that line occurs and then subset the doc from that point until the end of the character vector. Let’s first find the point where the = sequence occurs. We will again use the str_detect() function to find the pattern we are looking for (a contiguous sequence of =), but then we will pass the logical result to the which() function which will return the element index number of this match.\n\ndoc |&gt; \n  str_detect(pattern = \"=+\") |&gt; # match 1 or more `=`\n  which() # find vector index\n\n#&gt; [1] 31\n\n\nSo for this file 31 is the index in doc where the = sequence occurs. Now it is important to keep in mind that we are working with a single file from the sdac/ data. We need to be cautious to not create a pattern that may be matched multiple times in another document in the corpus. As the =+ pattern will match =, or ==, or ===, etc. it is not implausible to believe that there might be a = character on some other line in one of the other files. Let’s update our regular expression to avoid this potential scenario by only matching sequences of three or more =. In this case we will make use of the curly bracket operators {}.\n\ndoc |&gt; \n  str_detect(pattern = \"={3,}\") |&gt; # match 3 or more `=`\n  which() # find vector index\n\n#&gt; [1] 31\n\n\nWe will get the same result for this file, but will safeguard ourselves a bit as it is unlikely we will find multiple matches for ===, ====, etc.\n31 is the index for the = sequence, but we want the next line to be where we start reading the text section. To do this we increment the index by 1.\n\ntext_start_index &lt;- \n  doc |&gt; \n  str_detect(pattern = \"={3,}\") |&gt; # match 3 or more `=` \n  which() # find vector index\ntext_start_index &lt;- text_start_index + 1 # increment index by 1\n\nThe index for the end of the text is simply the length of the doc vector. We can use the length() function to get this index.\n\ntext_end_index &lt;- length(doc)\n\nWe now have the bookends, so to speak, for our text section. To extract the text we subset the doc vector by these indices.\n\ntext &lt;- doc[text_start_index:text_end_index] # extract text\nhead(text) # preview first lines of `text`\n\n#&gt; [1] \"\"                                      \n#&gt; [2] \"\"                                      \n#&gt; [3] \"o          A.1 utt1: Okay.  /\"         \n#&gt; [4] \"qw          A.1 utt2: {D So, }\"        \n#&gt; [5] \"\"                                      \n#&gt; [6] \"qy^d          B.2 utt1: [ [ I guess, +\"\n\n\nThe text has some extra whitespace on some lines and there are blank lines as well. We should do some cleaning up before moving forward to organize the data. To get rid of the whitespace we use the str_trim() function which by default will remove leading and trailing whitespace from each line.\n\ntext &lt;- str_trim(text) # remove leading and trailing whitespace\nhead(text) # preview first lines of `text`\n\n#&gt; [1] \"\"                                      \n#&gt; [2] \"\"                                      \n#&gt; [3] \"o          A.1 utt1: Okay.  /\"         \n#&gt; [4] \"qw          A.1 utt2: {D So, }\"        \n#&gt; [5] \"\"                                      \n#&gt; [6] \"qy^d          B.2 utt1: [ [ I guess, +\"\n\n\nTo remove blank lines we will use the a logical expression to subset the text vector. text != \"\" means return TRUE where lines are not blank, and FALSE where they are.\n\ntext &lt;- text[text != \"\"] # remove blank lines\nhead(text) # preview first lines of `text`\n\n#&gt; [1] \"o          A.1 utt1: Okay.  /\"                                                                  \n#&gt; [2] \"qw          A.1 utt2: {D So, }\"                                                                 \n#&gt; [3] \"qy^d          B.2 utt1: [ [ I guess, +\"                                                         \n#&gt; [4] \"+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\"\n#&gt; [5] \"+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\"                        \n#&gt; [6] \"qy          A.5 utt1: Does it say something? /\"\n\n\nOur first step towards a tidy dataset is to now combine the doc_id and each element of text in a data frame.\n\ndata &lt;- data.frame(doc_id, text) # tidy format `doc_id` and `text`\nslice_head(data, n = 5) |&gt; # preview first lines of `text`\n  knitr::kable(booktabs = TRUE)\n\n\n\nTable 6.9: First 5 observations of prelim data curation of the SDAC data.\n\n\n\n\n\ndoc_id\ntext\n\n\n\n4325\no A.1 utt1: Okay. /\n\n\n4325\nqw A.1 utt2: {D So, }\n\n\n4325\nqy^d B.2 utt1: [ [ I guess, +\n\n\n4325\n+ A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\n\n\n4325\n+ B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\n\n\n\n\n\n\nWith our data now in a data frame, its time to parse the text column and extract the damsl tags, speaker, speaker turn, utterance number, and the utterance text itself into separate columns. To do this we will make extensive use of regular expressions. Our aim is to find a consistent pattern that distinguishes each piece of information from other other text in a given row of data$text and extract it.\nThe best way to learn regular expressions is to use them. To this end I’ve included a link to the interactive regular expression practice website regex101.\nOpen this site and copy the text below into the ‘TEST STRING’ field.\no          A.1 utt1: Okay.  /\nqw          A.1 utt2: {D So, }\nqy^d          B.2 utt1: [ [ I guess, +\n+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\n+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /\nqy          A.5 utt1: Does it say something? /\nsd          B.6 utt1: I think it usually does.  /\nad          B.6 utt2: You might try, {F uh, }  /\nh          B.6 utt3: I don't know,  /\nad          B.6 utt4: hold it down a little longer,  /\n\n\n\n\nFigure 6.1: RegEx101\n\n\n\nNow manually type the following regular expressions into the ‘REGULAR EXPRESSION’ field one-by-one (each is on a separate line). Notice what is matched as you type and when you’ve finished typing. You can find out exactly what the component parts of each expression are doing by toggling the top right icon in the window or hovering your mouse over the relevant parts of the expression.\n^.+?\\s\n[AB]\\.\\d+\nutt\\d+\n:.+$\nAs you can now see, we have regular expressions that will match the damsl tags, speaker and speaker turn, utterance number, and the utterance text. To apply these expressions to our data and extract this information into separate columns we will make use of the mutate() and str_extract() functions. mutate() will take our data frame and create new columns with values we match and extract from each row in the data frame with str_extract(). Notice that str_extract() is different than str_extract_all(). When we work with mutate() each row will be evaluated in turn, therefore we only need to make one match per row in data$text.\nI’ve chained each of these steps in the code below, dropping the original text column with select(-text), and overwriting data with the results.\n\n# Extract column information from `text`\ndata &lt;- \n  data |&gt; # current dataset\n  mutate(damsl_tag = str_extract(string = text, pattern = \"^.+?\\\\s\")) |&gt;  # extract damsl tags\n  mutate(speaker_turn = str_extract(string = text, pattern = \"[AB]\\\\.\\\\d+\")) |&gt; # extract speaker_turn pairs\n  mutate(utterance_num = str_extract(string = text, pattern = \"utt\\\\d+\")) |&gt; # extract utterance number\n  mutate(utterance_text = str_extract(string = text, pattern = \":.+$\")) |&gt;  # extract utterance text\n  select(-text) # drop the `text` column\n\nglimpse(data) # preview the data set\n\n#&gt; Rows: 159\n#&gt; Columns: 5\n#&gt; $ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n#&gt; $ damsl_tag      &lt;chr&gt; \"o \", \"qw \", \"qy^d \", \"+ \", \"+ \", \"qy \", \"sd \", \"ad \", …\n#&gt; $ speaker_turn   &lt;chr&gt; \"A.1\", \"A.1\", \"B.2\", \"A.3\", \"B.4\", \"A.5\", \"B.6\", \"B.6\",…\n#&gt; $ utterance_num  &lt;chr&gt; \"utt1\", \"utt2\", \"utt1\", \"utt1\", \"utt1\", \"utt1\", \"utt1\",…\n#&gt; $ utterance_text &lt;chr&gt; \": Okay.  /\", \": {D So, }\", \": [ [ I guess, +\", \": What…\n\n\n\n\n\n\n\n\nTip\n\n\n\nOne twist you will notice is that regular expressions in R require double backslashes (\\\\\\\\) where other programming environments use a single backslash (\\\\).\n\n\nThere are a couple things left to do to the columns we extracted from the text before we move on to finishing up our tidy dataset. First, we need to separate the speaker_turn column into speaker and turn_num columns and second we need to remove unwanted characters from the damsl_tag, utterance_num, and utterance_text columns.\nTo separate the values of a column into two columns we use the separate() function. It takes a column to separate and character vector of the names of the new columns to create. By default the values of the input column will be separated by non-alphanumeric characters. In our case this means the . will be our separator.\n\ndata &lt;-\n  data |&gt; # current dataset\n  separate(col = speaker_turn, # source column\n           into = c(\"speaker\", \"turn_num\")) # separate speaker_turn into distinct columns: speaker and turn_num\n\nglimpse(data) # preview the data set\n\n#&gt; Rows: 159\n#&gt; Columns: 6\n#&gt; $ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n#&gt; $ damsl_tag      &lt;chr&gt; \"o \", \"qw \", \"qy^d \", \"+ \", \"+ \", \"qy \", \"sd \", \"ad \", …\n#&gt; $ speaker        &lt;chr&gt; \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n#&gt; $ turn_num       &lt;chr&gt; \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n#&gt; $ utterance_num  &lt;chr&gt; \"utt1\", \"utt2\", \"utt1\", \"utt1\", \"utt1\", \"utt1\", \"utt1\",…\n#&gt; $ utterance_text &lt;chr&gt; \": Okay.  /\", \": {D So, }\", \": [ [ I guess, +\", \": What…\n\n\nTo remove unwanted leading or trailing whitespace we apply the str_trim() function. For removing other characters we matching the character(s) and replace them with an empty string (\"\") with the str_replace() function. Again, I’ve chained these functions together and overwritten data with the results.\n\n# Clean up column information\ndata &lt;- \n  data |&gt; # current dataset\n  mutate(damsl_tag = str_trim(damsl_tag)) |&gt; # remove leading/ trailing whitespace\n  mutate(utterance_num = str_replace(string = utterance_num, pattern = \"utt\", replacement = \"\")) |&gt; # remove 'utt'\n  mutate(utterance_text = str_replace(string = utterance_text, pattern = \":\\\\s\", replacement = \"\")) |&gt; # remove ': '\n  mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace\n\nglimpse(data) # preview the data set\n\n#&gt; Rows: 159\n#&gt; Columns: 6\n#&gt; $ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n#&gt; $ damsl_tag      &lt;chr&gt; \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n#&gt; $ speaker        &lt;chr&gt; \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n#&gt; $ turn_num       &lt;chr&gt; \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n#&gt; $ utterance_num  &lt;chr&gt; \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n#&gt; $ utterance_text &lt;chr&gt; \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n\n\nTo round out our tidy dataset for this single conversation file we will connect the speaker_a_id and speaker_b_id with speaker A and B in our current dataset adding a new column speaker_id. The case_when() function does exactly this: allows us to map rows of speaker with the value “A” to speaker_a_id and rows with value “B” to speaker_b_id.\n\n# Link speaker with speaker_id\ndata &lt;- \n  data |&gt; # current dataset\n  mutate(speaker_id = case_when( # create speaker_id\n    speaker == \"A\" ~ speaker_a_id, # speaker_a_id value when A\n    speaker == \"B\" ~ speaker_b_id # speaker_b_id value when B\n  ))\n\nglimpse(data) # preview the data set\n\n#&gt; Rows: 159\n#&gt; Columns: 7\n#&gt; $ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n#&gt; $ damsl_tag      &lt;chr&gt; \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n#&gt; $ speaker        &lt;chr&gt; \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n#&gt; $ turn_num       &lt;chr&gt; \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n#&gt; $ utterance_num  &lt;chr&gt; \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n#&gt; $ utterance_text &lt;chr&gt; \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n#&gt; $ speaker_id     &lt;chr&gt; \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…\n\n\nWe now have the tidy dataset we set out to create. But this dataset only includes one conversation file! We want to apply this code to all 1155 conversation files in the sdac/ corpus. The approach will be to create a custom function which groups the code we’ve done for this single file and then iterative send each file from the corpus through this function and combine the results into one data frame.\nHere’s the custom function with some extra code to print a progress message for each file when it runs.\n\nextract_sdac_metadata &lt;- function(file) {\n  # Function: to read a Switchboard Corpus Dialogue file and extract meta-data\n  cat(\"Reading\", basename(file), \"...\")\n  \n  # Read `file` by lines\n  doc &lt;- read_lines(file) \n  \n  # Extract `doc_id`, `speaker_a_id`, and `speaker_b_id`\n  doc_speaker_info &lt;- \n    doc[str_detect(doc, \"\\\\d+_\\\\d+_\\\\d+\")] |&gt; # isolate pattern\n    str_extract(\"\\\\d+_\\\\d+_\\\\d+\") |&gt; # extract the pattern\n    str_split(pattern = \"_\") |&gt; # split the character vector\n    unlist() # flatten the list to a character vector\n  doc_id &lt;- doc_speaker_info[1] # extract `doc_id`\n  speaker_a_id &lt;- doc_speaker_info[2] # extract `speaker_a_id`\n  speaker_b_id &lt;- doc_speaker_info[3] # extract `speaker_b_id`\n  \n  # Extract `text`\n  text_start_index &lt;- # find where header info stops\n    doc |&gt; \n    str_detect(pattern = \"={3,}\") |&gt; # match 3 or more `=`\n    which() # find vector index\n  \n  text_start_index &lt;- text_start_index + 1 # increment index by 1\n  text_end_index &lt;- length(doc) # get the end of the text section\n  \n  text &lt;- doc[text_start_index:text_end_index] # extract text\n  text &lt;- str_trim(text) # remove leading and trailing whitespace\n  text &lt;- text[text != \"\"] # remove blank lines\n  \n  data &lt;- data.frame(doc_id, text) # tidy format `doc_id` and `text`\n  \n  # Extract column information from `text`\n  data &lt;- \n    data |&gt; \n    mutate(damsl_tag = str_extract(string = text, pattern = \"^.+?\\\\s\")) |&gt;  # extract damsl tags\n    mutate(speaker_turn = str_extract(string = text, pattern = \"[AB]\\\\.\\\\d+\")) |&gt; # extract speaker_turn pairs\n    mutate(utterance_num = str_extract(string = text, pattern = \"utt\\\\d+\")) |&gt; # extract utterance number\n    mutate(utterance_text = str_extract(string = text, pattern = \":.+$\")) |&gt;  # extract utterance text\n    select(-text)\n  \n  # Separate speaker_turn into distinct columns\n  data &lt;-\n    data |&gt; # current dataset\n    separate(col = speaker_turn, # source column\n             into = c(\"speaker\", \"turn_num\")) # separate speaker_turn into distinct columns: speaker and turn_num\n  \n  # Clean up column information\n  data &lt;- \n    data |&gt; \n    mutate(damsl_tag = str_trim(damsl_tag)) |&gt; # remove leading/ trailing whitespace\n    mutate(utterance_num = str_replace(string = utterance_num, pattern = \"utt\", replacement = \"\")) |&gt; # remove 'utt'\n    mutate(utterance_text = str_replace(string = utterance_text, pattern = \":\\\\s\", replacement = \"\")) |&gt; # remove ': '\n    mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace\n  \n  # Link speaker with speaker_id\n  data &lt;- \n    data |&gt; # current dataset\n    mutate(speaker_id = case_when( # create speaker_id\n      speaker == \"A\" ~ speaker_a_id, # speaker_a_id value when A\n      speaker == \"B\" ~ speaker_b_id # speaker_b_id value when B\n    ))\n  cat(\" done.\\n\")\n  return(data) # return the data frame object\n}\n\nAs a sanity check we will run the extract_sdac_metadata() function on a the conversation file we were just working on to make sure it works as expected.\n\nextract_sdac_metadata(file = \"../data/original/sdac/sw00utt/sw_0001_4325.utt\") |&gt; \n  glimpse()\n\n\n\n#&gt; Reading sw_0001_4325.utt ... done.\n#&gt; Rows: 159\n#&gt; Columns: 7\n#&gt; $ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n#&gt; $ damsl_tag      &lt;chr&gt; \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n#&gt; $ speaker        &lt;chr&gt; \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n#&gt; $ turn_num       &lt;chr&gt; \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n#&gt; $ utterance_num  &lt;chr&gt; \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n#&gt; $ utterance_text &lt;chr&gt; \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n#&gt; $ speaker_id     &lt;chr&gt; \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…\n\n\nLooks good!\nSo now it’s time to create a vector with the paths to all of the conversation files. fs::dir_ls() interfaces with our OS file system and will return the paths to the files in the specified directory. We also add a pattern to match conversation files (regexp = \\\\.utt$) so we don’t accidentally include other files in the corpus. recurse set to TRUE means we will get the full path to each file.\n\nsdac_files &lt;- \n  fs::dir_ls(path = \"../data/original/sdac/\", # source directory\n             recurse = TRUE, # traverse all sub-directories\n             type = \"file\", # only return files\n             regexp = \"\\\\.utt$\") # only return files ending in .utt\nhead(sdac_files) # preview file paths\n\n../data/original/sdac/sw00utt/sw_0001_4325.utt\n../data/original/sdac/sw00utt/sw_0002_4330.utt\n../data/original/sdac/sw00utt/sw_0003_4103.utt\n../data/original/sdac/sw00utt/sw_0004_4327.utt\n../data/original/sdac/sw00utt/sw_0005_4646.utt\n../data/original/sdac/sw00utt/sw_0006_4108.utt\no pass each conversation file in the vector of paths to our conversation files iteratively to the extract_sdac_metadata() function we use map(). This will apply the function to each conversation file and return a data frame for each. bind_rows() will then join the resulting data frames by rows to give us a single tidy dataset for all 1155 conversations. Note there is a lot of processing going on here we have to be patient.\n\n# Read files and return a tidy dataset\nsdac &lt;- \n  sdac_files |&gt; # pass file names\n  map(extract_sdac_metadata) |&gt; # read and tidy iteratively \n  bind_rows() # bind the results into a single data frame\n\nWe now see that we have 223606 observations (individual utterances in this dataset).\n\nglimpse(sdac) # preview complete curated dataset\n\n#&gt; Rows: 223,606\n#&gt; Columns: 7\n#&gt; $ doc_id         &lt;chr&gt; \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\", \"4325\",…\n#&gt; $ damsl_tag      &lt;chr&gt; \"o\", \"qw\", \"qy^d\", \"+\", \"+\", \"qy\", \"sd\", \"ad\", \"h\", \"ad…\n#&gt; $ speaker        &lt;chr&gt; \"A\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n#&gt; $ turn_num       &lt;chr&gt; \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"6\", \"6\", \"6\", \"6\", …\n#&gt; $ utterance_num  &lt;chr&gt; \"1\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"3\", \"4\", \"5\", …\n#&gt; $ utterance_text &lt;chr&gt; \"Okay.  /\", \"{D So, }\", \"[ [ I guess, +\", \"What kind of…\n#&gt; $ speaker_id     &lt;chr&gt; \"1632\", \"1632\", \"1519\", \"1632\", \"1519\", \"1632\", \"1519\",…\n\n\n\n6.3.3 Write datasets\nAgain as in the previous cases, we will write this dataset to disk to prepare for the next step in our text analysis project.\n\nfs::dir_create(path = \"../data/derived/sdac/\") # create sdac subdirectory\nwrite_csv(sdac, \n          file = \"../data/derived/sdac/sdac_curated.csv\") # write sdac to disk and label as the curated dataset\n\nThe directory structure now looks like this:\ndata/\n├── derived/\n│   └── sdac/\n│       └── sdac_curated.csv\n└── original/\n    └── sdac/\n        ├── README\n        ├── doc/\n        ├── sw00utt/\n        ├── sw01utt/\n        ├── sw02utt/\n        ├── sw03utt/\n        ├── sw04utt/\n        ├── sw05utt/\n        ├── sw06utt/\n        ├── sw07utt/\n        ├── sw08utt/\n        ├── sw09utt/\n        ├── sw10utt/\n        ├── sw11utt/\n        ├── sw12utt/\n        └── sw13utt/\n\n6.3.4 Summary\nIn this section we looked at semi-structured data. This type of data often requires the most work to organize into a tidy dataset. We continued to work with many of the R programming strategies introduced to this point in the coursebook. We also made more extensive use of regular expressions to pick out information from a semi-structured document format.\nTo round out this section I’ve provided a code summary of the steps involved to conduct the curation of the Switchboard Dialogue Act Corpus files. Note that I’ve added the extract_sdac_metadata() custom function to a file called curate_functions.R and sourced this file. This will make the code more succinct and legible here, as well in your own research projects.\n\n# Source the `extract_sdac_metadata()` function\nsource(\"../functions/curate_functions.R\") \n\n# Get list of the corpus files (.utt)\nsdac_files &lt;- \n  fs::dir_ls(path = \"../data/original/sdac/\", # source directory\n             recurse = TRUE, # traverse all sub-directories\n             type = \"file\", # only return files\n             regexp = \"\\\\.utt$\") # only return files ending in .utt\n\n# Read files and return a tidy dataset\nsdac &lt;- \n  sdac_files |&gt; # pass file names\n  map(extract_sdac_metadata) |&gt; # read and tidy iteratively \n  bind_rows() # bind the results into a single data frame\n  \n# Write curated dataset to disk\nfs::dir_create(path = \"../data/derived/sdac/\") # create sdac subdirectory\nwrite_csv(sdac, \n          file = \"../data/derived/sdac/sdac_curated.csv\") # write sdac to disk and label as the curated dataset"
  },
  {
    "objectID": "curate-datasets.html#documentation",
    "href": "curate-datasets.html#documentation",
    "title": "6  Curate data(sets)",
    "section": "\n6.4 Documentation",
    "text": "6.4 Documentation\nAt this stage we again want to ensure that the data that we have derived is well-documented. Where in the data acquisition process the documentation was focused on the sampling frame, curated datasets require documentation that describes the structure of the now rectangular dataset and its attributes. This documentation is known as a data dictionary. At the curation stage this documentation often contains the following information (“How to Make a Data Dictionary” 2021):\n\nnames of the variables (as they appear in the dataset)\nhuman-readable names for the variables\nshort prose descriptions of the variables, including units of measurement (where applicable)\n\nA data dictionary will take the format of a table and can be stored in a tabular-oriented file format (such as .csv). It is often easier to work with a spreadsheet to create this documentation. I suggest creating a .csv file with the basic structure of the documentation. You can do this however you choose, but I suggest using something along these lines as seen in the following custom function, data_dic_starter().\n\ndata_dic_starter &lt;- function(data, file_path) {\n  # Function:\n  # Creates a .csv file with the basic information\n  # to document a curated dataset\n  \n  tibble(variable_name = names(data), # column with existing variable names \n       name = \"\", # column for human-readable names\n       description = \"\") |&gt; # column for prose description\n  write_csv(file = file_path) # write to disk\n}\n\nRunning this function in the R Console on the curated dataset (in this case the sdac dataset), will provide this structure.\n\n\n\n\nTable 6.10: Data dictionary starter structure for the SDAC curated dataset.\n\nvariable_name\nname\ndescription\n\n\n\ndoc_id\n\n\n\n\ndamsl_tag\n\n\n\n\nspeaker\n\n\n\n\nturn_num\n\n\n\n\nutterance_num\n\n\n\n\nutterance_text\n\n\n\n\nspeaker_id\n\n\n\n\n\n\n\n\nThe resulting .csv file can then be opened with spreadsheet software (such as MS Excel, Google Sheets, etc.) and edited.1\n\n\n\n\n\n\n\n\nSave this file as a .csv file and replace the original starter file. Note that it is important to use a plain-text file format for the official documentation file and avoid proprietary formats to ensure open accessibility and future compatibility.2\nOur data/derived/ directory now looks like this.\ndata/\n└── derived/\n    └── sdac/\n        ├── sdac_curated.csv\n        └── data_dictionary_sdac.csv"
  },
  {
    "objectID": "curate-datasets.html#summary-3",
    "href": "curate-datasets.html#summary-3",
    "title": "6  Curate data(sets)",
    "section": "Summary",
    "text": "Summary\nIn this chapter we looked at the process of structuring data into a dataset. This included a discussion on three main types of data –unstructured, structured, and semi-structured. The level of structure of the original data(set) will vary from resource to resource and by the same token so will the file format used to support the level of meta-information included. The results from our data curation resulted in a curated dataset that is saved separate from the original data to maintain modularity between what the data(set) looked like before we intervene and afterwards. In addition to the code we use to derived the curated dataset’s structure, we also include a data dictionary which documents the names of the variables and provides sufficient description of these variables so that it is clear what our dataset contains.\nIt is important to recognized that this curated dataset will form the base for the next step in our text analysis project and the last step in data preparation for analysis: dataset transformation. This last step in preparing data for analysis is to convert this curated dataset into a dataset that is directly aligned with the research aims (i.e. analysis method(s)) of the project. Since there can be multiple analysis approaches applied the original data in a research project, this curated dataset serves as the point of departure for each of the subsequent datasets derived from the transformational steps."
  },
  {
    "objectID": "curate-datasets.html#activities",
    "href": "curate-datasets.html#activities",
    "title": "6  Curate data(sets)",
    "section": "Activities",
    "text": "Activities\n\n\n\n\n\n\nRecipe\n\n\n\nWhat: Regular Expressions and reshaping datasetsHow: Read Recipe 7 and participate in the Hypothes.is online social annotation.Why: To how regular expressions are helpful in developing strategies for matching, extracting, and/ or replacing patterns in character sequences and how to change the dimensions of a dataset to either expand or collapse columns or rows.\n\n\n\n\n\n\n\n\nLab\n\n\n\nWhat: Regular Expressions and reshaping datasetsHow: Clone, fork, and complete the steps in Lab 7.Why: To gain experience working with coding strategies reshaping data using tidyverse functions and regular expressions, to practice reading/ writing data from/ to disk, and to implement organizational strategies for organizing and documenting a dataset in reproducible fashion."
  },
  {
    "objectID": "curate-datasets.html#questions",
    "href": "curate-datasets.html#questions",
    "title": "6  Curate data(sets)",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\nConceptual questions\n\n\n\n\n…\n…\n\n\n\n\n\n\n\n\n\nTechnical exercises\n\n\n\n\n…\n…\n\n\n\n\n\n\n\nBroman, Karl W., and Kara H. Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10. https://doi.org/10.1080/00031305.2017.1375989.\n\n\n“How to Make a Data Dictionary.” 2021. OSF Guides. https://help.osf.io/hc/en-us/articles/360019739054-How-to-Make-a-Data-Dictionary."
  },
  {
    "objectID": "curate-datasets.html#footnotes",
    "href": "curate-datasets.html#footnotes",
    "title": "6  Curate data(sets)",
    "section": "",
    "text": "Note on RStudio Cloud you will need to download the .csv file and, after editing, upload the complete data dictionary file. Make sure to save the edited file as a .csv file.↩︎\nAlthough based on spreadsheets, Broman and Woo (2018) outlines many of the best for good data organization regardless of the technology.↩︎"
  },
  {
    "objectID": "transform-datasets.html#normalize",
    "href": "transform-datasets.html#normalize",
    "title": "7  Transform datasets",
    "section": "\n7.1 Normalize",
    "text": "7.1 Normalize\n\nThe process of normalizing datasets in essence is to santize the values of variable or set of variables such that there are no artifacts that will contaminate subsequent processing. It may be the case that non-linguistic metadata may require normalization but more often than not linguistic information is the most common target for normalization as text often includes artifacts from the acquisition process which will not be desired in the analysis.\nEuroparle Corpus\nConsider the curated Europarle Corpus dataset. I will read in the dataset. Since the dataset is quite large, I have also subsetted the dataset keeping only the first 1,000 observations for each of value of type for demonstration purposes.\n\neuroparle &lt;- read_csv(file = \"../data/derived/europarle/europarle_curated.csv\") |&gt;  # read curated dataset\n  filter(sentence_id &lt; 1001) # keep first 1000 observations for each type\n\nglimpse(europarle)\n\n\n\n#&gt; Rows: 2,000\n#&gt; Columns: 3\n#&gt; $ type        &lt;chr&gt; \"Source\", \"Target\", \"Source\", \"Target\", \"Source\", \"Target\"…\n#&gt; $ sentence_id &lt;dbl&gt; 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, …\n#&gt; $ sentence    &lt;chr&gt; \"Reanudación del período de sesiones\", \"Resumption of the …\n\n\nSimply looking at the first 14 lines of this dataset, we can see that if our goal is to work with the transcribed (‘Source’) and translated (‘Target’) language, there are lines which do not appear to be of interest.\n\n\n\n\nTable 7.1: Europarle Corpus curated dataset preview.\n\n\n\n\n\n\ntype\nsentence_id\nsentence\n\n\n\nSource\n1\nReanudación del período de sesiones\n\n\nTarget\n1\nResumption of the session\n\n\nSource\n2\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\n\n\nTarget\n2\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n\n\nSource\n3\nComo todos han podido comprobar, el gran “efecto del año 2000” no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\n\n\nTarget\n3\nAlthough, as you will have seen, the dreaded ‘millennium bug’ failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n\n\nSource\n4\nSus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\n\n\nTarget\n4\nYou have requested a debate on this subject in the course of the next few days, during this part-session.\n\n\nSource\n5\nA la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\n\n\nTarget\n5\nIn the meantime, I should like to observe a minute’ s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n\n\nSource\n6\nInvito a todos a que nos pongamos de pie para guardar un minuto de silencio.\n\n\nTarget\n6\nPlease rise, then, for this minute’ s silence.\n\n\nSource\n7\n(El Parlamento, de pie, guarda un minuto de silencio)\n\n\nTarget\n7\n(The House rose and observed a minute’ s silence)\n\n\n\n\n\n\nsentence_id 1 appears to be title and sentence_id 7 reflects description of the parliamentary session. Both of these are artifacts that we would like to remove from the dataset.\nTo remove these lines we can turn to the programming strategies we’ve previously worked with. Namely we will use filter() to filter observations in combination with str_detect() to detect matches for some pattern that is indicative of these lines that we want to remove and not of the other lines that we want to keep.\nBefore we remove any lines, let’s try craft a search pattern to identify these lines, and exclude the lines we will want to keep. Condition one is lines which start with an opening parenthesis (. Condition two is lines that do not end in standard sentence punctuation (., !, or ?). I’ve added both conditions to one filter() using the logical OR operator (|) to ensure that either condition is matched in the output.\n\n# Identify non-speech lines\neuroparle |&gt; \n  filter(str_detect(sentence, \"^\\\\(\") | str_detect(sentence, \"[^.!?]$\")) |&gt; # filter lines that detect a match for either condition 1 or 2\n  slice_sample(n = 10) |&gt; # random sample of 10 observations\n  knitr::kable(booktabs = TRUE)\n\n\n\nTable 7.2: Non-speech lines in the Europarle dataset.\n\n\n\n\n\n\ntype\nsentence_id\nsentence\n\n\n\nSource\n669\n(El Acta queda aprobada)\n\n\nSource\n66\nOrden de los trabajos\n\n\nSource\n672\nA5-0069/1999 del Sr. von Wogau, en nombre de la Comisión de Asuntos Económicos y Monetarios sobre el Libro Blanco de la Comisión sobre la modernización de las normas de aplicación de los artículos 85 y 86 del Tratado CE (COM(1999) 101 - C5-0105/1999 - 1999/2108(COS));\n\n\nTarget\n69\nRelating to Wednesday:\n\n\nTarget\n672\nThe next item is the joint debate on the following reports:\n\n\nTarget\n675\nA5-0087/1999 by Mr Jonckheer, on behalf of the Committee on Economic and Monetary Affairs, on the seventh survey on state aid in the European Union in the manufacturing and certain other sectors. [COM(1999) 148 - C5-0107/1999 - 1999/2110(COS)] (Report 1995-1997);\n\n\nSource\n110\n(El Parlamento rechaza la propuesta por 164 votos a favor, 166 votos en contra y 7 abstenciones)\n\n\nSource\n671\nDe conformidad con el orden del día, se procede al debate conjunto de los siguientes informes:\n\n\nSource\n673\nA5-0078/1999 del Sr. Rapkay, en nombre de la Comisión de Asuntos Económicos y Monetarios, sobre el XXVIII Informe de la Comisión Europea sobre la política de competencia - 1998 (SEC(1999) 743 - C5-121/1999 - 1999/2124(COS));\n\n\nSource\n69\nMiércoles :\n\n\n\n\n\n\nSince this search appears to match lines that we do not want to preserve, let’s move now to eliminate these lines from the dataset. To do this we will use the same regular expression patterns, but now each condition will have it’s own filter() call and the str_detect() will be negated with a prefixed !.\n\neuroparle &lt;- \n  europarle |&gt; # dataset\n  filter(!str_detect(sentence, pattern = \"^\\\\(\")) |&gt; # remove lines starting with (\n  filter(!str_detect(sentence, pattern = \"[^.!?]$\")) # remove lines not ending in ., !, or ?\n\nLet’s look at the first 14 lines again, now that we have eliminated these artifacts.\n\n\n\n\nTable 7.3: Europarle Corpus non-speech lines removed.\n\n\n\n\n\n\ntype\nsentence_id\nsentence\n\n\n\nSource\n2\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\n\n\nTarget\n2\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n\n\nSource\n3\nComo todos han podido comprobar, el gran “efecto del año 2000” no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\n\n\nTarget\n3\nAlthough, as you will have seen, the dreaded ‘millennium bug’ failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n\n\nSource\n4\nSus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\n\n\nTarget\n4\nYou have requested a debate on this subject in the course of the next few days, during this part-session.\n\n\nSource\n5\nA la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\n\n\nTarget\n5\nIn the meantime, I should like to observe a minute’ s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n\n\nSource\n6\nInvito a todos a que nos pongamos de pie para guardar un minuto de silencio.\n\n\nTarget\n6\nPlease rise, then, for this minute’ s silence.\n\n\nSource\n8\nSeñora Presidenta, una cuestión de procedimiento.\n\n\nTarget\n8\nMadam President, on a point of order.\n\n\nSource\n9\nSabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en Sri Lanka.\n\n\nTarget\n9\nYou will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n\n\n\n\n\n\nOne further issue that we may want to resolve concerns the fact that there are whitespaces between possessive forms (i.e. “minute’ s silence”). In this case we can employ str_replace_all() inside the mutate() function to overwrite the sentence values that match an apostrophe ' with whitespace (\\\\s) before s.\n\neuroparle &lt;- \n  europarle |&gt; # dataset\n  mutate(sentence = str_replace_all(string = sentence, \n                                    pattern = \"'\\\\ss\", \n                                    replacement = \"'s\")) # replace ' s with `s\n\nNow we have normalized text in the sentence column in the Europarle dataset.\nLast FM Lyrics\n\nLet’s look at another dataset we have worked with during this coursebook: the Lastfm lyrics. Reading in the lastfm_curated dataset from the data/derived/ directory we can see the structure for the curated structure.\n\nlastfm &lt;- read_csv(file = \"../data/derived/lastfm/lastfm_curated.csv\") # read in lastfm_curated dataset\n\n\n\n\n\nTable 7.4: Last fm lyrics dataset preview with one artist/ song per genre and the lyrics text truncated at 200 characters for display purposes.\n\n\n\n\n\n\n\nartist\nsong\nlyrics\ngenre\n\n\n\nAlan Jackson\nLittle Bitty\nHave a little love on a little honeymoonYou got a little dish and you got a little spoonA little bitty house and a little bitty yardA little bitty dog and a little bitty car Well, it’s alright to b…\ncountry\n\n\n50 Cent\nIn Da Club\nGo, go, go, go, go, goGo, shortyIt’s your birthdayWe gon’ party like it’s your birthdayWe gon’ sip Bacardi like it’s your birthdayAnd you know we don’t give a fuck it’s not your birthday You can fi…\nhip-hop\n\n\nBlack Sabbath\nParanoid\nFinished with my woman’Cause she couldn’t help me with my mindPeople think I’m insaneBecause I am frowning all the time All day long, I think of thingsBut nothing seems to satisfyThink I’ll lose my…\nmetal\n\n\na-ha\nTake On Me\nTalking awayI don’t know whatWhat to sayI’ll say it anywayToday is another day to find youShying awayOh, I’ll be coming for your love, okay? Take On Me (Take On Me)Take me on (Take On Me)I’ll be go…\npop\n\n\n3 Doors Down\nHere Without You\nA hundred days have made me olderSince the last time that I saw your pretty faceA thousand lies have made me colderAnd I don’t think I can look at this the same But all the miles that separateDisap…\nrock\n\n\n\n\n\n\nThere are a few things that we might want to clean out of the lyrics column’s values. First, there are lines from the original webscrape where the end of one stanza runs into the next without whitespace between them (i.e. “honeymoonYou”). These reflect contiguous end-new line segments where stanzas were joined in the curation process. Second, we see that there are what appear to be backing vocals which appear between parentheses (i.e. “(Take On Me)”).\nIn both cases we will use mutate(). With contiguous end-new line segments we will use str_replace_all() inside and for backing vocals in parentheses we will use str_remove_all().\nThe pattern to match for end-new lines from the stanzas will use some regular expression magic. The base pattern includes finding a pair of lowercase-uppercase letters (i.e. “nY”, in “honeymoonYou”). For this we can use the pattern [a-z][A-Z]. To replace this pattern using the lowercase letter then a space and then the uppercase letter we take advantage of the grouping syntax in regular expressions (...). So we add parentheses around the two groups to capture like this ([a-z])([A-Z]). In the replacement argument of the str_replace_all() function we then specify to use the captured groups in the order they appear \\\\1 for the lowercase letter match and \\\\2 for the uppercase letter match.\nNow, I’ve looked more extensively at the lyrics column and found that there are other combinations that are joined between stanzas. Namely that ', !, ,, ), ?, and I also may precede the uppercase letter. To make sure we capture these possibilities as well I’ve updated the regular expression to ([a-z'!,.)?I])([A-Z]).\nNow to remove the backing vocals, the regex pattern is \\\\(.+?\\\\) –match the parentheses and everything within the parentheses. The added ? after the + operator is what is known as a ‘lazy’ operator. This specifies that the .+ will match the minimal string that is enclosed by the trailing ). If we did not include this then we would get matches that span from the first parenthesis ( all the way to the last, which would match real lyrics, not just the backing vocals.\nPutting this to work let’s clean the lyrics column.\n\nlastfm &lt;- \n  lastfm |&gt; # dataset\n  mutate(lyrics = \n           str_replace_all(string = lyrics, \n                           pattern = \"([a-z'!,.)?I])([A-Z])\", # find contiguous end/ new line segments\n                           replacement = \"\\\\1 \\\\2\")) |&gt;  # replace with whitespace between\n  mutate(lyrics = str_remove_all(lyrics, \"\\\\(.+?\\\\)\")) # remove backing vocals (Take On Me)\n\n\n\n\n\nTable 7.5: Last fm lyrics with cleaned lyrics…\n\n\n\n\n\n\n\nartist\nsong\nlyrics\ngenre\n\n\n\nAlan Jackson\nLittle Bitty\nHave a little love on a little honeymoon You got a little dish and you got a little spoon A little bitty house and a little bitty yard A little bitty dog and a little bitty car Well, it’s alright t…\ncountry\n\n\n50 Cent\nIn Da Club\nGo, go, go, go, go, go Go, shorty It’s your birthday We gon’ party like it’s your birthday We gon’ sip Bacardi like it’s your birthday And you know we don’t give a fuck it’s not your birthday You c…\nhip-hop\n\n\nBlack Sabbath\nParanoid\nFinished with my woman’ Cause she couldn’t help me with my mind People think I’m insane Because I am frowning all the time All day long, I think of things But nothing seems to satisfy Think I’ll lo…\nmetal\n\n\na-ha\nTake On Me\nTalking away I don’t know what What to say I’ll say it anyway Today is another day to find you Shying away Oh, I’ll be coming for your love, okay? Take On Me Take me on I’ll be gone In a day or t…\npop\n\n\n3 Doors Down\nHere Without You\nA hundred days have made me older Since the last time that I saw your pretty face A thousand lies have made me colder And I don’t think I can look at this the same But all the miles that separate D…\nrock\n\n\n\n\n\n\nNow given the fact that songs are poems, there are many lines that are not complete sentences so there is no practical way to try to segment these into grammatical sentence units. So in this case, this seems like a good stopping point for normalizing the lastfm dataset."
  },
  {
    "objectID": "transform-datasets.html#td-recode",
    "href": "transform-datasets.html#td-recode",
    "title": "7  Transform datasets",
    "section": "\n7.2 Recode",
    "text": "7.2 Recode\n\nNormalizing text can be seen as an extension of dataset curation to some extent in that the structure of the dataset is maintained. In both the Europarle and Lastfm cases we saw this to be true. In the case of recoding, and other transformational steps, the aim will be to modify the dataset structure either by rows, columns, or both. Recoding processes can be characterized by the creation of structural changes which are derived from values in variables effectively recasting values as new variables to enable more direct access in our analyses.\nSwitchboard Dialogue Act Corpus\nThe Switchboard Dialogue Act Corpus dataset that was curated in the previous chapter contains a number of variables describing conversations between speakers of American English.\nLet’s read in this dataset and take a closer look.\n\nsdac &lt;- read_csv(file = \"../data/derived/sdac/sdac_curated.csv\") # read curated dataset\n\nAmong a number of metadata variables, curated dataset includes the utterance_text column which contains dialogue from the conversations interleaved with a disfluency annotation scheme.\n\n\n\n\nTable 7.6: 20 randomly sampled lines of the SDAC curated dataset.\n\n\n\n\n\n\n\n\n\n\ndoc_id\ndamsl_tag\nspeaker\nturn_num\nutterance_num\nutterance_text\nspeaker_id\n\n\n\n2953\nsd\nB\n127\n5\n{C and } it is not going to happen that way. /\n1229\n\n\n3688\n%\nB\n12\n1\n{D Well, } we’ve just, {F uh, } - /\n1478\n\n\n2800\nsd\nA\n11\n1\n{F Uh, } {C and } they have become, [ or, + ] also [ a, + a ] real bother sometimes. /\n1069\n\n\n3266\nba\nB\n92\n1\n {F Oh, } that poor child. /\n1373\n\n\n2524\nb\nB\n26\n1\nUh-huh. /\n1107\n\n\n2137\nsd\nA\n29\n1\n{D Well, } it seems that there’s some things like [ the, + {F uh, } the ] programs at least just go around [ [ and, + and, ] + and ] clean up streets [ [ and, + and, ] + and ] pick up trash and even aluminum cans and some of these kinds of things. /\n1039\n\n\n2821\nb\nA\n172\n1\n# Yeah. # /\n1253\n\n\n4032\nsd\nB\n6\n2\n{F Um, } I like to play tennis in the –\n1532\n\n\n2662\nb\nA\n91\n1\nYeah. /\n1069\n\n\n2967\nsd\nB\n68\n1\n{F Uh, } which was a, - /\n1072\n\n\n3168\nsv\nB\n36\n1\n{D Well, } {C and, } {D you know, } if they do find someone [ who, + who ] is, {F uh, } having a problem with drugs /\n1361\n\n\n2589\naa\nA\n23\n2\nI definitely agree. /\n1093\n\n\n2874\nb\nA\n10\n3\nuh-huh. /\n1258\n\n\n2482\n%\nA\n149\n1\nThey … -/\n1209\n\n\n3453\nsd\nB\n72\n1\n– {C because } it’s a V Six, /\n1383\n\n\n2924\nb\nA\n25\n1\n{F Oh } yeah, /\n1028\n\n\n3019\nsv\nB\n148\n2\nI think that is, - /\n1264\n\n\n3095\nsd\nA\n119\n1\n{D Well, } I am too. /\n1263\n\n\n3464\nsd\nA\n39\n2\nI think it’s put out by Landmark Productions, the same people who put out CHARIOTS OF FIRE. /\n1446\n\n\n2724\nsd\nA\n58\n2\nI didn’t know that, [ and, + ] {D you know, } until I came down here, and,\n1237\n\n\n\n\n\n\nLet’s drop a few variables from our dataset to rein in our focus. I will keep the doc_id, speaker_id, and utterance_text.\n\nsdac_simplified &lt;- \n  sdac |&gt; # dataset\n  select(doc_id, speaker_id, utterance_text) # columns to retain\n\n\n\n\n\nTable 7.7: First 10 lines of the simplified SDAC curated dataset.\n\n\n\n\n\n\ndoc_id\nspeaker_id\nutterance_text\n\n\n\n4325\n1632\nOkay. /\n\n\n4325\n1632\n{D So, }\n\n\n4325\n1519\n[ [ I guess, +\n\n\n4325\n1632\nWhat kind of experience [ do you, + do you ] have, then with child care? /\n\n\n4325\n1519\nI think, ] + {F uh, } I wonder ] if that worked. /\n\n\n4325\n1632\nDoes it say something? /\n\n\n4325\n1519\nI think it usually does. /\n\n\n4325\n1519\nYou might try, {F uh, } /\n\n\n4325\n1519\nI don’t know, /\n\n\n4325\n1519\nhold it down a little longer, /\n\n\n\n\n\n\nIn this disfluency annotation system, there are various conventions used for non-sentence elements. If say, for example, a researcher were to be interested in understanding the use of filled pauses (‘uh’ or ‘uh’), the aim would be to identify those lines where the {F ...} annotation is used around the utterances ‘uh’ and ‘um’.\nTo do this we turn to the str_count() function. This function will count the number of matches found for a pattern. We can use a regular expression to identify the pattern of interest which is all the instances of {F followed by either uh or um. Since the disfluencies may start an utterance, and therefore be capitalized we need to formulate a regular expression which allows for either U or u for each disfluency type. The result from each disfluency match will be added to a new column. To create a new column we will wrap each str_count() with mutate() and give the new column a meaningful name. In this case I’ve opted for uh and um.\n\nsdac_disfluencies &lt;- \n  sdac_simplified |&gt; # dataset\n  mutate(uh = str_count(utterance_text, \"\\\\{F [Uu]h\")) |&gt; # match {F Uh or {F uh}\n  mutate(um = str_count(utterance_text, \"\\\\{F [Uu]m\")) # match {F Um or {F um}\n\n\n\n\n\nTable 7.8: First 20 lines of SDAC dataset with counts for the disfluencies ‘uh’ and ‘um’.\n\n\n\n\n\n\n\n\ndoc_id\nspeaker_id\nutterance_text\nuh\num\n\n\n\n4325\n1632\nOkay. /\n0\n0\n\n\n4325\n1632\n{D So, }\n0\n0\n\n\n4325\n1519\n[ [ I guess, +\n0\n0\n\n\n4325\n1632\nWhat kind of experience [ do you, + do you ] have, then with child care? /\n0\n0\n\n\n4325\n1519\nI think, ] + {F uh, } I wonder ] if that worked. /\n1\n0\n\n\n4325\n1632\nDoes it say something? /\n0\n0\n\n\n4325\n1519\nI think it usually does. /\n0\n0\n\n\n4325\n1519\nYou might try, {F uh, } /\n1\n0\n\n\n4325\n1519\nI don’t know, /\n0\n0\n\n\n4325\n1519\nhold it down a little longer, /\n0\n0\n\n\n4325\n1519\n{C and } see if it, {F uh, } -/\n1\n0\n\n\n4325\n1632\nOkay . /\n\n0\n0\n\n\n4325\n1632\n&lt;&gt; {D Well, }\n\n0\n0\n\n\n4325\n1519\nOkay /\n0\n0\n\n\n4325\n1519\n[ I, +\n0\n0\n\n\n4325\n1632\nDoes it usually make a recording or s-, /\n0\n0\n\n\n4325\n1519\n{D Well, } I ] don’t remember. /\n0\n0\n\n\n4325\n1519\nIt seemed like it did, /\n0\n0\n\n\n4325\n1519\n{C but }  it might not. /\n\n0\n0\n\n\n4325\n1519\n[ I guess + –\n0\n0\n\n\n\n\n\n\nNow we have two new columns, uh and um which indicate how many times the relevant pattern was matched for a given utterance. By choosing to focus on disfluencies, however, we have made a decision to change the unit of observation from the utterance to the use of filled pauses (uh and um). This means that as the dataset stands, it is not in tidy format –where each observation corresponds to the observational unit. When datasets are misaligned in this particular way, there are in what is known as ‘wide’ format. What we want to do, then, is to restructure our dataset such that each row corresponds to the unit of observation –in this case each filled pause type.\nTo convert our current (wide) dataset to one where each filler type is listed and the counts are measured for each utterance we turn to the pivot_longer() function. This function creates two new columns, one in which the column names are listed and one for the values for each of the column names.\n\nsdac_disfluencies &lt;- \n  sdac_disfluencies |&gt; # dataset\n  pivot_longer(cols = c(\"uh\", \"um\"), # columns to convert\n               names_to = \"filler\", # column for the column names (i.e. filler types)\n               values_to = \"count\") # column for the column values (i.e. counts)\n\n\n\n\n\nTable 7.9: First 20 lines of SDAC dataset with tidy format for fillers as the unit of observation.\n\n\n\n\n\n\n\n\ndoc_id\nspeaker_id\nutterance_text\nfiller\ncount\n\n\n\n4325\n1632\nOkay. /\nuh\n0\n\n\n4325\n1632\nOkay. /\num\n0\n\n\n4325\n1632\n{D So, }\nuh\n0\n\n\n4325\n1632\n{D So, }\num\n0\n\n\n4325\n1519\n[ [ I guess, +\nuh\n0\n\n\n4325\n1519\n[ [ I guess, +\num\n0\n\n\n4325\n1632\nWhat kind of experience [ do you, + do you ] have, then with child care? /\nuh\n0\n\n\n4325\n1632\nWhat kind of experience [ do you, + do you ] have, then with child care? /\num\n0\n\n\n4325\n1519\nI think, ] + {F uh, } I wonder ] if that worked. /\nuh\n1\n\n\n4325\n1519\nI think, ] + {F uh, } I wonder ] if that worked. /\num\n0\n\n\n4325\n1632\nDoes it say something? /\nuh\n0\n\n\n4325\n1632\nDoes it say something? /\num\n0\n\n\n4325\n1519\nI think it usually does. /\nuh\n0\n\n\n4325\n1519\nI think it usually does. /\num\n0\n\n\n4325\n1519\nYou might try, {F uh, } /\nuh\n1\n\n\n4325\n1519\nYou might try, {F uh, } /\num\n0\n\n\n4325\n1519\nI don’t know, /\nuh\n0\n\n\n4325\n1519\nI don’t know, /\num\n0\n\n\n4325\n1519\nhold it down a little longer, /\nuh\n0\n\n\n4325\n1519\nhold it down a little longer, /\num\n0\n\n\n\n\n\n\nLast fm\n\nIn the previous example, we used a matching approach to extract information embedded in one column of the dataset and recoded the dataset to maintain the fidelity between the particular unit of observation and the other metadata.\nAnother common approach for recoding datasets in text analysis projects involves recoding linguistic units as smaller units; a process known as tokenization.\nLet’s return to the lastfm object we normalized earlier in the chapter to see the various ways one can choose to tokenize linguistic information.\n\n\n\n\nTable 7.10: Last fm dataset with normalized lyrics.\n\n\n\n\n\n\n\nartist\nsong\nlyrics\ngenre\n\n\n\nAlan Jackson\nLittle Bitty\nHave a little love on a little honeymoon You got a little dish and you got a little spoon A little bitty house and a little bitty yard A little bitty dog and a little bitty car Well, it’s alright t…\ncountry\n\n\n50 Cent\nIn Da Club\nGo, go, go, go, go, go Go, shorty It’s your birthday We gon’ party like it’s your birthday We gon’ sip Bacardi like it’s your birthday And you know we don’t give a fuck it’s not your birthday You c…\nhip-hop\n\n\nBlack Sabbath\nParanoid\nFinished with my woman’ Cause she couldn’t help me with my mind People think I’m insane Because I am frowning all the time All day long, I think of things But nothing seems to satisfy Think I’ll lo…\nmetal\n\n\na-ha\nTake On Me\nTalking away I don’t know what What to say I’ll say it anyway Today is another day to find you Shying away Oh, I’ll be coming for your love, okay? Take On Me Take me on I’ll be gone In a day or t…\npop\n\n\n3 Doors Down\nHere Without You\nA hundred days have made me older Since the last time that I saw your pretty face A thousand lies have made me colder And I don’t think I can look at this the same But all the miles that separate D…\nrock\n\n\n\n\n\n\nIn the current lastfm dataset, the unit of observation is the lyrics for the entire artist, song, and genre combination. If, however, we would like to change the unit to say words, we would like each word used to appear on its own row, while still maintaining the other relevant attributes associated with each word.\nThe tidytext package includes a very useful function unnest_tokens() which allows us to tokenize some textual input into smaller linguistic units. The ‘unnest’ part of the the function name refers to the process of extracting the unit of interest while maintaining the other relevant attributes. Let’s see this in action.\n\nlastfm |&gt; # dataset\n  unnest_tokens(output = word, # column for tokenized output\n                input = lyrics, # input column\n                token = \"words\") |&gt; # tokenize unit type\n  slice_head(n = 10) |&gt;  # preview first 10 lines\n  kable(booktabs = TRUE)\n\n\n\nTable 7.11: First 10 observations for lastfm dataset tokenized by words.\n\nartist\nsong\ngenre\nword\n\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhave\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlove\n\n\nAlan Jackson\nLittle Bitty\ncountry\non\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhoneymoon\n\n\nAlan Jackson\nLittle Bitty\ncountry\nyou\n\n\nAlan Jackson\nLittle Bitty\ncountry\ngot\n\n\n\n\n\n\nWe can see from the output, each word appears on a separate line in the order of appearance in the input text (lyrics). Furthermore, the output is in tidy format as each of the words is still associated with the relevant attribute values (artist, song, and genre). By default the tokenized text output is lowercased and the original text input column is dropped. These can be overridden, however, if desired.\nIn addition to ‘words’, the unnest_tokens() function provides easy access to a number of common tokenized units including ‘characters’, ‘sentences’, and ‘paragraphs’.\n\nlastfm |&gt; # dataset\n  unnest_tokens(output = character, # column for tokenized output\n                input = lyrics, # input column\n                token = \"characters\") |&gt; # tokenize unit type\n  slice_head(n = 10) |&gt;  # preview first 10 lines\n  kable(booktabs = TRUE)\n\n\n\nTable 7.12: First 10 observations for lastfm dataset tokenized by characters.\n\nartist\nsong\ngenre\ncharacter\n\n\n\nAlan Jackson\nLittle Bitty\ncountry\nh\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\n\n\nAlan Jackson\nLittle Bitty\ncountry\nv\n\n\nAlan Jackson\nLittle Bitty\ncountry\ne\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\n\n\nAlan Jackson\nLittle Bitty\ncountry\nl\n\n\nAlan Jackson\nLittle Bitty\ncountry\ni\n\n\nAlan Jackson\nLittle Bitty\ncountry\nt\n\n\nAlan Jackson\nLittle Bitty\ncountry\nt\n\n\nAlan Jackson\nLittle Bitty\ncountry\nl\n\n\n\n\n\n\nThe other two built-in options ‘sentences’ and ‘paragraphs’ depend on punctuation and/ or line breaks to function, so in this particular dataset, these options will not work given the particular characteristics of the lyrics variable.\nThere are even other options which allow for the creation of sequences of linguistic units. Say we want to tokenize our lyrics into two-word sequences, we can specify the token as ‘ngrams’ and then add the argument n = 2 to reflect we want two-word sequences.\n\nlastfm |&gt; \n  unnest_tokens(output = bigram, # column for tokenized output\n                input = lyrics, # input column\n                token = \"ngrams\", # tokenize unit type\n                n = 2) |&gt;  # size of word sequences \n  slice_head(n = 10) |&gt;  # preview first 10 lines\n  kable(booktabs = TRUE)\n\n\n\nTable 7.13: First 10 observations for lastfm dataset tokenized by bigrams\n\nartist\nsong\ngenre\nbigram\n\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhave a\n\n\nAlan Jackson\nLittle Bitty\ncountry\na little\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle love\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlove on\n\n\nAlan Jackson\nLittle Bitty\ncountry\non a\n\n\nAlan Jackson\nLittle Bitty\ncountry\na little\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle honeymoon\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhoneymoon you\n\n\nAlan Jackson\nLittle Bitty\ncountry\nyou got\n\n\nAlan Jackson\nLittle Bitty\ncountry\ngot a\n\n\n\n\n\n\nThe ‘n’ in ‘ngram’ refers to the number of word-sequence units we want to tokenize. Two-word sequences are known as ‘bigrams’, three-word sequences ‘trigrams’, and so on."
  },
  {
    "objectID": "transform-datasets.html#generate",
    "href": "transform-datasets.html#generate",
    "title": "7  Transform datasets",
    "section": "\n7.3 Generate",
    "text": "7.3 Generate\nIn the process of recoding a dataset the transformation of the dataset works with information that is already explicit. The process of generation, however, aims to make implicit information explicit. The most common type of operation involved in the generation process is the addition of linguistic annotation. This process can be accomplished manually by a researcher or research team or automatically through the use of pre-trained linguistic resources and/ or software. Ideally the annotation of linguistic information can be conducted automatically.\nThere are important considerations, however, that need to be taken into account when choosing whether linguistic annotation can be conducted automatically. First and foremost has to do with the type of annotation desired. Information such as part of speech (grammatical category) and morpho-syntactic information are the the most common types of linguistic annotation that can be conducted automatically. Second the degree to which the resource that will be used to annotate the linguistic information is aligned with the language variety and/or register is also a key consideration. As noted, automatic linguistic annotation methods are contingent on pre-trained resources. The language and language variety used to develop these resources may not be available for the language under investigation, or if it does, the language variety and/ or register may not align. The degree to which a resource does not align with the linguistic information targeted for annotation is directly related to the quality of the final annotations. To be clear, no annotation method, whether manual or automatic is guaranteed to be perfectly accurate.\nLet’s take a look at annotation some of the language from the Europarle dataset we normalized.\n\neuroparle |&gt; \n  filter(type == \"Target\") |&gt; \n  slice_head(n = 10) |&gt; \n  kable(booktabs = TRUE)\n\n\n\nTable 7.14: First 10 lines in English from the normalized SDAC dataset.\n\n\n\n\n\n\ntype\nsentence_id\nsentence\n\n\n\nTarget\n2\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n\n\nTarget\n3\nAlthough, as you will have seen, the dreaded ‘millennium bug’ failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n\n\nTarget\n4\nYou have requested a debate on this subject in the course of the next few days, during this part-session.\n\n\nTarget\n5\nIn the meantime, I should like to observe a minute’s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n\n\nTarget\n6\nPlease rise, then, for this minute’s silence.\n\n\nTarget\n8\nMadam President, on a point of order.\n\n\nTarget\n9\nYou will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n\n\nTarget\n10\nOne of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\n\n\nTarget\n11\nWould it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament’s regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\n\n\nTarget\n12\nYes, Mr Evans, I feel an initiative of the type you have just suggested would be entirely appropriate.\n\n\n\n\n\n\nWe will use the cleanNLP package to do our linguistic annotation. The annotation process depends on the pre-trained language models. There is a list of the models available to access. The load_model_udpipe() custom function below downloads the specified language model and initialized the udpipe engine (cnlp_init_udpipe()) for conducting annotations.\n\nload_model_udpipe &lt;- function(model_lang) {\n  # Function\n  # Download and load the specified udpipe language model\n  \n  cnlp_init_udpipe(model_lang) # to download the model, if not downloaded\nbase_path &lt;- system.file(\"extdata\", package = \"cleanNLP\") # get the base path\n  model_name &lt;- # extract the model_name\n    base_path |&gt; # extract the base path\n    dir() |&gt; # get the directory\n    stringr::str_subset(pattern = paste0(\"^\", model_lang)) # extract the name of the model\n  \n  model_path &lt;- udpipe::udpipe_load_model(file = file.path(base_path, model_name, fsep = \"/\")) # create the path to the downloaded model stored on disk\n    return(model_path)\n}\n\nIn a test case, let’s load the ‘english’ model to annotate a sentence line from the Europarle dataset to illustrate the basic workflow.\n\neng_model &lt;- load_model_udpipe(\"english\") # load and initialize the language model, 'english' in this case.\n\neng_annotation &lt;- \n  europarle |&gt; # dataset \n  filter(type == \"Target\" & sentence_id == 6) |&gt; # select English and sentence_id 6\n  cnlp_annotate(text_name = \"sentence\", # input text (sentence)\n                doc_name = \"sentence_id\") # specify the grouping column (sentence_id)\n\nglimpse(eng_annotation) # preview structure\n\n#&gt; List of 2\n#&gt;  $ token   : tibble [11 × 11] (S3: tbl_df/tbl/data.frame)\n#&gt;   ..$ doc_id       : num [1:11] 6 6 6 6 6 6 6 6 6 6 ...\n#&gt;   ..$ sid          : int [1:11] 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;   ..$ tid          : chr [1:11] \"1\" \"2\" \"3\" \"4\" ...\n#&gt;   ..$ token        : chr [1:11] \"Please\" \"rise\" \",\" \"then\" ...\n#&gt;   ..$ token_with_ws: chr [1:11] \"Please \" \"rise\" \", \" \"then\" ...\n#&gt;   ..$ lemma        : chr [1:11] \"please\" \"rise\" \",\" \"then\" ...\n#&gt;   ..$ upos         : chr [1:11] \"INTJ\" \"VERB\" \"PUNCT\" \"ADV\" ...\n#&gt;   ..$ xpos         : chr [1:11] \"UH\" \"VB\" \",\" \"RB\" ...\n#&gt;   ..$ feats        : chr [1:11] NA \"Mood=Imp|VerbForm=Fin\" NA \"PronType=Dem\" ...\n#&gt;   ..$ tid_source   : chr [1:11] \"2\" \"0\" \"2\" \"10\" ...\n#&gt;   ..$ relation     : chr [1:11] \"discourse\" \"root\" \"punct\" \"advmod\" ...\n#&gt;  $ document: tibble [1 × 2] (S3: tbl_df/tbl/data.frame)\n#&gt;   ..$ type  : chr \"Target\"\n#&gt;   ..$ doc_id: num 6\n#&gt;  - attr(*, \"class\")= chr [1:2] \"cnlp_annotation\" \"list\"\n\n\nWe see that the structure returned by the cnlp_annotate() function is a list. This list contains two data frames (tibbles). One for the tokens (and there annotation information) and the document (the metadata information). We can inspect the annotation characteristics for this one sentence by targetting the $tokens data frame. Let’s take a look at the linguistic annotation information returned.\n\n\n\n\nTable 7.15: Annotation information for a single English sentence from the Europarle dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndoc_id\nsid\ntid\ntoken\ntoken_with_ws\nlemma\nupos\nxpos\nfeats\ntid_source\nrelation\n\n\n\n6\n1\n1\nPlease\nPlease\nplease\nINTJ\nUH\nNA\n2\ndiscourse\n\n\n6\n1\n2\nrise\nrise\nrise\nVERB\nVB\nMood=Imp|VerbForm=Fin\n0\nroot\n\n\n6\n1\n3\n,\n,\n,\nPUNCT\n,\nNA\n2\npunct\n\n\n6\n1\n4\nthen\nthen\nthen\nADV\nRB\nPronType=Dem\n10\nadvmod\n\n\n6\n1\n5\n,\n,\n,\nPUNCT\n,\nNA\n10\npunct\n\n\n6\n1\n6\nfor\nfor\nfor\nADP\nIN\nNA\n10\ncase\n\n\n6\n1\n7\nthis\nthis\nthis\nDET\nDT\nNumber=Sing|PronType=Dem\n8\ndet\n\n\n6\n1\n8\nminute\nminute\nminute\nNOUN\nNN\nNumber=Sing\n10\nnmod:poss\n\n\n6\n1\n9\n’s\n’s\n’s\nPART\nPOS\nNA\n8\ncase\n\n\n6\n1\n10\nsilence\nsilence\nsilence\nNOUN\nNN\nNumber=Sing\n2\nconj\n\n\n6\n1\n11\n.\n.\n.\nPUNCT\n.\nNA\n2\npunct\n\n\n\n\n\n\nThere is quite a bit of information which is returned from cnlp_annotate(). First note that the input sentence has been tokenized by word. Each token includes the token, lemma, part of speech (upos and xpos), morphological features (feats), and syntactic relationships (tid_source and relation). It is also key to note that the doc_id, sid and tid maintain the relational attributes from the original dataset –and therefore maintains our annotated dataset in tidy format.\nLet’s now annotate the same sentence from the Europarle corpus for the Source (‘Spanish’) and note the similarities and differences.\n\nspa_model &lt;- load_model_udpipe(\"spanish\") # load and initialize the language model, 'spanish' in this case.\n\nspa_annotation &lt;- \n  europarle |&gt; # dataset \n  filter(type == \"Source\" & sentence_id == 6) |&gt; # select Spanish and sentence_id 6\n  cnlp_annotate(text_name = \"sentence\", # input text (sentence)\n                doc_name = \"sentence_id\") # specify the grouping column (sentence_id)\n\n\n\n\n\nTable 7.16: Annotation information for a single Spanish sentence from the Europarle dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndoc_id\nsid\ntid\ntoken\ntoken_with_ws\nlemma\nupos\nxpos\nfeats\ntid_source\nrelation\n\n\n\n6\n1\n1\nInvito\nInvito\nInvito\nVERB\nNA\nGender=Masc|Number=Sing|VerbForm=Fin\n0\nroot\n\n\n6\n1\n2\na\na\na\nADP\nNA\nNA\n3\ncase\n\n\n6\n1\n3\ntodos\ntodos\ntodo\nPRON\nNA\nGender=Masc|Number=Plur|PronType=Tot\n1\nobj\n\n\n6\n1\n4\na\na\na\nADP\nNA\nNA\n7\nmark\n\n\n6\n1\n5\nque\nque\nque\nSCONJ\nNA\nNA\n4\nfixed\n\n\n6\n1\n6\nnos\nnos\nyo\nPRON\nNA\nCase=Acc,Dat|Number=Plur|Person=1|PrepCase=Npr|PronType=Prs|Reflex=Yes\n7\niobj\n\n\n6\n1\n7\npongamos\npongamos\npongar\nVERB\nNA\nMood=Ind|Number=Plur|Person=1|Tense=Pres|VerbForm=Fin\n1\nadvcl\n\n\n6\n1\n8\nde\nde\nde\nADP\nNA\nNA\n9\ncase\n\n\n6\n1\n9\npie\npie\npie\nNOUN\nNA\nGender=Masc|Number=Sing\n7\nobl\n\n\n6\n1\n10\npara\npara\npara\nADP\nNA\nNA\n11\nmark\n\n\n6\n1\n11\nguardar\nguardar\nguardar\nVERB\nNA\nVerbForm=Inf\n1\nadvcl\n\n\n6\n1\n12\nun\nun\nuno\nDET\nNA\nDefinite=Ind|Gender=Masc|Number=Sing|PronType=Art\n13\ndet\n\n\n6\n1\n13\nminuto\nminuto\nminuto\nNOUN\nNA\nGender=Masc|Number=Sing\n11\nobj\n\n\n6\n1\n14\nde\nde\nde\nADP\nNA\nNA\n15\ncase\n\n\n6\n1\n15\nsilencio\nsilencio\nsilencio\nNOUN\nNA\nGender=Masc|Number=Sing\n13\nnmod\n\n\n6\n1\n16\n.\n.\n.\nPUNCT\nNA\nNA\n1\npunct\n\n\n\n\n\n\nFor the Spanish version of this sentence, we see the same variables. However, the feats variable has morphological information which is specific to Spanish –notably gender and mood.\n\n\n\n\n\n\nTip\n\n\n\nThe rsyntax package (Welbers and van Atteveldt 2022) can be used to recode and extract patterns from the output from automatic linguistic annotations using cleanNLP. See the documentation for more information."
  },
  {
    "objectID": "transform-datasets.html#merge",
    "href": "transform-datasets.html#merge",
    "title": "7  Transform datasets",
    "section": "\n7.4 Merge",
    "text": "7.4 Merge\n\nOne final class of transformations that can be applied to curated datasets to enhance their informativeness for a research project is the process of merging two or more datasets. To merge datasets it is required that the datasets share one or more attributes. With a common attribute two datasets can be joined to coordinate the attributes of one dataset with the other effectively adding attributes and one dataset with extended information. Another approach is to join datasets with the goal of filtering one of the datasets given the matching attribute.\nLet’s see this in practice. Take the lastfm dataset. Let’s tokenize the dataset into words, using unnest_tokens() such that our unit of observation is words.\n\nlastfm_words &lt;- \n  lastfm |&gt; # dataset\n  unnest_tokens(output = \"word\", # output column\n                input = \"lyrics\", # input column\n                token = \"words\") # tokenized unit (words)\n\nlastfm_words |&gt; # dataset\n  slice_head(n = 10) |&gt; # first 10 observations\n  kable(booktabs = TRUE)\n\n\n\nTable 7.17: First 10 observations for lastfm_words dataset.\n\nartist\nsong\ngenre\nword\n\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhave\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlove\n\n\nAlan Jackson\nLittle Bitty\ncountry\non\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhoneymoon\n\n\nAlan Jackson\nLittle Bitty\ncountry\nyou\n\n\nAlan Jackson\nLittle Bitty\ncountry\ngot\n\n\n\n\n\n\nConsider the get_sentiments() function which returns words which have been classified as ‘positive’- or ‘negative’-biased, if the lexicon is set to ‘bing’ (Hu and Liu 2004).\n\nsentiments_bing &lt;- \n  tidytext::get_sentiments(lexicon = \"bing\") # get 'bing' lexicon from get_sentiments\n\nsentiments_bing |&gt; \n  slice_head(n = 10) # preview first 10 observations\n\n#&gt; # A tibble: 10 × 2\n#&gt;    word        sentiment\n#&gt;    &lt;chr&gt;       &lt;chr&gt;    \n#&gt;  1 2-faces     negative \n#&gt;  2 abnormal    negative \n#&gt;  3 abolish     negative \n#&gt;  4 abominable  negative \n#&gt;  5 abominably  negative \n#&gt;  6 abominate   negative \n#&gt;  7 abomination negative \n#&gt;  8 abort       negative \n#&gt;  9 aborted     negative \n#&gt; 10 aborts      negative\n\n\nSince the sentiments_bing dataset and the lastfm_words dataset both share a column word (which has the same type of values) we can join these two datasets. The sentiments_bing dataset has 6786 unique words. Let’s check how many distinct words our lastfm_words dataset has.\n\nlastfm_words |&gt; # dataset\n  distinct(word) |&gt; # find unique words\n  nrow() # count distinct rows/ words\n\n#&gt; [1] 4614\n\n\nOne thing to note is that the sentiments_bing dataset does not include function words, that is words that are associated with closed-class categories (pronouns, determiners, prepositions, etc.) as these words do not have semantic content along the lines of positive and negative. So many of the words that appear in the lastfm_words will not be matched. Other thing to note is that the sentiments_bing lexicon will undoubtly have words that do not appear in the lastfm_words and vice versa.\nIf we want to keep all the words in the lastfm_words and add the sentiment information for those words that do match in both datasets, we can use the left_join() function. lastfm_words will be the dataset on the ‘left’ and therefore all rows in this dataset will be retained.\n\nleft_join(lastfm_words, sentiments_bing) |&gt; \n  slice_head(n = 10) |&gt; # first 10 observations\n  kable(booktabs = TRUE)\n\n\n\nTable 7.18: First 10 observations for the lastfm_words sentiments_bing` left join.\n\nartist\nsong\ngenre\nword\nsentiment\n\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhave\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlove\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\non\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\na\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhoneymoon\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\nyou\nNA\n\n\nAlan Jackson\nLittle Bitty\ncountry\ngot\nNA\n\n\n\n\n\n\nSo we see that quite a few of the words from lastfm_words are not matched. To focus in on those words in lastfm_words that do match, we’ll run the same join operation and filter for rows where sentiment is not empty (i.e. that there is a match in the sentiments_bing lexicon).\n\nleft_join(lastfm_words, sentiments_bing) |&gt;\n  filter(sentiment != \"\") |&gt; # return matched sentiments\n  slice_head(n = 10) |&gt; # first 10 observations\n  kable(booktabs = TRUE)\n\n\nFirst 10 observations for the lastfm_words sentiments_bing` left join. \n\nartist\nsong\ngenre\nword\nsentiment\n\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlove\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nwell\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nwell\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nwell\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nsmile\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nwell\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nwell\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nwell\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\nsmile\npositive\n\n\nAlan Jackson\nLittle Bitty\ncountry\ngood\npositive\n\n\n\n\n\nLet’s turn to another type of join: an anti-join. The purpose of an anti-join is to eliminate matches. This makes sense for a quick and dirty approach to removing function words (i.e. those grammatical words with little semantic content). In this case we use the get_stopwords() function to get the dataset. We’ll specify English as the language and we’ll use the default lexicon (‘Snowball’).\n\nenglish_stopwords &lt;- \n  get_stopwords(language = \"en\") # get English stopwords from the Snowball lexicon\n\nenglish_stopwords |&gt; \n  slice_head(n = 10) # preview first 10 observations\n\n#&gt; # A tibble: 10 × 2\n#&gt;    word      lexicon \n#&gt;    &lt;chr&gt;     &lt;chr&gt;   \n#&gt;  1 i         snowball\n#&gt;  2 me        snowball\n#&gt;  3 my        snowball\n#&gt;  4 myself    snowball\n#&gt;  5 we        snowball\n#&gt;  6 our       snowball\n#&gt;  7 ours      snowball\n#&gt;  8 ourselves snowball\n#&gt;  9 you       snowball\n#&gt; 10 your      snowball\n\n\nNow if we want to eliminate stopwords from our lastfm_words dataset we use anti_join(). All the observations in the lastfm_words where there is not a match in english_stopwords will be returned.\n\nanti_join(lastfm_words, english_stopwords) |&gt; \n  slice_head(n = 10) |&gt; \n  kable(booktabs = TRUE)\n\n\n\nTable 7.19: First 10 observations in lastfm_words after filtering for English stopwords.\n\nartist\nsong\ngenre\nword\n\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlove\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\nhoneymoon\n\n\nAlan Jackson\nLittle Bitty\ncountry\ngot\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\ndish\n\n\nAlan Jackson\nLittle Bitty\ncountry\ngot\n\n\nAlan Jackson\nLittle Bitty\ncountry\nlittle\n\n\nAlan Jackson\nLittle Bitty\ncountry\nspoon\n\n\n\n\n\n\nWe can also merge datasets that we generate in our analysis or that we import from other sources. This can be useful when there are cases in which a corpus has associated metadata that is contained in files separate from the corpus itself. This is the case for the Switchboard Dialogue Act Corpus.\nOur existing, disfluency recoded, version includes the following variables.\n\nsdac_disfluencies |&gt; # dataset\n  slice_head(n = 10) # preview first 10 observations\n\n#&gt; # A tibble: 10 × 5\n#&gt;    doc_id speaker_id utterance_text                                 filler count\n#&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;                                          &lt;chr&gt;  &lt;int&gt;\n#&gt;  1   4325       1632 Okay.  /                                       uh         0\n#&gt;  2   4325       1632 Okay.  /                                       um         0\n#&gt;  3   4325       1632 {D So, }                                       uh         0\n#&gt;  4   4325       1632 {D So, }                                       um         0\n#&gt;  5   4325       1519 [ [ I guess, +                                 uh         0\n#&gt;  6   4325       1519 [ [ I guess, +                                 um         0\n#&gt;  7   4325       1632 What kind of experience [ do you, + do you ] … uh         0\n#&gt;  8   4325       1632 What kind of experience [ do you, + do you ] … um         0\n#&gt;  9   4325       1519 I think, ] + {F uh, } I wonder ] if that work… uh         1\n#&gt; 10   4325       1519 I think, ] + {F uh, } I wonder ] if that work… um         0\n\n\nThe online documentation page provides a key file caller_tab.csv which contains speaker metadata information. Included in this .csv file is a column caller_no which contains the speaker_id we currently have in the sdac_disfluencies dataset. Let’s read this file into our R session renaming caller_no to speaker_id to prepare to join these datasets.\n\nsdac_speaker_meta &lt;- \n  read_csv(file = \"https://catalog.ldc.upenn.edu/docs/LDC97S62/caller_tab.csv\", \n           col_names = c(\"speaker_id\", # changed from `caller_no`\n                         \"pin\",\n                         \"target\",\n                         \"sex\",\n                         \"birth_year\",\n                         \"dialect_area\",\n                         \"education\",\n                         \"ti\",\n                         \"payment_type\",\n                         \"amt_pd\",\n                         \"con\",\n                         \"remarks\",\n                         \"calls_deleted\",\n                         \"speaker_partition\"))\n\nglimpse(sdac_speaker_meta)\n\n#&gt; Rows: 543\n#&gt; Columns: 14\n#&gt; $ speaker_id        &lt;dbl&gt; 1000, 1001, 1002, 1003, 1004, 1005, 1007, 1008, 1010…\n#&gt; $ pin               &lt;dbl&gt; 32, 102, 104, 5656, 123, 166, 274, 322, 445, 461, 57…\n#&gt; $ target            &lt;chr&gt; \"N\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y…\n#&gt; $ sex               &lt;chr&gt; \"FEMALE\", \"MALE\", \"FEMALE\", \"MALE\", \"FEMALE\", \"FEMAL…\n#&gt; $ birth_year        &lt;dbl&gt; 1954, 1940, 1963, 1947, 1958, 1956, 1965, 1939, 1932…\n#&gt; $ dialect_area      &lt;chr&gt; \"SOUTH MIDLAND\", \"WESTERN\", \"SOUTHERN\", \"NORTH MIDLA…\n#&gt; $ education         &lt;dbl&gt; 1, 3, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 3, 3, 2, 3…\n#&gt; $ ti                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ payment_type      &lt;chr&gt; \"CASH\", \"GIFT\", \"GIFT\", \"NONE\", \"GIFT\", \"GIFT\", \"CAS…\n#&gt; $ amt_pd            &lt;dbl&gt; 15, 10, 11, 7, 11, 22, 20, 3, 11, 9, 25, 9, 1, 16, 1…\n#&gt; $ con               &lt;chr&gt; \"N\", \"N\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"N\", \"N…\n#&gt; $ remarks           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ calls_deleted     &lt;dbl&gt; 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0…\n#&gt; $ speaker_partition &lt;chr&gt; \"DN2\", \"XP\", \"XP\", \"DN2\", \"XP\", \"ET\", \"DN1\", \"DN1\", …\n\n\nNow to join the sdac_disfluencies and sdac_speaker_meta. Let’s turn to left_join() again as we want to retain all the observations (rows) from sdac_disfluencies and add the columns for sdac_speaker_meta where the speaker_id column values match.\n\nsdac_disfluencies &lt;- \n  left_join(sdac_disfluencies, sdac_speaker_meta) # join by ``speaker_id`\n\nglimpse(sdac_disfluencies)\n\n#&gt; Rows: 447,212\n#&gt; Columns: 18\n#&gt; $ doc_id            &lt;dbl&gt; 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325…\n#&gt; $ speaker_id        &lt;dbl&gt; 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519…\n#&gt; $ utterance_text    &lt;chr&gt; \"Okay.  /\", \"Okay.  /\", \"{D So, }\", \"{D So, }\", \"[ […\n#&gt; $ filler            &lt;chr&gt; \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\"…\n#&gt; $ count             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n#&gt; $ pin               &lt;dbl&gt; 7713, 7713, 7713, 7713, 775, 775, 7713, 7713, 775, 7…\n#&gt; $ target            &lt;chr&gt; \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N…\n#&gt; $ sex               &lt;chr&gt; \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"F…\n#&gt; $ birth_year        &lt;dbl&gt; 1962, 1962, 1962, 1962, 1971, 1971, 1962, 1962, 1971…\n#&gt; $ dialect_area      &lt;chr&gt; \"WESTERN\", \"WESTERN\", \"WESTERN\", \"WESTERN\", \"SOUTH M…\n#&gt; $ education         &lt;dbl&gt; 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1…\n#&gt; $ ti                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ payment_type      &lt;chr&gt; \"CASH\", \"CASH\", \"CASH\", \"CASH\", \"CASH\", \"CASH\", \"CAS…\n#&gt; $ amt_pd            &lt;dbl&gt; 10, 10, 10, 10, 4, 4, 10, 10, 4, 4, 10, 10, 4, 4, 4,…\n#&gt; $ con               &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y…\n#&gt; $ remarks           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n#&gt; $ calls_deleted     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ speaker_partition &lt;chr&gt; \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UN…\n\n\nNow there are some metadata columns we may want to keep and others we may want to drop as they may not be of importance for our analysis. I’m going to assume that we want to keep sex, birth_year, dialect_area, and education and drop the rest.\n\nsdac_disfluencies &lt;- \n  sdac_disfluencies |&gt; # dataset\n  select(doc_id:count, sex:education) # subset key columns\n\n\n\n\n\nTable 7.20: First 10 observations for the sdac_disfluencies dataset with speaker metadata.\n\n\n\n\n\n\n\n\n\n\n\n\ndoc_id\nspeaker_id\nutterance_text\nfiller\ncount\nsex\nbirth_year\ndialect_area\neducation\n\n\n\n4325\n1632\nOkay. /\nuh\n0\nFEMALE\n1962\nWESTERN\n2\n\n\n4325\n1632\nOkay. /\num\n0\nFEMALE\n1962\nWESTERN\n2\n\n\n4325\n1632\n{D So, }\nuh\n0\nFEMALE\n1962\nWESTERN\n2\n\n\n4325\n1632\n{D So, }\num\n0\nFEMALE\n1962\nWESTERN\n2\n\n\n4325\n1519\n[ [ I guess, +\nuh\n0\nFEMALE\n1971\nSOUTH MIDLAND\n1\n\n\n4325\n1519\n[ [ I guess, +\num\n0\nFEMALE\n1971\nSOUTH MIDLAND\n1\n\n\n4325\n1632\nWhat kind of experience [ do you, + do you ] have, then with child care? /\nuh\n0\nFEMALE\n1962\nWESTERN\n2\n\n\n4325\n1632\nWhat kind of experience [ do you, + do you ] have, then with child care? /\num\n0\nFEMALE\n1962\nWESTERN\n2\n\n\n4325\n1519\nI think, ] + {F uh, } I wonder ] if that worked. /\nuh\n1\nFEMALE\n1971\nSOUTH MIDLAND\n1\n\n\n4325\n1519\nI think, ] + {F uh, } I wonder ] if that worked. /\num\n0\nFEMALE\n1971\nSOUTH MIDLAND\n1"
  },
  {
    "objectID": "transform-datasets.html#documentation",
    "href": "transform-datasets.html#documentation",
    "title": "7  Transform datasets",
    "section": "\n7.5 Documentation",
    "text": "7.5 Documentation\nDocumentation of the transformed dataset is just as important as the curated dataset. Therefore we use the same process as covered in the previous chapter. First we write the transformed dataset to disk and then we work to provide a data dictionary for this dataset. I’ve included the data_dic_starter() custom function to apply to our dataset(s).\n\ndata_dic_starter &lt;- function(data, file_path) {\n  # Function:\n  # Creates a .csv file with the basic information\n  # to document a curated dataset\n  \n  tibble(variable_name = names(data), # column with existing variable names \n       name = \"\", # column for human-readable names\n       description = \"\") |&gt; # column for prose description\n  write_csv(file = file_path) # write to disk\n}\n\nLet’s apply our function to the sdac_disfluencies dataset using the R console (not part of our project script to avoid overwriting our documentation!).\n\ndata_dic_starter(data = sdac_disfluencies, file_path = \"../data/derived/sdac/sdac_disfluencies_data_dictionary.csv\")\n\ndata/derived/\n└── sdac/\n    ├── data_dictionary_sdac.csv\n    ├── sdac_curated.csv\n    ├── sdac_disfluencies.csv\n    └── sdac_disfluencies_data_dictionary.csv\nOpen the data_dictionary_sdac_disfluencies.csv file in spreadsheet software and add the relevant description of the dataset."
  },
  {
    "objectID": "transform-datasets.html#summary",
    "href": "transform-datasets.html#summary",
    "title": "7  Transform datasets",
    "section": "Summary",
    "text": "Summary\nIn this chapter we covered the process of transforming datasets. The goal is to manipulate the curated dataset to make it align better for analysis. There are four general types of transformation steps: normalization, recoding, generation, and merging. In any given research project some or all of these steps will be employed –but not necessarily in the order presented in this chapter. Furthermore there may also be various datasets generated in at this stage each with a distinct analysis focus in mind. In any case it is important to write these datasets to disk and to document them according to the principles that we have established in the previous chapter.\nThis chapter concludes the section on data/ dataset preparation. The next section we turn to analyzing datasets. This is the stage where we interrogate the datasets to derive knowledge and insight either through inference, prediction, and/ or exploratory methods."
  },
  {
    "objectID": "transform-datasets.html#activities",
    "href": "transform-datasets.html#activities",
    "title": "7  Transform datasets",
    "section": "Activities",
    "text": "Activities\n\n\n\n\n\n\nRecipe\n\n\n\nWhat: Dataset manipulation: tokenization and joining datasetsHow: Read Recipe 8 and participate in the Hypothes.is online social annotation.Why: To work with to primary types of transformations, tokenization and joins. Tokenization is the process of recasting textual units as smaller textual units. The process of joining datasets aims to incorporate other datasets to augment or filter the dataset of interest.\n\n\n\n\n\n\n\n\nLab\n\n\n\nWhat: Dataset manipulation: tokenization and joining datasetsHow: Clone, fork, and complete the steps in Lab 8.Why: To gain experience working with coding strategies for transforming datasets using tidyverse functions and regular expressions, practice reading/ writing data from/ to disk, and implement organizational strategies for organizing and documenting a dataset in reproducible fashion."
  },
  {
    "objectID": "transform-datasets.html#questions",
    "href": "transform-datasets.html#questions",
    "title": "7  Transform datasets",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\nConceptual questions\n\n\n\n\n…\n…\n\n\n\n\n\n\n\n\n\nTechnical exercises\n\n\n\n\n…\n…\n\n\n\n\n\n\n\nHu, Minqing, and Bing Liu. 2004. “Mining and Summarizing Customer Reviews.” In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 168–77.\n\n\nWelbers, Kasper, and Wouter van Atteveldt. 2022. Rsyntax: Extract Semantic Relations from Text by Querying and Reshaping Syntax. https://CRAN.R-project.org/package=rsyntax."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "In this section we turn to the analysis of datasets, the evaluation of results, and the interpretation of the findings. We will outline the three main types of statistical analyses: Exploratory Data Analysis (EDA), Predictive Data Analysis (PDA), and Inferential Data Analysis (IDA). Each of these analysis types have distinct, non-overlapping aims and therefore should be determined from the outset of the research project and included as part of the research blueprint. The aim of this section is to establish a clearer picture of the goals, methods, and value of each of these approaches."
  },
  {
    "objectID": "exploration.html#eda-orientation",
    "href": "exploration.html#eda-orientation",
    "title": "8  Exploration",
    "section": "\n8.1 Orientation",
    "text": "8.1 Orientation\nThe aim of this section is to provide an overview of exploratory data analysis (EDA) for linguists, with a focus on descriptive methods such as frequency analysis and co-occurence analysis, as well as unsupervised learning approaches such as clustering, topic modeling, and vector space modeling. It will also include use cases of these methods.\n\n8.1.1 Research goals\nThe goal of exploratory data analysis is to discover, describe, and posit new hypotheses. The researcher does not start with a preconceived hypothesis or prediction, but rather the researcher aims to uncover patterns and associations from data allowing the data to guide the trajectory of the analysis. This analysis approach is best-suited for research where the literature on a research question is limited, or where the researcher is interested in exploring a new research question. Since the researcher does not start with a preconceived hypothesis, the researcher is not able to test a hypothesis and generalize to a population, but rather the researcher is able to describe the data and provide a new perspective to be qualitatively assessed. This is achieved through an iterative and inductive process of data exploration, where the researcher uses quantitative methods to summarize, reduce, and sort complex datasets and statistically and visually interrogate a dataset letting the data guide the analysis.\n\n8.1.2 Approaches\n\nThere is no fixed outcome variable rather there is only a set of predictors or covariates.\nThe data is mutable, meaning that the data can be changed or modified as needed to address the research question.\n\nAnalysis types\nThere are two main types of exploratory analysis: 1) descriptive analysis which statistically and/ or visually summarizes a dataset and 2) unsupervised learning which is a machine learning approach that does not assume any particular relationship between variables in a dataset. Either or both of these approaches can be used to explore a dataset.\n\n8.1.3 Workflow\nPrerequisites: - A working research question - A dataset which aligns with the research question or hypothesis in terms of its sampling frame and the variables it contains or can be derived from the text - A set of preliminary interests and/ or linguistic variables to explore in the dataset that align with the research question\nProcess: - Identify and extract the variables of interest in the dataset - Interrogate the dataset using descriptive analysis and/ or unsupervised learning -\n\nIdentify\n\nWith the research question in mind, identify the variables of interest in the dataset\nIdentify the linguistic variables that can be derived from the text (i.e. liguistic units: words, n-grams, sentences, etc.)\nConsider the operational measures of the variables derived from the text (i.e. frequency, dispersion, co-occurrence, etc.)\nConsider the other variables in the dataset that may be target for grouping or filtering the dataset (i.e. speaker information, document information, linguistic unit information, etc.)\nInspect\nInspect your data to ensure the quality of your data and understand its characteristics.\nInterrogate\nDescriptive analysis:\n\nFrequency analysis\nCo-occurrence analysis\n\nUnsupervised learning:\n\nClustering\nTopic modeling\nWord embedding\nInterpret\nExploratory methods will produce a set of statistical and/ or visual results. The researcher must interpret these results to determine if they are meaningful and if they provide a new perspective on the research question. Many times the results from one method will lead to new questions which can be explored with other methods. In some cases, the results may not be meaningful and the researcher may need to return to the data preparation stage to modify the dataset or the variables of interest. As the aim of exploratory analysis is just that, to explore, the researcher can pivot the approach to explore new questions and new variables. Ultimately, what is meaningful is determined by the researcher in the light of the research question and the potential insight obtained from the results."
  },
  {
    "objectID": "exploration.html#eda-analysis",
    "href": "exploration.html#eda-analysis",
    "title": "8  Exploration",
    "section": "\n8.2 Analysis",
    "text": "8.2 Analysis\n\n8.2.1 Descriptive analysis\nFrequency analysis\n\nFrequency analysis is a descriptive method that counts the number of times a linguistic unit (i.e. word, n-gram, sentence, etc.) occurs in a dataset. The results of frequency analysis can be used to describe the dataset and to identify linguistic units that are distinctive to a particular group or sub-group in the dataset.\n\nRaw frequency (counting)\n\nLinguistic units (characters, words, n-grams, sentences, etc.)\nRaw frequency (absolute frequency)\nZipf’s law (rank-frequency)\nIssues with raw frequency (\\(f\\)):\n\nIncomparable across sub-corpora, corpora, and time\n\n\n\n\nTerm frequency (normalization)\n\nRelative frequency (proportional frequency)\nMakes samples (more) comparable (divergence of corpus sizes can influences validity of comparison)\nIssues with term frequency (\\(tf\\)):\n\nDoes not reveal the distribution of terms across a text or set of texts\nDoes not highlight distinctive words\n\nOne way to address this is to filter out ‘stop words’ (i.e. the, a, an, etc.)\n\n\n\n\n\n\nAdjusted frequency (relevance)\n\nDispersion (distribution)\n\n\n\\(idf\\) (inverse document frequency)\n… Juilland’s \\(D\\) (equal/ non-equal parts)\nGries’ \\(DP\\) Deviation of Proportions (DP)\n\n\nWeights\n\n\n\\(tf-idf\\) (term frequency-inverse document frequency)\n\nDistinctive words (Used to identify the most relevant keywords in a given text.)\nIssues with \\(tf-idf\\): Does not reveal the context of terms, Does not reveal the relationship between terms\n\n\nweighted log odds (tidylo package)\n\nDistinctive words (Used to identify the most relevant keywords in a given text.)\nAdvantage over \\(tf-idf\\): does a better job dealing with different combinations of words and documents having different counts.\n\n\n\n\n\n\nCo-occurrence analysis\n\n\nConcordance\n\nKWIC (keyword in context) This section will discuss keyword in context (KWIC) analyses, which is used to identify meaningful keywords in a given text. It will discuss various ways to analyse a text and extract keywords, as well as discuss various practical applications of KWIC in linguistics.\n\n\nCollocation\n\nPMI (pointwise mutual information)\nDice’s coefficient\n…\n\n\n\n8.2.2 Unsupervised learning\nClustering\nThis section will discuss clustering techniques, which are used to partition data into clusters based on similarity. It will discuss various approaches to clustering, such as k-means and hierarchical clustering, as well as discuss their use cases in linguistics.\n\nK-means (pre-defined number of clusters)\nHierarchical clustering (dendrogram)\nDimensionality reduction\n\nPCA (principal component analysis)\nMDS (multidimensional scaling)\nTopic modeling\nThis section will discuss topic modeling techniques, which are used to identify and group semantically similar topics in unstructured data. It will discuss various approaches to topic modelling, such as Latent Dirichlet Allocation (LDA), and discuss their applications in linguistics.\n\nLDA (latent Dirichlet allocation)\nLSA (latent semantic analysis)\nWord embedding\nThis section will discuss word embedding techniques, which are used to represent words in a vector space. It will discuss various approaches to word embedding, such as Word2Vec and GloVe, and discuss their applications in linguistics.\n\nWord2vec (skip-gram)\nGloVe (global vectors for word representation)"
  },
  {
    "objectID": "exploration.html#summary",
    "href": "exploration.html#summary",
    "title": "8  Exploration",
    "section": "\n8.3 Summary",
    "text": "8.3 Summary\nExploratory data analysis is a set of methods that can be used to explore a dataset and to identify new questions and new variables of interest. The methods can be used to describe a dataset and to identify linguistic units that are distinctive to a particular group or sub-group in the dataset. The methods can also be used to identify semantically similar topics in unstructured data. The results of exploratory analysis can be used to inform the development of a hypothesis or to inform the design of a machine learning model."
  },
  {
    "objectID": "exploration.html#activities",
    "href": "exploration.html#activities",
    "title": "8  Exploration",
    "section": "Activities",
    "text": "Activities\n\n\n\n\n\n\nRecipe\n\n\n\nWhat: Exploratory methods: descriptive and unsupervised learning analysis methodsHow: Read Recipe 10 and participate in the Hypothes.is online social annotation.Why: To illustrate how to prepare a dataset for descriptive and unsupervised machine learning methods and evaluate the results for exploratory data analysis.\n\n\n\n\n\n\n\n\nLab\n\n\n\nWhat: Exploratory Data AnalysisHow: Clone, fork, and complete the steps in Lab 9.Why: To gain experience working with coding strategies to prepare, feature engineer, explore, and evaluate results from exploratory data analyses, practice transforming datasets into new object formats and visualizing relationships, and implement organizational strategies for organizing and reporting results in a reproducible fashion."
  },
  {
    "objectID": "exploration.html#questions",
    "href": "exploration.html#questions",
    "title": "8  Exploration",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\nConceptual questions\n\n\n\n\nWhat is exploratory data analysis?\nHow can exploratory data analysis be used to uncover patterns and associations?\nDescribe the workflow of exploratory data analysis?\nWhat are the advantages and disadvantages of descriptive analysis?\nWhat are the advantages and disadvantages of unsupervised learning?\nWhat is the difference between supervised and unsupervised learning?\nHow does exploratory data analysis differ from traditional hypothesis testing?\n\n\n\n\n\n\n\n\n\nTechnical exercises\n\n\n\n\nWrite a function in R to conduct a hierarchical cluster analysis on a dataset.\nImplement a k-means algorithm in R to identify clusters within a dataset.\nImplement a Principal Component Analysis (PCA) algorithm in R to identify patterns and associations within a dataset.\nWrite a function in R to produce a descriptive summary of a dataset.\nConduct a correlation analysis in R to identify relationships between variables in a dataset.\nLoad a dataset into R and conduct a frequency analysis on the dataset.\nLoad a dataset into R and conduct a keyword in context analysis on the dataset.\nLoad a dataset into R and conduct a keyword analysis on the dataset.\nLoad a dataset into R and conduct a sentiment analysis on the dataset.\nLoad a dataset into R and conduct a topic modelling analysis on the dataset."
  },
  {
    "objectID": "prediction.html#pda-orientation",
    "href": "prediction.html#pda-orientation",
    "title": "9  Prediction",
    "section": "\n9.1 Orientation",
    "text": "9.1 Orientation\nThe aim of this section is to introduce the reader to the concept of supervised learning and to provide a brief overview of the workflow for building predictive models for text analysis.\n\n9.1.1 Research goals\nSupervised learning is a type of machine learning that involves training a model on a labeled dataset where the input data and desired output are both provided. The model is able to make predictions or classifications based on the input data by learning the relationships between the input and output data. Supervised machine learning is an important tool for linguists studying language and communication, as it allows them to analyze language data to identify patterns or trends in language use, verify hypotheses, and prescribe actions. Supervised machine learning is an active area of research in linguistics, with many potential applications and areas for further exploration.\nPredictive analyses are more inductive than exploratory analyses, which are more deductive. This means that we are more interested in the relationship between a particular outcome variable and a set of predictor variables than we are in the relationship between the predictor variables themselves, as we would be in an exploratory analysis. In this sense, we have a particular outcome in mind from the outset. On the other hand, the input variables are mutable, meaning that they can be changed to see how they affect the outcome –which points to the exploratory aspect of predictive analyses.\n\n9.1.2 Approaches\n\nOutcome variable and any number of predictor variables\nPredictor variables are features derived from text and are mutable.\n\nAnalysis types\nThere are two main types of supervised machine learning algorithms: classification, which is used to predict a categorical outcome such as the genre of a text, and regression, which is used to predict a continuous outcome such as the sentiment of a text.\n\nSupervised learning\n\nClassification\n\nCategorical outcome variable\n\n\nRegression\n\nContinuous outcome variable\n\n\n\n\n\n9.1.3 Workflow\nPrerequisites: - A working research question or hypothesis - A dataset which aligns with the research question or hypothesis in terms of its sampling frame and the variables it contains or can be derived from the text and a target variable to be predicted. - A set of preliminary features to be derived from the text that are used to predict the target variable\n\nIdentify\nData cleaning and feature extraction are the first steps in the process of preparing data for supervised machine learning.\nIn order to use supervised machine learning, linguists must first identify measurable properties of the text use use as the input variables or features that are most likely to produce a model that performs well (i.e. that when used make accurate predictions). Once the feature types are identified, the data is processed to clean any extraneous elements and format the structure of the dataset given the requirements of the algorithm that will be used in subsequent steps.\nInspect\nThe next step is to inspect the data:\nPreprocess (missing, anomalies, outliers, transformations, etc.) Descriptive (summary statistics, visualizations, etc.)\nInclude dataset transformation to ensure that it is in the correct format for the machine learning algorithm that will be used.\nInterrogate\nModel training is the next step towards building a predictive model.\nIn this step, the data is split into training and testing sets. The training set is used to train the model, and the testing set is used to evaluate the model’s performance. The testing set is reserved and not used to train the model, so that the model’s performance can be evaluated on data that it has not seen before.\nThe model is then trained on the training data and evaluated on the testing data. The results are then evaluated and the hyperparameters of the model may be adjusted to optimize its performance.\nand the hyperparameters of the model may be adjusted to optimize its performance.\nHyperparameters are variables that are set prior to running a machine learning algorithm whose values influence the final result. In supervised machine learning, hyperparameters are typically used to control the learning process such as the learning rate, momentum, and batch size.\nSome applications of supervised machine learning in linguistics include text classification, part-of-speech tagging, and language identification. Supervised machine learning is an active area of research in linguistics, with many potential applications and areas for further exploration.\nInterpret\nTo either evaluate the training or testing set, the model is used to make predictions on the data in the set. The predictions are then compared to the actual values of the target variable in the set to evaluate the model’s performance. So how is the model’s performance evaluated?\nClassification\nFor classification, there are a number of metrics that can be used to evaluate the performance of a model, including accuracy, precision, recall, and F1 score. To understand these measures it is helpful to consider a confusion matrix, which is a table that describes the performance of a classification model on data for which the true values are known. The confusion matrix is a two-by-two matrix that shows the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN), as seen in Table 9.1.\n\n\n\n\nTable 9.1: A labeled confusion matrix\n\n\nPredicted positive\nPredicted negative\n\n\n\nActual positive\nTP\nFP\n\n\nActual negative\nFN\nTN\n\n\n\n\n\n\nNow let’s fill this confusion matrix with hypothetical values, as seen in Table 9.2 to see how the metrics are calculated.\n\n\n\n\nTable 9.2: Confusion matrix for a hypothetical model’s performance on a test set\n\n\nPredicted positive\nPredicted negative\n\n\n\nActual positive\n100\n10\n\n\nActual negative\n20\n50\n\n\n\n\n\n\n\nAccuracy is defined as the proportion of correct predictions made by the model.\n\n\\[\n\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n\\tag{9.1}\\]\nThe number of correct predictions is the sum of true positives and true negatives. So in our case this is 100 + 50 = 150. The total number of predictions is the sum of all four cells in the confusion matrix, so in our case this is 100 + 10 + 20 + 50 = 180. So the accuracy of our hypothetical model is 150/180 = 0.833.\n\nPrecision is defined as the proportion of positive predictions that are correct.\n\n\\[\n\\text{Precision} = \\frac{\\text{Number of true positives}}{\\text{Number of true positives + false positives}}\n\\tag{9.2}\\]\n\nRecall is defined as the proportion of actual positives that are correctly identified.\n\n\\[\n\\text{Recall} = \\frac{\\text{Number of true positives}}{\\text{Number of true positives + false negatives}}\n\\tag{9.3}\\]\n\nThe F1 score is the harmonic mean of precision and recall.\n\n\\[\n\\text{F1 score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\tag{9.4}\\]\nArea under the curve (AUC) is the area under the ROC (Receiver Operating Characteristic) curve, which is a graph of true positives (TPR) and false positives (FPR). The AUC is a measure of the model’s performance across all possible classification thresholds. The AUC is a number between 0 and 1, where 0.5 represents a model that is no better than random guessing, and 1 represents a perfect model.\nFeature importance\n\nparallel coordinate visualization\n\nIn a supervised text classification task, you can use parallel coordinate plots to visualize the distribution of class labels across different feature dimensions. This can help identify which features are most informative for distinguishing between classes and inform feature selection or dimensionality reduction techniques.\n\n# Libraries\nlibrary(tidyverse)\nlibrary(tidytext)\n\n# Faux data\ndata &lt;- data.frame(\n  text = c(\"I love cats\", \"Cats are amazing\", \"I hate dogs\", \"Dogs are annoying\"),\n  class = c(\"positive\", \"positive\", \"negative\", \"negative\")\n)\n\n# Tokenize data\ntokenized_data &lt;- data %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  count(class, word) %&gt;%\n  ungroup() %&gt;%\n  spread(class, n, fill = 0)\n\n# Normalize the term frequencies\nnormalized_data &lt;-\n  tokenized_data %&gt;%\n  mutate(\n    positive = positive / sum(positive),\n    negative = negative / sum(negative)\n  ) %&gt;%\n  gather(class, frequency, -word)\n\n# Generate the parallel coordinate plot\nggplot(normalized_data, aes(x = class, y = frequency, group = word, color = word)) +\n  geom_line() +\n  theme_minimal() +\n  labs(\n    title = \"Parallel Coordinate Visualization\",\n    subtitle = \"Text Classification Model Using Words as Features\",\n    x = \"Class\",\n    y = \"Normalized Term Frequency\"\n  )\n\n\n\nFigure 9.1: A parallel coordinate plot showing the distribution of class labels across different feature dimensions\n\n\n\nWe can look at the parallel coordinate plot in Figure 9.1 to see that the words “cats” and “love” are more common in the positive class, while the words “dogs” and “hate” are more common in the negative class. This suggests that these words are good features for distinguishing between the two classes.\nRegression\nFor regression, the most common metric is the root mean squared error (RMSE). The RMSE is the square root of the mean of the squared differences between the predicted values and the actual values. The lower the RMSE, the better the model fits the data.\nSupervised machine learning algorithms for regression are typically evaluated using measures of error such as mean squared error (MSE), root mean squared error (RMSE), and mean absolute error (MAE). MSE is used to measure the average of the squares of the errors, MAE is the average of the absolute differences between the prediction and the actual data, and RMSE is the square root of the mean squared error. For each of these statistics, the lower the value, the better the model fits the data. The differences between these statistics are shown in Table 9.3.\n\n\n\n\nTable 9.3: A table showing the differences between mean squared error, root mean squared error, and mean absolute error\n\n\n\n\n\n\nerror\nformula\ndescription\n\n\n\nMSE\n\\[\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\nThe average of the squared differences between the prediction and the actual data\n\n\nRMSE\n\\[\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\]\nThe square root of the mean squared error\n\n\nMAE\n\\[\\frac{1}{n} \\sum_{i=1}^{n} \\vert y_i - \\hat{y}_i \\vert\\]\nThe average of the absolute differences between the prediction and the actual data\n\n\n\n\n\n\nThe main advantages of using MSE, RMSE, and MAE are that they are all on the same scale as the dependent variable, and they are all differentiable, which makes them useful for optimization algorithms. MSE is the most commonly used metric for regression, but RMSE and MAE are also used. MSE is more sensitive to outliers than RMSE and MAE, so it is more useful when the data has outliers. RMSE and MAE are more useful when the data does not have outliers.\n\nPlot the actual and predicted values to see how well the model fits the data.\n\n\n\n\nA plot of the actual and predicted values for a regression model\n\n\n\nWe can now apply our error metrics to the results data to see how well the model fits the data.\n\n#|\n# Calculate the error metrics for the `results` data\nresults |&gt;\n  mutate(error = actual - predicted) |&gt; # calculate the error\n  summarise(\n    mse = mean(error^2), # calculate the MSE\n    rmse = sqrt(mse), # calculate the RMSE\n    mae = mean(abs(error)), # calculate the MAE\n    n = n()\n  ) |&gt; # calculate the number of observations\n  mutate(\n    mse = mse * 1 / n, # multiply by 1/n to get the MSE for n observations\n    rmse = rmse * 1 / n, # multiply by 1/n to get the RMSE for n observations\n    mae = mae * 1 / n\n  ) # multiply by 1/n to get the MAE for n observations\n\n&gt; # A tibble: 1 × 4\n&gt;      mse   rmse     mae     n\n&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n&gt; 1 0.0113 0.0106 0.00947   100\n\n\nFormula for calculating the MSE:\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\tag{9.5}\\]\nSo we can implement this in R subtracting the actual values from the predicted values, squaring the differences, then taking the mean of the all the squared differences, and finally multiplying by \\(1/n\\) to get the MSE for \\(n\\) observations.\n\nresults |&gt; # use the `results` data\n  mutate(error = actual - predicted) |&gt; # calculate the error\n  summarise(\n    mse = mean(error^2), # calculate the MSE\n    n = n()\n  ) |&gt; # calculate the number of observations\n  mutate(mse = mse * 1 / n) |&gt; # multiply by 1/n to get the MSE for n observations\n  pull(mse) # pull the MSE value out of the tibble\n\n&gt; [1] 0.0113\n\n\nFormula for calculating the RMSE:\n\\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n\\tag{9.6}\\]"
  },
  {
    "objectID": "prediction.html#pda-analysis",
    "href": "prediction.html#pda-analysis",
    "title": "9  Prediction",
    "section": "\n9.2 Analysis",
    "text": "9.2 Analysis\nRecap and introduction to the structure of the analysis subsection.\n\nIntroduce an algorithm\nBuild a model\n\nPreprocessing (tokenization, lemmatization, etc.)\nFeature extraction (TF-IDF, word embeddings, etc.)\nModel selection (logistic regression, SVM, etc.)\nModel training\n\n\nEvaluate (and adjust) the model on the training data\n\nCross-validation\nEvaluation metrics\nCompare to null and/ or other models\nAdjust the model (hyperparameters, regularization, etc.) as necessary\n\n\nEvaluate the model on the test data\n\nEvaluation metrics\nEvaluate feature importance\nEvaluate the features of correct and incorrect predictions\n\n\n\n\n9.2.1 Classification\nWe will first start with classification which is by far the most common text analysis approach in supervised machine learning. Again, classification is the task of predicting a categorical variable from a set of features. The features we use will be derived from the text but can take many forms. For example, we can use the raw text, the word counts, the TF-IDF values, or the word embeddings. We also will take into account the number of features we use. There is a trade-off, however, to the number of features: a) the more features we use, the more complex the model will be, and the more likely it will overfit the training data and b) the less features we use, the less complex the model will be, and the more likely it will underfit the training data. To find the optimal number of features we can use a technique called cross-validation.\nThe most common text classification algorithms are logistic regression, k-nearest neighbors, Naive Bayes, and support vector machines. We will start with logistic regression and k-nearest neighbors as they are the simplest to understand and implement. We will then move on to Naive Bayes and support vector machines as they are more complex and require more explanation.\nBuilding a null model for classification we simply predict the most common class in the training data. This makes sense as we have seen earlier, with categorical data the central tendency is estimated by the mode –i.e. the most common value.\nK-nearest neighbors\nK-nearest neighbors is a simple supervised machine learning method for classification. It is a non-parametric method, which means that it does not make any assumptions about the underlying distribution of the data. It is a lazy learning method, which means that it does not learn a discriminant function from the training data but instead stores the training data. It is a distance-based method, which means that it uses a distance metric to find the \\(k\\) nearest neighbors to a new observation. It is a simple method, which means that it is easy to understand and implement.\nLogistic regression\nLogistic regression is a supervised machine learning method for classification. It is a parametric method, which means that it makes assumptions about the underlying distribution of the data. It is an iterative method, which means that it uses an iterative algorithm to find the optimal parameters. To avoid overfitting it uses regularization such as ridge regression or lasso regression. These regularization methods penalize the model for having too many parameters. However, how does one know what the optimal number of parameters is? This is where cross-validation comes in.\nNaive Bayes\nNaive Bayes is a supervised machine learning method for classification. It is a parametric method, which means that it makes assumptions about the underlying distribution of the data. It is a probabilistic method, which means that it uses Bayes’ theorem to calculate the probability of a class given the predictor variables. It is a generative method, which means that it learns the joint probability distribution of the predictor variables and the outcome variable. It is a simple method, which means that it makes the assumption that the predictor variables are independent of each other. This assumption is called the naive assumption. Now this assumption does not theoretically hold for language data as words are not independent of each other. However, in practice, Naive Bayes’ models still perform well on many text classification tasks.\nDecision trees\nDecision trees for text classification are a supervised machine learning method for classification. They are a non-parametric method, which means that they do not make any assumptions about the underlying distribution of the data. They are a greedy method, which means that they use a greedy algorithm to find the optimal split of the predictor variables. They are a simple method, which means that they are easy to understand and implement.\nIn practical terms using decision trees for text classification can be very useful as they are easy to interpret. For example, we can see which words are most important for the classification of a text. However, they are prone to overfitting the training data. To avoid this we can use a technique called bagging. Bagging is a technique that uses multiple decision trees to make a prediction. The prediction is then the mode of the predictions of the individual decision trees. This is called a random forest.\n\n9.2.2 Regression\nIn supervised machine learning regression tasks contrast to classification tasks as the outcome variable is continuous. A typical example outside of language would be to predict the price of a house given the number of bedrooms, the number of bathrooms, the size of the house, etc. For language this means that the labled outcome variable is a number, not a class. For example, we can predict the number of words in a text given the number of characters in the text, the number of sentences in the text, etc. Other applications of regression in text analysis are sentiment analysis (where the outcome is a scalar value) and topic modeling (where the outcome is a probability distribution over topics).\nLinear regression\nLinear regression can be used to predict a continuous outcome variable from a set of features. It is a parametric method, which means that it makes assumptions about the underlying distribution of the data. It is an iterative method, which means that it uses an iterative algorithm to find the optimal parameters. To avoid overfitting it uses regularization such as ridge regression or lasso regression. These regularization methods penalize the model for having too many parameters. However, how does one know what the optimal number of parameters is? This is where cross-validation comes in.\nDecision trees (regression)\nDecision trees for regression are a supervised machine learning method for regression. They are a non-parametric method, which means that they do not make any assumptions about the underlying distribution of the data. They are a greedy method, which means that they use a greedy algorithm to find the optimal split of the predictor variables. They are a simple method, which means that they are easy to understand and implement.\nNeural networks\nNeural networks are a supervised machine learning method for regression and classification. They are a non-parametric method, which means that they do not make any assumptions about the underlying distribution of the data. They are an iterative method, which means that they use an iterative algorithm to find the optimal parameters. They are a complex method, which means that they are difficult to understand and implement. However, they are very powerful and can be used to solve a wide range of problems. However, they are expensive to train and require a lot of data. It is often the case that a simpler method will perform just as well as a neural network in certain contexts."
  },
  {
    "objectID": "prediction.html#pda-reporting",
    "href": "prediction.html#pda-reporting",
    "title": "9  Prediction",
    "section": "\n9.3 Reporting",
    "text": "9.3 Reporting\nWhen reporting the results of a supervised machine learning model it is important to report the evaluation metrics that are most relevant to the problem at hand. For example, if the problem is to predict a continuous outcome, then the most relevant evaluation metric is the mean squared error (MSE). It is often useful to report the root mean squared error (RMSE) as well.\nHowever, if the problem is to predict the class, then the most relevant evaluation metric is the accuracy. Other evaluation metrics that are often reported are the precision, recall, and F1-score. It can also be useful to report the confusion matrix. The confusion matrix shows the number of true positives, false positives, true negatives, and false negatives.\nIf the research goal is focused on prediction accuracy, then these statistics are the most relevant. But in other cases, the supervised learning alogrithm is a means to guage a relationship between the outcome and the predictor variables, namely to guage the most important predictor variables. The model can then be used to identify those predictor variables that support accurate predictions and even to identify those predictor variables that do not support accurate predictions. Note, however, that some supervised learning algorithms are not able to identify the most important predictor variables. For example, neural networks are not able to identify the most important predictor variables. These ‘black box’ algorithms may lead to accurate predictions, but they do not provide any insight into the underlying relationship between the outcome and the predictor variables."
  },
  {
    "objectID": "prediction.html#pda-summary",
    "href": "prediction.html#pda-summary",
    "title": "9  Prediction",
    "section": "\n9.4 Summary",
    "text": "9.4 Summary\nIn this chapter we have learned about supervised machine learning. We have learned about the different types of supervised machine learning methods and how they can be used to predict and classify. We have also learned about the different types of data structures that are used in supervised machine learning. Finally, we have learned about the different types of evaluation metrics that are used to evaluate the performance of supervised machine learning models."
  },
  {
    "objectID": "prediction.html#activities",
    "href": "prediction.html#activities",
    "title": "9  Prediction",
    "section": "Activities",
    "text": "Activities\n\n\n\n\n\n\nRecipe\n\n\n\nWhat: Predictive models: prep, train, test, and evaluateHow: Read Recipe 10 and participate in the Hypothes.is online social annotation.Why: To illustrate some common coding strategies for preparing datasets for inferential data analysis, as well as the steps conduct descriptive assessment, statistical interrogation, and evaluation and reporting of results.\n\n\n\n\n\n\n\n\nLab\n\n\n\nWhat: Predictive Data AnalysisHow: Clone, fork, and complete the steps in Lab 10.Why: To gain experience working with coding strategies to prepare, feature engineer, train and test a predictive model, and evaluate results from a predictive data analysis, practice transforming datasets into new object formats and visualizing relationships, and implement organizational strategies for organizing and reporting results in a reproducible fashion."
  },
  {
    "objectID": "prediction.html#questions",
    "href": "prediction.html#questions",
    "title": "9  Prediction",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\nConceptual questions\n\n\n\n\nWhat is the difference between a continuous and a categorical variable?\nWhat is the difference between a regression and a classification model?\nWhat is the difference between a training set and a testing set?\nWhat is the difference between a hyperparameter and a parameter?\nWhat is the difference between a supervised and an unsupervised machine learning model?\nWhat advantages and disadvantages do supervised machine learning models have over traditional methods of text analysis?\nWhat are some potential applications of supervised machine learning in linguistics?\n\n\n\n\n\n\n\n\n\nTechnical exercises\n\n\n\n\nWrite a program to build a classification model which uses a set of collected text features to predict a target variable.\nUse the classification model to classify a series of documents and assess the accuracy of the model.\nDevelop a regression model which uses text features to predict a continuous target variable.\nCreate a text mining application to analyze a large body of text and discover correlations between variables.\nUse a clustering algorithm to discover clusters in a large dataset, and create a visualization to present the identified clusters.\nAnalyze the structure of a text corpus and identify patterns in word usage and feature distributions.\nBuild a predictive model using text as an input and binary or categorical outcomes as the target.\nDevelop a natural language processing application which classifies text into predefined categories using a supervised learning algorithm.\nUse a supervised learning algorithm to build a predictive model which classifies a set of unseen texts into predefined categories.\nDevelop a web application which allows users to easily explore a set of text documents, visualize the content of the documents, and generate predictive models from the text.\n\n\n\n\n\n\n\n\n\nBaayen, R. Harald. 2011. “Corpus Linguistics and Naive Discriminative Learning.” Revista Brasileira de Lingu\\’\\istica Aplicada 11 (2): 295–328.\n\n\nDeshors, Sandra C, and Stefan Th. Gries. 2016. “Profiling Verb Complementation Constructions Across New Englishes.” International Journal of Corpus Linguistics. 21 (2): 192–218."
  },
  {
    "objectID": "inference.html#ida-orientation",
    "href": "inference.html#ida-orientation",
    "title": "10  Inference",
    "section": "\n10.1 Orientation",
    "text": "10.1 Orientation\nThe aim of this section is to provide an overview of inferential data analysis (IDA) and to introduce the three main types of inferential analysis approaches that are most common in text analysis research.\n\n10.1.1 Research goals\nThe goal of IDA is to detect, explain, and generalize. The relationship to detect is predtermined by the hypothesis. In this situation the researcher will have identified an outcome variable (often known as a dependent variable) and often a predictor or set of predictor variables (independent variables) that are directly linked to the hypothesis in a way that the results of the statistical analysis allows to either confirm that null hypothesis or reject it. Explaining the results of the statistical analysis is key to summarizing the findings and how they relate to the hypothesis. To the extent that the text sample used is representative of the population from which the data is sampled, the results can be generalized to the population. IDA is not an exploratory endeavor and as such that anlysis is performed on the data in a much more conservative manner than is the case in exploratory data analysis (EDA) or predictive data analysis (PDA).\n\n10.1.2 Approaches\n\nThe dependent variable and predictor variables are fixed (tied to hypothesis)\nDescriptive statistics and visualizations (plots or tables) are used to summarize the data and provide a preliminary assessment of the data\nInferential statistics are used to test the hypothesis\n\nAnalysis types\nThe type of analysis that is performed depends most heavily on the informational value of the dependent variable. The informational value of the dependent variable is determined by the type of data that is collected. Secondly, the number and informational types of the independent variables (predictor variables) also play a role in determining the type of analysis that is performed.\n\nCategorical dependent variable\n\nDescriptive statistics\n\nFrequency\nProportion\nConfidence intervals\n\n\nInferential statistics\n\nChi-square\nLogistic regression\n\n\n\n\nContinuous dependent variable\n\nDescriptive statistics\n\nMean\nStandard deviation\nConfidence intervals\n\n\nInferential statistics\n\nCorrelation\nLinear regression\n\n\n\n\n\n10.1.3 Workflow\nPrerequisites: - A testable hypothesis (covering the outcome space, i.e. null and alternative hypotheses). - A data set that aligns with the population targeted to generalize to. - A operationalized dependent and independent variable(s) that are tied to the hypothesis.\n\nIdentify\n\nIdentify the informational value of the dependent and independent variable(s).\nAssess the distribution of the independent and dependent variables with the appropriate descriptive statistics and visualizations.\nChoose an appropriate statistical test based on the informational value and distribution of the dependent and independent variables.\nApply transformations to the data as needed to meet the assumptions of the statistical tests.\nInterrogate\n\nApply the appropriate statistical test to the data:\n\nCategorical dependent variable\n\nChi-square (dependent and one independent variable)\nLogistic regression (dependent and one or more independent variables)\n\n\nContinuous dependent variable\n\nCorrelation (dependent and one independent variable)\nLinear regression (dependent and one or more independent variables)\n\n\n\n\nAssess the results of the statistical test (p-value, confidence intervals, effect size)\nInterpret\nReview the results of the statistical test and interpret the results in the context of the hypothesis."
  },
  {
    "objectID": "inference.html#ida-analysis",
    "href": "inference.html#ida-analysis",
    "title": "10  Inference",
    "section": "\n10.2 Analysis",
    "text": "10.2 Analysis\nRecap and introduction to the structure of the analysis subsection.\n\nCategorical dependent variable\n\nCategorical/ continuous independent variable(s)\n\nDescriptive assessment\n\n0-1 categorical independent variable: Tables summary statistics (contingency table)\nContinuous independent variable(s): Plots and summary statistics (boxplots, histograms, etc.)\n\n\nStatistical interrogation\n\nChi-square (dependent and one independent variable)\nLogistic regression (dependent and one or more independent variables)\n\n\nEvaluation of results\n\np-value\nConfidence intervals\nEffect size\n\n\n\n\n\n\nContinuous dependent variable\n\nContinuous/ categorical independent variable(s)\n\nDescriptive assessment\n\nTables, plots, and summary statistics\n\n\nStatistical interrogation\n\nCorrelation\nLinear regression\n\n\nEvaluation of results\n\np-value\nConfidence intervals\nEffect size\n\n\n\n\n\n\n\n\n10.2.1 Categorical dependent variable\nChi-square\nChi-square tests can be used for frequencies of categorical variables. The chi-square goodness of fit test is used to test whether the observed frequencies of a categorical variable match the expected frequencies of a single variable. The chi-square test of independence is used to test whether the observed frequencies of two categorical variables are independent of each other. For hypothesis tests that include more than two variables, the chi-square test is not appropriate. Instead, logistic regression is used.\nLogistic regression\nLogistic regression can handle more than one independent variable, and these variables need not be categorical. The dependent variable is a binary variable, and the independent variables can be continuous or categorical. The logistic regression model is used to predict the probability of the dependent variable being 1. The logistic regression model is used to test whether the independent variables are associated with the dependent variable.\nUsing the infer package, a logistic regression can be performed using the specify() and generate() functions. The specify() function is used to specify the dependent and independent variables. The generate() function is used to generate the model. The calculate() function is used to calculate the p-value and confidence intervals.\n\nlibrary(infer) # for inferential statistics\n\n# load the `dative` data from the `languageR` package\ndata(package = \"languageR\", data = \"dative\")\n\n# specify the dependent `RealizationOfRecipient` and independent variables `LengthOfTheme` and `AnimacyOfTheme` for a logistic regression model\nlog_reg &lt;- \n  dative %&gt;%\n  specify(RealizationOfRecipient ~ LengthOfTheme + AnimacyOfTheme) %&gt;%\n  generate(response = \"logistic\", type = \"response\") %&gt;%\n  calculate(stat = \"p.value\", order = \"descending\")\n\n\n10.2.2 Continuous dependent variable\nCorrelation\nLinear regression"
  },
  {
    "objectID": "inference.html#ida-reporting",
    "href": "inference.html#ida-reporting",
    "title": "10  Inference",
    "section": "\n10.3 Reporting",
    "text": "10.3 Reporting"
  },
  {
    "objectID": "inference.html#summary",
    "href": "inference.html#summary",
    "title": "10  Inference",
    "section": "\n10.4 Summary",
    "text": "10.4 Summary"
  },
  {
    "objectID": "inference.html#activities",
    "href": "inference.html#activities",
    "title": "10  Inference",
    "section": "Activities",
    "text": "Activities\n\n\n\n\n\n\nRecipe\n\n\n\nWhat: Statistical inference: prep, assess, interrogate, evaluate, and reportHow: Read Recipe 9 and participate in the Hypothes.is online social annotation.Why: To illustrate some common coding strategies for preparing datasets for inferential data analysis, as well as the steps conduct descriptive assessment, statistical interrogation, and evaluation and reporting of results.\n\n\n\n\n\n\n\n\nLab\n\n\n\nWhat: Statistical inference: prep, assess, interrogate, evaluate, and reportHow: Clone, fork, and complete the steps in Lab 9.Why: To gain experience working with coding strategies to prepare, assess, interrogate, evaluate, and report results from an inferential data analysis, practice transforming datasets and visualizing relationships, and implement organizational strategies for organizing and reporting results in a reproducible fashion."
  },
  {
    "objectID": "inference.html#questions",
    "href": "inference.html#questions",
    "title": "10  Inference",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\nConceptual questions\n\n\n\n\nWhat is the difference between a descriptive and inferential analysis?\nWhat are the steps involved in conducting a descriptive analysis?\nWhat are the steps involved in conducting an inferential analysis?\nWhat are the steps involved in preparing data for inferential analysis?\nWhat are the steps involved in conducting a statistical interrogation?\nWhat are the steps involved in evaluating the results of an inferential analysis?\nWhat are the steps involved in reporting the results of an inferential analysis?\nWould word freqency differences between two groups of words be better assessed using a t-test or a chi-squared distribution?\nWould word lengths between two groups of words be better assessed using a t-test or a chi-squared distribution?\nWhat type of visualization would be best for exploring the relationship between two categorical variables?\nWhat type of visualization would be best for exploring the relationship between two non-categorical variables?\nWhat type of visualization would be best for exploring the relationship between a categorical and a non-categorical variable?\nWhat is the role of confidence intervals in inferential data analysis? How is this similar or differnt to the role of p-values?\nWhat is the role of effect size in inferential data analysis? How is this similar or differnt to the role of p-values?\n\n\n\n\n\n\n\n\n\nTechnical exercises\n\n\n\n\nUse the lm() function to create a linear model, assess the summary statistics, and evaluate the results.\nUse the glm() function to assess the relationship between two categorical variables and evaluate the results.\nApply a chi-squared distribution to explore categorical dependent variables and evaluate the results.\nCalculate correlation coefficients between two non-categorical variables and evaluate the results.\nRead in a dataset and transform it to prepare it for inferential analysis.\nDecide which type of visualization is most appropriate for the dataset and then implement it using ggplot2.\nUse the effectsize() function to calculate effect size and confidence intervals."
  },
  {
    "objectID": "inference.html#preparation",
    "href": "inference.html#preparation",
    "title": "10  Inference",
    "section": "\n10.5 Preparation",
    "text": "10.5 Preparation\nAt this point let’s now get familiar with the datasets and prepare them for analysis. The first dataset to consider is the dative dataset. This dataset can be loaded from the languageR package (R. H. Baayen and Shafaei-Bajestan 2019).\n\ndative &lt;- \n  languageR::dative |&gt; # load the `dative` dataset  \n  as_tibble() # convert the data frame to a tibble object\n  \nglimpse(dative) # preview structure \n\n#&gt; Rows: 3,263\n#&gt; Columns: 15\n#&gt; $ Speaker                &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#&gt; $ Modality               &lt;fct&gt; written, written, written, written, written, wr…\n#&gt; $ Verb                   &lt;fct&gt; feed, give, give, give, offer, give, pay, bring…\n#&gt; $ SemanticClass          &lt;fct&gt; t, a, a, a, c, a, t, a, a, a, a, a, t, a, c, a,…\n#&gt; $ LengthOfRecipient      &lt;int&gt; 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1,…\n#&gt; $ AnimacyOfRec           &lt;fct&gt; animate, animate, animate, animate, animate, an…\n#&gt; $ DefinOfRec             &lt;fct&gt; definite, definite, definite, definite, definit…\n#&gt; $ PronomOfRec            &lt;fct&gt; pronominal, nonpronominal, nonpronominal, prono…\n#&gt; $ LengthOfTheme          &lt;int&gt; 14, 3, 13, 5, 3, 4, 4, 1, 11, 2, 3, 3, 5, 2, 8,…\n#&gt; $ AnimacyOfTheme         &lt;fct&gt; inanimate, inanimate, inanimate, inanimate, ina…\n#&gt; $ DefinOfTheme           &lt;fct&gt; indefinite, indefinite, definite, indefinite, d…\n#&gt; $ PronomOfTheme          &lt;fct&gt; nonpronominal, nonpronominal, nonpronominal, no…\n#&gt; $ RealizationOfRecipient &lt;fct&gt; NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP,…\n#&gt; $ AccessOfRec            &lt;fct&gt; given, given, given, given, given, given, given…\n#&gt; $ AccessOfTheme          &lt;fct&gt; new, new, new, new, new, new, new, new, accessi…\n\n\nFrom glimpse() we can see that this dataset contains 3,263 observations and 15 columns.\nThe R Documentation can be consulted using ?dative in the R Console. The description states:\n\nData describing the realization of the dative as NP or PP in the Switchboard corpus and the Treebank Wall Street Journal collection.\n\nFor a bit more context, a dative is the phrase which reflects the entity that takes the recipient role in a ditransitive clause. In English, the recipient (dative) can be realized as either a noun phrase (NP) as seen in (1) or as a prepositional phrase (PP) as seen in (2) below.\n\nThey give [you NP] a drug test.\nThey give a drug test [to you PP].\n\nTogether these two syntactic options are known as the Dative Alternation.\nThe observational unit for this dataset is RealizationOfRecipient variable which is either ‘NP’ or ‘PP’. For the purposes of this chapter I will select a subset of the key variables we will use in the upcoming analyses and drop the others.\n\ndative &lt;- \n  dative |&gt; # dataset\n  select(RealizationOfRecipient, Modality, LengthOfRecipient, LengthOfTheme) |&gt; # select key variables\n  janitor::clean_names() # normalize variable names\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\n\n\n\n\nrealization_of_recipient\nmodality\nlength_of_recipient\nlength_of_theme\n\n\n\nNP\nwritten\n1\n14\n\n\nNP\nwritten\n2\n3\n\n\nNP\nwritten\n1\n13\n\n\nNP\nwritten\n1\n5\n\n\nNP\nwritten\n2\n3\n\n\nNP\nwritten\n2\n4\n\n\nNP\nwritten\n2\n4\n\n\nNP\nwritten\n1\n1\n\n\nNP\nwritten\n1\n11\n\n\nNP\nwritten\n1\n2\n\n\n\n\n\nIn Table 10.1 I’ve created a data dictionary describing the variables in our new dative dataset based on the variable descriptions in the languageR::dative documentation.\n\n\n\n\nTable 10.1: Data dictionary for the dative dataset.\n\n\n\n\n\n\nvariable_name\nname\ndescription\n\n\n\nrealization_of_recipient\nRealization of Recipient\nA factor with levels NP and PP coding the realization of the dative.\n\n\nmodality\nLanguage Modality\nA factor with levels spoken, written.\n\n\nlength_of_recipient\nLength of Recipient\nA numeric vector coding the number of words comprising the recipient.\n\n\nlength_of_theme\nLength of Theme\nA numeric vector coding the number of words comprising the theme.\n\n\n\n\n\n\nThe second dataset that we will use in this chapter is the sdac_disfluencies dataset that we worked to derived in the previous chapter. Let’s read in the dataset and preview the structure.\n\nsdac_disfluencies &lt;- \n  read_csv(file = \"../data/derived/sdac/sdac_disfluencies.csv\") # read transformed dataset\n\nglimpse(sdac_disfluencies) # preview structure\n\n\n\n#&gt; Rows: 447,212\n#&gt; Columns: 9\n#&gt; $ doc_id         &lt;dbl&gt; 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4…\n#&gt; $ speaker_id     &lt;dbl&gt; 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519, 1…\n#&gt; $ utterance_text &lt;chr&gt; \"Okay.  /\", \"Okay.  /\", \"{D So, }\", \"{D So, }\", \"[ [ I …\n#&gt; $ filler         &lt;chr&gt; \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"…\n#&gt; $ count          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0…\n#&gt; $ sex            &lt;chr&gt; \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMA…\n#&gt; $ birth_year     &lt;dbl&gt; 1962, 1962, 1962, 1962, 1971, 1971, 1962, 1962, 1971, 1…\n#&gt; $ dialect_area   &lt;chr&gt; \"WESTERN\", \"WESTERN\", \"WESTERN\", \"WESTERN\", \"SOUTH MIDL…\n#&gt; $ education      &lt;dbl&gt; 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1…\n\n\nWe prepared a data dictionary that reflects this transformed dataset. Let’s read that file and then view it in Table 10.2.\n\nsdac_disfluencies_dictionary &lt;- read_csv(file = \"../data/derived/sdac/sdac_disfluencies_data_dictionary.csv\") # read data dictionary\n\n\n\n\n\nTable 10.2: Data dictionary for the sdac_disfluencies dataset.\n\n\n\n\n\n\nvariable_name\nname\ndescription\n\n\n\ndoc_id\nDocument ID\nUnique identifier for each conversation file.\n\n\nspeaker_id\nSpeaker ID\nUnique identifier for each speaker in the corpus.\n\n\nutterance_text\nUtterance Text\nTranscribed utterances for each conversation. Includes disfluency annotation tags.\n\n\nfiller\nFiller\nFiller type either uh or um.\n\n\ncount\nCount\nNumber of fillers for each utterance.\n\n\nsex\nSex\nSex for each speaker either male or female.\n\n\nbirth_year\nBirth Year\nThe year each speaker was born.\n\n\ndialect_area\nDialect Area\nRegion from the US where the speaker spent first 10 years.\n\n\neducation\nEducation\nHighest educational level attained: values 0, 1, 2, 3, and 9.\n\n\n\n\n\n\nFor our analysis purposes we will reduce this dataset, as we did for the dative dataset, retaining only the variables of interest for the upcoming analyses.\n\nsdac_disfluencies &lt;- \n  sdac_disfluencies |&gt; # dataset\n  select(speaker_id, filler, count, sex, birth_year, education) # select key variables\n\nLet’s preview this simplified sdac_disfluencies dataset.\n\n\n\n\nTable 10.3: First 10 observations of simplified sdac_disfluencies dataset.\n\nspeaker_id\nfiller\ncount\nsex\nbirth_year\neducation\n\n\n\n1632\nuh\n0\nFEMALE\n1962\n2\n\n\n1632\num\n0\nFEMALE\n1962\n2\n\n\n1632\nuh\n0\nFEMALE\n1962\n2\n\n\n1632\num\n0\nFEMALE\n1962\n2\n\n\n1519\nuh\n0\nFEMALE\n1971\n1\n\n\n1519\num\n0\nFEMALE\n1971\n1\n\n\n1632\nuh\n0\nFEMALE\n1962\n2\n\n\n1632\num\n0\nFEMALE\n1962\n2\n\n\n1519\nuh\n1\nFEMALE\n1971\n1\n\n\n1519\num\n0\nFEMALE\n1971\n1\n\n\n\n\n\n\nNow the sdac_disfluencies dataset needs some extra transformation to better prepare it for statistical interrogation. On the one hand the variables birth_year and education are not maximally informative. First it would be more ideal if birth_year would reflect the age of the speaker at the time of the conversation(s) and furthermore the coded values of education are not explicit as far what the numeric values refer to.\nThe second issue has to do with preparing the sdac_disfluencies dataset for statistical analysis. This involves converting our column types to the correct vector types for statistical methods. Specifically we need to convert our categorical variables to the R type ‘factor’ (fct). This includes of our current variables which are character vectors, but also the speaker_id and education which appear as numeric but do not reflect a continuous variables; one is merely a code which uniquely labels each speaker and the other is an ordinal list of educational levels.\nThis will be a three step process, first we will normalize the birth_year to reflect the age of the speaker, second we will convert all the relevant categorical variables to factors, and third we will convert the education variable to a factor adding meaningful labels for the levels of this factor.\nConsulting the online manual for this corpus, we see that the recording date for these conversations took place in 1992, so we can simply subtract the birth_year from 1992 to get each participant’s age. We’ll rename this new column age and drop the birth_year column.\n\nsdac_disfluencies &lt;- \n  sdac_disfluencies |&gt; # dataset\n  mutate(age = (1992 - birth_year)) |&gt; # calculate age\n  select(-birth_year) # drop `birth_year` column\n\nNow let’s convert all the variables which are character vectors. We can do this using the the factor() function; first on speaker_id and then, with the help of mutate_if(), to all the other variables which are character vectors.\n\nsdac_disfluencies &lt;- \n  sdac_disfluencies |&gt; # dataset\n  mutate(speaker_id = factor(speaker_id)) |&gt; # convert numeric to factor\n  mutate_if(is.character, factor) # convert all character to factor\n\nWe know from the data dictionary that the education column contains four values (0, 1, 2, 3, and 9). Again, consulting the corpus manual we can see what these values mean.\nEDUCATION    COUNT\n--------------------\n\n0            14      less than high school\n1            39      less than college\n2            309     college\n3            176     more than college\n9            4       unknown\nSo let’s convert education to a factor adding these descriptions as factor level labels. The function factor() can take an argument labels = which we can manually assign the label names for the factor levels in the order of the factor levels. Since the original values were numeric, the factor level ordering defaults to ascending order.\n\nsdac_disfluencies &lt;- \n  sdac_disfluencies |&gt; # dataset\n  mutate(education = factor(education, \n                            labels = c(\"less than high school\", # value 0\n                                       \"less than college\", # value 1\n                                       \"college\", # value 2\n                                       \"more than college\", # value 3 \n                                       \"unknown\"))) # value 9\n\nSo let’s take a look at the sdac_disfluencies dataset we’ve prepared for analysis.\n\nglimpse(sdac_disfluencies)\n\n#&gt; Rows: 447,212\n#&gt; Columns: 6\n#&gt; $ speaker_id &lt;fct&gt; 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519, 1519,…\n#&gt; $ filler     &lt;fct&gt; uh, um, uh, um, uh, um, uh, um, uh, um, uh, um, uh, um, uh,…\n#&gt; $ count      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n#&gt; $ sex        &lt;fct&gt; FEMALE, FEMALE, FEMALE, FEMALE, FEMALE, FEMALE, FEMALE, FEM…\n#&gt; $ education  &lt;fct&gt; college, college, college, college, less than college, less…\n#&gt; $ age        &lt;dbl&gt; 30, 30, 30, 30, 21, 21, 30, 30, 21, 21, 30, 30, 21, 21, 21,…\n\n\nNow the datasets dative and sdac_disfluencies are ready to be statistically interrogated."
  },
  {
    "objectID": "inference.html#univariate-analysis",
    "href": "inference.html#univariate-analysis",
    "title": "10  Inference",
    "section": "\n10.6 Univariate analysis",
    "text": "10.6 Univariate analysis\nIn what follows I will provide a description of inferential data analysis when only one variable is to be interrogated. This is known as a univariate analysis, or one-variable analysis. We will consider a case when the variable is categorical and the other continuous.\n\n10.6.1 Categorical\nAs an example of a univariate analysis where the variable used in the analysis is categorical we will look at the dative dataset. In this analysis we may be interested in knowing whether the recipient role in a ditransitive construction is realized more as an ‘NP’ or ‘PP’.\nDescriptive assessment\nThe realization_of_recipient variable contains the relevant information. Let’s take a first look using the skimr package.\n\ndative |&gt; # dataset\n  select(realization_of_recipient) |&gt; # select the variable\n  skim() |&gt; # get data summary\n  yank(\"factor\") # only show factor-oriented information\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\nrealization_of_recipient\n0\n1\nFALSE\n2\nNP: 2414, PP: 849\n\n\n\n\nThe output from skim() produces various pieces of information that can be helpful. On the one hand we get diagnostics that tell us if there are missing cases (NA values), what the proportion of complete cases is, if the the factor is ordered, how many distinct levels the factor has, as well as the level counts.\nLooking at the top_counts we can see that of the 3,263 observations, in 2,414 the dative is expressed as an ‘NP’ and 849 as ‘PP’. Numerically we can see that there is a difference between the use of the alternation types. A visualization is often helpful for descriptive purposes in statistical analysis. In this particular case, however, we are considering a single categorical variable with only two levels (values) so a visualization is not likely to be more informative than the numeric values we have already obtained. But for demonstration purposes and to get more familiar with building plots, let’s create a visualization.\n\ndative |&gt; # dataset\n  ggplot(aes(x = realization_of_recipient)) + # mapping\n  geom_bar() + # geometry\n  labs(x = \"Realization of recipient\", y = \"Count\") # labels\n\n\n\nBarplot visualizing the realization of recipient\n\n\n\nThe question we want to address, however, is whether this numerical difference is in fact a statistical difference.\nStatistical interrogation\n\nTo statistical assess the distribution for a categorical variable, we will turn to the Chi-squared test. This test aims to gauge whether the numerical differences between ‘NP’ and ‘PP’ counts observed in the data is greater than what would be expected by chance. Chance in the case where there are only two possible outcome levels is 50/50. For our particular data where there are 3,263 observations half would be ‘NP’ and the other half ‘PP’ –specifically 1631.5 for each.\nTo run this test we first will need to create a cross-tabulation of the variable. We will use the xtabs() function to create the table.\n\nror_table &lt;- \n  xtabs(formula = ~ realization_of_recipient, # formula selecting the variable\n        data = dative) # dataset\n\nror_table # preview\n\n#&gt; realization_of_recipient\n#&gt;   NP   PP \n#&gt; 2414  849\n\n\nNo new information here, but the format (i.e. an object of class ‘table’) is what is important for the input argument for the chisq.test() function we will use to run the test.\n\nc1 &lt;- chisq.test(x = ror_table) # apply the chi-squared test to `ror_table`\n\nc1 # preview the test results\n\n#&gt; \n#&gt;  Chi-squared test for given probabilities\n#&gt; \n#&gt; data:  ror_table\n#&gt; X-squared = 751, df = 1, p-value &lt;2e-16\n\n\nThe preview of the c1 object reveals the main information of interest including the Chi-squared statistic, the degrees of freedom, and the \\(p\\)-value (albeit in scientific notation). However, the c1 is an ‘htest’ object an includes a number of other pieces information about the test.\n\nnames(c1) # preview column names\n\n#&gt; [1] \"statistic\" \"parameter\" \"p.value\"   \"method\"    \"data.name\" \"observed\" \n#&gt; [7] \"expected\"  \"residuals\" \"stdres\"\n\n\nFor our purposes let’s simply confirm that the \\(p\\)-value is lower than the standard .05 threshold for statistical significance.\n\nc1$p.value &lt; .05 # confirm p-value below .05\n\n#&gt; [1] TRUE\n\n\nOther information can be organized in a more readable format using the broom package’s augment() function.\n\nc1 |&gt; # statistical result\n  augment() # view detailed statistical test information\n\n#&gt; # A tibble: 2 × 6\n#&gt;   realization_of_recipient .observed .prop .expected .resid .std.resid\n#&gt;   &lt;fct&gt;                        &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1 NP                            2414 0.740     1632.   19.4       27.4\n#&gt; 2 PP                             849 0.260     1632.  -19.4      -27.4\n\n\nHere we can see the observed and expected counts and the proportions for each level of realization_of_recipient. We also get additional information concerning residuals, but we will leave these aside.\nEvaluation\nAt this point we may think we are done. We have statistically interrogated the realization_of_recipient variable and found that the difference between ‘NP’ and ‘PP’ realization in the datives in this dataset is statistically significant. However, we need to evaluate the size (‘effect size’) and the reliability of the effect (‘confidence interval’). The effectsize package provides a function effectsize() that can provide us both the effect size and confidence interval.\n\neffects &lt;- \n  effectsize(c1) # evaluate effect size and generate a confidence interval (fei type given 2x1 contingency table)\n\neffects # preview effect size and confidence interval\n\n#&gt; Fei  |       95% CI\n#&gt; -------------------\n#&gt; 0.48 | [0.45, 1.00]\n#&gt; \n#&gt; - Adjusted for uniform expected probabilities.\n#&gt; - One-sided CIs: upper bound fixed at [1.00].\n\n\neffectsize() recognizes the type of test results in c1 and calculates the appropriate effect size measure and generates a confidence interval. Since the effect statistic (“Fei”) falls between the 95% confidence interval this suggests the results are reliably interpreted (chances of Type I (false positive) or Type II (false negative) are low).\nNow, the remaining question is to evaluate whether the significant result here is a strong effect or not. To do this we can pass the effect size measure to the interpret_r() function.\n\ninterpret_r(effects$Fei) # interpret the effect size \n\n#&gt; [1] \"very large\"\n#&gt; (Rules: funder2019)\n\n\nTurns out we have a strong effect; the realization of dative alternation heavily favors the ‘NP’ form in our data. The potential reasons why are not considered in this univariate analysis, but we will return to this question later as we add independent variables to the statistical analysis.\n\n10.6.2 Continuous\nNow let’s turn to a case when the variable we aim to interrogate is non-categorical. For this case we will turn to the sdac_disfluencies dataset. Specifically we will aim to test whether the use of fillers is normally distributed across speakers.\n\n\n\n\n\n\nTip\n\n\n\nThis is an important step when working with numeric dependent variables as the type of distribution will dictate decisions about whether we will use parametric or non-parametric tests if we consider the extent to which an independent variable (or variables) can explain the variation of the dependent variable.\n\n\nSince the dataset is currently organized around fillers as the observational unit, I will first transform this dataset to sum the use of fillers for each speaker in the dataset.\n\nsdac_speaker_fillers &lt;- \n  sdac_disfluencies |&gt; # dataset\n  group_by(speaker_id) |&gt; # group by each speaker\n  summarise(sum = sum(count)) |&gt; # add up all fillers used\n  ungroup() # remove grouping parameter\n\n\n\n\n\nTable 10.4: First 10 observations of sdac_speaker_fillers dataset.\n\nspeaker_id\nsum\n\n\n\n155\n28\n\n\n1000\n45\n\n\n1001\n264\n\n\n1002\n54\n\n\n1004\n45\n\n\n1005\n129\n\n\n1007\n0\n\n\n1008\n27\n\n\n1010\n2\n\n\n1011\n54\n\n\n\n\n\n\nDescriptive assessment\nLet’s perform some descriptive assessement of the variable of interest sum. First let’s apply the skim() function and retrieve just the relevant numeric descriptors with yank(). One twist here, however, is that I’ve customized the skim() function using the skim_with() to remove the default histogram and add the Interquartile Range (IQR) to the output. This new skim function num_skim() will take the place of skim().\n\nnum_skim &lt;- \n  skim_with(numeric = sfl(hist = NULL, # remove hist skim\n                                   iqr = IQR)) # add IQR to skim\n\nsdac_speaker_fillers |&gt; # dataset\n  select(sum) |&gt; # variable of interest\n  num_skim() |&gt; # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\niqr\n\n\nsum\n0\n1\n87.1\n108\n0\n16\n45\n114\n668\n98\n\n\n\n\nWe see here that the mean use of fillers is 87.1 across speakers. However, the standard deviation and IQR are large relative to this mean which indicates that the dispersion is quite large, in other words this suggests that there are large differences between speakers. Furthermore, since the median (p50) is smaller than the mean, the distribution is right skewed.\nLet’s look a couple visualizations of this distribution to appreciate these descriptives. A histogram will provide us a view of the distribution using the counts of the values of sum and a density plot will provide a smooth curve which represents the scaled distribution of the observed data.\n\n\np1 &lt;- \n  sdac_speaker_fillers |&gt; # dataset\n  ggplot(aes(sum)) + # mapping\n  geom_histogram() +  # geometry\n  labs(x = \"Fillers\", y = \"Count\")\n\np2 &lt;- \n  sdac_speaker_fillers |&gt; # dataset\n  ggplot(aes(sum)) + # mapping\n  geom_density() + # geometry\n  geom_rug() +  # visualize individual observations\n  labs(x = \"Fillers\", y = \"Density\")\n\np1 + p2 + plot_annotation(\"Filler count distributions.\")\n\n\n\nFigure 10.1: ?(caption)\n\n\n\nFrom the plots in Figure 10.1 we see that our initial intuitions about the distribution of sum are correct. There is large dispersion between speakers and the data distribution is right skewed.\n\n\n\n\n\n\nTip\n\n\n\nNote that I’ve used the patchwork package for organizing the display of plots and including a plot annotation label.\n\n\nSince our aim is to test for normality, we can generate a Quantile-Quantile plots (QQ Plot).\n\nsdac_speaker_fillers |&gt; # dataset\n  ggplot(aes(sample = sum)) + # mapping\n  stat_qq() + # calculate expected quantile-quantile distribution\n  stat_qq_line() # plot the qq-line\n\n\n\n\n\n\n\nSince many points do not fall on the expected normal distribution line we have even more evidence to support the notion that the distribution of sum is non-normal.\nStatistical interrogation\nAlthough the descriptives and visualizations strongly suggest that we do not have normally distributed data let’s run a normality test. For this we turn to the shapiro.test() function which performs the Shapiro-Wilk test of normality. We pass the sum variable to this function to run the test.\n\ns1 &lt;- shapiro.test(sdac_speaker_fillers$sum) # apply the normality test to `sum`\n\ns1 # preview the test results\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  sdac_speaker_fillers$sum\n#&gt; W = 0.8, p-value &lt;2e-16\n\n\nAs we saw with the results from the chisq.test() function, the shapiro.test() function produces an object with information about the test including the \\(p\\)-value. Let’s run our logical test to see if the test is statistically significant.\n\ns1$p.value &lt; .05 # \n\n#&gt; [1] TRUE\n\n\nEvaluation\nThe results from the Shapiro-Wilk Normality Test tell us that the distribution of sum is statistically found to differ from the normal distribution. So in this case, statistical significance suggests that sum cannot be used as a parametric dependent variable. For our aims this is all the evaluation required. Effect size and confidence intervals are not applicable.\nIt is of note, however, that the expectation that the variable sum would conform to the normal distribution was low from the outset as we are working with count data. Count data, or frequencies, are in a strict sense not continuous, but rather discrete –meaning that they are real numbers (whole numbers which are always positive). This is a common informational type to encounter in text analysis."
  },
  {
    "objectID": "inference.html#bivariate-analysis",
    "href": "inference.html#bivariate-analysis",
    "title": "10  Inference",
    "section": "\n10.7 Bivariate analysis",
    "text": "10.7 Bivariate analysis\nA more common scenario in statistical analysis is the consideration of the relationship between two-variables, known as bivariate analysis.\n\n10.7.1 Categorical\nLet’s build on our univariate analysis of realization_of_recipient and include an explanatory, or independent variable which we will explore to test whether it can explain our earlier finding that ‘NP’ datives are more common that ‘PP’ datives. The question to test, then, is whether modality explains the distribution of the realization_of_recipient.\nDescriptive assessment\nBoth the realization_of_recipient and modality variables are categorical, specifically nominal as we can see by using skim().\n\ndative |&gt; \n  select(realization_of_recipient, modality) |&gt; # select key variables\n  skim() |&gt; # get custom data summary\n  yank(\"factor\") # only show factor-oriented information\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nrealization_of_recipient\n0\n1\nFALSE\n2\nNP: 2414, PP: 849\n\n\nmodality\n0\n1\nFALSE\n2\nspo: 2360, wri: 903\n\n\n\n\n\nFor this reason measures of central tendency are not applicable and we will turn to a contingency table to summarize the relationship. The janitor package has a set of functions, the primary function being tabyl(). Other functions used here are to adorn the contingency table with totals, percentages, and to format the output for readability.\n\ndative |&gt; \n  tabyl(realization_of_recipient, modality) |&gt; # cross-tabulate\n  adorn_totals(c(\"row\", \"col\")) |&gt; # provide row and column totals\n  adorn_percentages(\"col\") |&gt; # add percentages to the columns\n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |&gt; # round the digits\n  adorn_ns() |&gt; # add observation number\n  adorn_title(\"combined\") |&gt; # add a header title\n  kable(booktabs = TRUE) # pretty table)\n\n\n\nTable 10.5: Contingency table for realization_of_recipient and modality.\n\n\n\n\n\n\n\nrealization_of_recipient/modality\nspoken\nwritten\nTotal\n\n\n\nNP\n79% (1859)\n61% (555)\n74% (2414)\n\n\nPP\n21% (501)\n39% (348)\n26% (849)\n\n\nTotal\n100% (2360)\n100% (903)\n100% (3263)\n\n\n\n\n\n\nTo gain a better appreciation for this relationship let’s generate a couple plots one which shows cross-tabulated counts and the other calculated proportions.\n\np1 &lt;- \n  dative |&gt; # dataset\n  ggplot(aes(x = realization_of_recipient, fill = modality)) + # mappings\n  geom_bar() + # geometry\n  labs(y = \"Count\", x = \"Realization of recipient\") # labels\n\np2 &lt;- \n  dative |&gt; # dataset\n  ggplot(aes(x = realization_of_recipient, fill = modality)) + # mappings\n  geom_bar(position = \"fill\") + # geometry, with fill for proportion plot\n  labs(y = \"Proportion\", x = \"Realization of recipient\", fill = \"Modality\") # labels\n\np1 &lt;- p1 + theme(legend.position = \"none\") # remove legend from left plot\n\np1 + p2 + plot_annotation(\"Relationship between Realization of recipient and Modality.\")\n\n\n\n\n\n\n\nLooking at the count plot (in the left pane) we see that large difference between the realization of the dative as an ‘NP’ or ‘PP’ obscures to some degree our ability to see to what degree modality is related to the realization of the dative. So, a proportion plot (in the right pane) standardizes each level of realization_of_recipient to provide a more comparable view. From the proportion plot we see that there appears to be a trend towards more use of ‘PP’ than ‘NP’ in the written modality.\nStatistical interrogation\nAlthough the proportion plot is visually helpful, we use the raw counts to statistically analyze this relationship. Again, as we are working with categorical variables, now for a dependent and independent variable, we use the Chi-squared test. And as before we need to create the cross-tabulation table to pass to the chisq.test() to perform the test.\n\nror_mod_table &lt;- \n  xtabs(formula = ~ realization_of_recipient + modality, # formula \n        data = dative) # dataset\n\nc2 &lt;- chisq.test(ror_mod_table) # apply the chi-squared test to `ror_mod_table`\n\nc2 # # preview the test results\n\n#&gt; \n#&gt;  Pearson's Chi-squared test with Yates' continuity correction\n#&gt; \n#&gt; data:  ror_mod_table\n#&gt; X-squared = 101, df = 1, p-value &lt;2e-16\n\nc2$p.value &lt; .05 # confirm p-value below .05\n\n#&gt; [1] TRUE\n\n\nWe can preview the result and provide a confirmation of the \\(p\\)-value. This evidence suggests that there is a difference between the distribution of dative realization according to modality.\nWe can also see more details about the test.\n\nc2 |&gt; # statistical result\n  augment() # view detailed statistical test information\n\n#&gt; # A tibble: 4 × 9\n#&gt;   realization_of_…¹ modal…² .obse…³ .prop .row.…⁴ .col.…⁵ .expe…⁶ .resid .std.…⁷\n#&gt;   &lt;fct&gt;             &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 NP                spoken     1859 0.570   0.770   0.788   1746.   2.71    10.1\n#&gt; 2 PP                spoken      501 0.154   0.590   0.212    614.  -4.56   -10.1\n#&gt; 3 NP                written     555 0.170   0.230   0.615    668.  -4.37   -10.1\n#&gt; 4 PP                written     348 0.107   0.410   0.385    235.   7.38    10.1\n#&gt; # … with abbreviated variable names ¹​realization_of_recipient, ²​modality,\n#&gt; #   ³​.observed, ⁴​.row.prop, ⁵​.col.prop, ⁶​.expected, ⁷​.std.resid\n\n\nEvaluation\nNow we want to calculate the effect size and the confidence interval to provide measures of assurance that our finding is robust.\n\neffects &lt;- effectsize(c2) # evaluate effect size and generate a confidence interval\n\neffects # preview effect size and confidence interval\n\n#&gt; Cramer's V (adj.) |       95% CI\n#&gt; --------------------------------\n#&gt; 0.18              | [0.15, 1.00]\n#&gt; \n#&gt; - One-sided CIs: upper bound fixed at [1.00].\n\ninterpret_r(effects$Cramers_v) # interpret the effect size\n\n#&gt; [1] \"small\"\n#&gt; (Rules: funder2019)\n\n\nWe get effect size and confidence interval information. Note that the effect size, reflected by Cramer’s V, for this relationship is weak. This points out an important aspect to evaluation of statistical tests. The fact that a test is significant does not mean that it is meaningful. A small effect size suggests that we should be cautious about the extent to which this significant finding is robust in the population from which the data is sampled.\n\n10.7.2 Continuous\nFor a bivariate analysis in which the dependent variable is not categorical, we will turn to the sdac_disfluencies dataset. The question we will pose to test is whether the use of fillers is related to the type of filler (‘uh’ or ‘um’).\nDescriptive assessment\nThe key variables to assess in this case are the variables count and filler. But before we start to explore this relationship we will need to transform the dataset such that each speaker’s use of the levels of filler is summed. We will use group_by() to group speaker_id and filler combinations and then use summarize() to then sum the counts for each filler type for each speaker\n\nsdac_fillers &lt;- \n  sdac_disfluencies |&gt; # dataset\n  group_by(speaker_id, filler) |&gt; # grouping parameters\n  summarize(sum = sum(count)) |&gt; # summed counts for each speaker-filler combination\n  ungroup() # remove the grouping parameters\n\nLet’s preview this transformation.\n\n\n\n\nTable 10.6: First 10 observations from sdac_fillers dataset.\n\nspeaker_id\nfiller\nsum\n\n\n\n155\nuh\n28\n\n\n155\num\n0\n\n\n1000\nuh\n37\n\n\n1000\num\n8\n\n\n1001\nuh\n262\n\n\n1001\num\n2\n\n\n1002\nuh\n34\n\n\n1002\num\n20\n\n\n1004\nuh\n30\n\n\n1004\num\n15\n\n\n\n\n\n\nLet’s take a look at them together by grouping the dataset by filler and then using the custom skim function num_skim() for the numeric variablecount.\n\nsdac_fillers |&gt; # dataset\n  group_by(filler) |&gt; # grouping parameter\n  num_skim() |&gt; # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nfiller\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\niqr\n\n\n\nsum\nuh\n0\n1\n71.4\n91.5\n0\n14\n39\n91\n661\n77\n\n\nsum\num\n0\n1\n15.7\n31.0\n0\n0\n4\n16\n265\n16\n\n\n\n\n\nWe see here that the standard deviation and IQR for both ‘uh’ and ‘um’ are relatively large for the respective means (71.4 and 15.7) suggesting the distribution is quite dispersed. Let’s take a look at a boxplot to visualize the counts in sum for each level of filler.\n\np1 &lt;- \n  sdac_fillers |&gt; # dataset\n  ggplot(aes(x = filler, y = sum)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  labs(x = \"Filler\", y = \"Counts\") # labels\n\np2 &lt;- \n  sdac_fillers |&gt; # dataset\n  ggplot(aes(x = filler, y = sum)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  ylim(0, 100) + # scale the y axis to trim outliers\n  labs(x = \"Filler\", y = \"\") # labels\n\np1 + p2\n\n\n\n\n\n\n\nIn the plot in the left pane we see a couple things. First, it appears that there is in fact quite a bit of dispersion as there are quite a few outliers (dots) above the lines extending from the boxes. Recall that the boxes represent the first and third quantile, that is the IQR and that the notches represent the confidence interval. Second, when we compare the boxes and their notches we see that there is little overlap (looking horizontally). In the right pane I’ve zoomed in a bit trimming some outliers to get a better view of the relationship between the boxes. Since the overlap is minimal and in particular the notches do not overlap at all, this is a good indication that there is a significant trend.\nFrom the descriptive statistics and the visual summary it appears that the filler ‘uh’ is more common than ‘um’. It’s now time to submit this to statistical interrogation.\nStatistical interrogation\n\nIn a bivariate (and multivariate) analysis where the dependent variable is non-categorical we apply Linear Regression Modeling (LM). The default assumption of linear models, however, is that the dependent variable is normally distributed. As we have seen our variable sum does not conform to the normal distribution. We know this because of our tests in the univariate case, but as mentioned at the end of that section, we are working with count data which by nature is understood as discrete and not continuous in a strict technical sense. So instead of using the linear model for our regression analysis we will use the Generalized Linear Model (GLM) (R. Harald Baayen 2008; Gries 2013).\nThe function glm() implements generalized linear models. In addition to the formula (sum ~ filler) and the dataset to use, we also include an appropriate distribution family for the dependent variable. For count and frequency data the appropriate family is the “Poisson” distribution.\n\nm1 &lt;- \n  glm(formula = sum ~ filler, # formula\n      data = sdac_fillers, # dataset\n      family = \"poisson\") # distribution family\n\nsummary(m1) # preview the test results\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = sum ~ filler, family = \"poisson\", data = sdac_fillers)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;    Min      1Q  Median      3Q     Max  \n#&gt; -11.95   -5.61   -3.94    0.80   41.99  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)  4.26794    0.00564     757   &lt;2e-16 ***\n#&gt; fillerum    -1.51308    0.01327    -114   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for poisson family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 72049  on 881  degrees of freedom\n#&gt; Residual deviance: 55071  on 880  degrees of freedom\n#&gt; AIC: 58524\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6\n\n\nLet’s focus on the coefficients, specifically for the ‘fillerum’ line. Since our factor filler has two levels one level is used as the reference to contrast with the other level. In this case by default the first level is used as the reference. Therefore the coefficients we see in ‘fillerum’ are ‘um’ in contrast to ‘uh’. Without digging into the details of the other parameter statistics, let’s focus on the last column which contains the \\(p\\)-value. A convenient aspect of the summary() function when applied to regression model results is that it provides statistical significance codes. In this case we can see that the contrast between ‘uh’ and ‘um’ is signficant at \\(p &lt; .001\\) which of course is lower than our standard threshold of \\(.05\\).\nTherefore we can say with some confidence that the filler ‘uh’ is more frequent than ‘um’.\nEvaluation\nGiven we have found a significant effect for filler, let’s look at evaluating the effect size and the confidence interval. Again, we use the effectsize() function. We then can preview the effects object. Note that effect size of interest is in the second row of the coefficient (Std_Coefficient) so we subset this column to extract only the effect coefficient for the filler contrast.\n\neffects &lt;- effectsize(m1) # evaluate effect size and generate a confidence interval\n\neffects # preview effect size and confidence interval\n\n#&gt; # Standardization method: refit\n#&gt; \n#&gt; Parameter   | Std. Coef. |         95% CI\n#&gt; -----------------------------------------\n#&gt; (Intercept) |       4.27 | [ 4.26,  4.28]\n#&gt; fillerum    |      -1.51 | [-1.54, -1.49]\n#&gt; \n#&gt; - Response is unstandardized.\n\ninterpret_r(effects$Std_Coefficient[2]) # interpret the effect size\n\n#&gt; [1] \"very large\"\n#&gt; (Rules: funder2019)\n\n\nThe coefficient statistic falls within the confidence interval and the effect size is strong so we can be confident that our findings are reliable given this data."
  },
  {
    "objectID": "inference.html#multivariate-analysis",
    "href": "inference.html#multivariate-analysis",
    "title": "10  Inference",
    "section": "\n10.8 Multivariate analysis",
    "text": "10.8 Multivariate analysis\nThe last case to consider is when we have more than one independent variable we want to use to assess their potential relationship to the dependent variable. Again we will consider a categorical and non-categorical dependent variable. But, in this case the implementation methods are quite similar, as we will see.\n\n10.8.1 Categorical\nFor the categorical multivariate case we will again consider the dative dataset and build on the previous analyses. The question to be posed is whether modality in combination with the length of the recipient (length_of_recipient) together explain the distribution of the realization of the recipient (realization_of_recipient).\nDescriptive assessment\nNow that we have three variables, there is more to summarize to get our descriptive information. Luckily, however, the same process can be applied to three (or more) variables using the group_by() function and then passed to skim(). In this case we have two categorical variables and one numeric variable. So we will group by both the categorical variables and then pass the numeric variable to the custom num_skim() function –pulling out only the relevant descriptive information for numeric variables with yank().\n\ndative |&gt; # dataset\n  select(realization_of_recipient, modality, length_of_recipient) |&gt; # select key variables\n  group_by(realization_of_recipient, modality) |&gt; # grouping parameters\n  num_skim() |&gt; # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nrealization_of_recipient\nmodality\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\niqr\n\n\n\nlength_of_recipient\nNP\nspoken\n0\n1\n1.14\n0.60\n1\n1\n1\n1\n12\n0\n\n\nlength_of_recipient\nNP\nwritten\n0\n1\n1.95\n1.59\n1\n1\n2\n2\n17\n1\n\n\nlength_of_recipient\nPP\nspoken\n0\n1\n2.30\n2.04\n1\n1\n2\n3\n15\n2\n\n\nlength_of_recipient\nPP\nwritten\n0\n1\n4.75\n4.10\n1\n2\n4\n6\n31\n4\n\n\n\n\n\nThere is much more information now that we are considering multiple independent variables, but if we look over the measures of dispersion we can see that the median and the IQR are relatively similar to their respective means suggesting that there are fewer outliers and relativley little skew.\nLet’s take a look at a visualization of this information. Since we are working with a categorical dependent variable and there is one non-categorical variable we can use a boxplot. The addition here is to include a color mapping which will provide a distinct box for each level of modality (‘written’ and ‘spoken’).\n\np1 &lt;- \n  dative |&gt; # dataset\n  ggplot(aes(x = realization_of_recipient, y = length_of_recipient, color = modality)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  labs(x = \"Realization of recipient\", y = \"Length of recipient (in words)\", color = \"Modality\") # labels\n\np2 &lt;- \n  dative |&gt; # dataset\n  ggplot(aes(x = realization_of_recipient, y = length_of_recipient, color = modality)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  ylim(0, 15) + # scale the y axis to trim outliers\n  labs(x = \"Realization of recipient\", y = \"\", color = \"Modality\") # labels\n\np1 &lt;- p1 + theme(legend.position = \"none\") # remove the legend from the left pane plot\n\np1 + p2\n\n\n\n\n\n\n\nIn the left pane we see the entire visualization including all outliers. From this view it appears that there is a potential trend that the length of the recipient is larger when the realization of the recipient is ‘PP’. There is also a potential trend for modality with written language showing longer recipient lengths overall. The pane on the right is scaled to get a better view of the boxes by scaling the y-axis down and as such trimming the outliers. This plot shows more clearly that the length of the recipient is longer when the recipient is realized as a ‘PP’. Again, the contrast in modality is also a potential trend, but the boxes (of the same color), particularly for the spoken modality overlap to some degree.\nSo we have some trends in mind which will help us interpret the statistical interrogation so let’s move there next.\nStatistical interrogation\nOnce we involve more than two variables, the choice of statistical method turns towards regression. In the case that the dependent variable is categorical, however, we will use Logistic Regression. The workhorse function glm() can be used for a series of regression models, including logistic regression. The requirement, however, is that we specify the family of the distribution. For logistic regression the family is “binomial”. The formula includes the dependent variable as a function of our other two variables, each are separated by the + operator.\n\nm1 &lt;- glm(formula = realization_of_recipient ~ modality + length_of_recipient, # formula\n          data = dative, # dataset\n          family = \"binomial\") # distribution family\n\nsummary(m1) # preview the test results\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = realization_of_recipient ~ modality + length_of_recipient, \n#&gt;     family = \"binomial\", data = dative)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;    Min      1Q  Median      3Q     Max  \n#&gt; -4.393  -0.598  -0.598   0.132   1.924  \n#&gt; \n#&gt; Coefficients:\n#&gt;                     Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)          -2.3392     0.0797  -29.35   &lt;2e-16 ***\n#&gt; modalitywritten      -0.0483     0.1069   -0.45     0.65    \n#&gt; length_of_recipient   0.7081     0.0420   16.86   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 3741.1  on 3262  degrees of freedom\n#&gt; Residual deviance: 3104.7  on 3260  degrees of freedom\n#&gt; AIC: 3111\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 5\n\n\nThe results from the model again provide a wealth of information. But the key information to focus on is the coefficients. In particular the coefficients for the independent variables modality and length_of_recipient. What we notice, is that the \\(p\\)-value for length_of_recipient is significant, but the contrast between ‘written’ and ‘spoken’ for modality is not. If you recall, we used this same dataset to explore modality as a single indpendent variable earlier –and it was found to be significant. So why now is it not? The answer is that when multiple variables are used to explain the distribution of a measure (dependent variable) each variable now adds more information to explain the dependent variable –each has it’s own contribution. Since length_of_recipient is significant, this suggests that the explanatory power of modality is weak, especially when compared to length_of_recipient. This make sense as we saw in the earlier model the fact that the effect size for modality was not strong and that is now more evident that the length_of_recipient is included in the model.\nEvaluation\nNow let’s move on and gauge the effect size and calculate the confidence interval for length_of_recipient in our model. We apply the effectsize() function to the model and then use interpret_r() on the coefficient of interest (which is in the fourth row of the Std_Coefficients column).\n\neffects &lt;- effectsize(m1) # evaluate effect size and generate a confidence interval\n\neffects # preview effect size and confidence interval\n\n#&gt; # Standardization method: refit\n#&gt; \n#&gt; Parameter           | Std. Coef. |         95% CI\n#&gt; -------------------------------------------------\n#&gt; (Intercept)         |      -1.03 | [-1.15, -0.92]\n#&gt; modalitywritten     |      -0.05 | [-0.26,  0.16]\n#&gt; length_of_recipient |       1.46 | [ 1.30,  1.64]\n#&gt; \n#&gt; - Response is unstandardized.\n\ninterpret_r(effects$Std_Coefficient[4]) # interpret the effect size\n\n#&gt; [1] NA\n#&gt; (Rules: funder2019)\n\n\nWe see we have a coefficient that falls within the confidence interval and the effect size is large. So we can saw with some confidence that the length of the recipient is a significant predictor of the use of ‘PP’ as the realization of the recipient in the dative alternation.\n\n10.8.2 Continuous\nThe last case we will consider here is when the dependent variable is non-categorical and we have more than one independent variable. The question we will pose is whether the type of filler and the sex of the speaker can explain the use of fillers in conversational speech.\nWe will need to prepare the data before we get started as our current data frame sdac_fillers has filler and the sum count for each filler grouped by speaker –but it does not include the sex of each speaker. The sdac_disfluencies data frame does have the sex column, but it has not been grouped by speaker. So let’s transform the sdac_disfluencies summarizing it to only get the speaker_id and sex combinations. This should result in a data frame with 441 observations, one observation for each speaker in the corpus.\n\nsdac_speakers_sex &lt;- \n  sdac_disfluencies |&gt; # dataset\n  distinct(speaker_id, sex) # summarize for distinct `speaker_id` and `sex` values\n\nLet’s preview the first 10 observations form this transformation.\n\n\n\n\nTable 10.7: First 10 observations of the sdac_speakers_sex data frame.\n\nspeaker_id\nsex\n\n\n\n155\nNA\n\n\n1000\nFEMALE\n\n\n1001\nMALE\n\n\n1002\nFEMALE\n\n\n1004\nFEMALE\n\n\n1005\nFEMALE\n\n\n1007\nFEMALE\n\n\n1008\nFEMALE\n\n\n1010\nMALE\n\n\n1011\nFEMALE\n\n\n\n\n\n\nGreat, now we have each speaker_id and sex for all 441 speakers. One thing to note, however, is that speaker ‘155’ does not have a value for sex –this seems to be an error in the metadata that we will need to deal with before we proceed in our analysis. Let’s move on to join our new sdac_speakers_sex data frame and the sdac_fillers data frame.\nNow that we have a complete dataset with speaker_id and sex we will now join this dataset with our sdac_fillers dataset effectively adding the column sex. We want to keep all the observations in sdac_fillers and add the column sex for observations that correspond between each data frame for the column speaker_id so we will use a left join with the function left_join() with the sdac_fillers dataset on the left.\n\nsdac_fillers_sex &lt;- \n  left_join(sdac_fillers, sdac_speakers_sex) # join\n\nNow let’s preview the first observations in this new sdac_fillers_sex data frame.\n\n\n\n\nTable 10.8: First 10 observations of the sdac_fillers_sex data frame.\n\nspeaker_id\nfiller\nsum\nsex\n\n\n\n155\nuh\n28\nNA\n\n\n155\num\n0\nNA\n\n\n1000\nuh\n37\nFEMALE\n\n\n1000\num\n8\nFEMALE\n\n\n1001\nuh\n262\nMALE\n\n\n1001\num\n2\nMALE\n\n\n1002\nuh\n34\nFEMALE\n\n\n1002\num\n20\nFEMALE\n\n\n1004\nuh\n30\nFEMALE\n\n\n1004\num\n15\nFEMALE\n\n\n\n\n\n\nAt this point let’s drop this speaker from the sdac_speakers_sex data frame.\n\nsdac_fillers_sex &lt;- \n  sdac_fillers_sex |&gt; # dataset\n  filter(speaker_id != \"155\") # drop speaker_id 155\n\nWe are now ready to proceed in our analysis.\nDescriptive assessment\nThe process by now should be quite routine for getting our descriptive statistics: select the key variables, group by the categorical variables, and finally pull the descriptives for the numeric variable.\n\nsdac_fillers_sex |&gt; # dataset\n  select(sum, filler, sex) |&gt; # select key variables\n  group_by(filler, sex) |&gt; # grouping parameters\n  num_skim() |&gt; # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nfiller\nsex\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\niqr\n\n\n\nsum\nuh\nFEMALE\n0\n1\n63.22\n76.5\n0\n12.0\n39.0\n81.8\n509\n69.8\n\n\nsum\nuh\nMALE\n0\n1\n78.74\n102.6\n0\n15.2\n37.5\n101.5\n661\n86.2\n\n\nsum\num\nFEMALE\n0\n1\n22.38\n36.3\n0\n1.0\n9.0\n28.0\n265\n27.0\n\n\nsum\num\nMALE\n0\n1\n9.92\n24.2\n0\n0.0\n1.0\n8.0\n217\n8.0\n\n\n\n\n\nLooking at these descriptives, it seems like there is quite a bit of variability for some combinations and not others. In short, it’s a mixed bag. Let’s try to make sense of these numbers with a boxplot.\n\np1 &lt;- \n  sdac_fillers_sex |&gt; # dataset\n  ggplot(aes(x = filler, y = sum, color = sex)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  labs(x = \"Filler\", y = \"Counts\", color = \"Sex\") # labels\n\np2 &lt;- \n  sdac_fillers_sex |&gt; # dataset\n  ggplot(aes(x = filler, y = sum, color = sex)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  ylim(0, 200) + # scale the y axis to trim outliers\n  labs(x = \"Filler\", y = \"\", color = \"Sex\") # labels\n\np1 &lt;- p1 + theme(legend.position = \"none\") # drop the legend from the left pane plot\n\np1 + p2\n\n\n\n\n\n\n\nWe can see that ‘uh’ is used more than ‘um’ overall. But that whereas men and women use ‘uh’ in similar ways, women use more ‘um’ than men. This is known as an interaction. So we will approach our statistical analysis with this in mind.\nStatistical interrogation\nWe will again use a generalized linear model with the glm() function to conduct our test. The distribution family will be the same has we are again using the sum as our dependent variable which contains discrete count values. The formula we will use, however, is new. Instead of adding a new variable to our independent variables, we will test the possible interaction between filler and sex that we noted in the descriptive assessment. To encode an interaction the * operator is used. So our formula will take the form sum ~ filler * sex. Let’s generate the model and view the summary of the test results as we have done before.\n\nm1 &lt;- \n  glm(formula = sum ~ filler * sex, # formula\n      data = sdac_fillers_sex, # dataset\n      family = \"poisson\") # distribution family\n\nsummary(m1) # preview the test results\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = sum ~ filler * sex, family = \"poisson\", data = sdac_fillers_sex)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;    Min      1Q  Median      3Q     Max  \n#&gt; -12.55   -6.21   -3.64    1.08   40.60  \n#&gt; \n#&gt; Coefficients:\n#&gt;                  Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)       4.14660    0.00876   473.2   &lt;2e-16 ***\n#&gt; fillerum         -1.03827    0.01714   -60.6   &lt;2e-16 ***\n#&gt; sexMALE           0.21955    0.01145    19.2   &lt;2e-16 ***\n#&gt; fillerum:sexMALE -1.03344    0.02791   -37.0   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for poisson family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 71956  on 879  degrees of freedom\n#&gt; Residual deviance: 53543  on 876  degrees of freedom\n#&gt; AIC: 56994\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6\n\n\nAgain looking at the coefficients we something new. First we see that there is a row for the filler contrast and the sex contrast but also the interaction between filler and sex (‘fillerum:sexMALE’). All rows show significant effects. It is important to note that when an interaction is explored and it is found to be significant, the other simple effects, known as main effects (‘fillerum’ and ‘sexMALE’), are ignored. Only the higer-order effect is considered significant.\nNow what does the ‘fillerum:sexMALE’ row mean. It means that there is an interaction between filler and sex. the directionality of that interaction should be interpreted using our descriptive assessment, in particular the visual boxplots we generated. In sum, women use more ‘um’ than men or stated another way men use ‘um’ less than women.\nEvaluation\nWe finalize our analysis by looking at the effect size and confidence intervals.\n\neffects &lt;- effectsize(m1) # evaluate effect size and generate a confidence interval\n\neffects # preview effect size and confidence interval\n\n#&gt; # Standardization method: refit\n#&gt; \n#&gt; Parameter        | Std. Coef. |         95% CI\n#&gt; ----------------------------------------------\n#&gt; (Intercept)      |       4.15 | [ 4.13,  4.16]\n#&gt; fillerum         |      -1.04 | [-1.07, -1.00]\n#&gt; sexMALE          |       0.22 | [ 0.20,  0.24]\n#&gt; fillerum:sexMALE |      -1.03 | [-1.09, -0.98]\n#&gt; \n#&gt; - Response is unstandardized.\n\ninterpret_r(effects$Std_Coefficient[4]) # interpret the effect size\n\n#&gt; [1] \"very large\"\n#&gt; (Rules: funder2019)\n\n\nWe can conclude, then, that there is a strong interaction effect for filler and sex and that women use more ‘um’ than men."
  },
  {
    "objectID": "inference.html#summary-1",
    "href": "inference.html#summary-1",
    "title": "10  Inference",
    "section": "\n10.9 Summary",
    "text": "10.9 Summary\nIn this chapter we have discussed various approaches to conducting inferential data analysis. Each configuration, however, always includes a descriptive assessment, statistical interrogation, and an evaluation of the results. We considered univariate, bivariate, and multivariate analyses using both categorical and non-categorical dependent variables to explore the similarities and differences between these approaches.\n\n\n\n\n\nBaayen, R. Harald. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics Using r. Cambridge Univ Pr.\n\n\nBaayen, R. H., and Elnaz Shafaei-Bajestan. 2019. languageR: Analyzing Linguistic Data: A Practical Introduction to Statistics. https://CRAN.R-project.org/package=languageR.\n\n\nGries, Stefan Th. 2013. Statistics for Linguistics with r. A Practical Introduction. 2nd revise."
  },
  {
    "objectID": "communication.html",
    "href": "communication.html",
    "title": "Communication",
    "section": "",
    "text": "In this section I cover the steps in presenting the findings of the research both as a research document and as a reproducible research project. Both research documents and reproducible projects are fundamental components of modern scientific inquiry. On the one hand a research document provides readers a detailed summary of the main import of the research study. On the other hand making the research project available to interested readers ensures that the scientific community can gain insight into the process implemented in the research and thus enables researchers to vet and extend this research to build a more robust and verifiable research base."
  },
  {
    "objectID": "reporting.html#questions",
    "href": "reporting.html#questions",
    "title": "11  Reporting",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\nConceptual questions\n\n\n\n\n…\n…\n\n\n\n\n\n\n\n\n\nTechnical exercises\n\n\n\n\n…\n…"
  },
  {
    "objectID": "collaboration.html#questions",
    "href": "collaboration.html#questions",
    "title": "12  Collaboration",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\n\nConceptual questions\n\n\n\n\n…\n…\n\n\n\n\n\n\n\n\n\nTechnical exercises\n\n\n\n\n…\n…"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ackoff, Russell L. 1989. “From Data to Wisdom.” Journal\nof Applied Systems Analysis 16 (1): 3–9.\n\n\nÄdel, Annelie. 2020. “Corpus Compilation.” In A\nPractical Handbook of Corpus Linguistics, edited by Magali Paquot\nand Stefan Th. Gries, 3–24. Switzerland: Springer.\n\n\nAllaire, JJ. 2022. Quarto: R Interface to Quarto Markdown Publishing\nSystem. https://github.com/quarto-dev/quarto-r.\n\n\nAllaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier\nLuraschi, Kevin Ushey, Aron Atkins, et al. 2023. Rmarkdown: Dynamic\nDocuments for r. https://CRAN.R-project.org/package=rmarkdown.\n\n\nAlmeida, Tiago A, José María Gómez Hildago, and Akebo Yamakami. 2011.\n“Contributions to the Study of SMS Spam Filtering: New Collection\nand Results.” In Proceedings of the 2011 ACM Symposium on\nDocument Engineering (DOCENG’11), 4. Mountain View, CA.\n\n\nBaayen, R. Harald. 2004. “Statistics in Psycholinguistics: A\nCritique of Some Current Gold Standards.” Mental Lexicon\nWorking Papers 1 (1): 1–47.\n\n\n———. 2008. Analyzing Linguistic Data: A Practical Introduction to\nStatistics Using r. Cambridge Univ Pr.\n\n\n———. 2011. “Corpus Linguistics and Naive Discriminative\nLearning.” Revista Brasileira de Lingu\\’\\istica Aplicada\n11 (2): 295–328.\n\n\nBaayen, R. Harald, L. B. Feldman, and R. Schreuder. 2006.\n“Morphological Influences on the Recognition of Monosyllabic\nMonomorphemic Words.” Journal of Memory and Language 55:\n290–313. https://doi.org/10.1016/j.jml.2006.03.008.\n\n\nBaayen, R. H., and Elnaz Shafaei-Bajestan. 2019. languageR:\nAnalyzing Linguistic Data: A Practical Introduction to Statistics.\nhttps://CRAN.R-project.org/package=languageR.\n\n\nBao, Wang, Ning Lianju, and Kong Yue. 2019. “Integration of\nUnsupervised and Supervised Machine Learning Algorithms for Credit Risk\nAssessment.” Expert Systems with Applications 128\n(August): 301–15. https://doi.org/10.1016/j.eswa.2019.02.033.\n\n\nBenoit, Kenneth. 2020. Quanteda.corpora: A Collection of Corpora for\nQuanteda. http://github.com/quanteda/quanteda.corpora.\n\n\nBenoit, Kenneth, David Muhr, and Kohei Watanabe. 2021. Stopwords:\nMultilingual Stopword Lists. https://github.com/quanteda/stopwords.\n\n\nBroman, Karl W., and Kara H. Woo. 2018. “Data Organization in\nSpreadsheets.” The American Statistician 72 (1): 2–10.\nhttps://doi.org/10.1080/00031305.2017.1375989.\n\n\nBrown, Keith. 2005. Encyclopedia of Language and Linguistics.\nVol. 1. Elsevier.\n\n\nBuckheit, Jonathan B., and David L. Donoho. 1995. “Wavelab and\nReproducible Research.” In Wavelets and Statistics,\n55–81. Springer.\n\n\nBychkovska, Tetyana, and Joseph J. Lee. 2017. “At the Same Time:\nLexical Bundles in L1 and L2 University Student Argumentative\nWriting.” Journal of English for Academic Purposes 30\n(November): 38–52. https://doi.org/10.1016/j.jeap.2017.10.008.\n\n\nCampbell, Lyle. 2001. “The History of Linguistics.” In\nThe Handbook of Linguistics, edited by Mark Aronoff and Janie\nRees-Miller, 81–104. Blackwell Handbooks in Linguistics. Blackwell\nPublishers.\n\n\nCarmi, Elinor, Simeon J. Yates, Eleanor Lockley, and Alicja Pawluczuk.\n2020. “Data Citizenship: Rethinking Data Literacy in the Age of\nDisinformation, Misinformation, and Malinformation.” Internet\nPolicy Review 9 (2).\n\n\nChambers, John M. 2020. “S, r, and Data Science.”\nProceedings of the ACM on Programming Languages 4 (HOPL): 1–17.\nhttps://doi.org/10.1145/3386334.\n\n\nChan, Sin-wai. 2014. Routledge Encyclopedia of Translation\nTechnology. Routledge.\n\n\nConway, Lucian Gideon, Laura Janelle Gornick, Chelsea Burfeind, Paul\nMandella, Andrea Kuenzli, Shannon C. Houck, and Deven Theresa Fullerton.\n2012. “Does Complex or Simple Rhetoric Win Elections? An\nIntegrative Complexity Analysis of u.s. Presidential Campaigns.”\nPolitical Psychology 33 (5): 599–618. https://doi.org/10.1111/j.1467-9221.2012.00910.x.\n\n\nCross, Nigel. 2006. “Design as a Discipline.”\nDesignerly Ways of Knowing, 95–103.\n\n\n“Data Never Sleeps 7.0 Infographic.” 2019.\nhttps://www.domo.com/learn/infographic/data-never-sleeps-7.\n\n\nDeshors, Sandra C, and Stefan Th. Gries. 2016. “Profiling Verb\nComplementation Constructions Across New Englishes.”\nInternational Journal of Corpus Linguistics. 21 (2): 192–218.\n\n\nDesjardins, Jeff. 2019. “How Much Data Is Generated Each\nDay?” Visual Capitalist.\n\n\nDonoho, David. 2017. “50 Years of Data Science.”\nJournal of Computational and Graphical Statistics 26 (4):\n745–66. https://doi.org/10.1080/10618600.2017.1384734.\n\n\nDubnjakovic, Ana, and Patrick Tomlin. 2010. A Practical Guide to\nElectronic Resources in the Humanities. Elsevier.\n\n\nEisenstein, Jacob, Brendan O’Connor, Noah A Smith, and Eric P Xing.\n2012. “Mapping the Geographical Diffusion of New Words.”\nComputation and Language, 1–13. https://doi.org/10.1371/journal.pone.0113114.\n\n\nFrancom, Jerid. 2022. “Corpus Studies of Syntax.” In\nThe Cambridge Handbook of Experimental Syntax, edited by Grant\nGoodall, 687–713. Cambridge Handbooks in Language and Linguistics.\nCambridge University Press.\n\n\n———. 2023. Qtalrkit: Quantitative Text Analysis for Linguists\nResource Kit.\n\n\nGandrud, Christopher. 2015. Reproducible\nResearch with r and r Studio. Second edition. CRC Press.\n\n\nGentleman, Robert, and Duncan Temple Lang. 2007. “Statistical\nAnalyses and Reproducible Research.” Journal of Computational\nand Graphical Statistics 16 (1): 1–23.\n\n\nGilquin, Gaëtanelle, and Stefan Th Gries. 2009. “Corpora and\nExperimental Methods: A State-of-the-Art Review.” Corpus\nLinguistics and Linguistic Theory 5 (1): 1–26. https://doi.org/10.1515/CLLT.2009.001.\n\n\nGomez-Uribe, Carlos A., and Neil Hunt. 2015. “The Netflix\nRecommender System: Algorithms, Business Value, and Innovation.”\nACM Transactions on Management Information Systems (TMIS) 6\n(4): 1–19.\n\n\nGries, Stefan Th. 2021. Statistics for Linguistics with r. De\nGruyter Mouton.\n\n\nGries, Stefan Th. 2013. Statistics for Linguistics with r. A\nPractical Introduction. 2nd revise.\n\n\nGrieve, Jack, Andrea Nini, and Diansheng Guo. 2018. “Mapping\nLexical Innovation on American Social Media.” Journal of\nEnglish Linguistics 46 (4): 293–319.\n\n\nHead, Megan L., Luke Holman, Rob Lanfear, Andrew T. Kahn, and Michael D.\nJennions. 2015. “The Extent and Consequences of p-Hacking in\nScience.” PLOS Biology 13 (3): e1002106. https://doi.org/10.1371/journal.pbio.1002106.\n\n\nHicks, Stephanie C., and Roger D. Peng. 2019. “Elements and\nPrinciples for Characterizing Variation Between Data Analyses.”\narXiv. https://doi.org/10.48550/arXiv.1903.07639.\n\n\n“How to Make a Data Dictionary.” 2021. OSF Guides.\nhttps://help.osf.io/hc/en-us/articles/360019739054-How-to-Make-a-Data-Dictionary.\n\n\nHu, Minqing, and Bing Liu. 2004. “Mining and Summarizing Customer\nReviews.” In Proceedings of the Tenth ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining,\n168–77.\n\n\nIde, Nancy, Collin Baker, Christiane Fellbaum, Charles Fillmore, and\nRebecca Passonneau. 2008. “MASC: The Manually Annotated Sub-Corpus\nof American English.” In 6th International Conference on\nLanguage Resources and Evaluation, LREC 2008, 2455–60. European\nLanguage Resources Association (ELRA).\n\n\nIgnatow, Gabe, and Rada Mihalcea. 2017. An Introduction to Text\nMining: Research Design, Data Collection, and Analysis. Sage\nPublications.\n\n\nJaeger, T Florian, and Neal Snider. 2007. “Implicit Learning and\nSyntactic Persistence: Surprisal and Cumulativity.”\nUniversity of Rochester Working Papers in the Language Sciences\n3 (1).\n\n\nKaur, Jashanjot, and P. Kaur Buttar. 2018. “A Systematic Review on\nStopword Removal Algorithms.” International Journal on Future\nRevolution in Computer Science & Communication Engineering 4\n(4): 207–10.\n\n\nKearney, Michael W., Lluís Revilla Sancho, and Hadley Wickham. 2023.\nRtweet: Collecting Twitter Data. https://CRAN.R-project.org/package=rtweet.\n\n\nKerr, Norbert L. 1998. “HARKing: Hypothesizing After the Results\nAre Known.” Personality and Social Psychology Review 2\n(3): 196–217.\n\n\nKloumann, IM, CM Danforth, KD Harris, and CA Bliss. 2012.\n“Positivity of the English Language.” PloS One.\n\n\nKostić, Aleksandar, Tanja Marković, and Aleksandar Baucal. 2003.\n“Inflectional Morphology and Word Meaning: Orthogonal or\nCo-Implicative Cognitive Domains?” In Morphological Structure\nin Language Processing, edited by R. Harald Baayen and Robert\nSchreuder, 1–44. De Gruyter Mouton. https://doi.org/10.1515/9783110910186.1.\n\n\nKowsari, Kamran, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana\nMendu, Laura E. Barnes, and Donald E. Brown. 2019. “Text\nClassification Algorithms: A Survey.” Information 10\n(4): 150. https://doi.org/10.3390/info10040150.\n\n\nKrathwohl, David R. 2002. “A Revision of Bloom’s Taxonomy: An\nOverview.” Theory into Practice 41 (4): 212–18.\n\n\nKross, Sean, Nick Carchedi, Bill Bauer, and Gina Grdina. 2020.\nSwirl: Learn r, in r. http://swirlstats.com.\n\n\nKucera, H, and W N Francis. 1967. Computational Analysis of Present\nDay American English. Brown University Press Providence.\n\n\nLantz, Brett. 2013. Machine Learning with r. Birmingham: Packt\nPublishing.\n\n\nLewis, Michael. 2004. Moneyball: The Art of Winning an Unfair\nGame. WW Norton & Company.\n\n\nLozano, Cristóbal. 2009. “CEDEL2: Corpus Escrito Del Español\nL2.” Applied Linguistics Now: Understanding Language and\nMind/La Lingüística Aplicada Hoy: Comprendiendo El Lenguaje y La Mente.\nAlmería: Universidad de Almería, 197–212.\n\n\nMagueresse, Alexandre, Vincent Carles, and Evan Heetderks. 2020.\n“Low-Resource Languages: A Review of Past Work and Future\nChallenges.” arXiv. https://arxiv.org/abs/2006.07264.\n\n\nManning, Christopher. 2003. “Probabilistic Syntax.” In\nProbabilistic Linguistics, edited by Bod, Jennifer Hay, and\nJannedy, 289–341. Cambridge, MA: MIT Press.\n\n\nMarcus, Mitchell P, Beatrice Santorini, and Mary Ann Marcinkiewicz.\n1993. “Building a Large Annotated Corpus of English: The Penn\nTreebank.” Computational Linguistics 19 (2): 313–30.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging\nData Analytical Work Reproducibly Using r (and Friends).” The\nAmerican Statistician 72 (1): 80–88.\n\n\nMosteller, Frederick, and David L Wallace. 1963. “Inference in an\nAuthorship Problem.” Journal of the American Statistical\nAssociation 58 (302): 275–309. https://www.jstor.org/stable/2283270.\n\n\nMuñoz, Carmen, ed. 2006. Age and the Rate of Foreign Language\nLearning. 1st ed. Vol. 19. Second Language Acquisition Series.\nClevedon: Multilingual Matters.\n\n\nNivre, Joakim, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg,\nJan Hajič, Christopher D Manning, Ryan McDonald, et al. 2016.\n“Universal Dependencies V1: A Multilingual Treebank\nCollection.” Proceedings of the Tenth International\nConference on Language Resources and Evaluation (LREC’16), 1659–66.\n\n\nOlohan, Maeve. 2008. “Leave It Out! Using a Comparable Corpus to\nInvestigate Aspects of Explicitation in Translation.”\nCadernos de Tradução, 153–69.\n\n\nPaquot, Magali, and Stefan Th. Gries, eds. 2020. A Practical\nHandbook of Corpus Linguistics. Switzerland: Springer.\n\n\nRoediger, H. L. L, and K. B. B McDermott. 2000. “Distortions of\nMemory.” The Oxford Handbook of Memory, 149–62.\n\n\nRowley, Jennifer. 2007. “The Wisdom Hierarchy: Representations of\nthe DIKW Hierarchy.” Journal of Information Science 33\n(2): 163–80. https://doi.org/10.1177/0165551506070706.\n\n\nSaxena, Shweta, and Manasi Gyanchandani. 2020. “Machine Learning\nMethods for Computer-Aided Breast Cancer Diagnosis Using Histopathology:\nA Narrative Review.” Journal of Medical Imaging and Radiation\nSciences 51 (1): 182–93.\n\n\nTalarico, Jennifer M., and David C. Rubin. 2003. “Confidence, Not\nConsistency, Characterizes Flashbulb Memories.” Psychological\nScience 14 (5): 455–61. https://doi.org/10.1111/1467-9280.02453.\n\n\nVoigt, Rob, Nicholas P. Camp, Vinodkumar Prabhakaran, William L.\nHamilton, Rebecca C. Hetey, Camilla M. Griffiths, David Jurgens, Dan\nJurafsky, and Jennifer L. Eberhardt. 2017. “Language from Police\nBody Camera Footage Shows Racial Disparities in Officer Respect.”\nProceedings of the National Academy of Sciences 114 (25):\n6521–26.\n\n\nWasow, Thomas, and Jennifer Arnold. 2005. “Intuitions in\nLinguistic Argumentation.” Lingua 115 (11): 1481–96.\n\n\nWelbers, Kasper, and Wouter van Atteveldt. 2022. Rsyntax: Extract\nSemantic Relations from Text by Querying and Reshaping Syntax. https://CRAN.R-project.org/package=rsyntax.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of\nStatistical Software 59 (10). https://doi.org/10.18637/jss.v059.i10.\n\n\n———. 2022. Rvest: Easily Harvest (Scrape) Web Pages. https://CRAN.R-project.org/package=rvest.\n\n\n———. 2023. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Jennifer Bryan, Malcolm Barrett, and Andy Teucher.\n2023. Usethis: Automate Package and Project Setup. https://CRAN.R-project.org/package=usethis.\n\n\nWickham, Hadley, Jim Hester, Winston Chang, and Jennifer Bryan. 2022.\nDevtools: Tools to Make Developing r Packages Easier. https://CRAN.R-project.org/package=devtools.\n\n\nWulff, S, A Stefanowitsch, and Stefan Th. Gries. 2007. “Brutal\nBrits and Persuasive Americans.” Aspects of Meaning.\n\n\nXie, Yihui. 2023. Tinytex: Helper Functions to Install and Maintain\nTeX Live, and Compile LaTeX Documents. https://github.com/rstudio/tinytex."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Appendix A — Data",
    "section": "",
    "text": "…"
  }
]