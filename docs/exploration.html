<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>An Introduction to Quantitative Text Analysis for Linguistics - 8&nbsp; Exploration</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./prediction.html" rel="next">
<link href="./analysis.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script async="" src="https://hypothes.is/embed.js"></script><script src="site_libs/kePrint-0.0.1/kePrint.js"></script><link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><link rel="stylesheet" href="assets/css/mini.css">
</head>
<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./analysis.html">Analysis</a></li><li class="breadcrumb-item"><a href="./exploration.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Exploration</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">An Introduction to Quantitative Text Analysis for Linguistics</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com" rel="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://github.com/qtalr/book" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./qtalr-manuscript.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./orientation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Orientation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Text analysis in context</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./understanding-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Understanding data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./approaching-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Approaching analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./framing-research.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Framing research</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preparation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./acquire-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Acquire data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./curate-datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Curate datasets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./transform-datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transform datasets</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exploration.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Exploration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Inference</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./communication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Communication</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./contributing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Contributing</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./feedback.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Feedback <i class="fa-solid fa-comment" aria-label="comment"></i></span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-eda-orientation" id="toc-sec-eda-orientation" class="nav-link active" data-scroll-target="#sec-eda-orientation"><span class="header-section-number">8.1</span> Orientation</a>
  <ul class="collapse">
<li><a href="#sec-eda-research-goal" id="toc-sec-eda-research-goal" class="nav-link" data-scroll-target="#sec-eda-research-goal"><span class="header-section-number">8.1.1</span> Research goal</a></li>
  <li><a href="#sec-eda-approach" id="toc-sec-eda-approach" class="nav-link" data-scroll-target="#sec-eda-approach"><span class="header-section-number">8.1.2</span> Approach</a></li>
  </ul>
</li>
  <li>
<a href="#sec-eda-analysis" id="toc-sec-eda-analysis" class="nav-link" data-scroll-target="#sec-eda-analysis"><span class="header-section-number">8.2</span> Analysis</a>
  <ul class="collapse">
<li><a href="#sec-eda-descriptive" id="toc-sec-eda-descriptive" class="nav-link" data-scroll-target="#sec-eda-descriptive"><span class="header-section-number">8.2.1</span> Descriptive analysis</a></li>
  <li><a href="#sec-eda-unsupervised" id="toc-sec-eda-unsupervised" class="nav-link" data-scroll-target="#sec-eda-unsupervised"><span class="header-section-number">8.2.2</span> Unsupervised learning</a></li>
  </ul>
</li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">8.3</span> Summary</a></li>
  <li><a href="#activities" id="toc-activities" class="nav-link" data-scroll-target="#activities">Activities</a></li>
  <li><a href="#questions" id="toc-questions" class="nav-link" data-scroll-target="#questions">Questions</a></li>
  </ul><div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/qtalr/book/blob/main/exploration.qmd" class="toc-action">View source</a></p><p><a href="https://github.com/qtalr/book/edit/main/exploration.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/qtalr/book/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-exploration" class="quarto-section-identifier"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Exploration</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><div class="callout callout-style-default callout-tip callout-titled" title="Draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Draft
</div>
</div>
<div class="callout-body-container callout-body">
<p>Ready for review.</p>
</div>
</div>
<!--

Content:

- [ ] edit: consider the `tidycluster` package for clustering?

Exercises:

- [ ] add concept questions to Activities
- [ ] add exercises to Activities
- [ ] add thought questions/ case studies to prose sections

Formatting:

-->
<!---
> The real voyage of discovery consists not in seeking new landscapes, but in having new eyes.
>
> --- Marcel Proust
--->
<blockquote class="blockquote">
<p>The data speaks for itself, but only if we are willing to listen.</p>
<p>— Nate Silver</p>
</blockquote>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-regular fa-list-alt" aria-label="list-alt"></i> Outcomes</strong></p>
<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->
<ul>
<li>Identify when an exploratory data analysis approach is the best fit for a given research project.</li>
<li>Describe the fundamental methods of descriptive analysis and unsupervised learning, recognizing their strengths in revealing patterns and summarizing data.</li>
<li>Interpret the basic insights gained from data summarization and pattern recognition, considering how these insights could guide further questions or research.</li>
</ul>
</div>
</div>
</div>
<p>In this chapter, we examine a wide range of strategies for deriving insight from data in cases where the researcher does not start with a preconceived hypothesis or prediction, but rather the researcher aims to uncover patterns and associations from data allowing the data to guide the trajectory of the analysis. The chapter outlines two main branches of exploratory data analysis: 1) descriptive analysis which statistically and/ or visually summarizes a dataset and 2) unsupervised learning which is a machine learning approach that does not assume any particular relationship between variables in a dataset. Either through descriptive or unsupervised learning methods, exploratory data analysis employs quantitative methods to summarize, reduce, and sort complex datasets and statistically and visually interrogate a dataset in order to provide the researcher novel perspective to be qualitatively assessed.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-solid fa-terminal" aria-label="terminal"></i> Lessons</strong></p>
<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->
<!--
- [ ] Update lesson name,
- [ ] update lesson purpose
-->
<p><strong>What</strong>: <a href="https://github.com/qtalr/lessons">Matrices, Exploratory Visualization</a><br><strong>How</strong>: In the R Console pane load <code>swirl</code>, run <code>swirl()</code>, and follow prompts to select the lesson.<br><strong>Why</strong>: Learn how to work with matrices to store and analyze numeric data using <code>quanteda</code> and to further your understanding of graphically representing data using <code>ggplot2</code> and other packages for more advanced plotting.</p>
</div>
</div>
</div>
<section id="sec-eda-orientation" class="level2" data-number="8.1"><h2 data-number="8.1" class="anchored" data-anchor-id="sec-eda-orientation">
<span class="header-section-number">8.1</span> Orientation</h2>
<p>The aim of this section is to provide an overview of exploratory data analysis (EDA). We will delve into various descriptive methods, such as frequency analysis and co-occurrence analysis, which are fundamental tools in linguistic research. However, our exploration won’t stop there. We will also integrate modern exploratory methods from unsupervised learning approaches, including clustering, dimensionality reductin, and vector space modeling. This may sound overwhelming, but I will strive to keep explanations clear and concise, ensuring their practicality and relevance to your linguistic inquiries is apparent. To this end, we will provide real-world examples to exemplify the applicability of these methodologies.</p>
<section id="sec-eda-research-goal" class="level3" data-number="8.1.1"><h3 data-number="8.1.1" class="anchored" data-anchor-id="sec-eda-research-goal">
<span class="header-section-number">8.1.1</span> Research goal</h3>
<p>As discussed in <a href="approaching-analysis.html#sec-aa-explore"><span>Section&nbsp;3.2.1</span></a> and <a href="framing-research.html#sec-fr-aim"><span>Section&nbsp;4.3.1</span></a>, the goal of exploratory data analysis is to discover, describe, and posit new hypotheses. The researcher does not start with a preconceived hypothesis or prediction, but rather the researcher aims to uncover patterns and associations from data allowing the data to guide the trajectory of the analysis. This analysis approach is best-suited for research where the literature on a research question is limited, or where the researcher is interested in exploring a new research question.</p>
<p>Since the researcher does not start with a preconceived hypothesis, the researcher is not able to test a hypothesis and generalize to a population, but rather the researcher is able to describe the data and provide a new perspective to be qualitatively assessed. This is achieved through an iterative and inductive process of data exploration, where the researcher uses quantitative methods to summarize, reduce, and sort complex datasets and statistically and visually interrogate a dataset letting the data guide the analysis.</p>
</section><section id="sec-eda-approach" class="level3" data-number="8.1.2"><h3 data-number="8.1.2" class="anchored" data-anchor-id="sec-eda-approach">
<span class="header-section-number">8.1.2</span> Approach</h3>
<!-- Note:

This section should cover the following:
- Workflow: Identify, Inspect, Interrogate, Interpret, (Iterate)
  - Add a few words on the common aspects with other research approaches
  - And point out the differences (use of data, results, interpretation, etc.)
-->
<p>The approach to exploratory data analysis is iterative and inductive. To reign in the analysis, however, it is important to have a research question to guide the analysis. The research question will often be broad and exploratory in nature, but it will provide a framework for the analysis including the unit of analysis and sometimes the units of observation. Yet the units of observation can be modified as needed to address the research question. Furthermore, the methods applied to the data can evolve as the research unfolds. The researcher may start with a descriptive analysis and then move to an unsupervised learning approach, or vice versa. The researcher may also pivot the approach to explore new questions and new variables. Ultimately, the researcher is guided by the data and the research question, but the researcher is not bound by a preconceived hypothesis or prediction.</p>
<!-- Workflow -->
<p>With a research question and relevant data in hand, we can look to conduct the analysis. The general workflow for exploratory data analysis is shown in <a href="#tbl-eda-workflow">Table&nbsp;<span>8.1</span></a>.</p>
<div id="tbl-eda-workflow" class="anchored">
<table class="table-striped table">
<caption>Table&nbsp;8.1: Workflow for exploratory data analysis</caption>
<colgroup>
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 52%">
</colgroup>
<thead><tr class="header">
<th style="text-align: center;">Step</th>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: left;">Identify</td>
<td style="text-align: left;">Consider the research question and identify variables of potential interest to provide insight into our question.</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: left;">Inspect</td>
<td style="text-align: left;">Check for missing data, outliers, <em>etc</em>. and check data distributions and transform if necessary.</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: left;">Interrogate</td>
<td style="text-align: left;">Submit the selected variables to descriptive (frequency, keyword, co-occurrence analysis, <em>etc.</em>) or unsupervised learning (clustering, dimensionality reduction, vector spacing modeling, <em>etc.</em>) methods to provide quantitative measures to evaluate.</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: left;">Interpret</td>
<td style="text-align: left;">Evaluate the results and determine if they are valid and meaningful to respond to the research question.</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: left;">(Optional) Iterate</td>
<td style="text-align: left;">Repeat steps 1-4 as new questions emerge from your interpretation.</td>
</tr>
</tbody>
</table>
</div>
<p>Let’s elaborate on each of these steps. First, we want to consider our research question and identify the variables of potential interest to provide insight to our question. Starting with a transformed dataset means that much of the data preparation has already been done, but we may need to further transform the data, either up front or as we explore the data. In text analysis, this often includes identifying and extracting the linguistic variables of interest, such as words, <span class="math inline">\(n\)</span>-grams, sentences, <em>etc</em>. Depending on the annotation scheme, other linguistic variables may be of interest, such as part-of-speech tags, syntactic dependencies, semantic roles, <em>etc</em>.</p>
<p>We may also want to consider the operational measures of the variables derived from the text, such as frequency, dispersion, co-occurrence, keyness, <em>etc</em>. We may also want to consider the other variables in the dataset that may be target for grouping or filtering the dataset, such as speaker information, document information, linguistic unit information, <em>etc</em>.</p>
<p>During or after extracting and operationalizing the variables of interest, we want to inspect the dataset to ensure the quality of the data and understand its characteristics. This may include checking for missing data, checking for outliers, checking for errors, checking for inconsistencies, <em>etc</em>. We may also want to inspect the distribution of the variables of interest to understand their characteristics. Summary statistics and visualizations, such as those covered in <a href="approaching-analysis.html#sec-aa-diagnose"><span>Section&nbsp;3.1</span></a>, are useful for inspecting the dataset and also provide a foundation for interrogating the dataset.</p>
<p>Once we have identified the variables of interest and inspected the dataset, we can interrogate the dataset using descriptive analysis and/ or unsupervised learning. Descriptive analysis is a set of methods that statistically and/ or visually summarizes a dataset. Descriptive analysis can be used to describe a dataset and to identify linguistic units (frequency analysis) or co-occuring (co-occurrence analysis) units that are distinctive to a particular group or sub-group in the dataset. Unsupervised learning is a machine learning approach that does not assume any particular relationship between variables in a dataset. It can be used to identify groupings (clustering) in the data including patterning of linguistic units, identifying semantically similar topics (topic modeling), and estimating word context relationships (vector space modeling).</p>
<!-- Interpret -->
<p>Exploratory methods will produce a set of statistical and/ or visual results. The researcher must interpret these results to determine if they are meaningful and if they provide a new perspective on the research question. Many times the results from one method will lead to new questions which can be explored with other methods. In some cases, the results may not be meaningful and the researcher may need to return to the data preparation stage to modify the dataset or the variables of interest. As the aim of exploratory analysis is just that, to explore, the researcher can pivot the approach to explore new questions and new variables. Ultimately, what is meaningful is determined by the researcher in the light of the research question and the potential insight obtained from the results.</p>
</section></section><section id="sec-eda-analysis" class="level2" data-number="8.2"><h2 data-number="8.2" class="anchored" data-anchor-id="sec-eda-analysis">
<span class="header-section-number">8.2</span> Analysis</h2>
<p>In this section will discuss exploratory data analysis (EDA) for linguists, with a focus on descriptive methods such as frequency analysis and co-occurence analysis, as well as unsupervised learning approaches such as clustering, topic modelling, and word embedding. To ground the discussion, we will use the the Manually Annotated Sub-Corpus (MASC) of the American National Corpus. The data dictionary for the <code>masc_transformed</code> dataset is shown in <a href="#tbl-eda-masc-dd-show">Table&nbsp;<span>8.2</span></a>.</p>
<!-- Show data dictionary -->
<div class="cell" data-hash="exploration_cache/html/tbl-eda-masc-dd-show_435f41d90b88926df60db95ad8c5aa25">
<div class="cell-output-display">
<div id="tbl-eda-masc-dd-show" class="anchored">
<table class="table table-sm table-striped small" data-quarto-postprocess="true">
<caption>Table&nbsp;8.2: Data dictionary for the MASC dataset.</caption>
<thead><tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">variable</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">name</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">variable_type</th>
<th style="text-align: left;" data-quarto-table-cell-role="th">description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">doc_id</td>
<td style="text-align: left;">Document ID</td>
<td style="text-align: left;">numeric</td>
<td style="text-align: left;">Unique identifier for each document</td>
</tr>
<tr class="even">
<td style="text-align: left;">description</td>
<td style="text-align: left;">Description</td>
<td style="text-align: left;">categorical</td>
<td style="text-align: left;">Description of the content of the document</td>
</tr>
<tr class="odd">
<td style="text-align: left;">modality</td>
<td style="text-align: left;">Modality</td>
<td style="text-align: left;">categorical</td>
<td style="text-align: left;">The form in which the document is presented (written or spoken)</td>
</tr>
<tr class="even">
<td style="text-align: left;">genre</td>
<td style="text-align: left;">Genre</td>
<td style="text-align: left;">categorical</td>
<td style="text-align: left;">The category or type of the document</td>
</tr>
<tr class="odd">
<td style="text-align: left;">domain</td>
<td style="text-align: left;">Domain</td>
<td style="text-align: left;">categorical</td>
<td style="text-align: left;">The subject or field to which the document belongs</td>
</tr>
<tr class="even">
<td style="text-align: left;">term_num</td>
<td style="text-align: left;">Term Number</td>
<td style="text-align: left;">numeric</td>
<td style="text-align: left;">Index number term per document</td>
</tr>
<tr class="odd">
<td style="text-align: left;">term</td>
<td style="text-align: left;">Term</td>
<td style="text-align: left;">categorical</td>
<td style="text-align: left;">Individual word forms in the document</td>
</tr>
<tr class="even">
<td style="text-align: left;">lemma</td>
<td style="text-align: left;">Lemma</td>
<td style="text-align: left;">categorical</td>
<td style="text-align: left;">Base or dictionary form of the term</td>
</tr>
<tr class="odd">
<td style="text-align: left;">pos</td>
<td style="text-align: left;">Part of Speech</td>
<td style="text-align: left;">categorical</td>
<td style="text-align: left;">Grammatical category of the term (modified PENN Treebank tagset)</td>
</tr>
</tbody>
</table>
</div>


</div>
</div>
<!-- Load the MASC dataset/ preview -->
<p>We will work with the MASC as our dataset to approach a task, more than a question. The task will be to identify relevant materials for an English Language Learner (ELL) textbook. This will involve multiple research questions and allow us to illustrate some very fundamental concepts that will emerge across text analysis research.</p>
<p>First, I’ll read in the dataset and only keep the variables that will pertain to our task, dropping the <code>description</code> and <code>domain</code> variables, and preview the dataset in <a href="#exm-eda-masc-read">Example&nbsp;<span>8.1</span></a>.</p>
<div id="exm-eda-masc-read" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.1 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/exm-eda-masc-read-show_d0e195c304882076f326cb5dc33cee92">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Read and subset the MASC dataset</span></span>
<span><span class="va">masc_tbl</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu">read_csv</span><span class="op">(</span><span class="st">"../data/masc/masc_transformed.csv"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="va">description</span>, <span class="op">-</span><span class="va">domain</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Preview the MASC dataset</span></span>
<span><span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="exploration_cache/html/exm-eda-masc-read-run_a82b35cc064447920b2c94171f872461">
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 5 × 7
&gt;   doc_id modality genre   term_num term         lemma        pos  
&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;
&gt; 1      1 Written  Letters        0 December     december     NNP  
&gt; 2      1 Written  Letters        1 1998         1998         CD   
&gt; 3      1 Written  Letters        2 Your         your         PRP$ 
&gt; 4      1 Written  Letters        3 contribution contribution NN   
&gt; 5      1 Written  Letters        4 to           to           TO</code></pre>
</div>
</div>
</div>
<p>From the output in <a href="#exm-eda-masc-read">Example&nbsp;<span>8.1</span></a>, we should note a couple of things. First the <code>doc_id</code> is treated as numeric <code>&lt;dbl&gt;</code> and it is not a quantitative variable –we should change this vector type to <code>&lt;chr&gt;</code>. Second, at some point in our analysis we may need to recode some of the character variables to factor variables as analysis methods may require this.</p>
<div id="exm-eda-masc-doc-id" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.2 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/exm-eda-masc-doc-id_0c84c0c1e95551a5d3d4e4d43f8cd747">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Change doc_id to character</span></span>
<span><span class="va">masc_tbl</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>doc_id <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html">as.character</a></span><span class="op">(</span><span class="va">doc_id</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>To get a better sense of distribution of the dataset, let’s use <code><a href="https://docs.ropensci.org/skimr/reference/skim.html">skim()</a></code> from the <code>skimr</code> package to generate a summary of the dataset. In particular, let’s just focus on the character variables by using <code>yank("character")</code>, as seen in <a href="#exm-eda-masc-skim">Example&nbsp;<span>8.3</span></a>.</p>
<div id="exm-eda-masc-skim" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.3 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/tbl-exm-eda-masc-skim_ac03e10081e52d572683546474417ced">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Load package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://docs.ropensci.org/skimr/">skimr</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate summary of the MASC dataset</span></span>
<span><span class="va">masc_tbl_skm</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://docs.ropensci.org/skimr/reference/skim.html">skim</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Pull character variables</span></span>
<span><span class="va">masc_tbl_skm</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://docs.ropensci.org/skimr/reference/partition.html">yank</a></span><span class="op">(</span><span class="st">"character"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">kable</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="tbl-exm-eda-masc-skim" class="anchored">
<table data-quarto-postprocess="true" class="table table-sm table-striped small">
<caption>Table&nbsp;8.3: Summary of the MASC dataset.</caption>
<thead><tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">skim_variable</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">n_missing</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">complete_rate</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">min</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">max</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">empty</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">n_unique</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">whitespace</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">doc_id</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">392</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">modality</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">genre</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">term</td>
<td style="text-align: right;">25</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">99</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">39470</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">lemma</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">99</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">28008</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">pos</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">39</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>


</div>
</div>
</div>
<p>Looking at <a href="#tbl-exm-eda-masc-skim">Table&nbsp;<span>8.3</span></a>, we see that there are 392 documents, two modalities, 18 genres, over 30k unique terms (which are words), over 28k lemmas (word base forms), and 39 distinct part-of-speech tags.</p>
<!--
[ ] the skim shows that there are some 'words' up to 99 characters long, which are likely errors
[ ] there are 25 missing tokens and 4 missing lemmas
-->
<section id="sec-eda-descriptive" class="level3" data-number="8.2.1"><h3 data-number="8.2.1" class="anchored" data-anchor-id="sec-eda-descriptive">
<span class="header-section-number">8.2.1</span> Descriptive analysis</h3>
<p>Descriptive analysis includes common techniques such as frequency analysis to determine the most frequent words or phrases, dispersion analysis to see how terms or topics are distributed throughout a document or corpus, keyword analysis to identify distinctive terms, and/ or co-occurrence analysis to see what terms tend to appear together.</p>
<p>Using the MASC dataset, we will entertain questions such as:</p>
<ul>
<li>What are the most common terms a beginning ELL should learn?</li>
<li>Are there term differences between spoken and written discourses that should be emphasized?</li>
<li>What are the most common verb particle constructions?</li>
</ul>
<p>Along the way, we will introduce some fundamental concepts in text analysis such as tokens and types and frequency, dispersion, and co-occurrence measures. In addition, we will apply various descriptive analysis techniques and visualizations to explore the dataset and identify new questions and new variables of interest.</p>
<section id="sec-eda-frequency" class="level4"><h4 class="anchored" data-anchor-id="sec-eda-frequency">Frequency analysis</h4>
<!-- 4 I's: identify, inspect, interrogate, interpret -->
<p>At its core, frequency analysis is a descriptive method that counts the number of times a linguistic unit, or term, (<em>i.e.</em> word, <span class="math inline">\(n\)</span>-gram, sentence, <em>etc</em>.) occurs in a dataset. The results of frequency analysis can be used to describe the dataset and to identify terms that are linguistically distinctive or distinctive to a particular group or sub-group in the dataset.</p>
<!--- Raw frequency (counting) --->
<section id="sec-eda-frequency-raw" class="level5"><h5 class="anchored" data-anchor-id="sec-eda-frequency-raw">Raw frequency</h5>
<p>Let’s consider what the most common words in the MASC dataset are as a starting point to making inroads on our task by identifying relevant vocabulary for an ELL textbook.</p>
<p>In the <code>masc_tbl</code> data frame we have the linguistic unit <code>term</code> which corresponds to the word-level annotation of the MASC. The <code>lemma</code> corresponds to the base form of each term, for words with inflectional morphology, the lemma is the word sans the inflection (<em>e.g.</em> is - be, are - be). For other words, the <code>term</code> and the <code>lemma</code> will be the same (<em>e.g.</em> the - the, in - in). These two variables pose a choice point for us: do we consider words to be the actual forms or the base forms? There is an argument to be made for both. In this case I will operationalize our linguistic unit as the <code>lemma</code> variable, as this will allow us to group words with inflectional morphology together.</p>
<p>To perform a basic word frequency analysis, we start by using the <code>count()</code> function from the <code>dplyr</code> package to count the number of times each lemma occurs in the dataset. We’ll sort by the most frequent lemmas, as seen in <a href="#exm-eda-masc-count">Example&nbsp;<span>8.4</span></a>.</p>
<div id="exm-eda-masc-count" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.4 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-count_c513dfde85310625f0166b0c4d80be74">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Lemma count, sorted</span></span>
<span><span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">count</span><span class="op">(</span><span class="va">lemma</span>, sort <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 28,009 × 2
&gt;    lemma     n
&gt;    &lt;chr&gt; &lt;int&gt;
&gt;  1 ,     27113
&gt;  2 .     26258
&gt;  3 the   26137
&gt;  4 be    19466
&gt;  5 to    13548
&gt;  6 and   12528
&gt;  7 of    12005
&gt;  8 a     10480
&gt;  9 in     8374
&gt; 10 i      7783
&gt; # ℹ 27,999 more rows</code></pre>
</div>
</div>
</div>
<p>The output of this frequency tabulation in <a href="#exm-eda-masc-count">Example&nbsp;<span>8.4</span></a> is a data frame with two columns: <code>lemma</code> and <code>n</code>. The <code>lemma</code> column contains the unique lemmas in the dataset, and the <code>n</code> column contains the frequency of each lemma. The data frame is sorted in descending order by the frequency of lemmas. Now the result includes over 28,000 rows –which corresponds to the number of unique lemmas in the dataset.</p>
<p>At this point, it is important to define a few key concepts that are fundamental to working with text. First, a <strong>term</strong> is a defined linguistic unit extracted from a corpus. In our dataset, the terms are words, such as ‘the’, ‘houses’, ‘are’. A lemma is an annotated recoding of words which represent the uninflected base form of a word. In either case, the term or lemma is an instance of a linguistic unit. These instances are called <strong>tokens</strong>. When we count the number of times a term or lemma occurs in a dataset, we are counting the number of tokens (<code>n</code>), such as in <a href="#exm-eda-masc-count">Example&nbsp;<span>8.4</span></a>. Now, the list of unique linguistic units is a list of <strong>types</strong> (<code>lemma</code>). By definition, then, there will always be at least as many tokens as types, but more often than not (many) more tokens than types.</p>
<p>Our first pass at calculating lemma frequency in <a href="#exm-eda-masc-count">Example&nbsp;<span>8.4</span></a> should bring something else to our attention. As we can see among the most frequent lemmas are non-words such as <code>,</code>, and <code>.</code>. As you can imagine, given the conventions of written and transcriptional language, these types are very frequent. For a frequency analysis focusing on words, however, we should probably remove them. Thinking ahead, there may also be other non-words that we want to remove, such as symbols, numbers, <em>etc</em>. Let’s take a look at <a href="#fig-eda-masc-pos">Figure&nbsp;<span>8.1</span></a>, where I’ve counted the part-of-speech tags <code>pos</code> in the dataset to see what other non-words we might want to remove.</p>
<div id="exm-eda-masc-pos" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.5 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/fig-eda-masc-pos_9cef9156db704fe37e90e3a34e3cbc6b">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Part-of-speech tags</span></span>
<span><span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">count</span><span class="op">(</span><span class="va">pos</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span></span>
<span>    <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/reorder.factor.html">reorder</a></span><span class="op">(</span><span class="va">pos</span>, <span class="fu">desc</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span>, y <span class="op">=</span> <span class="va">n</span><span class="op">)</span></span>
<span>    <span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_col</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>axis.text.x <span class="op">=</span> <span class="fu">element_text</span><span class="op">(</span>angle <span class="op">=</span> <span class="fl">90</span>, hjust <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Part-of-speech tags"</span>, y <span class="op">=</span> <span class="st">"Token frequency"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-eda-masc-pos" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-pos-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption">Figure&nbsp;8.1: Part-of-speech tags in the MASC dataset.</figcaption></figure>
</div>
</div>
</div>
</div>
<p>Consulting the <a href="https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html">PENN Tagset online</a>, we can see that the <code>pos</code> variable includes a number of non-words or other elements to exclude including:</p>
<ul>
<li>‘CD’ - Cardinal number</li>
<li>‘FW’ - Foreign word</li>
<li>‘LS’ - List item marker</li>
<li>‘SYM’ - Symbol</li>
</ul>
<p>This modified tagset has grouped the punctuation tags into a single tag, ‘PUNCT’.</p>
<p>We can use this information to remove lemmas that are tagged with either of these values. We can do this by filtering the data frame to only include lemmas that are not tagged with the <code>pos</code> values listed above, as seen in <a href="#exm-eda-masc-count-filter">Example&nbsp;<span>8.6</span></a>.</p>
<div id="exm-eda-masc-count-filter" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.6 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-count-filter_52370080fd042aa27647f6cb2680e679">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Filter out lemmas with PUNCT or SYM for pos</span></span>
<span><span class="va">masc_tbl</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="op">!</span><span class="op">(</span><span class="va">pos</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"CD"</span>, <span class="st">"FW"</span>, <span class="st">"LS"</span>, <span class="st">"SYM"</span>, <span class="st">"PUNCT"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Lemma count, sorted (again)</span></span>
<span><span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">count</span><span class="op">(</span><span class="va">lemma</span>, sort <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 26,164 × 2
&gt;    lemma     n
&gt;    &lt;chr&gt; &lt;int&gt;
&gt;  1 the   26137
&gt;  2 be    19466
&gt;  3 to    13548
&gt;  4 and   12528
&gt;  5 of    12005
&gt;  6 a     10461
&gt;  7 in     8374
&gt;  8 i      7783
&gt;  9 that   7082
&gt; 10 you    5276
&gt; # ℹ 26,154 more rows</code></pre>
</div>
</div>
</div>
<p>Now we are only viewing the most frequent words in the dataset, which reduces the number of observations to around 26k. Let’s now explore the frequency distribution of the tokens. In <a href="#fig-eda-masc-count-plots">Figure&nbsp;<span>8.2</span></a>, I’ve created three plots which include: 1) all the types, 2) the top 100 types, and 3) the top 10 types in the dataset.</p>
<div>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># [ ] consider how to present the 'all types' plot better, more concisely</span></span>
<span></span>
<span><span class="co"># Plot lemma count for all types</span></span>
<span><span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">count</span><span class="op">(</span><span class="va">lemma</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">arrange</span><span class="op">(</span><span class="fu">desc</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/reorder.factor.html">reorder</a></span><span class="op">(</span><span class="va">lemma</span>, <span class="fu">desc</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span>, y <span class="op">=</span> <span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_col</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Types"</span>, y <span class="op">=</span> <span class="st">"Token frequency"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>axis.text.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot lemma count for top 100 types</span></span>
<span><span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">count</span><span class="op">(</span><span class="va">lemma</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">arrange</span><span class="op">(</span><span class="fu">desc</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/reorder.factor.html">reorder</a></span><span class="op">(</span><span class="va">lemma</span>, <span class="fu">desc</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span>, y <span class="op">=</span> <span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_col</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Types"</span>, y <span class="op">=</span> <span class="st">"Token frequency"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>axis.text.x <span class="op">=</span> <span class="fu">element_text</span><span class="op">(</span>angle <span class="op">=</span> <span class="fl">90</span>, hjust <span class="op">=</span> <span class="fl">1.3</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot lemma count for top 10 types</span></span>
<span><span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">count</span><span class="op">(</span><span class="va">lemma</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">arrange</span><span class="op">(</span><span class="fu">desc</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/reorder.factor.html">reorder</a></span><span class="op">(</span><span class="va">lemma</span>, <span class="fu">desc</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span>, y <span class="op">=</span> <span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_col</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Types"</span>, y <span class="op">=</span> <span class="st">"Token frequency"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>axis.text.x <span class="op">=</span> <span class="fu">element_text</span><span class="op">(</span>angle <span class="op">=</span> <span class="fl">65</span>, hjust <span class="op">=</span> <span class="fl">1.3</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-eda-masc-count-plots" class="cell quarto-layout-panel" data-hash="exploration_cache/html/fig-eda-masc-count-plots_c53dc66d78a70cf568f8a8f5821a5720">
<figure class="figure"><div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-eda-masc-count-plots-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-count-plots-1.png" class="img-fluid figure-img" data-ref-parent="fig-eda-masc-count-plots" width="384"></p>
<figcaption class="figure-caption">(a) All types</figcaption></figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-eda-masc-count-plots-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-count-plots-2.png" class="img-fluid figure-img" data-ref-parent="fig-eda-masc-count-plots" width="384"></p>
<figcaption class="figure-caption">(b) Top 100 types</figcaption></figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-eda-masc-count-plots-3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-count-plots-3.png" class="img-fluid figure-img" data-ref-parent="fig-eda-masc-count-plots" width="384"></p>
<figcaption class="figure-caption">(c) Top 10 types</figcaption></figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;8.2: Frequency plots of tokens in the MASC dataset</figcaption><p></p>
</figure>
</div>
</div>
<p>The distributions we see in <a href="#fig-eda-masc-count-plots">Figure&nbsp;<span>8.2</span></a> are highly right-skewed (in <a href="#fig-eda-masc-count-plots-1">Figure&nbsp;<span>8.2 (a)</span></a> in a very extreme way!). This is typical of natural language distributions, notably documented by George Kingsley Zipf <span class="citation" data-cites="Zipf1949">(<a href="references.html#ref-Zipf1949" role="doc-biblioref">Zipf 1949</a>)</span>. This type of distribution approaches the theoretical Zipf distribution. A Zipf (or Zipfian) distribution is characterized by the fact that the frequency of any word is inversely proportional to its rank in the frequency table. In other words, the most frequent word occurs approximately twice as often as the second most frequent word, three times as often as the third most frequent word, and so on.</p>
<p>As we can see, our distribuions to not follow the Zipf distribution exactly. This is because the Zipf distribution is a theoretical distribution, and the actual distribution of words in a corpus is affected by various sampling factors, including the size of the corpus. The larger the corpus, the closer the distribution will be to the Zipf distribution.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-solid fa-medal" aria-label="medal"></i> Dive deeper</strong></p>
<p>As stated above, Zipfian distributions are typical of natural language and are observed a various linguistic levels. This is because natural language is a complex system, and complex systems tend to exhibit Zipfian distributions. Other examples of complex systems that exhibit Zipfian distributions include the size of cities, the frequency of species in ecological communities, the frequency of links in the World Wide Web, <em>etc.</em></p>
</div>
</div>
</div>
<p>The observation captured in the Zipf distribution is key to understanding quantitative text analysis. It demonstrates that most of the types in a corpus occur (relatively) infrequently, while a small number of types occur very frequently. In fact, if we calculate the cumulative frequency of the lemmas in the <code>masc_tbl</code> data frame, we can see that the top 10 types account for over 20% of the lemmas used in the dataset –by 100 types that increases to over 40%, as seen in <a href="#exm-eda-masc-count-cumulative">Example&nbsp;<span>8.7</span></a>.</p>
<div id="exm-eda-masc-count-cumulative" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.7 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/fig-eda-masc-count-cumulative_a8a3d953835244958b6511625171d3a4">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Calculate cumulative frequency</span></span>
<span><span class="va">lemma_cumul_freq</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">count</span><span class="op">(</span><span class="va">lemma</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">arrange</span><span class="op">(</span><span class="fu">desc</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>cumulative <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>percent <span class="op">=</span> <span class="va">cumulative</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lemma_cumul_freq</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">2000</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/reorder.factor.html">reorder</a></span><span class="op">(</span><span class="va">lemma</span>, <span class="fu">desc</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span>, y <span class="op">=</span> <span class="va">percent</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_col</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_vline</span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">10</span>, linetype <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_vline</span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fl">100</span>, linetype <span class="op">=</span> <span class="st">"dashed"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="co"># annotate("text", x = 10+10, y = 0.5, label = "10 lemmas") +</span></span>
<span>  <span class="co"># annotate("text", x = 100+10, y = 0.5, label = "100 lemmas") +</span></span>
<span>  <span class="fu">scale_y_continuous</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="va"><a href="https://scales.r-lib.org/reference/percent_format.html">percent</a></span>, limits <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Types"</span>, y <span class="op">=</span> <span class="st">"Cumulative frequency percent"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme</span><span class="op">(</span>axis.text.x <span class="op">=</span> <span class="fu">element_blank</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-eda-masc-count-cumulative" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-count-cumulative-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption">Figure&nbsp;8.3: Cumulative frequency of lemmas in the MASC dataset</figcaption></figure>
</div>
</div>
</div>
</div>
<p>If we look at the types that appear within the first 100 most frequent, you can likely also appreciate another thing about language use. Let’s list the top 100 types in <a href="#exm-eda-masc-count-top-100">Example&nbsp;<span>8.8</span></a>.</p>
<div id="exm-eda-masc-count-top-100" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.8 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/tbl-eda-masc-count-top-100_c36c56ebc2de010739711d853f023e69">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Top 100 types</span></span>
<span><span class="va">lemma_cumul_freq</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">pull</span><span class="op">(</span><span class="va">lemma</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span>ncol <span class="op">=</span> <span class="fl">10</span>, byrow <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">kable</span><span class="op">(</span>col.names <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="tbl-eda-masc-count-top-100" class="anchored">
<table data-quarto-postprocess="true" class="table table-sm table-striped small">
<caption>Table&nbsp;8.4: Top 100 lemma types in the MASC dataset.</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;">the</td>
<td style="text-align: left;">be</td>
<td style="text-align: left;">to</td>
<td style="text-align: left;">and</td>
<td style="text-align: left;">of</td>
<td style="text-align: left;">a</td>
<td style="text-align: left;">in</td>
<td style="text-align: left;">i</td>
<td style="text-align: left;">that</td>
<td style="text-align: left;">you</td>
</tr>
<tr class="even">
<td style="text-align: left;">have</td>
<td style="text-align: left;">it</td>
<td style="text-align: left;">for</td>
<td style="text-align: left;">on</td>
<td style="text-align: left;">do</td>
<td style="text-align: left;">with</td>
<td style="text-align: left;">we</td>
<td style="text-align: left;">as</td>
<td style="text-align: left;">this</td>
<td style="text-align: left;">not</td>
</tr>
<tr class="odd">
<td style="text-align: left;">at</td>
<td style="text-align: left;">from</td>
<td style="text-align: left;">he</td>
<td style="text-align: left;">but</td>
<td style="text-align: left;">by</td>
<td style="text-align: left;">will</td>
<td style="text-align: left;">my</td>
<td style="text-align: left;">or</td>
<td style="text-align: left;">n't</td>
<td style="text-align: left;">they</td>
</tr>
<tr class="even">
<td style="text-align: left;">your</td>
<td style="text-align: left;">an</td>
<td style="text-align: left;">say</td>
<td style="text-align: left;">what</td>
<td style="text-align: left;">so</td>
<td style="text-align: left;">his</td>
<td style="text-align: left;">if</td>
<td style="text-align: left;">'s</td>
<td style="text-align: left;">can</td>
<td style="text-align: left;">go</td>
</tr>
<tr class="odd">
<td style="text-align: left;">all</td>
<td style="text-align: left;">there</td>
<td style="text-align: left;">me</td>
<td style="text-align: left;">would</td>
<td style="text-align: left;">about</td>
<td style="text-align: left;">know</td>
<td style="text-align: left;">get</td>
<td style="text-align: left;">make</td>
<td style="text-align: left;">out</td>
<td style="text-align: left;">up</td>
</tr>
<tr class="even">
<td style="text-align: left;">think</td>
<td style="text-align: left;">our</td>
<td style="text-align: left;">she</td>
<td style="text-align: left;">more</td>
<td style="text-align: left;">time</td>
<td style="text-align: left;">just</td>
<td style="text-align: left;">no</td>
<td style="text-align: left;">when</td>
<td style="text-align: left;">their</td>
<td style="text-align: left;">like</td>
</tr>
<tr class="odd">
<td style="text-align: left;">her</td>
<td style="text-align: left;">who</td>
<td style="text-align: left;">which</td>
<td style="text-align: left;">other</td>
<td style="text-align: left;">see</td>
<td style="text-align: left;">people</td>
<td style="text-align: left;">new</td>
<td style="text-align: left;">s</td>
<td style="text-align: left;">take</td>
<td style="text-align: left;">now</td>
</tr>
<tr class="even">
<td style="text-align: left;">work</td>
<td style="text-align: left;">some</td>
<td style="text-align: left;">year</td>
<td style="text-align: left;">how</td>
<td style="text-align: left;">them</td>
<td style="text-align: left;">use</td>
<td style="text-align: left;">come</td>
<td style="text-align: left;">into</td>
<td style="text-align: left;">well</td>
<td style="text-align: left;">than</td>
</tr>
<tr class="odd">
<td style="text-align: left;">look</td>
<td style="text-align: left;">its</td>
<td style="text-align: left;">may</td>
<td style="text-align: left;">right</td>
<td style="text-align: left;">then</td>
<td style="text-align: left;">could</td>
<td style="text-align: left;">because</td>
<td style="text-align: left;">only</td>
<td style="text-align: left;">us</td>
<td style="text-align: left;">these</td>
</tr>
<tr class="even">
<td style="text-align: left;">want</td>
<td style="text-align: left;">any</td>
<td style="text-align: left;">also</td>
<td style="text-align: left;">need</td>
<td style="text-align: left;">way</td>
<td style="text-align: left;">where</td>
<td style="text-align: left;">back</td>
<td style="text-align: left;">him</td>
<td style="text-align: left;">here</td>
<td style="text-align: left;">'</td>
</tr>
</tbody>
</table>
</div>


</div>
</div>
</div>
<p>For the most part, the most frequent words are not content words, but rather function words (<em>e.g.</em> determiners, prepositions, pronouns, auxiliary verbs). Function words include a closed class of relatively few words that are used to express grammatical relationships between content words. It then is no surprise that they are the comprise many of the most frequent words in a corpus.</p>
<p>Another key observation is that among the most frequency content words (<em>e.g.</em> nouns, verbs, adjectives, adverbs) are words that are quite semantically generic –that is, they are words that are used in a wide range of contexts and take a wide range of meanings. Take for example the adjective ‘good’. It can be used to describe a wide range of nouns, such as ‘good food’, ‘good people’, ‘good times’, <em>etc</em>. A sometimes near-synonym of ‘good’, for example ‘good student’, is the word ‘studious’. Yet, ‘studious’ is not as frequent as ‘good’ as it is used to describe a narrower range of nouns, such as ‘studious student’, ‘studious scholar’, ‘studious researcher’, <em>etc</em>. In this way, ‘studious’ is more semantically specific than ‘good’.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-regular fa-lightbulb" aria-label="lightbulb"></i> Consider this</strong></p>
<p>Based on what you now know about the expected distribution of words in a corpus, what if your were asked to predict what the most frequency English word used is in each U.S. State? What would you predict? How confident would you be in your prediction? What if you were asked to predict what the most frequency word used is in the language of a given country? What would you want to know before making your prediction?</p>
</div>
</div>
</div>
<p>So common across corpus samples, in some analyses these usual suspects of the most common words are considered irrelvant and are filtered out. In our ELL materials task, however, we might exclude them for this simple fact that it will be a given that we will teach these words given their grammatical importance. If we want to focus on the most common content words, we can filter out the function words.</p>
<p>One approach to filtering out these words is to use a pre-determined list of <strong>stopwords</strong>. The <code>tidytext</code> package includes a data frame <code>stop_words</code> of stopword lexicons for English. We can select a lexicon from <code>stop_words</code> and use <code>anti_join()</code> to filter out the words that appear in the <code>word</code> variable from the <code>lemma</code> variable in the <code>masc_tbl</code> data frame. In <a href="#exm-eda-masc-count-stop-words">Example&nbsp;<span>8.9</span></a>, I perform this filtering and then re-run the frequency analysis for the top 100 lemmas.</p>
<div id="exm-eda-masc-count-stop-words" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.9 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/tbl-eda-masc-count-stop-words_df2157385fd2e133aed30d579970e5ff">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Load package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/juliasilge/tidytext">tidytext</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Select stopword lexicon</span></span>
<span><span class="va">stopwords</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">stop_words</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">lexicon</span> <span class="op">==</span> <span class="st">"SMART"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Filter out stop words</span></span>
<span><span class="fu">anti_join</span><span class="op">(</span></span>
<span>  x <span class="op">=</span> <span class="va">masc_tbl</span>,</span>
<span>  y <span class="op">=</span> <span class="va">stopwords</span>,</span>
<span>  by <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"lemma"</span> <span class="op">=</span> <span class="st">"word"</span><span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">count</span><span class="op">(</span><span class="va">lemma</span>, sort <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">pull</span><span class="op">(</span><span class="va">lemma</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span>ncol <span class="op">=</span> <span class="fl">10</span>, byrow <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">as_tibble</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">kable</span><span class="op">(</span>col.names <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="tbl-eda-masc-count-stop-words" class="anchored">
<table data-quarto-postprocess="true" class="table table-sm table-striped small">
<caption>Table&nbsp;8.5: Frequency of tokens in the MASC dataset after filtering out stopwords</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;">n't</td>
<td style="text-align: left;">'s</td>
<td style="text-align: left;">make</td>
<td style="text-align: left;">time</td>
<td style="text-align: left;">people</td>
<td style="text-align: left;">work</td>
<td style="text-align: left;">year</td>
<td style="text-align: left;">back</td>
<td style="text-align: left;">'</td>
<td style="text-align: left;">find</td>
</tr>
<tr class="even">
<td style="text-align: left;">give</td>
<td style="text-align: left;">day</td>
<td style="text-align: left;">thing</td>
<td style="text-align: left;">jack</td>
<td style="text-align: left;">man</td>
<td style="text-align: left;">yeah</td>
<td style="text-align: left;">good</td>
<td style="text-align: left;">call</td>
<td style="text-align: left;">world</td>
<td style="text-align: left;">president</td>
</tr>
<tr class="odd">
<td style="text-align: left;">state</td>
<td style="text-align: left;">question</td>
<td style="text-align: left;">service</td>
<td style="text-align: left;">change</td>
<td style="text-align: left;">life</td>
<td style="text-align: left;">leave</td>
<td style="text-align: left;">subject</td>
<td style="text-align: left;">set</td>
<td style="text-align: left;">long</td>
<td style="text-align: left;">place</td>
</tr>
<tr class="even">
<td style="text-align: left;">write</td>
<td style="text-align: left;">show</td>
<td style="text-align: left;">child</td>
<td style="text-align: left;">end</td>
<td style="text-align: left;">feel</td>
<td style="text-align: left;">hand</td>
<td style="text-align: left;">system</td>
<td style="text-align: left;">school</td>
<td style="text-align: left;">information</td>
<td style="text-align: left;">part</td>
</tr>
<tr class="odd">
<td style="text-align: left;">group</td>
<td style="text-align: left;">follow</td>
<td style="text-align: left;">run</td>
<td style="text-align: left;">support</td>
<td style="text-align: left;">today</td>
<td style="text-align: left;">point</td>
<td style="text-align: left;">read</td>
<td style="text-align: left;">provide</td>
<td style="text-align: left;">uh</td>
<td style="text-align: left;">send</td>
</tr>
<tr class="even">
<td style="text-align: left;">turn</td>
<td style="text-align: left;">include</td>
<td style="text-align: left;">talk</td>
<td style="text-align: left;">fact</td>
<td style="text-align: left;">&amp;</td>
<td style="text-align: left;">live</td>
<td style="text-align: left;">put</td>
<td style="text-align: left;">word</td>
<td style="text-align: left;">number</td>
<td style="text-align: left;">start</td>
</tr>
<tr class="odd">
<td style="text-align: left;">law</td>
<td style="text-align: left;">case</td>
<td style="text-align: left;">company</td>
<td style="text-align: left;">money</td>
<td style="text-align: left;">great</td>
<td style="text-align: left;">open</td>
<td style="text-align: left;">home</td>
<td style="text-align: left;">city</td>
<td style="text-align: left;">issue</td>
<td style="text-align: left;">woman</td>
</tr>
<tr class="even">
<td style="text-align: left;">job</td>
<td style="text-align: left;">american</td>
<td style="text-align: left;">important</td>
<td style="text-align: left;">result</td>
<td style="text-align: left;">book</td>
<td style="text-align: left;">hear</td>
<td style="text-align: left;">sparrow</td>
<td style="text-align: left;">house</td>
<td style="text-align: left;">problem</td>
<td style="text-align: left;">um</td>
</tr>
<tr class="odd">
<td style="text-align: left;">america</td>
<td style="text-align: left;">walk</td>
<td style="text-align: left;">family</td>
<td style="text-align: left;">begin</td>
<td style="text-align: left;">country</td>
<td style="text-align: left;">date</td>
<td style="text-align: left;">face</td>
<td style="text-align: left;">friend</td>
<td style="text-align: left;">report</td>
<td style="text-align: left;">move</td>
</tr>
<tr class="even">
<td style="text-align: left;">order</td>
<td style="text-align: left;">head</td>
<td style="text-align: left;">id</td>
<td style="text-align: left;">watch</td>
<td style="text-align: left;">form</td>
<td style="text-align: left;">program</td>
<td style="text-align: left;">market</td>
<td style="text-align: left;">week</td>
<td style="text-align: left;">area</td>
<td style="text-align: left;">figure</td>
</tr>
</tbody>
</table>
</div>


</div>
</div>
</div>
<p>The resulting list in <a href="#tbl-eda-masc-count-stop-words">Table&nbsp;<span>8.5</span></a> paints a different picture of the most frequent words in the dataset. The most frequent words are now content words, and included in most frequent words are more semantically specific words.</p>
<p>Eliminating words in this fashion, however, may not always be the best approach. Available lists of stopwords vary in their contents and are determined by other researchers for other potential uses. We may instead opt to create our own stopword list that is tailored to the task, or we may opt to use a statistical approach based on their distribution in the dataset using a combination of frequency and dispersion measures, as we will see in [the next section.]</p>
<p>For our case, however, we have another strategy to apply. Since our task is to identify relevant vocabulary, beyond the fundamental function words in English, we can use the part-of-speech tags to reduce our dataset to just the content words, that is nouns, verbs, adjectives, and adverbs. We need to consult the Penn Tagset again, to ensure we are selecting the correct tags. I will assign this data frame to <code>masc_content_tbl</code> to keep it separate from our main data frame <code>masc_tbl</code>, seen in <a href="#exm-eda-masc-filter-pos">Example&nbsp;<span>8.10</span></a>.</p>
<div id="exm-eda-masc-filter-pos" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.10 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-filter-pos_3a34ca8e38a298d00295944d2ee064e8">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Penn Tagset for content words</span></span>
<span><span class="co"># Nouns: NN, NNS,</span></span>
<span><span class="co"># Verbs: VB, VBD, VBG, VBN, VBP, VBZ</span></span>
<span><span class="co"># Adjectives: JJ, JJR, JJS</span></span>
<span><span class="co"># Adverbs: RB, RBR, RBS</span></span>
<span></span>
<span><span class="va">content_pos</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"NN"</span>, <span class="st">"NNS"</span>, <span class="st">"VB"</span>, <span class="st">"VBD"</span>, <span class="st">"VBG"</span>, <span class="st">"VBN"</span>, <span class="st">"VBP"</span>, <span class="st">"VBZ"</span>, <span class="st">"JJ"</span>, <span class="st">"JJR"</span>, <span class="st">"JJS"</span>, <span class="st">"RB"</span>, <span class="st">"RBR"</span>, <span class="st">"RBS"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Select content words</span></span>
<span><span class="va">masc_content_tbl</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">pos</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="va">content_pos</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>We now have reduced the number of observations by 50% focusing on the content words. We are getting closer to identifying the vocabulary that we want to include in our ELL materials, but we will need some more tools to help us identify the most relevant vocabulary.</p>
</section><section id="sec-eda-frequency-dispersion" class="level5"><h5 class="anchored" data-anchor-id="sec-eda-frequency-dispersion">Dispersion</h5>
<p><strong>Dispersion</strong> is a measure of how evenly distributed a linguistic unit is across a dataset. This is a key concept in text analysis, as important as frequency. It is important to recognize that frequency and dispersion are measures of different characteristics. We can have two words that occur with the same frequency, but one word may be more evenly distributed across a dataset than the other. Depending on the researcher’s aims, this may be an important distinction to make. For our task, it is likely the case that we want to capture words that are well-dispersed across the dataset as words that have a high frequency and a low dispersion tend to be connected to a particular context, whether that be a particular genre, a particular speaker, a particular topic, <em>etc</em>. In other research, aim may be the reverse; to identify words that are highly frequent and highly concentrated in a particular context to identify words that are distinctive to that context.</p>
<p>This a wide variety of measures that can be used to estimate the distribution of types across a dataset. Let’s focus on three measures: document frequency (<span class="math inline">\(df\)</span>), inverse document frequency (<span class="math inline">\(idf\)</span>), and Gries’ Deviation of Proportions (<span class="math inline">\(dp\)</span>).</p>
<p>The most basic measure is <strong>document frequency</strong> (<span class="math inline">\(df\)</span>). This is the number of documents in which a type appears at least once. For example, if a type appears in 10 documents, then the document frequency is 10. This is a very basic measure, but it is a good starting point.</p>
<p>A nuanced version of document frequency is <strong>inverse document frequency</strong> (<span class="math inline">\(idf\)</span>). This measure takes the total number of documents and divides it by the document frequency. This results in a measure that is inversely proportional to the document frequency. That is, the higher the document frequency, the lower the inverse document frequency. This measure is often log-transformed to spread out the values.</p>
<p>One thing to consider about <span class="math inline">\(df\)</span> and <span class="math inline">\(idf\)</span> is that niether takes into account the length of the documents in which the type appears nor the spread of the type within each document. To take these factors into account, we can use Gries’ Deviation of Proportions (<span class="math inline">\(dp\)</span>) measure <span class="citation" data-cites="Gries2023">(<a href="references.html#ref-Gries2023" role="doc-biblioref">Gries 2023, 87–88</a>)</span>. The <span class="math inline">\(dp\)</span> measure is calculated as the difference between the proportion of a tokens in a document and tokens in the corpus. The metric can be subtracted from 1 to create a normalized measure of dispersion ranging between 0 and 1, with lower values being more dispersed.</p>
<p>Let’s consider how these measures differ with three scenarios:</p>
<p>Imagine a type with a token frequency of 100 appears in each of the 10 documents in a corpus.</p>
<p>A. Each of the documents is 100 words long. The type appears 10 times in each document. B. Each of the documents is 100 words long. But now the type appears once in 9 documents and 91 times in 1 document. C. Nine of the documents constitute 99% of the corpus. The type appears once in each of the 9 documents and 91 times in the 10th document.</p>
<p>Scenario A is the most dispersed, scenario B is less dispersed, and scenario C is the least dispersed. Yet, the type’s <span class="math inline">\(df\)</span> and <span class="math inline">\(idf\)</span> scores will be the same. But the <span class="math inline">\(dp\)</span> score will reflect increasing concentration of the type from A to B to C. You may wonder why we would want to use <span class="math inline">\(df\)</span> or <span class="math inline">\(idf\)</span> at all. The answer is some combination of the fact that they are computationally less expensive to calculate, they are widely used (especially <span class="math inline">\(idf\)</span>), and/ or in many practical situations they often highly correlated with <span class="math inline">\(dp\)</span>.</p>
<p>So for our task we will use <span class="math inline">\(dp\)</span> as our measure of dispersion. The <code>qtalrkit</code> package includes the <code><a href="https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html">calc_type_metrics()</a></code> function which calculates, among other metrics, the dispersion metrics <span class="math inline">\(df\)</span>, <span class="math inline">\(idf\)</span>, and/ or <span class="math inline">\(dp\)</span>. Let’s select <code>dp</code> and assign the result to <code>masc_lemma_disp</code>, as seen in <a href="#exm-eda-masc-dp">Example&nbsp;<span>8.11</span></a>.</p>
<div id="exm-eda-masc-dp" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.11 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-dp_734468c5c3a30dfaa38c96c918a51685">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Load package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/qtalr/qtalrkit">qtalrkit</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calculate deviance of proportions (DP)</span></span>
<span><span class="va">masc_lemma_disp</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_content_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html">calc_type_metrics</a></span><span class="op">(</span></span>
<span>    type <span class="op">=</span> <span class="va">lemma</span>,</span>
<span>    documents <span class="op">=</span> <span class="va">doc_id</span>,</span>
<span>    dispersion <span class="op">=</span> <span class="st">"dp"</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">arrange</span><span class="op">(</span><span class="va">dp</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Preview</span></span>
<span><span class="va">masc_lemma_disp</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 10 × 3
&gt;    type      n    dp
&gt;    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;
&gt;  1 be    19231 0.123
&gt;  2 have   5136 0.189
&gt;  3 not    2279 0.240
&gt;  4 make   1149 0.266
&gt;  5 other   882 0.270
&gt;  6 more   1005 0.276
&gt;  7 take    769 0.286
&gt;  8 only    627 0.286
&gt;  9 time    931 0.314
&gt; 10 see     865 0.327</code></pre>
</div>
</div>
</div>
<p>We would like to identify lemmas that are frequent and well-dispersed. But an important question arises, what is the threshold for frequency and dispersion that we should use to identify the lemmas that we want to include in our ELL materials?</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-regular fa-lightbulb" aria-label="lightbulb"></i> Consider this</strong></p>
<p>You may be wondering why the Inverse Document Frequency is, in fact, the inverse of the document counts, instead of just a count of the documents that each type appears in. The <span class="math inline">\(idf\)</span> is a very common measure in machine learning that is used in combination with (term) frequency to calculate the <span class="math inline">\(tf-idf\)</span> (term frequency-inverse document frequency) measure. That is, the product of the frequency of a term and the inverse document frequency of the term. This serves as a weighting measure that lowers the <span class="math inline">\(tf-idf\)</span> score for terms that are frequent across documents and increases the <span class="math inline">\(tf-idf\)</span> score for terms that are infrequent across documents.</p>
<p>Consider what types will end up with a high or a low <span class="math inline">\(tf-idf\)</span> score. What use(s) could this measure have for distinguishing between types in a corpus?</p>
<p>Hint: consider the earlier discussion of stopword lists.</p>
</div>
</div>
</div>
<p>There are statistical approaches to identifying natural breakpoints, including clustering, but a visual inspection is often good enough for practical purposes. Let’s create a density plot to see if there is a natural break in the distribution of our dispersion measure, as seen in <a href="#fig-eda-masc-dp-density">Figure&nbsp;<span>8.4</span></a>.</p>
<div id="exm-eda-masc-dp-density" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.12 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/fig-eda-masc-dp-density_809ba4b170c3e1793e4e890d280ac10d">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Density plot of dp</span></span>
<span><span class="va">masc_lemma_disp</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">dp</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_density</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_x_continuous</span><span class="op">(</span>breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">.1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Deviation of Proportions"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-eda-masc-dp-density" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-dp-density-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption">Figure&nbsp;8.4: Density plot of Deviation of Proportions for lemmas in the MASC dataset</figcaption></figure>
</div>
</div>
</div>
</div>
<p>What we are looking for is an elbow in the distribution of dispersion measures. In <a href="#fig-eda-masc-dp-density">Figure&nbsp;<span>8.4</span></a>, we can see that there is distinctive bend in the distribution between .85 and .97. We can split the difference and use this as a threshold to filter out lemmas that are less dispersed. In <a href="#exm-eda-masc-dp-filter">Example&nbsp;<span>8.13</span></a>, I filter out lemmas that have a dispersion measure less than .91. Then in <a href="#tbl-eda-masc-dp-filter">Table&nbsp;<span>8.6</span></a>, I preview the top and bottom 50 lemmas in the dataset.</p>
<div id="exm-eda-masc-dp-filter" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.13 </strong></span>&nbsp;</p>
<div>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Filter for lemmas with dp &lt;= .91</span></span>
<span><span class="va">masc_lemma_disp_thr</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_lemma_disp</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">dp</span> <span class="op">&lt;=</span> <span class="fl">.91</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">arrange</span><span class="op">(</span><span class="fu">desc</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Preview top</span></span>
<span><span class="va">masc_lemma_disp_thr</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">50</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">pull</span><span class="op">(</span><span class="va">type</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span>ncol <span class="op">=</span> <span class="fl">10</span>, byrow <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">kable</span><span class="op">(</span>col.names <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span>
<span><span class="co"># Preview bottom</span></span>
<span><span class="va">masc_lemma_disp_thr</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_tail</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">50</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">pull</span><span class="op">(</span><span class="va">type</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span>ncol <span class="op">=</span> <span class="fl">10</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">kable</span><span class="op">(</span>col.names <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="tbl-eda-masc-dp-filter" class="cell tbl-parent quarto-layout-panel anchored" data-hash="exploration_cache/html/tbl-eda-masc-dp-filter_1e98fccd9aec7a96718cb18b2be8da69">
<div class="panel-caption table-caption">
<p>Table&nbsp;8.6: Frequency of tokens in the MASC dataset after filtering out lemmas with a Deviation of Proportions less than .91</p>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div id="tbl-eda-masc-dp-filter-1" class="quarto-layout-cell quarto-layout-cell-subref anchored" data-ref-parent="tbl-eda-masc-dp-filter" style="flex-basis: 50.0%;justify-content: center;">
<div id="tbl-eda-masc-dp-filter-1" data-ref-parent="tbl-eda-masc-dp-filter" class="anchored">
<table data-quarto-postprocess="true" class="table table-sm table-striped small">
<caption>(a) Top 50 lemmas</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;">be</td>
<td style="text-align: left;">have</td>
<td style="text-align: left;">do</td>
<td style="text-align: left;">not</td>
<td style="text-align: left;">n't</td>
<td style="text-align: left;">say</td>
<td style="text-align: left;">go</td>
<td style="text-align: left;">know</td>
<td style="text-align: left;">get</td>
<td style="text-align: left;">make</td>
</tr>
<tr class="even">
<td style="text-align: left;">think</td>
<td style="text-align: left;">more</td>
<td style="text-align: left;">just</td>
<td style="text-align: left;">time</td>
<td style="text-align: left;">so</td>
<td style="text-align: left;">other</td>
<td style="text-align: left;">see</td>
<td style="text-align: left;">people</td>
<td style="text-align: left;">take</td>
<td style="text-align: left;">now</td>
</tr>
<tr class="odd">
<td style="text-align: left;">work</td>
<td style="text-align: left;">year</td>
<td style="text-align: left;">come</td>
<td style="text-align: left;">use</td>
<td style="text-align: left;">well</td>
<td style="text-align: left;">look</td>
<td style="text-align: left;">then</td>
<td style="text-align: left;">right</td>
<td style="text-align: left;">only</td>
<td style="text-align: left;">want</td>
</tr>
<tr class="even">
<td style="text-align: left;">also</td>
<td style="text-align: left;">way</td>
<td style="text-align: left;">need</td>
<td style="text-align: left;">back</td>
<td style="text-align: left;">here</td>
<td style="text-align: left;">new</td>
<td style="text-align: left;">find</td>
<td style="text-align: left;">give</td>
<td style="text-align: left;">thing</td>
<td style="text-align: left;">tell</td>
</tr>
<tr class="odd">
<td style="text-align: left;">t</td>
<td style="text-align: left;">first</td>
<td style="text-align: left;">help</td>
<td style="text-align: left;">day</td>
<td style="text-align: left;">many</td>
<td style="text-align: left;">man</td>
<td style="text-align: left;">ask</td>
<td style="text-align: left;">very</td>
<td style="text-align: left;">much</td>
<td style="text-align: left;">even</td>
</tr>
</tbody>
</table>
</div>
<p><strong>?(caption)</strong></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">


</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div id="tbl-eda-masc-dp-filter-2" class="quarto-layout-cell quarto-layout-cell-subref anchored" data-ref-parent="tbl-eda-masc-dp-filter" style="flex-basis: 50.0%;justify-content: center;">
<div id="tbl-eda-masc-dp-filter-2" data-ref-parent="tbl-eda-masc-dp-filter" class="anchored">
<table data-quarto-postprocess="true" class="table table-sm table-striped small">
<caption>(b) Bottom 50 lemmas</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;">fortunately</td>
<td style="text-align: left;">ignorance</td>
<td style="text-align: left;">liability</td>
<td style="text-align: left;">brave</td>
<td style="text-align: left;">summarize</td>
<td style="text-align: left;">liberty</td>
<td style="text-align: left;">wound</td>
<td style="text-align: left;">nostalgic</td>
<td style="text-align: left;">accidentally</td>
<td style="text-align: left;">wax</td>
</tr>
<tr class="even">
<td style="text-align: left;">dump</td>
<td style="text-align: left;">sting</td>
<td style="text-align: left;">tuition</td>
<td style="text-align: left;">unleash</td>
<td style="text-align: left;">blur</td>
<td style="text-align: left;">going</td>
<td style="text-align: left;">devote</td>
<td style="text-align: left;">shy</td>
<td style="text-align: left;">protective</td>
<td style="text-align: left;">faith-based</td>
</tr>
<tr class="odd">
<td style="text-align: left;">instrument</td>
<td style="text-align: left;">mainstream</td>
<td style="text-align: left;">awaken</td>
<td style="text-align: left;">prosperous</td>
<td style="text-align: left;">resistance</td>
<td style="text-align: left;">awkward</td>
<td style="text-align: left;">alright</td>
<td style="text-align: left;">proximity</td>
<td style="text-align: left;">preside</td>
<td style="text-align: left;">decidedly</td>
</tr>
<tr class="even">
<td style="text-align: left;">triumph</td>
<td style="text-align: left;">wildly</td>
<td style="text-align: left;">hook</td>
<td style="text-align: left;">buzz</td>
<td style="text-align: left;">absurd</td>
<td style="text-align: left;">afterwards</td>
<td style="text-align: left;">evolutionary</td>
<td style="text-align: left;">sandy</td>
<td style="text-align: left;">rethink</td>
<td style="text-align: left;">resolute</td>
</tr>
<tr class="odd">
<td style="text-align: left;">harsh</td>
<td style="text-align: left;">dismiss</td>
<td style="text-align: left;">fetch</td>
<td style="text-align: left;">presume</td>
<td style="text-align: left;">qualify</td>
<td style="text-align: left;">eve</td>
<td style="text-align: left;">envy</td>
<td style="text-align: left;">interfere</td>
<td style="text-align: left;">strictly</td>
<td style="text-align: left;">evidently</td>
</tr>
</tbody>
</table>
</div>
<p><strong>?(caption)</strong></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">


</div>
</div>
</div>
</div>
</div>
<p>We now have a good candidate list of common vocabulary that is spread well across the corpus.</p>
</section><section id="sec-eda-frequency-relative" class="level5"><h5 class="anchored" data-anchor-id="sec-eda-frequency-relative">Relative frequency</h5>
<p>Gauging frequency and dispersion across the entire corpus is a good starting point for any frequency analysis, but it is often the case that we want to compare the frequency and dispersion of linguistic units across corpora or sub-corpora.</p>
<p>In the case of the MASC dataset, for example, we may want to compare metrics across the two modalities or the various genres. Simply comparing frequency counts across these sub-corpora is not a good approach, and can be misleading, as the sub-corpora may vary in size. For example, if one sub-corpus is twice as large as another sub-corpus, then, all else being equal, the frequency counts will be twice as large in the larger sub-corpus. This is why we use relative frequency measures, which are normalized by the size of the sub-corpus.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-regular fa-lightbulb" aria-label="lightbulb"></i> Consider this</strong></p>
<p>A variable in the MASC dataset that has yet to be used is the <code>pos</code> part-of-speech variable. How could we use this variable to refine our frequency and dispersion analysis of lemma types?</p>
<p>Hint: consider lemma forms that may be tagged with different parts-of-speech.</p>
</div>
</div>
</div>
<p>To normalize the frequency of linguistic units across sub-corpora, we can use the <strong>relative frequency</strong> measure. This is the frequency of a linguistic unit divided by the total number of linguistic units in the sub-corpus. This bakes in the size of the sub-corpus into the measure. The notion of relative frequency is key to all research working with text, as it is the basis for the statistical approach to text analysis where comparisons are made.</p>
<p>There are some field-specific terms that are used to refer to relative frequency measures. For example, in information retrieval and Natural Language Processing, the relative frequency measure is often referred to as the <strong>term frequency</strong>. In corpus linguistics, the relative frequency measure is often modified slightly to include a constant (<em>e.g.</em> <span class="math inline">\(rf * 100\)</span>) which is known as the <strong>observed relative frequency</strong>. Athough the observed relative frequency per number of tokens is not strictly necessary, it is often used to make the values more interpretable as we can now talk about an observed relative frequency of 1.5 as a linguistic unit that occurs 1.5 times per 100 linguistic units.</p>
<p>Let’s consider how we might compare the frequency and dispersion of lemmas across the two modalities in the MASC dataset, spoken and written. To make this a bit more interesting and more relevant, let’s add the <code>pos</code> variable to our analysis. The intent, then, will be to identify lemmas tagged with particular parts of speech that are particularly indicative of each of the modaliites.</p>
<p>We can do this by collapsing the <code>lemma</code> and <code>pos</code> variables into a single variable, <code>lemma_pos</code>, with the <code>str_c()</code> function, as seen in <a href="#exm-eda-masc-type">Example&nbsp;<span>8.14</span></a>.</p>
<div id="exm-eda-masc-type" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.14 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-type_b20221b6ad527f9743442a083bab66f9">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Collapse lemma and pos into type</span></span>
<span><span class="va">masc_content_tbl</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_content_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>lemma_pos <span class="op">=</span> <span class="fu">str_c</span><span class="op">(</span><span class="va">lemma</span>, <span class="va">pos</span>, sep <span class="op">=</span> <span class="st">"_"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Preview</span></span>
<span><span class="va">masc_content_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 10 × 8
&gt;    doc_id modality genre   term_num term         lemma        pos   lemma_pos   
&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;       
&gt;  1 1      Written  Letters        3 contribution contribution NN    contributio…
&gt;  2 1      Written  Letters        7 mean         mean         VB    mean_VB     
&gt;  3 1      Written  Letters        8 more         more         JJR   more_JJR    
&gt;  4 1      Written  Letters       12 know         know         VB    know_VB     
&gt;  5 1      Written  Letters       15 help         help         VB    help_VB     
&gt;  6 1      Written  Letters       17 see          see          VB    see_VB      
&gt;  7 1      Written  Letters       19 much         much         JJ    much_JJ     
&gt;  8 1      Written  Letters       21 contribution contribution NN    contributio…
&gt;  9 1      Written  Letters       22 means        mean         VBZ   mean_VBZ    
&gt; 10 1      Written  Letters       25 'm           be           VBP   be_VBP</code></pre>
</div>
</div>
</div>
<p>Now this will increase the number of lemma types in the dataset as we are now considering lemmas where the same lemma form is tagged with different parts-of-speech.</p>
<p>Getting back to calculating the frequency and dispersion of lemmas in each modality, we can use the <code><a href="https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html">calc_type_metrics()</a></code> function with <code>lemma_pos</code> as our type argument. We will, however, need to apply this function to each sub-corpus independently and then concatenate the two data frames. This function returns a (raw) frequency measure by default, but we can specify the<code>frequency</code> argument to <code>rf</code> to calculate the relative frequency of the linguistic units as in <a href="#exm-eda-masc-metrics-modality">Example&nbsp;<span>8.15</span></a>.</p>
<div id="exm-eda-masc-metrics-modality" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.15 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-metrics-modality_4629f5a03e98c19fcacc31d492de6cd7">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Calculate relative frequency</span></span>
<span><span class="co"># Spoken</span></span>
<span><span class="va">masc_spoken_metrics</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_content_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">modality</span> <span class="op">==</span> <span class="st">"Spoken"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html">calc_type_metrics</a></span><span class="op">(</span></span>
<span>    type <span class="op">=</span> <span class="va">lemma_pos</span>,</span>
<span>    documents <span class="op">=</span> <span class="va">doc_id</span>,</span>
<span>    frequency <span class="op">=</span> <span class="st">"rf"</span>,</span>
<span>    dispersion <span class="op">=</span> <span class="st">"dp"</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>modality <span class="op">=</span> <span class="st">"Spoken"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">arrange</span><span class="op">(</span><span class="fu">desc</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Written</span></span>
<span><span class="va">masc_written_metrics</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_content_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">modality</span> <span class="op">==</span> <span class="st">"Written"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html">calc_type_metrics</a></span><span class="op">(</span></span>
<span>    type <span class="op">=</span> <span class="va">lemma_pos</span>,</span>
<span>    documents <span class="op">=</span> <span class="va">doc_id</span>,</span>
<span>    frequency <span class="op">=</span> <span class="st">"rf"</span>,</span>
<span>    dispersion <span class="op">=</span> <span class="st">"dp"</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>modality <span class="op">=</span> <span class="st">"Written"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">arrange</span><span class="op">(</span><span class="fu">desc</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Concatenate spoken and written metrics</span></span>
<span><span class="va">masc_metrics</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu">bind_rows</span><span class="op">(</span><span class="va">masc_spoken_metrics</span>, <span class="va">masc_written_metrics</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Preview</span></span>
<span><span class="va">masc_metrics</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 10 × 5
&gt;    type         n      rf     dp modality
&gt;    &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   
&gt;  1 be_VBZ    2612 0.0489  0.0842 Spoken  
&gt;  2 be_VBP    1282 0.0240  0.111  Spoken  
&gt;  3 be_VBD    1020 0.0191  0.301  Spoken  
&gt;  4 n't_RB     829 0.0155  0.139  Spoken  
&gt;  5 have_VBP   766 0.0143  0.152  Spoken  
&gt;  6 do_VBP     728 0.0136  0.180  Spoken  
&gt;  7 be_VB      655 0.0123  0.147  Spoken  
&gt;  8 not_RB     638 0.0119  0.137  Spoken  
&gt;  9 just_RB    404 0.00756 0.267  Spoken  
&gt; 10 so_RB      387 0.00725 0.357  Spoken</code></pre>
</div>
</div>
</div>
<p>With the <code>rf</code> measure, we are now in a position to compare ‘apples to apples’, as you might say. We can now compare the relative frequency of lemmas across the two modalities. Let’s preview the top 10 lemmas in each modality, as seen in <a href="#exm-eda-masc-relative-frequency-top">Example&nbsp;<span>8.16</span></a>.</p>
<div id="exm-eda-masc-relative-frequency-top" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.16 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-relative-frequency-top_7e746737ecc0cc6c755cb33283c92852">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Preview top 10 lemmas in each modality</span></span>
<span><span class="va">masc_metrics</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">group_by</span><span class="op">(</span><span class="va">modality</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_max</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span>, order_by <span class="op">=</span> <span class="va">rf</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 20 × 5
&gt;    type         n      rf     dp modality
&gt;    &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   
&gt;  1 be_VBZ    2612 0.0489  0.0842 Spoken  
&gt;  2 be_VBP    1282 0.0240  0.111  Spoken  
&gt;  3 be_VBD    1020 0.0191  0.301  Spoken  
&gt;  4 n't_RB     829 0.0155  0.139  Spoken  
&gt;  5 have_VBP   766 0.0143  0.152  Spoken  
&gt;  6 do_VBP     728 0.0136  0.180  Spoken  
&gt;  7 be_VB      655 0.0123  0.147  Spoken  
&gt;  8 not_RB     638 0.0119  0.137  Spoken  
&gt;  9 just_RB    404 0.00756 0.267  Spoken  
&gt; 10 so_RB      387 0.00725 0.357  Spoken  
&gt; 11 be_VBZ    4745 0.0248  0.230  Written 
&gt; 12 be_VBD    3317 0.0173  0.366  Written 
&gt; 13 be_VBP    2617 0.0137  0.237  Written 
&gt; 14 be_VB     1863 0.00974 0.218  Written 
&gt; 15 not_RB    1640 0.00858 0.259  Written 
&gt; 16 have_VBP  1227 0.00642 0.290  Written 
&gt; 17 n't_RB     905 0.00473 0.540  Written 
&gt; 18 have_VBD   859 0.00449 0.446  Written 
&gt; 19 have_VBZ   777 0.00406 0.335  Written 
&gt; 20 say_VBD    710 0.00371 0.609  Written</code></pre>
</div>
</div>
</div>
<p>We can appreciate, now, that there are similarities and a few differences between the most frequent lemmas for each modality. First, there are similar lemmas in written and spoken modalities, such as ‘be’, ‘have’, and ‘not’. Second, the top 10 include verbs and adverbs. Now we are looking at the most frequent types, so it is not surprising that we see more in common than not. However, looking close we can see that contracted forms are more frequent in the spoken modality, such as ‘isn’t’, ‘don’t’, and ‘can’t’ and that ordering of the verb tenses differs to some degree. Whether these are important distinctions for our task is something we will need to consider.</p>
<p>We can further cull our results by filtering out lemmas that are not well-dispersed across the sub-corpora. Although it may be tempting to use the threshold we used earlier, we should consider that the size of the sub-corpora are different and the distribution of the dispersion measure may be different. With this in mind, we need to visualize the distribution of the dispersion measure for each modality, as seen in <a href="#fig-eda-masc-dispersion-threshold">Figure&nbsp;<span>8.5</span></a>.</p>
<div class="cell" data-hash="exploration_cache/html/fig-eda-masc-dispersion-threshold_96770a698dd6faad97d36d3fa6446637">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Density plot of dp by modality</span></span>
<span><span class="va">masc_metrics</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">dp</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_density</span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_x_continuous</span><span class="op">(</span>breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">.1</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="st">"Deviation of Proportions"</span>, y <span class="op">=</span> <span class="st">"Density"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">facet_wrap</span><span class="op">(</span><span class="op">~</span> <span class="va">modality</span>, ncol <span class="op">=</span> <span class="fl">2</span>, scales <span class="op">=</span> <span class="st">"free_x"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-eda-masc-dispersion-threshold" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-dispersion-threshold-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption">Figure&nbsp;8.5: Density plot of Deviation of Proportions for lemmas in the MASC dataset by modality</figcaption></figure>
</div>
</div>
</div>
<p>As expected, the density plots point to different thresholds for each modality.The written subcorpus follows closely with the previous distribution, but the spoken subcorpus has more than one bend in the distribution. Why are there multiple peaks in the density plot? It points to some level of inconsistency in the spoken data, either potentially some level of context-dependent language use (genres, topics, speech styles) or it could be due to the fact that the spoken subcorpus’ size is too small to provide a reliable distribution.</p>
<p>In any case, we can estimate the threshold for the spoken corpus making use of the largest peak in the distribution as the reference point. With this approach in mind, lets maintain the <span class="math inline">\(.91\)</span> threshold for the written subcorpus and use a <span class="math inline">\(.79\)</span> threshold for the spoken subcorpus. Let’s filter out lemmas that have a dispersion measure less than .91 for the written subcorpus and less than .79 for the spoken subcorpus, as seen in <a href="#exm-eda-masc-subcorpora-filtered">Example&nbsp;<span>8.17</span></a>.</p>
<div id="exm-eda-masc-subcorpora-filtered" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.17 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-subcorpora-filtered_97a5605d1f9727b7ef317eb92c0e1ce8">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Filter for lemmas with</span></span>
<span><span class="co"># dp &lt;= .91 for written and</span></span>
<span><span class="co"># dp &lt;= .79 for spoken</span></span>
<span><span class="va">masc_metrics_thr</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_metrics</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span></span>
<span>    <span class="op">(</span><span class="va">modality</span> <span class="op">==</span> <span class="st">"Written"</span> <span class="op">&amp;</span> <span class="va">dp</span> <span class="op">&lt;=</span> <span class="fl">.91</span><span class="op">)</span> <span class="op">|</span></span>
<span>    <span class="op">(</span><span class="va">modality</span> <span class="op">==</span> <span class="st">"Spoken"</span> <span class="op">&amp;</span> <span class="va">dp</span> <span class="op">&lt;=</span> <span class="fl">.79</span><span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">arrange</span><span class="op">(</span><span class="fu">desc</span><span class="op">(</span><span class="va">rf</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>Filtering the less-dispersed types reduces the dataset from 33637 to 4860 observations. This will provide us with a more succinct list of common and well-dispersed lemmas that are used in each modality.</p>
<p>As much as the frequency and dispersion measures can provide us with a good starting point, it does not provide an understanding of what types are more indicative of a particular sub-corpus, modality subcorpora in our case. We can do this by calculating the log odds ratio of each lemma in each modality.</p>
<p>The <strong>log odds ratio</strong> is a measure that quantifies the difference between the frequencies of a type in two corpora or sub-corpora. In spirit and in name, it compares the odds of a type occurring in one corpus versus the other. The values range from negative to positive infinity, with negative values indicating that the type is more frequent in the first corpus and positive values indicating that the lemma is more frequent in the second corpus. The magnitude of the value indicates the strength of the association.</p>
<p>The <code>tidylo</code> package provides a convenient function <code><a href="https://juliasilge.github.io/tidylo/reference/bind_log_odds.html">bind_log_odds()</a></code> to calculate the log odds ratio, and a weighed variant, for each type in each sub-corpus. Let’s use this function to calculate the log odds ratio for each lemma in each modality, as seen in <a href="#exm-eda-masc-log-odds">Example&nbsp;<span>8.18</span></a>.</p>
<div id="exm-eda-masc-log-odds" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.18 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-log-odds_5e48c54932c8052d839ad031a60c1af7">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Load package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://juliasilge.github.io/tidylo/">tidylo</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calculate log odds ratio</span></span>
<span><span class="va">masc_metrics_thr</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_metrics_thr</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://juliasilge.github.io/tidylo/reference/bind_log_odds.html">bind_log_odds</a></span><span class="op">(</span></span>
<span>    set <span class="op">=</span> <span class="va">modality</span>,</span>
<span>    feature <span class="op">=</span> <span class="va">type</span>,</span>
<span>    n <span class="op">=</span> <span class="va">n</span>,</span>
<span>    unweighted <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Preview (ordered by log_odds)</span></span>
<span><span class="co"># Spoken</span></span>
<span><span class="va">masc_metrics_thr</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_max</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span>, order_by <span class="op">=</span> <span class="va">log_odds</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 10 × 7
&gt;    type                  n       rf    dp modality log_odds log_odds_weighted
&gt;    &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;             &lt;dbl&gt;
&gt;  1 understanding_NN     45 0.000843 0.649 Spoken      0.955              6.41
&gt;  2 meeting_NNS          42 0.000786 0.702 Spoken      0.955              6.19
&gt;  3 testimony_NN         36 0.000674 0.785 Spoken      0.955              5.73
&gt;  4 administration_NN    34 0.000637 0.650 Spoken      0.955              5.57
&gt;  5 trial_NN             33 0.000618 0.769 Spoken      0.955              5.49
&gt;  6 governor_NN          28 0.000524 0.597 Spoken      0.955              5.05
&gt;  7 intelligent_JJ       28 0.000524 0.777 Spoken      0.955              5.05
&gt;  8 faith_NN             27 0.000506 0.524 Spoken      0.955              4.96
&gt;  9 walk_VBZ             27 0.000506 0.761 Spoken      0.955              4.96
&gt; 10 gun_NNS              26 0.000487 0.577 Spoken      0.955              4.87</code></pre>
</div>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Written</span></span>
<span><span class="va">masc_metrics_thr</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_min</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span>, order_by <span class="op">=</span> <span class="va">log_odds</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 10 × 7
&gt;    type             n        rf    dp modality log_odds log_odds_weighted
&gt;    &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;             &lt;dbl&gt;
&gt;  1 correct_JJ      13 0.0000680 0.900 Written    -0.461             -3.75
&gt;  2 mean_VBP        38 0.000199  0.776 Written    -0.411             -5.00
&gt;  3 president_NN    37 0.000194  0.840 Written    -0.406             -4.81
&gt;  4 board_NN        27 0.000141  0.830 Written    -0.405             -4.10
&gt;  5 argument_NN     24 0.000126  0.798 Written    -0.356             -3.07
&gt;  6 meeting_NN      44 0.000230  0.852 Written    -0.343             -3.90
&gt;  7 question_NN     80 0.000418  0.630 Written    -0.330             -4.94
&gt;  8 say_VBN         20 0.000105  0.767 Written    -0.325             -2.42
&gt;  9 read_VBN        10 0.0000523 0.895 Written    -0.325             -1.71
&gt; 10 cut_NN           9 0.0000471 0.879 Written    -0.325             -1.62</code></pre>
</div>
</div>
</div>
<p>The distinctive terms in each modality from <a href="#exm-eda-masc-log-odds">Example&nbsp;<span>8.18</span></a> may not jibe with your intuitio, and that’s understandable. This is likely because we are comparing sub-corpora of different sizes and with different document lengths. The log odds ratio is a measure that is sensitive to these differences.</p>
<p>The second measure produced by <code><a href="https://juliasilge.github.io/tidylo/reference/bind_log_odds.html">bind_log_odds()</a></code> function, is the weighted log odds ratio. This measure provides a more robust and interpretable measure for comparing term frequencies across corpora, especially when term frequencies are low or when corpora are of different sizes. The weighting (or standardization) also makes it easier to identify terms that are particularly distinctive or characteristic of one corpus over another. Note that the weighted measure’s interpretation is slightly different that the log odds’s. The larger positive values in each corpus indicate that the type is more indicative of that (sub-)corpus, and the larger negative values indicate that the type is less indicative.</p>
<p>Let’s imagine we would like to extract the most indicative verbs for each modality using the weighted log odds as our measure. We can do this with a little regular expression magic. Let’s use the <code>str_subset()</code> function to filter for lemmas that start with ‘V’ and then use <code>slice_max()</code> to extract the top 10 most indicative verb lemmas, as seen in <a href="#exm-eda-masc-log-odds-weighted-verbs">Example&nbsp;<span>8.19</span></a>.</p>
<div id="exm-eda-masc-log-odds-weighted-verbs" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.19 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-log-odds-weighted-verbs_6e0df26d1b2e60e4e52c9de2685b485c">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Preview (ordered by log_odds_weighted)</span></span>
<span><span class="co"># Spoken and written</span></span>
<span><span class="va">masc_metrics_thr</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">group_by</span><span class="op">(</span><span class="va">modality</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="fu">str_detect</span><span class="op">(</span><span class="va">type</span>, <span class="st">"_V"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_max</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span>, order_by <span class="op">=</span> <span class="va">log_odds_weighted</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="va">n</span>, <span class="op">-</span><span class="va">log_odds</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 20 × 5
&gt;    type                rf     dp modality log_odds_weighted
&gt;    &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;
&gt;  1 be_VBZ        0.0489   0.0842 Spoken               14.0 
&gt;  2 do_VBP        0.0136   0.180  Spoken               10.3 
&gt;  3 be_VBP        0.0240   0.111  Spoken                8.66
&gt;  4 think_VBP     0.00655  0.259  Spoken                8.32
&gt;  5 have_VBP      0.0143   0.152  Spoken                8.00
&gt;  6 know_VBP      0.00528  0.260  Spoken                7.03
&gt;  7 go_VBG        0.00534  0.207  Spoken                6.47
&gt;  8 do_VBD        0.00603  0.321  Spoken                5.97
&gt;  9 mean_VBP      0.00247  0.543  Spoken                5.94
&gt; 10 do_VB         0.00455  0.207  Spoken                5.61
&gt; 11 don_VB        0.000361 0.839  Written               4.04
&gt; 12 doe_VBZ       0.000350 0.871  Written               3.98
&gt; 13 walk_VBD      0.000319 0.790  Written               3.80
&gt; 14 associate_VBN 0.000303 0.777  Written               3.70
&gt; 15 reply_VBD     0.000293 0.838  Written               3.64
&gt; 16 develop_VBG   0.000288 0.812  Written               3.60
&gt; 17 require_VBN   0.000272 0.793  Written               3.50
&gt; 18 fall_VBD      0.000267 0.757  Written               3.47
&gt; 19 meet_VB       0.000241 0.729  Written               3.30
&gt; 20 regard_VBG    0.000225 0.823  Written               3.19</code></pre>
</div>
</div>
</div>
<p>Note that the log odds are larger for the spoken modality than the written modality. This indicates that theses types are more strongly indicative of the spoken modality than the types in the written modality are indicative of the written modality. This is not surprising, as the written modality is typically more diverse in terms of lexical usage than the spoken modality, where the terms tend to be repeated more often, including verbs.</p>
</section></section><section id="sec-eda-co-occurrence" class="level4"><h4 class="anchored" data-anchor-id="sec-eda-co-occurrence">Co-occurrence analysis</h4>
<p>Moving forward on our task, we have a good idea of the general vocabulary that we want to include in our ELL materials and can identify lemma types that are particularly indicative of each modality. Another useful approach to complement our analysis is to identify words that co-occur with our target lemmas –in particular verbs. In English it is common for verbs to appear with a preposition or adverb, such as ‘give up’, ‘look after’. These ‘phrasal verbs’ form a semantic unit that is distinct from the verb alone.</p>
<p>In cases such as this, we are aiming to do a co-occurrence analysis. Co-occurrence analysis is a set of methods that are used to identify words that appear in close proximity to a target type.</p>
<!-- Concordances -->
<p>An exploratory, primarily qualitative, approach is to display the co-occurrence of words in a Keyword in Context (KWIC). This is a table that displays the target word in the center of the table and the words that appear before and after the target word. This is a useful approach for spot identifying collocations of a target word or phrase.</p>
<p>The <code>quanteda</code> package includes a function <code><a href="https://quanteda.io/reference/kwic.html">kwic()</a></code> that can be used to create a KWIC table. It does require some transformation to the data, however. We need to collapse the <code>lemma</code> column into a single string for each document from the original transformed dataset, <code>masc_tbl</code>. Then we can apply the <code><a href="https://quanteda.io/reference/corpus.html">corpus()</a></code> and then <code>tokens</code> function to create a quanteda tokens object. Then we can apply the <code><a href="https://quanteda.io/reference/kwic.html">kwic()</a></code> function to create a KWIC table.</p>
<div id="exm-eda-masc-kwic" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.20 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-kwic_6aeb506b07a2d18b49f6243fd237533b">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Collapse lemma, pos into a single string</span></span>
<span><span class="va">masc_text_tbl</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>lemma_pos <span class="op">=</span> <span class="fu">str_c</span><span class="op">(</span><span class="va">lemma</span>, <span class="va">pos</span>, sep <span class="op">=</span> <span class="st">"_"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">group_by</span><span class="op">(</span><span class="va">doc_id</span>, <span class="va">modality</span>, <span class="va">genre</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">summarize</span><span class="op">(</span>text <span class="op">=</span> <span class="fu">str_c</span><span class="op">(</span><span class="va">lemma_pos</span>, collapse <span class="op">=</span> <span class="st">" "</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Load package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://quanteda.io">quanteda</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">masc_corpus</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_text_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://quanteda.io/reference/corpus.html">corpus</a></span><span class="op">(</span></span>
<span>    text_field <span class="op">=</span> <span class="st">"text"</span>,</span>
<span>    docid_field <span class="op">=</span> <span class="st">"doc_id"</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="va">masc_corpus</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://quanteda.io/reference/tokens.html">tokens</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://quanteda.io/reference/kwic.html">kwic</a></span><span class="op">(</span></span>
<span>    pattern <span class="op">=</span> <span class="fu"><a href="https://quanteda.io/reference/phrase.html">phrase</a></span><span class="op">(</span><span class="st">"*_V* *_IN*"</span><span class="op">)</span>,</span>
<span>    window <span class="op">=</span> <span class="fl">3</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">as_tibble</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">docname</span>, <span class="va">pre</span>, <span class="va">keyword</span>, <span class="va">post</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_sample</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 10 × 4
&gt;    docname pre                               keyword             post           
&gt;    &lt;chr&gt;   &lt;chr&gt;                             &lt;chr&gt;               &lt;chr&gt;          
&gt;  1 171     other_JJ hollywood_NNP star_NNS   come_VBD as_IN      well_RB simply…
&gt;  2 242     not_RB only_RB i_PRP              find_VBD out_IN     more_JJR about…
&gt;  3 206     long_JJ have_VBP you_PRP          live_VBD in_IN      charlotte_NNP …
&gt;  4 76      fire_NN let_VBG it_PRP            burn_VBP out_IN     by_IN itself_P…
&gt;  5 170     s_POS paranoia_NN by_IN           flee_VBG into_IN    egypt_NNP with…
&gt;  6 382     be_VBZ currently_RB be_VBG        examine_VBN by_IN   regulator_NNS …
&gt;  7 217     collection_NN of_IN nestorian_NNP cross_VBZ from_IN   the_DT yuan_NN…
&gt;  8 98      destroy_VBN save_VB for_IN        reserve_VBN for_IN  the_DT smithso…
&gt;  9 233     secret_JJ in_NNP attorney_NNS     charge_VBN with_IN  prosecute_VBG …
&gt; 10 153     the_DT artist_NN have_VBD         reach_VBN within_IN himself_PRP it…</code></pre>
</div>
</div>
</div>
<!-- N-grams -->
<p>A straightforward quantitative way to explore co-occurrence is to set the unit of observation to an <span class="math inline">\(n-gram\)</span> of terms. An <span class="math inline">\(n-gram\)</span> is a sequence of <span class="math inline">\(n\)</span> words. For example, a 2-gram is a sequence of two words, a 3-gram is a sequence of three words, and so on. Then, the frequency and dispersion metrics can be calculated for each <span class="math inline">\(n-gram\)</span>.</p>
<p>In general, deriving <span class="math inline">\(n-grams\)</span> from a corpus is a straightforward process. The <code>tidytext</code> package includes a function <code><a href="https://rdrr.io/pkg/tidytext/man/unnest_tokens.html">unnest_tokens()</a></code> that can be used to create <span class="math inline">\(n-grams\)</span> from a corpus. The function can take a single column of untokenized text or a tokenized column in combination with a variable to use as the grouping variable. In the <code>masc_tbl</code> dataset, we have tokenized text in the <code>lemma</code> column and a grouping variable in the <code>doc_id</code> column. We can use the <code><a href="https://rdrr.io/pkg/tidytext/man/unnest_tokens.html">unnest_tokens()</a></code> function to create a new data frame with a row for each <span class="math inline">\(n-gram\)</span> in each document, as seen in <a href="#exm-eda-masc-bigrams-tidytext">Example&nbsp;<span>8.21</span></a>.</p>
<div id="exm-eda-masc-bigrams-tidytext" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.21 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-bigrams-tidytext_67bc3204c5d82579960cac4f19619623">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Load package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/juliasilge/tidytext">tidytext</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create bigrams</span></span>
<span><span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tidytext/man/unnest_tokens.html">unnest_tokens</a></span><span class="op">(</span></span>
<span>    output <span class="op">=</span> <span class="va">bigrams</span>,</span>
<span>    input <span class="op">=</span> <span class="va">lemma</span>,</span>
<span>    token <span class="op">=</span> <span class="st">"ngrams"</span>,</span>
<span>    n <span class="op">=</span> <span class="fl">2</span>,</span>
<span>    collapse <span class="op">=</span> <span class="st">"doc_id"</span></span>
<span>    <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 10 × 2
&gt;    doc_id bigrams          
&gt;    &lt;chr&gt;  &lt;chr&gt;            
&gt;  1 1      december your    
&gt;  2 1      your contribution
&gt;  3 1      contribution to  
&gt;  4 1      to goodwill      
&gt;  5 1      goodwill will    
&gt;  6 1      will mean        
&gt;  7 1      mean more        
&gt;  8 1      more than        
&gt;  9 1      than you         
&gt; 10 1      you may</code></pre>
</div>
</div>
</div>
<p>The result of this operation can be joined with the original dataset using the <code>doc_id</code> as the key variable. Then we can calculate the frequency and dispersion metrics for each <span class="math inline">\(n-gram\)</span> in each modality. However, this approach is not ideal for our task. The reason is that we are interested in identifying <span class="math inline">\(n-grams\)</span> that include verbs. The <code><a href="https://rdrr.io/pkg/tidytext/man/unnest_tokens.html">unnest_tokens()</a></code> function does not allow us to filter the <span class="math inline">\(n-grams\)</span> by part-of-speech.</p>
<p>Another, more informative approach is to create a new variable that combines the lemma and part-of-speech for each observation before using <code><a href="https://rdrr.io/pkg/tidytext/man/unnest_tokens.html">unnest_tokens()</a></code> to generate the bigrams. We can use the <code>str_c()</code> function from the <code>stringr</code> package to join the <code>lemma</code> and <code>pos</code> columns into a single string, so that we have a variable <code>lemma_pos</code> with the lemma and part-of-speech joined by an underscore.</p>
<p>One consideration that we need to take for our goal to identify verb particle constructions, is how we ultimately want to group our <code>lemma_pos</code> values. This is particularly important given the fact that our <code>pos</code> tags for verbs include information about the verb’s tense and person. This means that a verb in a verb particle bigram, such as ‘look after’, will be represented by multiple <code>lemma_pos</code> values, such as ‘look_VB’, ‘look_VBP’, ‘look_VBD’, and ‘look_VBG’. If we want this level of detail, we just proceed as described above. However, if we want to group the verb particle bigrams by a single verb value, we need to recode the <code>pos</code> values for verbs. We can do this with the <code>case_match()</code> function from the <code>dplyr</code> package.</p>
<p>In <a href="#exm-eda-masc-lemma-pos">Example&nbsp;<span>8.22</span></a>, I recode the <code>pos</code> values for verbs to ‘V’ and then join the <code>lemma</code> and <code>pos</code> columns into a single string.</p>
<div id="exm-eda-masc-lemma-pos" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.22 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-lemma-pos_2aa4c7f8a4776b75bd93cdd72ae587c4">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Collapse lemma into a single string</span></span>
<span><span class="va">masc_lemma_pos_tbl</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>pos <span class="op">=</span> <span class="fu">case_when</span><span class="op">(</span></span>
<span>    <span class="fu">str_detect</span><span class="op">(</span><span class="va">pos</span>, <span class="st">"^V"</span><span class="op">)</span> <span class="op">~</span> <span class="st">"V"</span>,</span>
<span>    <span class="cn">TRUE</span> <span class="op">~</span> <span class="va">pos</span></span>
<span>  <span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">group_by</span><span class="op">(</span><span class="va">doc_id</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>lemma_pos <span class="op">=</span> <span class="fu">str_c</span><span class="op">(</span><span class="va">lemma</span>, <span class="va">pos</span>, sep <span class="op">=</span> <span class="st">"_"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Preview</span></span>
<span><span class="va">masc_lemma_pos_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 10 × 8
&gt;    doc_id modality genre   term_num term         lemma        pos   lemma_pos   
&gt;    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;       
&gt;  1 1      Written  Letters        0 December     december     NNP   december_NNP
&gt;  2 1      Written  Letters        2 Your         your         PRP$  your_PRP$   
&gt;  3 1      Written  Letters        3 contribution contribution NN    contributio…
&gt;  4 1      Written  Letters        4 to           to           TO    to_TO       
&gt;  5 1      Written  Letters        5 Goodwill     goodwill     NNP   goodwill_NNP
&gt;  6 1      Written  Letters        6 will         will         MD    will_MD     
&gt;  7 1      Written  Letters        7 mean         mean         V     mean_V      
&gt;  8 1      Written  Letters        8 more         more         JJR   more_JJR    
&gt;  9 1      Written  Letters        9 than         than         IN    than_IN     
&gt; 10 1      Written  Letters       10 you          you          PRP   you_PRP</code></pre>
</div>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">masc_lemma_pos_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tidytext/man/unnest_tokens.html">unnest_tokens</a></span><span class="op">(</span></span>
<span>    output <span class="op">=</span> <span class="va">bigrams</span>,</span>
<span>    input <span class="op">=</span> <span class="va">lemma_pos</span>,</span>
<span>    token <span class="op">=</span> <span class="st">"ngrams"</span>,</span>
<span>    n <span class="op">=</span> <span class="fl">2</span>,</span>
<span>    to_lower <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>    collapse <span class="op">=</span> <span class="st">"doc_id"</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="fu">str_detect</span><span class="op">(</span><span class="va">bigrams</span>, <span class="st">"_V.*_IN"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://qtalr.github.io/qtalrkit/reference/calc_type_metrics.html">calc_type_metrics</a></span><span class="op">(</span></span>
<span>    type <span class="op">=</span> <span class="va">bigrams</span>,</span>
<span>    documents <span class="op">=</span> <span class="va">doc_id</span>,</span>
<span>    frequency <span class="op">=</span> <span class="st">"rf"</span>,</span>
<span>    dispersion <span class="op">=</span> <span class="st">"dp"</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">arrange</span><span class="op">(</span><span class="fu">desc</span><span class="op">(</span><span class="va">rf</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 4,512 × 4
&gt;    type                n      rf    dp
&gt;    &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
&gt;  1 be_V in_IN        359 0.0259  0.366
&gt;  2 be_V that_IN      210 0.0151  0.504
&gt;  3 look_V at_IN      157 0.0113  0.528
&gt;  4 say_V that_IN     134 0.00966 0.580
&gt;  5 be_V on_IN        120 0.00865 0.523
&gt;  6 talk_V about_IN   114 0.00821 0.622
&gt;  7 be_V of_IN        110 0.00793 0.503
&gt;  8 know_V that_IN     98 0.00706 0.605
&gt;  9 think_V that_IN    90 0.00649 0.643
&gt; 10 be_V about_IN      76 0.00548 0.608
&gt; # ℹ 4,502 more rows</code></pre>
</div>
</div>
</div>
<p>We have identified and derived frequency and dispersion metrics for <span class="math inline">\(n-grams\)</span> that include verb particle construction candidates. Yet, there is a problem with this approach. The problem is that the <span class="math inline">\(n-grams\)</span> are not necessarily verb particle constructions in the sense that they form a semantic unit. Second, frequency and dispersion metrics are not necessarily the best measures for identifying the co-occurrence relationship between the verb and the particle. In other words, just because a two-word sequence is frequent and well-dispersed does not mean that the two words form a semantic unit.</p>
<!-- Collocation -->
<p>To address these issues, we can use a statistical measures to estimate collocational strength between two words. A <strong>collocation</strong> is a sequence of words that co-occur more often than would be expected by chance. The most common measure of collocation is the <strong>pointwise mutual information</strong> (PMI) measure. The PMI measure is calculated as the log ratio of the observed frequency of two words co-occurring to the expected frequency of the two words co-occurring. The expected frequency is calculated as the product of the frequency of each word. The PMI measure is a log ratio, so the values range from negative to positive infinity, with negative values indicating that the two words co-occur less often than would be expected by chance and positive values indicating that the two words co-occur more often than would be expected by chance. The magnitude of the value indicates the strength of the association.</p>
<p>Let’s calculate the PMI for all the bigrams in the MASC dataset. We can use the <code><a href="https://qtalr.github.io/qtalrkit/reference/calc_assoc_metrics.html">calc_assoc_metrics()</a></code> function from <code>qtalrkit</code>. We need to specify the <code>association</code> argument to <code>pmi</code> and the <code>type</code> argument to <code>bigrams</code>, as seen in <a href="#exm-eda-masc-bigrams-pmi">Example&nbsp;<span>8.23</span></a>.</p>
<div id="exm-eda-masc-bigrams-pmi" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.23 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-bigrams-pmi_cb90fce12072336f68d3b90276ddb83c">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">masc_lemma_pos_assoc</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_lemma_pos_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://qtalr.github.io/qtalrkit/reference/calc_assoc_metrics.html">calc_assoc_metrics</a></span><span class="op">(</span></span>
<span>    doc_index <span class="op">=</span> <span class="va">doc_id</span>,</span>
<span>    token_index <span class="op">=</span> <span class="va">term_num</span>,</span>
<span>    type <span class="op">=</span> <span class="va">lemma_pos</span>,</span>
<span>    association <span class="op">=</span> <span class="st">"pmi"</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Preview</span></span>
<span><span class="va">masc_lemma_pos_assoc</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">arrange</span><span class="op">(</span><span class="fu">desc</span><span class="op">(</span><span class="va">pmi</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 10 × 4
&gt;    x               y                   n   pmi
&gt;    &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;
&gt;  1 #Christian_NN   bigot_NN            1  12.4
&gt;  2 #FAIL_NN        phenomenally_RB     1  12.4
&gt;  3 #NASCAR_NN      #indycar_NN         1  12.4
&gt;  4 #PALM_NN        merchan_NN          1  12.4
&gt;  5 #Twitter_NN     #growth_NN          1  12.4
&gt;  6 #college_NN     #jobs_NN            1  12.4
&gt;  7 #education_NN   #teaching_NN        1  12.4
&gt;  8 #faculty_NN     #cites_NN           1  12.4
&gt;  9 #fb_NN          siebel_NNP          1  12.4
&gt; 10 #glitchmyass_NN reps_NNP            1  12.4</code></pre>
</div>
</div>
</div>
<p>One caveat to using the PMI measure is that it is sensitive to the frequency of the words. If the words in a bigram pair are infrequent, and especially if they only occur once, then the PMI measure will be inflated. To mitigate this issue, we can apply a frequency threshold to the bigrams before calculating the PMI measure. Let’s filter out bigrams that occur less than 10 times, as seen in <a href="#exm-eda-masc-bigrams-pmi-filtered">Example&nbsp;<span>8.24</span></a>.</p>
<div id="exm-eda-masc-bigrams-pmi-filtered" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.24 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-bigrams-pmi-filtered_3559df1d0bc142ce6d1793656b75eb73">
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Filter for bigrams that occur &gt;= 10 times</span></span>
<span><span class="va">masc_lemma_pos_assoc_thr</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_lemma_pos_assoc</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">n</span> <span class="op">&gt;=</span> <span class="fl">10</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">arrange</span><span class="op">(</span><span class="fu">desc</span><span class="op">(</span><span class="va">pmi</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Preview</span></span>
<span><span class="va">masc_lemma_pos_assoc_thr</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 10 × 4
&gt;    x             y                n   pmi
&gt;    &lt;chr&gt;         &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;
&gt;  1 pianista_NN   irlandesa_NN    10 10.0 
&gt;  2 costa_NNP     rica_NNP        10  9.95
&gt;  3 nanowrimo_NNP novel_NNP       12  9.87
&gt;  4 bin_NN        laden_NNP       11  9.79
&gt;  5 osama_NNP     bin_NN          11  9.79
&gt;  6 bin_NNP       ladin_NNP       11  9.70
&gt;  7 los_NNP       angeles_NNP     11  9.64
&gt;  8 chilean_JJ    seabass_NNS     13  9.64
&gt;  9 novel_NNP     ch_NNP          12  9.58
&gt; 10 st_NNP        zip_NNP         10  9.52</code></pre>
</div>
</div>
</div>
<p>Now we are in a position to identify verb particle constructions. We can filter for bigrams that include a verb and a particle and that have a PMI measure greater than 0, as seen in <a href="#exm-eda-masc-bigrams-pmi-filtered-vpc">Example&nbsp;<span>8.25</span></a>.</p>
<div id="exm-eda-masc-bigrams-pmi-filtered-vpc" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.25 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-bigrams-pmi-filtered-vpc_fdfa9974349fdd136f01a764b084f78e">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Filter for bigrams that include a verb and a particle</span></span>
<span><span class="co"># and that have a PMI measure greater than 0</span></span>
<span><span class="va">masc_verb_part_assoc</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_lemma_pos_assoc_thr</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="fu">str_detect</span><span class="op">(</span><span class="va">x</span>, <span class="st">"_V"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="fu">str_detect</span><span class="op">(</span><span class="va">y</span>, <span class="st">"_IN"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">pmi</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">arrange</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Preview</span></span>
<span><span class="va">masc_verb_part_assoc</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 10 × 4
&gt;    x             y           n   pmi
&gt;    &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;
&gt;  1 account_V     for_IN     17  3.74
&gt;  2 acknowledge_V that_IN    13  3.62
&gt;  3 act_V         as_IN      14  2.96
&gt;  4 agree_V       with_IN    45  3.21
&gt;  5 agree_V       that_IN    14  1.94
&gt;  6 appear_V      on_IN      12  1.98
&gt;  7 appear_V      in_IN      24  1.76
&gt;  8 argue_V       that_IN    20  3.26
&gt;  9 arrive_V      at_IN      18  3.39
&gt; 10 arrive_V      in_IN      10  1.48</code></pre>
</div>
</div>
</div>
<p>We can clean up the results a bit by removing the part-of-speech tags from the <code>x</code> and <code>y</code> variables, up our minimum PMI value, and create a network plot to visualize the results, as seen in <a href="#fig-eda-masc-verb-part-network">Figure&nbsp;<span>8.6</span></a>.</p>
<div class="cell" data-hash="exploration_cache/html/fig-eda-masc-verb-part-network_5028ecd96c4d2168be52c23a829a44c4">
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Clean up results</span></span>
<span><span class="va">masc_verb_part_assoc_plot</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_verb_part_assoc</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">pmi</span> <span class="op">&gt;=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="fu">str_remove</span><span class="op">(</span><span class="va">x</span>, <span class="st">"_V.*"</span><span class="op">)</span>,</span>
<span>    y <span class="op">=</span> <span class="fu">str_remove</span><span class="op">(</span><span class="va">y</span>, <span class="st">"_IN"</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Create an association network plot</span></span>
<span><span class="co"># `x` and `y` are the nodes</span></span>
<span><span class="co"># `pmi` is the edge weight</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r.igraph.org/">igraph</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggraph.data-imaginist.com">ggraph</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">masc_verb_part_assoc_plot</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://r.igraph.org/reference/graph_from_data_frame.html">graph_from_data_frame</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://ggraph.data-imaginist.com/reference/ggraph.html">ggraph</a></span><span class="op">(</span>layout <span class="op">=</span> <span class="st">"nicely"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggraph.data-imaginist.com/reference/geom_edge_link.html">geom_edge_link</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>color <span class="op">=</span> <span class="va">pmi</span><span class="op">)</span>,</span>
<span>    alpha <span class="op">=</span> <span class="fl">0.8</span>,</span>
<span>    edge_width <span class="op">=</span> <span class="fl">0.8</span>,</span>
<span>    arrow <span class="op">=</span> <span class="fu">grid</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/grid/arrow.html">arrow</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggraph.data-imaginist.com/reference/geom_node_point.html">geom_node_point</a></span><span class="op">(</span>color <span class="op">=</span> <span class="st">"black"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggraph.data-imaginist.com/reference/geom_node_text.html">geom_node_text</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>label <span class="op">=</span> <span class="va">name</span><span class="op">)</span>, repel <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggraph.data-imaginist.com/reference/scale_edge_colour.html">scale_edge_color_gradient</a></span><span class="op">(</span>low <span class="op">=</span> <span class="st">"grey90"</span>, high <span class="op">=</span> <span class="st">"grey20"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_void</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-eda-masc-verb-part-network" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-verb-part-network-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption">Figure&nbsp;8.6: Network plot of verb particle constructions in the MASC dataset</figcaption></figure>
</div>
</div>
</div>
<p>From this plot, and from the underlying data, we can explore verb particle constructions. We could go further and apply our co-occurrence methods to each modality separately, if we wanted to identify verb particle constructions that are distinctive to each modality. We could also apply our co-occurrence methods to other parts-of-speech, such as adjectives and nouns, to identify collocations of these parts-of-speech. There is much more to explore with co-occurrence analysis, but this should give you a good idea of the types of questions that can be addressed with co-occurrence analysis.</p>
</section></section><section id="sec-eda-unsupervised" class="level3" data-number="8.2.2"><h3 data-number="8.2.2" class="anchored" data-anchor-id="sec-eda-unsupervised">
<span class="header-section-number">8.2.2</span> Unsupervised learning</h3>
<p>Aligned in purpose with descriptive approaches, unsupervised learning approaches to exploratory data analysis are used to identify patterns in the data from an algorithmic perspective. Common methods in text analysis include principle component analysis, clustering, and vector space modeling.</p>
<p>We will continue to use the MASC dataset as we develop materials for our ELL textbook to illustrate unsupervised learning methods. In the process, we will explore the following questions:</p>
<ul>
<li>Can we identify and group documents based on linguistic features or co-occurrence patterns of the data itself?</li>
<li>Do the groups of documents map to the labels in the dataset?</li>
<li>Can we estimate the semantics of words based on their co-occurrence patterns?</li>
</ul>
<p>Through these questions we will build on our knowledge of frequency, dispersion, and co-occurrence analysis and introduce concepts and methods associated with machine learning.</p>
<section id="sec-eda-clustering" class="level4"><h4 class="anchored" data-anchor-id="sec-eda-clustering">Clustering</h4>
<p><strong>Clustering</strong> is a unsupervised learning technique that can be used to group similar items in the text data, helping to organize the data into distinct categories and discover relationships between different elements in the text. The main steps in the procedure includes identifying the relevant linguistic features to use for clustering, representing the features in a way that can be used for clustering, and applying a clustering algorithm to the data. However, it is important to consider the strengths and weaknesses of the clustering algorithm for a particular task and how the results will be evaluated.</p>
<p>In our ELL textbook task, we may very well want to explore the similiarities and/ or differences between the documents based on the distribution of linguistic features. This provides us a view to evaluate to what extent the labels in the dataset (modality and genre) map to the distribution of linguistic features. Based on this evaluation, we may want to consider re-labeling the documents, collapsing labels, or even adding new labels.</p>
<p>Enter clustering. Instead of relying entirely on the labels in the MASC dataset, we can let the data itself say something about how related the documents are. Yet, a pivotal question is what features should we use, otherwise known as <strong>feature selection</strong>. We could use terms or lemmas, but we may want to consider other features, such as parts-of-speech or some co-occurrence patterns. We are not locked into using one criterion, and we can perform clustering multiple times with different features, but we should consider the implications of our feature selection for our interpretation of the results.</p>
<p>Another key question is what clustering algorithm to use. Again, we are not married to one algorithm, and we can perform clustering multiple times with different algorithms, but not all algorithms are created equal. Some algorithms are better suited for certain types of data and certain types of tasks. For example, <strong>Hierarchical clustering</strong> is a good choice when we are not sure how many clusters we want to identify, as it does not require us to specify the number of clusters from the outset. However, it is not a good choice when we have a large dataset, as it can be computationally expensive compared to some other algorithms. <strong>K-means clustering</strong>, on the other hand, is a good choice when we want to identify a pre-defined number of clusters, and the aim is to gauge how well the data fit the clusters. These two clustering techniques, therefore complement each other with Hierarchical clustering being a good choice for initial exploration and K-means clustering being a good choice for targeted evaluation.</p>
<p>With these considerations in mind, let’s start by identifying the linguistic features that we want to use for clustering. Imagine that among the various features that we are interested in associating documents, we consider lemma use and part-of-speech use. However, we need to operationalize what we mean by ‘use’. In machine learning, this process is known as <strong>feature engineering</strong>. Since we aim to compare documents it is logical for us to use the document-normalized features. So in both lemma and part-of-speech tags, we will use the relative frequency. An additional operation that we can apply to the lemma feature is to weight the relative frequency by the dispersion of the lemma. This will give us a measure of the distinctiveness of the lemma in the document. A common implementation of this approach is to use the <span class="math inline">\(tf-idf\)</span> measure, which is the product of the relative frequency and the inverse document frequency.</p>
<p>Each of these engineered feature sets represents a different aspect of the lexical nature of the documents. The relative frequency of lemmas represents the lexical diversity of the documents, the dispersion-weighted <span class="math inline">\(tf-idf\)</span> of lemmas represents the distinctiveness of the lemmas in the documents, and the relative frequency of part-of-speech tags represents the grammatical diversity of the documents <span class="citation" data-cites="Petrenz2011">(<a href="references.html#ref-Petrenz2011" role="doc-biblioref">Petrenz and Webber 2011</a>)</span>. //FIXME CITATIONS</p>
<p>The next question to address in any analysis is how to represent the features. In our case, we want to represent the features in each document. In machine learning, the most common way to represent features is in a matrix. In our case, we want to create a matrix with the documents in the rows and the features in the columns. The values in the matrix will be the operationalization of lexical use in each document for each of our three candidate measures. This configuration is known as a <strong>document-term matrix</strong> (DTM).</p>
<p>To recast a data frame into a DTM, we can use the <code><a href="https://rdrr.io/pkg/tidytext/man/document_term_casters.html">cast_dtm()</a></code> function from the <code>tidytext</code> package. This function takes a data frame with a document identifier, a feature identifier, and a value for each observation and casts it into a matrix. Operations such as normalization are easily and efficiently performed in R on matrices, so initially we can cast a frequency table of lemmas and part-of-speech tags into a matrix and then normalize the matrix by documents. For the <span class="math inline">\(tf-idf\)</span> measure we use the <code><a href="https://rdrr.io/pkg/tidytext/man/bind_tf_idf.html">bind_tf_idf()</a></code> function from the <code>tidytext</code> package. This function takes a DTM and calculates the <span class="math inline">\(tf-idf\)</span> measure for each feature in each document. This is a normalized measure, so we do not need to normalize the matrix by documents. Let’s see how this works with the MASC dataset in <a href="#exm-eda-masc-dtms">Example&nbsp;<span>8.26</span></a>.</p>
<div id="exm-eda-masc-dtms" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.26 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-dtms_199e6fc0efcaa9d2c92b473e2919ed9a">
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Load packages</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/juliasilge/tidytext">tidytext</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a document-term matrix of lemmas</span></span>
<span><span class="va">masc_lemma_dtm</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">count</span><span class="op">(</span><span class="va">doc_id</span>, <span class="va">lemma</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tidytext/man/document_term_casters.html">cast_dtm</a></span><span class="op">(</span><span class="va">doc_id</span>, <span class="va">lemma</span>, <span class="va">n</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a document-term matrix of part-of-speech tags</span></span>
<span><span class="va">masc_pos_dtm</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">count</span><span class="op">(</span><span class="va">doc_id</span>, <span class="va">pos</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tidytext/man/document_term_casters.html">cast_dtm</a></span><span class="op">(</span><span class="va">doc_id</span>, <span class="va">pos</span>, <span class="va">n</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a document-term matrix of tf-idf weighted lemmas</span></span>
<span><span class="va">masc_lemma_tfidf_dtm</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">count</span><span class="op">(</span><span class="va">doc_id</span>, <span class="va">lemma</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tidytext/man/bind_tf_idf.html">bind_tf_idf</a></span><span class="op">(</span><span class="va">doc_id</span>, <span class="va">lemma</span>, <span class="va">n</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tidytext/man/document_term_casters.html">cast_dtm</a></span><span class="op">(</span><span class="va">doc_id</span>, <span class="va">lemma</span>, <span class="va">tf_idf</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>Note preview the a subset of the contents of a matrix, such as in <a href="#exm-eda-masc-dtms">Example&nbsp;<span>8.26</span></a>, we use bracket syntax <code>[]</code> instead of the <code><a href="https://rdrr.io/r/utils/head.html">head()</a></code> function. Let’s take a look at the first 5 rows and 5 columns of the matrices, as seen in <a href="#exm-eda-masc-dtms-preview">Example&nbsp;<span>8.27</span></a>.</p>
<div id="exm-eda-masc-dtms-preview" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.27 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-dtms-preview_40777982056e6e452472847f8c933023">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Preview</span></span>
<span><span class="va">masc_lemma_dtm</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt;      Terms
&gt; Docs  'd 's M.  a account
&gt;   1    1  1  1 15       1
&gt;   10   0  0  0  7       0
&gt;   100  0  0  0  0       0
&gt;   101  0  0  0  2       0
&gt;   102  0  0  0  1       0</code></pre>
</div>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">masc_pos_dtm</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt;      Terms
&gt; Docs  CC DT EX IN JJ
&gt;   1   14 35  1 44 27
&gt;   10  11 38  0 39 18
&gt;   100  0  2  0  2  3
&gt;   101  3 16  0 23  7
&gt;   102 20 29  0 34 20</code></pre>
</div>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">masc_lemma_tfidf_dtm</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt;      Terms
&gt; Docs      'd      's    M.        a account
&gt;   1   0.0556 0.00324 0.228 0.006549  0.0414
&gt;   10  0.0000 0.00000 0.000 0.003320  0.0000
&gt;   100 0.0000 0.00000 0.000 0.000000  0.0000
&gt;   101 0.0000 0.00000 0.000 0.001064  0.0000
&gt;   102 0.0000 0.00000 0.000 0.000485  0.0000</code></pre>
</div>
</div>
</div>
<p>Now we can normalize the lemma and pos matrices by documents. We can do this by dividing each feature count by the total count in each document. This is a row-wise transformation, so we can use the <code><a href="https://rdrr.io/r/base/colSums.html">rowSums()</a></code> function from base R to calculate the total count in each document. Then each count divided by its row’s total count, as seen in <a href="#exm-eda-masc-dtms-normalized">Example&nbsp;<span>8.28</span></a>.</p>
<div id="exm-eda-masc-dtms-normalized" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.28 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-dtms-normalized_7bed91dfa690ac75ed3b8fca9d2b1e50">
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Normalize lemma and pos matrices by documents</span></span>
<span><span class="va">masc_lemma_dtm</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_lemma_dtm</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">rowSums</a></span><span class="op">(</span><span class="va">masc_lemma_dtm</span><span class="op">)</span></span>
<span></span>
<span><span class="va">masc_pos_dtm</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_pos_dtm</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">rowSums</a></span><span class="op">(</span><span class="va">masc_pos_dtm</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>There are two concerns to address before we can proceed with clustering. First, clustering algorithm performance tends to degrade with the number of features. If we consider either the relative frequency of lemmas or the dispersion-weighted <span class="math inline">\(tf-idf\)</span> of lemmas, we are looking at over 25k features! Second, clustering algorithms perform better with more informative features. That is to say, features that are more distinct across the documents provide better information for deriving useful clusters.</p>
<p>We can address both of these concerns by reducing the number of features and increasing the informativeness of the features. To accomplish this is to use <strong>dimensionality reduction</strong>. Dimensionality reduction is a set of methods that are used to reduce the number of features in a dataset while retaining as much information as possible. The most common method for dimensionality reduction is <strong>principle component analysis</strong> (PCA). PCA is a method that transforms a set of correlated variables into a set of uncorrelated variables, known as principle components. The principle components are ordered by the amount of variance that they explain in the data. The first principle component explains the most variance, the second principle component explains the second most variance, and so on.</p>
<p>We can apply PCA to each of these features and assess how well the features account for the variation in the data. We can then use the features that account for the most variation in the data for clustering. The <code><a href="https://rdrr.io/r/stats/prcomp.html">prcomp()</a></code> function from base R can be used to perform PCA. Let’s apply PCA to each of our candidate feature matrices, as seen in <a href="#exm-eda-masc-dtms-pca">Example&nbsp;<span>8.29</span></a>.</p>
<div id="exm-eda-masc-dtms-pca" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.29 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-dtms-pca_5d54e76d2fe640c9b922484c0abb9c75">
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Set seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Apply PCA to each feature matrix</span></span>
<span><span class="va">masc_lemma_pca</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_lemma_dtm</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">masc_pos_pca</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_pos_dtm</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">masc_lemma_tfidf_pca</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_lemma_tfidf_dtm</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>We can visualize the amount of variance explained by each principle component with a scree plot. The <code><a href="https://rdrr.io/pkg/factoextra/man/eigenvalue.html">fviz_eig()</a></code> function from the <code>factoextra</code> package can be used to create a scree plot. The <code><a href="https://rdrr.io/pkg/factoextra/man/eigenvalue.html">fviz_eig()</a></code> function takes the output of the <code><a href="https://rdrr.io/r/stats/prcomp.html">prcomp()</a></code> function as its argument and an argument <code>ncp = 10</code> to specify the number of principle components to include in the plot. Let’s create a scree plot for each of our candidate feature matrices, as seen in <a href="#fig-eda-masc-dtms-pca-scree">Figure&nbsp;<span>8.7</span></a>.</p>
<div>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Load package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sthda.com/english/rpkgs/factoextra">factoextra</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Scree plot: lemma relative frequency</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/eigenvalue.html">fviz_eig</a></span><span class="op">(</span><span class="va">masc_lemma_pca</span>, ncp <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Scree plot: lemma dispersion-weighted tf-idf</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/eigenvalue.html">fviz_eig</a></span><span class="op">(</span><span class="va">masc_lemma_tfidf_pca</span>, ncp <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Scree plot: part-of-speech relative frequency</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/eigenvalue.html">fviz_eig</a></span><span class="op">(</span><span class="va">masc_pos_pca</span>, ncp <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-eda-masc-dtms-pca-scree" class="cell quarto-layout-panel" data-hash="exploration_cache/html/fig-eda-masc-dtms-pca-scree_36baef5c635e4f2802afa13c482d0d7b">
<figure class="figure"><div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-eda-masc-dtms-pca-scree-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-dtms-pca-scree-1.png" class="img-fluid figure-img" data-ref-parent="fig-eda-masc-dtms-pca-scree" width="384"></p>
<figcaption class="figure-caption">(a) Lemma relative frequency</figcaption></figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-eda-masc-dtms-pca-scree-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-dtms-pca-scree-2.png" class="img-fluid figure-img" data-ref-parent="fig-eda-masc-dtms-pca-scree" width="384"></p>
<figcaption class="figure-caption">(b) Lemma dispersion-weighted tf-idf</figcaption></figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-eda-masc-dtms-pca-scree-3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-dtms-pca-scree-3.png" class="img-fluid figure-img" data-ref-parent="fig-eda-masc-dtms-pca-scree" width="384"></p>
<figcaption class="figure-caption">(c) Part-of-speech relative frequency</figcaption></figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;8.7: Scree plot of the principle components of the MASC dataset</figcaption><p></p>
</figure>
</div>
</div>
<p>From <a href="#fig-eda-masc-dtms-pca-scree">Figure&nbsp;<span>8.7</span></a>, we can see the plots are different. All trend toward less variance explained as the number of dimensions increase. But for our purposes, we are interested in the largest variance explained with the fewest dimensions. To that end, the <a href="#fig-eda-masc-dtms-pca-scree-3">Figure&nbsp;<span>8.7 (c)</span></a> plot is the most reduced and most informative. The amount of variance explained is over 30% for the first dimension alone, however, the variance explained decreases between 4 and 5 dimensions. This is a good indication that we should use 4 dimensions for our clustering algorithm.</p>
<p>To calculate the amount of variance explained by each principle component we can square the standard deviations of the principle components and divide by the sum of the squared standard deviations. Let’s calculate the amount of variance explained by each principle component for each of our candidate feature matrices, as seen in <a href="#exm-eda-masc-dtms-pca-variance">Example&nbsp;<span>8.30</span></a>.</p>
<div id="exm-eda-masc-dtms-pca-variance" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.30 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-dtms-pca-variance_341f0b7fc9b6a757c90e8baa7677c6da">
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Calculate variance explained for first 3 principle components</span></span>
<span><span class="co"># lemma</span></span>
<span><span class="va">masc_lemma_pca_var</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_lemma_pca</span><span class="op">$</span><span class="va">sdev</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">masc_lemma_pca</span><span class="op">$</span><span class="va">sdev</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="fl">100</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">masc_lemma_pca_var</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; [1] 24.3</code></pre>
</div>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># pos</span></span>
<span><span class="va">masc_pos_pca_var</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_pos_pca</span><span class="op">$</span><span class="va">sdev</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">masc_pos_pca</span><span class="op">$</span><span class="va">sdev</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="fl">100</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">masc_pos_pca_var</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; [1] 68.2</code></pre>
</div>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># lemma tf-idf</span></span>
<span><span class="va">masc_lemma_tfidf_pca_var</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_lemma_tfidf_pca</span><span class="op">$</span><span class="va">sdev</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">masc_lemma_tfidf_pca</span><span class="op">$</span><span class="va">sdev</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">*</span> <span class="fl">100</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">masc_lemma_tfidf_pca_var</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; [1] 7.76</code></pre>
</div>
</div>
</div>
<p>Combining the findings in the Scree plots and the variance explained calculations, we can see that the first four principle components of the part-of-speech features account for a good proportion of the variance. Therefore, all else being equal, we should use the part-of-speech features. As with all things exploratory, however, it is important to consider the implications of our feature selection for our interpretation of the results. In this case, the part-of-speech features approximate grammatical diversity of the documents, more so than lexical diversity. This means that the clusters that we identify will be based on a particular measure of grammatical diversity of the documents. If, for example, we want to identify clusters based on the lexical diversity of the documents, we may opt to use the lemma features, or some other operationalized measure of lexical diversity.</p>
<p>Before we leave PCA, let’s also take a look at the principle components themselves. The <code><a href="https://rdrr.io/pkg/factoextra/man/get_pca.html">get_pca_var()</a></code> function from the <code>factoextra</code> package can be used to extract the principle components from the output of the <code><a href="https://rdrr.io/r/stats/prcomp.html">prcomp()</a></code> function. The <code><a href="https://rdrr.io/pkg/factoextra/man/get_pca.html">get_pca_var()</a></code> function takes the output of the <code><a href="https://rdrr.io/r/stats/prcomp.html">prcomp()</a></code> function as its argument and an argument <code>ncp = 10</code> to specify the number of principle components to include in the plot. Let’s create a plot of the first five principle components for the part-of-speech data, as seen in <a href="#fig-eda-masc-dtms-pca-pc">Figure&nbsp;<span>8.8</span></a>.</p>
<div class="cell" data-hash="exploration_cache/html/fig-eda-masc-dtms-pca-pc_455b47a362352edb1507450fc116229f">
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Load package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sthda.com/english/rpkgs/factoextra">factoextra</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot principle components: pos</span></span>
<span><span class="va">masc_pos_pca</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_contrib.html">fviz_contrib</a></span><span class="op">(</span></span>
<span>    choice <span class="op">=</span> <span class="st">"var"</span>,</span>
<span>    axes <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">4</span>,</span>
<span>    top <span class="op">=</span> <span class="fl">20</span></span>
<span>  <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-eda-masc-dtms-pca-pc" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-dtms-pca-pc-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption">Figure&nbsp;8.8: Feature contributions to the PCA of the MASC dataset</figcaption></figure>
</div>
</div>
</div>
<p>From <a href="#fig-eda-masc-dtms-pca-pc">Figure&nbsp;<span>8.8</span></a>, we can see that the four principle components are dominated by the relative frequency of nouns, personal pronouns, prepositions, and determiners. This information can help us better understand the results of the clustering algorithm.</p>
<p>Now that we have identified the features that we want to use for clustering and we have represented the features in a way that can be used for clustering, we can apply a clustering algorithm to the data. For Hiearchical clustering, we can use the <code><a href="https://rdrr.io/r/stats/hclust.html">hclust()</a></code> function from base R. The <code><a href="https://rdrr.io/r/stats/hclust.html">hclust()</a></code> function takes a distance matrix as its argument and an argument <code>method = "average"</code> to specify the average linkage method. The average linkage method takes the average of the dissimilarities between all pairs in two clusters. It is less sensitive to outliers compared to other methods. Let’s apply the clustering algorithm to the part-of-speech features, as seen in <a href="#exm-eda-masc-pos-hclust">Example&nbsp;<span>8.31</span></a>.</p>
<div id="exm-eda-masc-pos-hclust" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.31 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/fig-eda-masc-pos-hclust_6cef311c9336c0d68c3d2b72cd61bb1a">
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Extract first 4 principle components</span></span>
<span><span class="va">masc_pos_pca_pc</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_pos_pca</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Create distance matrix</span></span>
<span><span class="va">masc_pos_dist</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_pos_pca_pc</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/dist.html">dist</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"manhattan"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Apply the clustering algorithm</span></span>
<span><span class="va">masc_pos_hc</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_pos_dist</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"average"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Visualize</span></span>
<span><span class="va">masc_pos_hc</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_dend.html">fviz_dend</a></span><span class="op">(</span>show_labels <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-eda-masc-pos-hclust" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-pos-hclust-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption">Figure&nbsp;8.9: Hierarchical clustering of the MASC dataset</figcaption></figure>
</div>
</div>
</div>
</div>
<p>Since we are exploring the usefulness of the 18 genre labels used in the MASC dataset we have a good idea of how many clusters we want to start with. This is a good case to employ the K-means clustering algorithm. In K-means clustering, we specify the number of clusters that we want to identify. For each cluster number, a random center is generated. Then each observation is assigned to the cluster with the nearest center. The center of each cluster is then recalculated based on the distribution of the observations in the cluster. This process is iterates either a pre-defined number of times, or until the centers converge (<em>i.e</em> observations stop switching clusters).</p>
<p>We can use the <code><a href="https://rdrr.io/r/stats/kmeans.html">kmeans()</a></code> function from base R to apply the K-means clustering algorithm. The <code><a href="https://rdrr.io/r/stats/kmeans.html">kmeans()</a></code> function takes the matrix of features as its first argument and the number of clusters as its second argument. We can specify the number of clusters with the <code>centers</code> argument. The <code><a href="https://rdrr.io/r/stats/kmeans.html">kmeans()</a></code> function also takes an argument <code>nstart</code> to specify the number of random starts. The K-means algorithm is sensitive to the initial starting points, so it is a good idea to run the algorithm multiple times with different starting points. The <code>nstart</code> argument specifies the number of random starts. The default value is 1, but we can increase this to 10 or 20 to increase the likelihood of finding a good solution.</p>
<p>Our goal, then, will be to assess how well this number of clusters fits the data. If it does not fit the data well, we can try a different number of clusters. We can then compare the results of the clustering with the genre labels to see how well the clusters map to the labels and make ajustments to the way we group the labels as necessary.</p>
<p>Let’s start with 18 clusters, assuming the target of the number of genres in the MASC dataset. We can apply the K-means clustering algorithm to the part-of-speech features, as seen in <a href="#exm-eda-masc-pos-kmeans">Example&nbsp;<span>8.32</span></a>.</p>
<div id="exm-eda-masc-pos-kmeans" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.32 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-pos-kmeans_81ea9ca08c8c2bc6239be4c0668355fa">
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Extract first 4 principle components</span></span>
<span><span class="va">masc_pos_pca_pc</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_pos_pca</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># K-means clustering</span></span>
<span><span class="va">masc_pos_kmeans</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_pos_pca_pc</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/kmeans.html">kmeans</a></span><span class="op">(</span></span>
<span>    centers <span class="op">=</span> <span class="fl">18</span>,</span>
<span>    nstart <span class="op">=</span> <span class="fl">25</span>,</span>
<span>    iter.max <span class="op">=</span> <span class="fl">20</span></span>
<span>  <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>The <code>factoextra</code> package provides <code><a href="https://rdrr.io/pkg/factoextra/man/fviz_cluster.html">fviz_cluster()</a></code> function for visualizing the results of clustering algorithms. The <code><a href="https://rdrr.io/pkg/factoextra/man/fviz_cluster.html">fviz_cluster()</a></code> function takes the output of the <code><a href="https://rdrr.io/r/stats/kmeans.html">kmeans()</a></code> function as its first argument and the matrix of features as its second argument. The <code><a href="https://rdrr.io/pkg/factoextra/man/fviz_cluster.html">fviz_cluster()</a></code> function can be used to visualize the clusters in the data, as seen in <a href="#fig-eda-masc-pos-kmeans">Figure&nbsp;<span>8.10</span></a>.</p>
<div class="cell" data-hash="exploration_cache/html/fig-eda-masc-pos-kmeans_a914270748e83b8b875087b5795ec810">
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Visualize</span></span>
<span><span class="va">masc_pos_kmeans</span> <span class="op">|&gt;</span>  <span class="co"># output of kmeans()</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_cluster.html">fviz_cluster</a></span><span class="op">(</span></span>
<span>    data <span class="op">=</span> <span class="va">masc_pos_pca_pc</span>, <span class="co"># matrix of features</span></span>
<span>    ellipse.type <span class="op">=</span> <span class="st">"norm"</span>,</span>
<span>    ellipse.level <span class="op">=</span> <span class="fl">0.95</span>,</span>
<span>    geom <span class="op">=</span> <span class="st">"point"</span>,</span>
<span>    pointsize <span class="op">=</span> <span class="fl">1</span>,</span>
<span>    palette <span class="op">=</span> <span class="st">"grey"</span>,</span>
<span>    ggtheme <span class="op">=</span> <span class="fu">theme_qtalr</span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-eda-masc-pos-kmeans" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-pos-kmeans-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption">Figure&nbsp;8.10: K-means clustering of the MASC dataset</figcaption></figure>
</div>
</div>
</div>
<p>The ellipses in a k-means plot represent the 95% confidence interval for each cluster. The ellipses are based on the multivariate normal distribution of the data in each cluster. The size and shape of the ellipses tell us about the variance of the data in each cluster. The larger the ellipse the greater the dispersion of values. A wide ellipse suggests high within cluster variability. The distance between the clusters also tells us about the similarity between the clusters. The closer the clusters are to each other, the more similar they are. The further the clusters are from each other, the more dissimilar they are.</p>
<p>So in <a href="#fig-eda-masc-pos-kmeans">Figure&nbsp;<span>8.10</span></a>, we see a mix of shapes and sizes. This suggests that some clusters are more homogeneous than others. We also see separation between the large wide clusters towards the top of the plot and the smaller, more circular clusters towards the center. With 18 clusters, we have a lot of clusters, so it is difficult to interpret the results as there is a large amount of overlap. In sum, 18 clusters is likely not an optimal number for this clustering approach.</p>
<p>We could run the code in <a href="#exm-eda-masc-pos-kmeans">Example&nbsp;<span>8.32</span></a> for different values for <span class="math inline">\(k\)</span> and plot each in turn. But a more effective way to determine the optimal number of clusters is to plot the within-cluster sum of squares (WSS) for a range of values for <span class="math inline">\(k\)</span>. The WSS is the sum of the squared distance between each observation and its cluster center. With a plot of the WSS for a range of values for <span class="math inline">\(k\)</span>, we can identify the value for <span class="math inline">\(k\)</span> where the WSS begins to level off. This is known as the <strong>elbow method</strong>. The elbow method is a heuristic, so it is not always clear where the elbow is. However, it is a good starting point for identifying the optimal number of clusters.</p>
<p>Again, the <code>factoextra</code> package has us covered. The <code><a href="https://rdrr.io/pkg/factoextra/man/fviz_nbclust.html">fviz_nbclust()</a></code> function can be used to plot the WSS for a range of values for <span class="math inline">\(k\)</span>. The <code><a href="https://rdrr.io/pkg/factoextra/man/fviz_nbclust.html">fviz_nbclust()</a></code> function takes the <code><a href="https://rdrr.io/r/stats/kmeans.html">kmeans()</a></code> function as its first argument and the matrix of features as its second argument. The <code><a href="https://rdrr.io/pkg/factoextra/man/fviz_nbclust.html">fviz_nbclust()</a></code> function also takes arguments <code>method = "wss"</code> to specify the WSS method and <code>k.max = 20</code> to specify the maximum number of clusters to plot. Let’s plot the WSS for a range of values for <span class="math inline">\(k\)</span>, as seen in <a href="#fig-eda-masc-pos-kmeans-elbow">Figure&nbsp;<span>8.11</span></a>.</p>
<div class="cell" data-hash="exploration_cache/html/fig-eda-masc-pos-kmeans-elbow_1bc145ff50f19de0aa4297539dbc3342">
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Determine the optimal number of clusters</span></span>
<span><span class="va">masc_pos_pca_pc</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_nbclust.html">fviz_nbclust</a></span><span class="op">(</span></span>
<span>    FUNcluster <span class="op">=</span> <span class="va">kmeans</span>,</span>
<span>    method <span class="op">=</span> <span class="st">"wss"</span>, <span class="co"># method</span></span>
<span>    k.max <span class="op">=</span> <span class="fl">20</span>,</span>
<span>    nstart <span class="op">=</span> <span class="fl">25</span>,</span>
<span>    iter.max <span class="op">=</span> <span class="fl">20</span></span>
<span>  <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-eda-masc-pos-kmeans-elbow" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-pos-kmeans-elbow-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption">Figure&nbsp;8.11: Elbow method for k-means clustering of the MASC dataset</figcaption></figure>
</div>
</div>
</div>
<p>It is clear that there is significant gains in cluster fit from 1 to 4 clusters, but the gains begin to level off after 5-7 clusters. Now we can skip ahead and try 4 clusters, as seen in <a href="#exm-eda-masc-pos-kmeans-fit">Example&nbsp;<span>8.33</span></a>.</p>
<div id="exm-eda-masc-pos-kmeans-fit" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.33 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/fig-eda-masc-pos-kmeans-fit_7db478b902feba431b12a55e25bd7151">
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Set seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># K-means: for 5 clusters</span></span>
<span><span class="va">masc_pos_kmeans_fit</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_pos_pca_pc</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/kmeans.html">kmeans</a></span><span class="op">(</span></span>
<span>    centers <span class="op">=</span> <span class="fl">4</span>,</span>
<span>    nstart <span class="op">=</span> <span class="fl">25</span>,</span>
<span>    iter.max <span class="op">=</span> <span class="fl">20</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Visualize</span></span>
<span><span class="va">masc_pos_kmeans_fit</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_cluster.html">fviz_cluster</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">masc_pos_pca_pc</span>,</span>
<span>  ellipse.type <span class="op">=</span> <span class="st">"norm"</span>,</span>
<span>  ellipse.level <span class="op">=</span> <span class="fl">0.95</span>,</span>
<span>  geom <span class="op">=</span> <span class="st">"point"</span>,</span>
<span>  pointsize <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  palette <span class="op">=</span> <span class="st">"grey"</span>,</span>
<span>  ggtheme <span class="op">=</span> <span class="fu">theme_qtalr</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-eda-masc-pos-kmeans-fit" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-pos-kmeans-fit-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption">Figure&nbsp;8.12: K-means clustering of the MASC dataset with 4 clusters</figcaption></figure>
</div>
</div>
</div>
</div>
<p>The results are much more interpretable with 4 clusters. We can see that the clusters are more homogeneous and more distinct from each other, in particular clusters 1 and 2, and are generally similar in shape and size. However, there is still some overlap between the clusters, in particular for clusters 3 and 4. We expect there to be noise as we have paired down the number of features from over 25k to 4, so this is a good working solution.</p>
<p>From this point we can join document-cluster pairings produced by the k-means algorithm with the original dataset. We can then explore the clusters in terms of the original features. We can also explore the clusters in terms of the original labels. Let’s join the cluster assignments to the original dataset, as seen in <a href="#exm-eda-masc-pos-kmeans-join">Example&nbsp;<span>8.34</span></a>.</p>
<div id="exm-eda-masc-pos-kmeans-join" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.34 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-pos-kmeans-join_4e08a4ad06f03ad373e2cd069f4fa385">
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Organize k-means clusters into a tibble</span></span>
<span><span class="va">masc_pos_cluster_tbl</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu">tibble</span><span class="op">(</span></span>
<span>    doc_id <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">masc_pos_kmeans_fit</span><span class="op">$</span><span class="va">cluster</span><span class="op">)</span>,</span>
<span>    cluster <span class="op">=</span> <span class="va">masc_pos_kmeans_fit</span><span class="op">$</span><span class="va">cluster</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Join cluster assignments to original dataset</span></span>
<span><span class="va">masc_cluster_tbl</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_tbl</span><span class="op">|&gt;</span></span>
<span>  <span class="fu">left_join</span><span class="op">(</span></span>
<span>    <span class="va">masc_pos_cluster_tbl</span>,</span>
<span>    by <span class="op">=</span> <span class="st">"doc_id"</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Preview</span></span>
<span><span class="va">masc_cluster_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; # A tibble: 5 × 8
&gt;   doc_id modality genre   term_num term         lemma        pos   cluster
&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;   &lt;int&gt;
&gt; 1 1      Written  Letters        2 Your         your         PRP$        1
&gt; 2 1      Written  Letters        3 contribution contribution NN          1
&gt; 3 1      Written  Letters        4 to           to           TO          1
&gt; 4 1      Written  Letters        6 will         will         MD          1
&gt; 5 1      Written  Letters        7 mean         mean         VB          1</code></pre>
</div>
</div>
</div>
<p>We now see that the cluster assignments from the k-means algorithm have been joined to the original dataset. We can now explore the clusters in terms of the original features. For example, let’s look at the distribution of the clusters across modality first, as seen in <a href="#exm-eda-masc-pos-kmeans-modality">Example&nbsp;<span>8.35</span></a>. To do this, we first need to reduce our dataset to the distinct combinations of modality, genre, and cluster. Then, we can use the <code>janitor</code> package’s <code><a href="https://sfirke.github.io/janitor/reference/tabyl.html">tabyl()</a></code> function to provided formatted percentages.</p>
<div id="exm-eda-masc-pos-kmeans-modality" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.35 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-pos-kmeans-modality_bbd6c231a241137e6164a9e17b6686c1">
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Load package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/sfirke/janitor">janitor</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Reduce to distinct combinations of modality, genre, and cluster</span></span>
<span><span class="va">masc_meta_tbl</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_cluster_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">distinct</span><span class="op">(</span><span class="va">modality</span>, <span class="va">genre</span>, <span class="va">cluster</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Tabulate: cluster by modality</span></span>
<span><span class="va">masc_meta_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://sfirke.github.io/janitor/reference/tabyl.html">tabyl</a></span><span class="op">(</span><span class="va">cluster</span>, <span class="va">modality</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://sfirke.github.io/janitor/reference/adorn_percentages.html">adorn_percentages</a></span><span class="op">(</span><span class="st">"row"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://sfirke.github.io/janitor/reference/adorn_pct_formatting.html">adorn_pct_formatting</a></span><span class="op">(</span>digits <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt;  cluster Spoken Written
&gt;        1  30.8%   69.2%
&gt;        2   0.0%  100.0%
&gt;        3   0.0%  100.0%
&gt;        4   8.3%   91.7%</code></pre>
</div>
</div>
</div>
<p>From <a href="#exm-eda-masc-pos-kmeans-modality">Example&nbsp;<span>8.35</span></a>, we can see that the clusters are not evenly distributed across the modalities. In particular, cluster 1 is where the great majority of the spoken modality appears. We can also appreciate that there may be some written genres that are more similar to spoken genres than other written genres. This may be something we could like to explore further.</p>
<p>Let’s dive into genres and limit our analysis to the written modality, as seen in <a href="#exm-eda-masc-pos-kmeans-genre">Example&nbsp;<span>8.36</span></a>. To do this, we first need to filter the dataset to the written modality. In addition to the row-wise percentages which capture the proportion of the genre that contributes to each cluster, let’s also consider the column-wise percentages to consider how each genre is distrributed across the clusters.</p>
<div id="exm-eda-masc-pos-kmeans-genre" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.36 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-pos-kmeans-genre_4059538108384f2db7750a475dd1d81c">
<div class="sourceCode" id="cb74"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Tabulate: cluster by genre for written modality</span></span>
<span><span class="va">masc_meta_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">modality</span> <span class="op">==</span> <span class="st">"Written"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://sfirke.github.io/janitor/reference/tabyl.html">tabyl</a></span><span class="op">(</span><span class="va">cluster</span>, <span class="va">genre</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://sfirke.github.io/janitor/reference/adorn_percentages.html">adorn_percentages</a></span><span class="op">(</span><span class="st">"row"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://sfirke.github.io/janitor/reference/adorn_pct_formatting.html">adorn_pct_formatting</a></span><span class="op">(</span>digits <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt;  cluster  Blog Email Essay Fiction Fictlets Government Jokes Journal Letters
&gt;        1 11.1% 11.1% 11.1%   11.1%    11.1%       0.0% 11.1%   11.1%   11.1%
&gt;        2  0.0% 20.0% 20.0%    0.0%     0.0%       0.0%  0.0%    0.0%    0.0%
&gt;        3 20.0% 20.0%  0.0%    0.0%     0.0%       0.0%  0.0%    0.0%   20.0%
&gt;        4  9.1%  9.1%  9.1%    9.1%     0.0%       9.1%  0.0%    9.1%    9.1%
&gt;  Newspaper Non-fiction Technical Travel Guide Twitter
&gt;      11.1%        0.0%      0.0%         0.0%    0.0%
&gt;      20.0%       20.0%     20.0%         0.0%    0.0%
&gt;      20.0%        0.0%      0.0%         0.0%   20.0%
&gt;       9.1%        9.1%      9.1%         9.1%    0.0%</code></pre>
</div>
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Tabulate: cluster by genre for written modality</span></span>
<span><span class="va">masc_meta_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">modality</span> <span class="op">==</span> <span class="st">"Written"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://sfirke.github.io/janitor/reference/tabyl.html">tabyl</a></span><span class="op">(</span><span class="va">cluster</span>, <span class="va">genre</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://sfirke.github.io/janitor/reference/adorn_percentages.html">adorn_percentages</a></span><span class="op">(</span><span class="st">"col"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://sfirke.github.io/janitor/reference/adorn_pct_formatting.html">adorn_pct_formatting</a></span><span class="op">(</span>digits <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt;  cluster  Blog Email Essay Fiction Fictlets Government  Jokes Journal Letters
&gt;        1 33.3% 25.0% 33.3%   50.0%   100.0%       0.0% 100.0%   50.0%   33.3%
&gt;        2  0.0% 25.0% 33.3%    0.0%     0.0%       0.0%   0.0%    0.0%    0.0%
&gt;        3 33.3% 25.0%  0.0%    0.0%     0.0%       0.0%   0.0%    0.0%   33.3%
&gt;        4 33.3% 25.0% 33.3%   50.0%     0.0%     100.0%   0.0%   50.0%   33.3%
&gt;  Newspaper Non-fiction Technical Travel Guide Twitter
&gt;      25.0%        0.0%      0.0%         0.0%    0.0%
&gt;      25.0%       50.0%     50.0%         0.0%    0.0%
&gt;      25.0%        0.0%      0.0%         0.0%  100.0%
&gt;      25.0%       50.0%     50.0%       100.0%    0.0%</code></pre>
</div>
</div>
</div>
<p>On the other hand, looking at the written genres, we see that there are some genres which are grouped entirely in cluster 1. In other words, these genres, such as ‘Fictlets’ and ‘Jokes’, align with spoken genres in terms of their grammatical diversity. This is an interesting finding that we may want to explore further. Furthermore, it may be of interest to explore individual documents in genres which have a signifcant proportion of documents in cluster 1. There are too many possibilities to explore here but this is a good example of how exploratory data analysis can be used to identify new questions and new variables of interest.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-regular fa-lightbulb" aria-label="lightbulb"></i> Consider this</strong></p>
<p>Given the cluster assignments derived using the distribution of part-of-speech tags, what other relationships between the clusters and the original features could one explore? What are the limitations of this approach? What are the implications of this approach for the interpretation of the results?</p>
</div>
</div>
</div>
</section><section id="sec-eda-vector-space-models" class="level4"><h4 class="anchored" data-anchor-id="sec-eda-vector-space-models">Vector space models</h4>
<p>In our discussion of clustering, we targeted associations between documents based on the distribution of linguistic features. We now turn to targeting associations between linguistic features based on their distribution across documents. The technique we will introduce is known as <strong>vector space modeling</strong>. Vector space modeling aims to represent linguistic features as numerical vectors which reflect the various linguistic contexts in which the features appear. Together these vectors form a feature-context space in which features with similar contextual distributions are closer together.</p>
<p>An interesting property of vector space models is that are able to capture semantic and/ or syntactic relationships between features based on their distribution. In this way, vector space modeling can be seen as an implementation of the <strong>distributional hypothesis</strong> –that words that appear in similar linguistic contexts tend to have similar meanings <span class="citation" data-cites="Harris1954">(<a href="references.html#ref-Harris1954" role="doc-biblioref">Harris 1954</a>)</span>. As <span class="citation" data-cites="Firth1957">Firth (<a href="references.html#ref-Firth1957" role="doc-biblioref">1957</a>)</span> states “you shall know a word by the company it keeps”.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-regular fa-file-alt" aria-label="file-alt"></i> Case study</strong></p>
<p><span class="citation" data-cites="Garg2018">Garg et al. (<a href="references.html#ref-Garg2018" role="doc-biblioref">2018</a>)</span> quantify and compare gender and ethnic stereotypes over time using word embeddings. The authors explore the temporal dynamics of stereotypes using word embeddings as a quantitative measure of bias. The data used includes word embeddings from the Google News dataset for contemporary analysis, as well as embeddings from the COHA and Google Books datasets for historical analysis. Additional validation is done using embeddings from the New York Times Annotated Corpus. Several word lists representing gender, ethnicity, and neutral words are collated for analysis. The main finding is that language reflects and perpetuates cultural stereotypes, and the analysis shows consistency in the relationships between embedding bias and external metrics across datasets over time. The results also highlight the impact of historical events, such as the women’s movement of the 1960s, on the encoding of stereotypes.</p>
</div>
</div>
</div>
<p>Let’s assume in our textbook project we are interested in gathering information about English’s expression of the semantic concepts of manner and motion. For learners of English, this can be an area of difficulty as languages differ in how these semantic properties are expressed. English is a good example of a “satellite-framed” language, that is that manner and motion are often encoded in the same verb with a particle encoding the motion path (“rush out”, “climb up”). Other languages such as Spanish, Turkish, and Japanese are “verb-framed” languages, that is that motion but not manner is encoded in the verb (“salir corriendo”, “koşarak çıkmak”, “走り出す”).</p>
<p>We can use vector space modeling to represent the distribution of verbs in the MASC dataset and then target the concepts of manner and motion to then explore how English encodes these concepts. The question will be what will our features be. They could be terms, lemmas, pos tags, etc. Or they could be some combination. Considering the task at hand which we will ultimately want to know something about verbs, it makes sense to include the part of speech information in combination with either the term or the lemma.</p>
<p>If we include term and pos then we have a feature for every morphological variant of the term (e.g.&nbsp;house_VB, housed_VBD, housing_VBG). This can make the model more sizeable than it needs to be. If we include lemma and pos then we have a feature for every lemma with a distinct grammatical category (e.g.&nbsp;house_NN, house_VB). Note that as the pos tags are from the Penn tagset, many morphological variants appear in the tag itself (e.g.&nbsp;house_VB, houses_VBZ, housing_VBG). This is a good example of how the choice of features can impact the size of the model. In our case, it is not clear that we need to include the morphological variants of the verbs, so we will use lemma and a simplified pos as our features.</p>
<p>To engineer these features we will need to simplify the tags. We will conflate Penn tagset distinctions between nouns, verbs, adjectives, and adverbs. This will give us a feature for every lemma with a distinct grammatical category (e.g.&nbsp;house_NOUN, house_VERB), and no more than that. To do this we can apply the <code>case_when()</code> function from the <code>dplyr</code> package. The <code>case_when()</code> function takes a series of logical statements and returns a value based on the first logical statement that is <code>TRUE</code>. Let’s conflate the Penn tagset distinctions between nouns, verbs, adjectives, and adverbs, as seen in <a href="#exm-eda-masc-vsm-lemma-pos">Example&nbsp;<span>8.37</span></a>.</p>
<!--
- Nouns: `NN` (singular common noun), `NNS` (plural common noun), `NNP` (singular proper noun), `NNPS` (plural proper noun)
- Verbs: `VB` (base form), `VBD` (past tense), `VBG` (gerund), `VBN` (past participle), `VBP` (non-3rd person singular present), `VBZ` (3rd person singular present)
- Adjectives: `JJ` (adjective), `JJR` (comparative adjective), `JJS` (superlative adjective)
- Adverbs: `RB` (adverb), `RBR` (comparative adverb), `RBS` (superlative adverb)
-->
<div id="exm-eda-masc-vsm-lemma-pos" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.37 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-vsm-lemma-pos_ce6eb209206d64e0d1efaaee18fd61f7">
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Conflate Penn tagset into Universal-like tagset</span></span>
<span><span class="va">masc_tbl</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>xpos <span class="op">=</span> <span class="fu">case_when</span><span class="op">(</span></span>
<span>    <span class="va">pos</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"NN"</span>, <span class="st">"NNS"</span>, <span class="st">"NNP"</span>, <span class="st">"NNPS"</span><span class="op">)</span> <span class="op">~</span> <span class="st">"NOUN"</span>,</span>
<span>    <span class="va">pos</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"VB"</span>, <span class="st">"VBD"</span>, <span class="st">"VBG"</span>, <span class="st">"VBN"</span>, <span class="st">"VBP"</span>, <span class="st">"VBZ"</span><span class="op">)</span> <span class="op">~</span> <span class="st">"VERB"</span>,</span>
<span>    <span class="va">pos</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"JJ"</span>, <span class="st">"JJR"</span>, <span class="st">"JJS"</span><span class="op">)</span> <span class="op">~</span> <span class="st">"ADJ"</span>,</span>
<span>    <span class="va">pos</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"RB"</span>, <span class="st">"RBR"</span>, <span class="st">"RBS"</span><span class="op">)</span> <span class="op">~</span> <span class="st">"ADV"</span>,</span>
<span>    <span class="cn">TRUE</span> <span class="op">~</span> <span class="va">pos</span> <span class="co"># keep other tags</span></span>
<span>  <span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Lemma + pos</span></span>
<span><span class="va">masc_tbl</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>lemma_pos <span class="op">=</span> <span class="fu">str_c</span><span class="op">(</span><span class="va">lemma</span>, <span class="va">xpos</span>, sep <span class="op">=</span> <span class="st">"_"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Preview</span></span>
<span><span class="va">masc_tbl</span> <span class="op">|&gt;</span> <span class="fu">glimpse</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; Rows: 439,539
&gt; Columns: 9
&gt; $ doc_id    &lt;chr&gt; "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", …
&gt; $ modality  &lt;chr&gt; "Written", "Written", "Written", "Written", "Written", "Writ…
&gt; $ genre     &lt;chr&gt; "Letters", "Letters", "Letters", "Letters", "Letters", "Lett…
&gt; $ term_num  &lt;dbl&gt; 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20,…
&gt; $ term      &lt;chr&gt; "Your", "contribution", "to", "will", "mean", "more", "than"…
&gt; $ lemma     &lt;chr&gt; "your", "contribution", "to", "will", "mean", "more", "than"…
&gt; $ pos       &lt;chr&gt; "PRP$", "NN", "TO", "MD", "VB", "JJR", "IN", "PRP", "MD", "V…
&gt; $ xpos      &lt;chr&gt; "PRP$", "NOUN", "TO", "MD", "VERB", "ADJ", "IN", "PRP", "MD"…
&gt; $ lemma_pos &lt;chr&gt; "your_PRP$", "contribution_NOUN", "to_TO", "will_MD", "mean_…</code></pre>
</div>
</div>
</div>
<p>When VSM is applied to words, it is known as <strong>word embedding</strong>. To calculate word embeddings there are various algorithms that can be used (BERT, word2vec, GloVe, <em>etc.</em>) The most common algorithm is <strong>word2vec</strong> <span class="citation" data-cites="Mikolov2013b">(<a href="references.html#ref-Mikolov2013b" role="doc-biblioref">Mikolov et al. 2013</a>)</span>. Word2vec is a neural network-based algorithm that learns word embeddings from a large corpus of text. In the word2vec algorithm the researcher can choose to learn embeddings from a Continuous Bag of Words (CBOW) or a Skip-gram model. The CBOW model predicts a target word based on the context words. The Skip-gram model predicts the context words based on the target word. The CBOW model is faster to train and is better for frequent words. The Skip-gram model is slower to train and is better for infrequent words.</p>
<p>Another consideration to take into account is the size of the corpus used to train the model. VSM provide more reliable results when trained on larger corpora. The MASC dataset is relatively small. We’ve simplified our features in order to have a smaller vocabulary in hopes to offset this limitation to a degree. But the choice of either CBOW or Skip-gram can also help to offset this limitation. CBOW can be better for smaller corpora as it aggregates context infomation.</p>
<p>To implement the word2vec algorithm on our lemma + pos features, we will use the <code>word2vec</code> package. The <code><a href="https://rdrr.io/pkg/word2vec/man/word2vec.html">word2vec()</a></code> function takes a text file and uses it to train the vector representations. To prepare the MASC dataset for training, we will need to write the lemma + pos features to a text file as a single character string. We can do this by first collapsing the <code>lemma_pos</code> variable into a single string for each document using the <code>str_c()</code> function from the <code>stringr</code> package. Then we can use the <code><a href="https://rdrr.io/r/base/writeLines.html">writeLines()</a></code> function to write the string to a text file. Let’s prepare the MASC dataset for training, as seen in <a href="#exm-eda-masc-vsm-word2vec-text">Example&nbsp;<span>8.38</span></a>.</p>
<div id="exm-eda-masc-vsm-word2vec-text" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.38 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-vsm-word2vec-text-show_776cc8cf09b49969e0798157c78bccf0">
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Prepare data for word2vec training</span></span>
<span><span class="va">masc_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">summarize</span><span class="op">(</span>text <span class="op">=</span> <span class="fu">str_c</span><span class="op">(</span><span class="va">lemma_pos</span>, collapse <span class="op">=</span> <span class="st">" "</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">pull</span><span class="op">(</span><span class="va">text</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/writeLines.html">writeLines</a></span><span class="op">(</span></span>
<span>    con <span class="op">=</span> <span class="st">"../data/derived/masc_word2vec.txt"</span></span>
<span>  <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>With our <em>masc_word2vect.txt</em> file, we read in. apply the word2vec algorithm using the <code>word2vec</code> package, and write the model to disk. By default, te <code><a href="https://rdrr.io/pkg/word2vec/man/word2vec.html">word2vec()</a></code> function applies the CBOW model, with 50 dimensions, a window size of 5, and a minimum word count of 5. We can change these parameters as needed, but let’s apply the default algorithm to the text file spliting features by sentence punctuation, as seen in <a href="#exm-eda-masc-vsm-word2vec-train">Example&nbsp;<span>8.39</span></a>.</p>
<div id="exm-eda-masc-vsm-word2vec-train" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.39 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-vsm-word2vec-train-show_3b1084f3653f3e0f1e51d70f5b7c401f">
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Load package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/bnosac/word2vec">word2vec</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Traing word2vec model</span></span>
<span><span class="va">masc_model</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/word2vec/man/word2vec.html">word2vec</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="st">"../data/derived/masc_word2vec.txt"</span>,</span>
<span>    split <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">" "</span>, <span class="st">".?!"</span><span class="op">)</span>,</span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Write model to disk</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/word2vec/man/write.word2vec.html">write.word2vec</a></span><span class="op">(</span></span>
<span>  <span class="va">masc_model</span>,</span>
<span>  file <span class="op">=</span> <span class="st">"../data/derived/masc_word2vec.bin"</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="exploration_cache/html/eda-masc-vsm-word2vec-train-run_0bd2d3b98f82d5d9796bd2617d8b08e5">
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Set seed for reproducibility</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Load package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/bnosac/word2vec">word2vec</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Traing word2vec model</span></span>
<span><span class="va">masc_model</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/word2vec/man/word2vec.html">word2vec</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="st">"data/masc_word2vec.txt"</span>,</span>
<span>    split <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">" "</span>, <span class="st">".?!"</span><span class="op">)</span>,</span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Write model to disk</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/word2vec/man/write.word2vec.html">write.word2vec</a></span><span class="op">(</span></span>
<span>  <span class="va">masc_model</span>,</span>
<span>  file <span class="op">=</span> <span class="st">"data/masc_word2vec.bin"</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>Writing the model to disk is important as it allows us to read the model in without having to retrain it. In cases where the corpus is large, this can save a lot of computational time.</p>
<p>Now that we have a trained model, we can read it in with the <code><a href="https://rdrr.io/pkg/wordVectors/man/read.vectors.html">read.vectors()</a></code> function from the <code>wordVectors</code> package.</p>
<div id="exm-eda-masc-vsm-word2vec-read" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.40 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-vsm-word2vec-read-show_d128914dd5268470cd78db9347533b05">
<div class="sourceCode" id="cb83"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Load package</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://github.com/bmschmidt/wordVectors">wordVectors</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Read word2vec model</span></span>
<span><span class="va">masc_model</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/wordVectors/man/read.vectors.html">read.vectors</a></span><span class="op">(</span></span>
<span>    filename <span class="op">=</span> <span class="st">"../data/derived/masc_word2vec.bin"</span></span>
<span>  <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>The <code><a href="https://rdrr.io/pkg/wordVectors/man/read.vectors.html">read.vectors()</a></code> function returns a matrix where each row is a term in the model and each column is a dimension in the vector space, as seen in <a href="#exm-eda-masc-vsm-word2vec-vector-object">Example&nbsp;<span>8.41</span></a>.</p>
<div id="exm-eda-masc-vsm-word2vec-vector-object" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.41 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-vsm-word2vec-vector-object_87cc7e9c8968e0822307f4ae83e3210e">
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Inspect</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">masc_model</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; [1] 5892   50</code></pre>
</div>
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Preview</span></span>
<span><span class="va">masc_model</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; A VectorSpaceModel object of  5  words and  5  vectors
&gt;                     [,1]  [,2]   [,3]   [,4]    [,5]
&gt; map_VERB           0.476 0.590  0.572 -0.547 -0.0146
&gt; KGCUBS10_NOUN     -0.304 1.506 -0.407  1.518  0.1857
&gt; MICKEY3_NOUN      -0.259 0.854 -0.925  0.759  0.1816
&gt; amenity_NOUN       1.284 0.502 -0.921  0.488 -1.1896
&gt; transmission_NOUN -1.580 1.493 -0.205  1.457 -1.7360
&gt; attr(,".cache")
&gt; &lt;environment: 0x7fdc08066940&gt;</code></pre>
</div>
</div>
</div>
<p>The row-wise vector in the model is the vector representation of each feature. The notion is that these values can now be compared with other terms to explore distributional relatedness. We can extract specific features from the matrix using the <code>[]</code> operator.</p>
<p>As an example, let’s compare the vectors for noun-verb pairs for the lemmas ‘run’ and ‘walk’. To do this we extract these features from the model. To appreciate the relatedness of these features it is best to visualize them. We can do this by first reducing the dimensionality of the vectors using principal components analysis (PCA). We can then plot the first two principle components, as seen in <a href="#fig-eda-masc-vsm-word2vec-similarity">Figure&nbsp;<span>8.13</span></a>.</p>
<div id="exm-eda-masc-vsm-word2vec-similarity" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.42 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/fig-eda-masc-vsm-word2vec-similarity_584c453bc0a3ca7df0188e16d772b7fa">
<div class="sourceCode" id="cb88"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Extract vectors</span></span>
<span><span class="va">word_vectors</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">masc_model</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"run_VERB"</span>, <span class="st">"walk_VERB"</span>, <span class="st">"run_NOUN"</span>, <span class="st">"walk_NOUN"</span><span class="op">)</span>, <span class="op">]</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">pca</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">word_vectors</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/scale.html">scale</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">pca_tbl</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu">as_tibble</span><span class="op">(</span><span class="va">pca</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>word <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">word_vectors</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">pca_tbl</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">PC1</span>, y <span class="op">=</span> <span class="va">PC2</span>, label <span class="op">=</span> <span class="va">word</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggrepel</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/ggrepel/man/geom_text_repel.html">geom_text_repel</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-eda-masc-vsm-word2vec-similarity" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-vsm-word2vec-similarity-1.png" class="img-fluid figure-img" width="384"></p>
<figcaption class="figure-caption">Figure&nbsp;8.13: Similarity between ‘run’ and ‘walk’ in the MASC dataset</figcaption></figure>
</div>
</div>
</div>
</div>
<!--

[ ] If PC1 and PC2 are orthogonal and appear to be picking up part of speech vs. meaning, should the other values select pos-ness or meaning-ness?

-->
<p>From <a href="#fig-eda-masc-vsm-word2vec-similarity">Figure&nbsp;<span>8.13</span></a>, we can see that each of these features occupies a distinct position in the reduced vector space. But on closer inspection, we can see that there is a relationship between the lemma pairs. Remember that PCA reduces the dimensionality of the data by identifying the dimensions that capture the greatest amount of variance in the data. This means that of the 50 dimensions in the model, the PC1 and PC2 correspond to orthogonal dimensions that capture the greatest amount of variance in the data. If we look along PC1, we can see that there is a distinction between part-of-speech. Looking along PC2, we see some pariety between lemma meanings. Given these features, we can see that meaning and grammatical category can be captured in the vector space.</p>
<p>An interesting property of vector space models is that we can build up a dimension of meaning by adding vectors that are expect to approximate that meaning. For example, we can add the vectors for typical motion verbs to create a vector for motion-similarity and one for manner-similarity. We can then compare the feature vectors for all verbs and assess their motion-similarity and manner-similarity.</p>
<p>To do this let’s first subset the model to only include verbs, as in <a href="#exm-eda-masc-vsm-word2vec-verbs">Example&nbsp;<span>8.43</span></a>. We will also remove the part-of-speech tags from the rownames of the matrix as they are no longer needed.</p>
<div id="exm-eda-masc-vsm-word2vec-verbs" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.43 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-vsm-word2vec-verbs_42275fa1ca7abab8424f25daab4d1a6a">
<div class="sourceCode" id="cb89"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Filter to verbs</span></span>
<span><span class="va">verbs</span> <span class="op">&lt;-</span> <span class="fu">str_subset</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">masc_model</span><span class="op">)</span>, <span class="st">".*_VERB"</span><span class="op">)</span></span>
<span><span class="va">verb_vectors</span> <span class="op">&lt;-</span> <span class="va">masc_model</span><span class="op">[</span><span class="va">verbs</span>, <span class="op">]</span></span>
<span></span>
<span><span class="co"># Remove part-of-speech tags</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">verb_vectors</span><span class="op">)</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">verb_vectors</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">str_replace_all</span><span class="op">(</span><span class="st">"_VERB"</span>, <span class="st">""</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Inspect</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">verb_vectors</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; [1] 1115   50</code></pre>
</div>
<div class="sourceCode" id="cb91"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Preview</span></span>
<span><span class="va">verb_vectors</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; A VectorSpaceModel object of  5  words and  5  vectors
&gt;             [,1]  [,2]   [,3]    [,4]   [,5]
&gt; map      -1.4875 1.510 -0.662 -1.2129 -0.646
&gt; whip      0.0496 0.919 -1.020  0.3087  0.350
&gt; enroll   -0.6251 1.620 -1.878  0.0861 -0.428
&gt; tuck      0.5610 1.054 -1.331 -0.5298  1.598
&gt; suppress  0.3824 1.928 -1.750 -0.3229 -0.643
&gt; attr(,".cache")
&gt; &lt;environment: 0x7fa1f92fcf00&gt;</code></pre>
</div>
</div>
</div>
<p>We now have <code>verb_vectors</code> which includes the vector representations for all verbs 1115 in the MASC dataset. Next, let’s seed the vectors for motion-similarity and manner-similarity and calculate the vector ‘closeness’ to the motion and manner seed vectors with the <code><a href="https://rdrr.io/pkg/wordVectors/man/closest_to.html">closest_to()</a></code> function from the <code>wordVectors()</code> package.</p>
<div id="exm-eda-masc-vsm-word2vec-manner-motion" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.44 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/fig-eda-masc-vsm-word2vec-add_0dcdf527b384e046231db28dacbfba7b">
<div class="sourceCode" id="cb93"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Add vectors for motion-similarity and manner-similarity</span></span>
<span><span class="va">motion</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"go"</span>, <span class="st">"come"</span>, <span class="st">"leave"</span>, <span class="st">"arrive"</span>, <span class="st">"enter"</span>, <span class="st">"exit"</span>, <span class="st">"depart"</span>, <span class="st">"return"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">motion_similarity</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">verb_vectors</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/pkg/wordVectors/man/closest_to.html">closest_to</a></span><span class="op">(</span><span class="va">motion</span>, n <span class="op">=</span> <span class="cn">Inf</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Preview</span></span>
<span><span class="va">motion_similarity</span> <span class="op">|&gt;</span> <span class="fu">glimpse</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; Rows: 1,115
&gt; Columns: 2
&gt; $ word                   &lt;chr&gt; "sever", "hang", "nod", "splash", "return", "sl…
&gt; $ `similarity to motion` &lt;dbl&gt; 0.781, 0.776, 0.769, 0.760, 0.755, 0.755, 0.745…</code></pre>
</div>
<div class="sourceCode" id="cb95"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">manner</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"run"</span>, <span class="st">"walk"</span>, <span class="st">"jump"</span>, <span class="st">"crawl"</span>, <span class="st">"swim"</span>, <span class="st">"fly"</span>, <span class="st">"drive"</span>, <span class="st">"ride"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">manner_similarity</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">verb_vectors</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/pkg/wordVectors/man/closest_to.html">closest_to</a></span><span class="op">(</span><span class="va">manner</span>, n <span class="op">=</span> <span class="cn">Inf</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Preview</span></span>
<span><span class="va">manner_similarity</span> <span class="op">|&gt;</span> <span class="fu">glimpse</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; Rows: 1,115
&gt; Columns: 2
&gt; $ word                   &lt;chr&gt; "crawl", "drop", "step", "hang", "walk", "throw…
&gt; $ `similarity to manner` &lt;dbl&gt; 0.894, 0.880, 0.867, 0.861, 0.858, 0.857, 0.850…</code></pre>
</div>
</div>
</div>
<p>The <code>motion_similarity</code> and <code>motion_similarity</code> data frames each contain all the verbs with a corresponding closeness measure. We can join these two data frames by feature to create a single data frame with the motion-similarity and manner-similarity measures, as seen in <a href="#exm-eda-masc-vsm-word2vec-manner-motion">Example&nbsp;<span>8.45</span></a>.</p>
<div id="exm-eda-masc-vsm-word2vec-manner-motion" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.45 </strong></span>&nbsp;</p>
<div class="cell" data-hash="exploration_cache/html/eda-masc-vsm-word2vec-manner-motion_abc6d11f2779903f3b4b0b061d9f92b8">
<div class="sourceCode" id="cb97"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Join motion-similarity and manner-similarity</span></span>
<span><span class="va">manner_motion_similarity</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">manner_similarity</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">inner_join</span><span class="op">(</span><span class="va">motion_similarity</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Preview</span></span>
<span><span class="va">manner_motion_similarity</span> <span class="op">|&gt;</span> <span class="fu">glimpse</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&gt; Rows: 1,115
&gt; Columns: 3
&gt; $ word                   &lt;chr&gt; "crawl", "drop", "step", "hang", "walk", "throw…
&gt; $ `similarity to manner` &lt;dbl&gt; 0.894, 0.880, 0.867, 0.861, 0.858, 0.857, 0.850…
&gt; $ `similarity to motion` &lt;dbl&gt; 0.742, 0.743, 0.692, 0.776, 0.695, 0.627, 0.702…</code></pre>
</div>
</div>
</div>
<p>The result of <a href="#exm-eda-masc-vsm-word2vec-manner-motion">Example&nbsp;<span>8.45</span></a> is a data frame with the motion-similarity and manner-similarity measures for all verbs in the MASC dataset. We can now visualize the distribution of motion-similarity and manner-similarity measures, as seen in <a href="#fig-eda-masc-vsm-word2vec-manner-motion-compare">Figure&nbsp;<span>8.14</span></a>.</p>
<div class="cell" data-hash="exploration_cache/html/fig-eda-masc-vsm-word2vec-manner-motion-compare_3dc07d68e628e35bd581423441985aa5">
<div class="cell-output-display">
<div id="fig-eda-masc-vsm-word2vec-manner-motion-compare" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="exploration_files/figure-html/fig-eda-masc-vsm-word2vec-manner-motion-compare-1.png" class="img-fluid figure-img" width="768"></p>
<figcaption class="figure-caption">Figure&nbsp;8.14: Motion-similarity and manner-similarity of verbs in the MASC dataset</figcaption></figure>
</div>
</div>
</div>
<p>From <a href="#fig-eda-masc-vsm-word2vec-manner-motion-compare">Figure&nbsp;<span>8.14</span></a>, we can see that the manner-similarity is plotted on the x-axis and the motion-similarity on the y-axis. I’ve added horizontal and vertical lines to break the scatterplot into quadrants –the top-right corresponding to high manner- and motion-similiarity and the bottom-left corresponding to low manner- and motion-similarity. This captures the majority of the verbs in the dataset. The verbs in the top-left quadrant have high motion-similarity but lower manner similarity, and verbs in the bottom-right quadrant have high manner-similarity but lower motion-similarity.</p>
<p>I’ve randomly sampled 50 verbs from the dataset and plotted them as text labels. I’ve also plotted the motion and manner seed vectors as triangle and box points, respectively. We can see that motion- and manner-similiarity seed verbs are found in the top-left quandrant together, showing that they are semantically related. Verbs in the other quadrants are either lower in motion- or manner-similarity, or both. From a qualitative point of view it appears that many of the verbs coincide with inuition. Some, however, less so. This is to be expected to some degree as the model is trained on a relatively small corpus. All in all, this serves as an example of how vector space modeling can be used to explore semantic relationships between linguistic features.</p>
</section></section></section><section id="summary" class="level2" data-number="8.3"><h2 data-number="8.3" class="anchored" data-anchor-id="summary">
<span class="header-section-number">8.3</span> Summary</h2>
<p>In this chapter we surveyed a range of methods for uncovering insights from data, particularly when we do not have a predetermined hypothesis. We broke the chapter discussion along the two central branches of exploratory data analysis: descriptive analysis and unsupervised learning. Descriptive analysis offers statistical or visual summaries of datasets through frequency, dispersion, and co-occurrence measures, while unsupervised learning utilizes machine learning techniques to uncover patterns without predefining variable relationships. Here we covered a few unsupervised learning methods including clustering, diminensionality reduction, and vector space modeling. Through either descriptive or unsupervised learning methodologies, we probe questions in a data-driven fashion and apply methods to summarize, reduce, and sort complex datasets. This in turn facilitates novel, quantitative perspectives that can subsequently be evaluated qualitatively, offering us a robust approach to exploring and generating research questions.</p>
</section><section id="activities" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="activities">Activities</h2>
<!-- [ ] Add description of the activites -->
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-regular fa-file-code" aria-label="file-code"></i> Recipe</strong></p>
<p><strong>What</strong>: <a href="https://qtalr.github.io/qtalrkit/articles/recipe-8.html">Exploratory methods: descriptive and unsupervised learning analysis methods</a><br><strong>How</strong>: Read Recipe 8 and participate in the Hypothes.is online social annotation.<br><strong>Why</strong>: To illustrate how to prepare a dataset for descriptive and unsupervised machine learning methods and evaluate the results for exploratory data analysis.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-solid fa-flask" aria-label="flask"></i> Lab</strong></p>
<!-- Analyze, evaluate, and create verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->
<!-- [ ] update lab -->
<p><strong>What</strong>: <a href="https://github.com/qtalr/lab-08">Exploratory Data Analysis</a><br><strong>How</strong>: Clone, fork, and complete the steps in Lab 8.<br><strong>Why</strong>: To gain experience working with coding strategies to prepare, feature engineer, explore, and evaluate results from exploratory data analyses, practice transforming datasets into new object formats and visualizing relationships, and implement organizational strategies for organizing and reporting results in a reproducible fashion.</p>
</div>
</div>
</div>
</section><section id="questions" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="questions">Questions</h2>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><i class="fa-solid fa-wrench" aria-label="wrench"></i> <strong>Conceptual questions</strong></p>
<ol type="1">
<li>What is exploratory data analysis?</li>
<li>How can exploratory data analysis be used to uncover patterns and associations?</li>
<li>Describe the workflow of exploratory data analysis?</li>
<li>What are the advantages and disadvantages of descriptive analysis?</li>
<li>What are the advantages and disadvantages of unsupervised learning?</li>
<li>What is the difference between supervised and unsupervised learning?</li>
<li>How does exploratory data analysis differ from traditional hypothesis testing?</li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><i class="fa-solid fa-wrench" aria-label="wrench"></i> <strong>Technical questions</strong></p>
<ol type="1">
<li>Write a function in R to conduct a hierarchical cluster analysis on a dataset.</li>
<li>Implement a k-means algorithm in R to identify clusters within a dataset.</li>
<li>Implement a Principal Component Analysis (PCA) algorithm in R to identify patterns and associations within a dataset.</li>
<li>Write a function in R to produce a descriptive summary of a dataset.</li>
<li>Conduct a correlation analysis in R to identify relationships between variables in a dataset.</li>
<li>Load a dataset into R and conduct a frequency analysis on the dataset.</li>
<li>Load a dataset into R and conduct a keyword in context analysis on the dataset.</li>
<li>Load a dataset into R and conduct a keyword analysis on the dataset.</li>
<li>Load a dataset into R and conduct a sentiment analysis on the dataset.</li>
<li>Load a dataset into R and conduct a topic modelling analysis on the dataset.</li>
</ol>
</div>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-Firth1957" class="csl-entry" role="listitem">
Firth, John R. 1957. <em>Papers in Linguistics</em>. Oxford University Press.
</div>
<div id="ref-Garg2018" class="csl-entry" role="listitem">
Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018. <span>“Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes.”</span> <em>Proceedings of the National Academy of Sciences</em> 115 (16): E3635–44. <a href="https://doi.org/10.1073/pnas.1720347115">https://doi.org/10.1073/pnas.1720347115</a>.
</div>
<div id="ref-Gries2023" class="csl-entry" role="listitem">
Gries, Stefan Th. 2023. <span>“Statistical Methods in Corpus Linguistics.”</span> In <em>Readings in Corpus Linguistics: A Teaching and Research Guide for Scholars in Nigeria and Beyond,</em> 78–114.
</div>
<div id="ref-Harris1954" class="csl-entry" role="listitem">
Harris, Zellig S. 1954. <span>“Distributional Structure.”</span> <em>Word</em> 10 (2-3): 146–62. <a href="https://doi.org/10.1080/00437956.1954.11659520">https://doi.org/10.1080/00437956.1954.11659520</a>.
</div>
<div id="ref-Mikolov2013b" class="csl-entry" role="listitem">
Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>“Distributed Representations of Words and Phrases and Their Compositionality.”</span> In <em>Advances in Neural Information Processing Systems</em>, 3111–19.
</div>
<div id="ref-Petrenz2011" class="csl-entry" role="listitem">
Petrenz, Philipp, and Bonnie Webber. 2011. <span>“Stable Classification of Text Genres.”</span> <em>Computational Linguistics</em> 37 (2): 385–93. <a href="https://doi.org/10.1162/COLI_a_00052">https://doi.org/10.1162/COLI_a_00052</a>.
</div>
<div id="ref-Zipf1949" class="csl-entry" role="listitem">
Zipf, George Kingsley. 1949. <em>Human Behavior and the Principle of Least Effort</em>. Oxford, England: Addison-Wesley Press.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./analysis.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Analysis</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./prediction.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Prediction</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>