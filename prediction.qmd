---
execute: 
  echo: true
---

# Prediction {#sec-prediction}

```{r}
#| label: setup-options
#| child: "_common.qmd"
#| cache: false
```

::: {.callout-tip title="Draft"}
Ready for review.
:::

<!-- 

Content:

- [x] Note on hypothesis testing?
  - Confidence intervals?
  - MuPDAR approach (Multifactorial Prediction and Deviation Analysis with Regressions) [@Deshors2016; @Gries2014] and @Baayen2011
    - Training on group
    - Testing on another group (or groups)

- [ ] Make sure to standardize the use of 'variable' and 'feature'. 

Exercises:

- [ ] add concept questions to Activities
- [ ] add exercises to Activities
- [ ] add thought questions/ case studies to prose sections

Formatting:

- [ ] add citations 
  - [ ] for study rationale
  - [ ] for packages

-->

> All models are wrong, but some are useful. 
> 
> --- George E.P. Box

::: {.callout}
**{{< fa regular list-alt >}} Outcomes**

<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

- Identify the research goals of predictive data analysis
- Describe the workflow for predictive data analysis
- Recognize quantitative and qualitative methods for evaluating predictive models
:::

```{r}
#| label: prediction-data-packages
#| echo: false
#| message: false

```

In this chapter, I introduce supervised learning as an approach to data analysis, specifically focusing on its applications in text analysis. Supervised learning aims to establish a relationship between a target (or outcome) variable and a set of feature variables derived from text data. By leveraging this relationship, statistical generalizations (models) can be created to accurately predict values of the target variable based on the values of the feature variables. Throughout the chapter, we explore practical tasks and theoretical applications of statistical learning in text analysis. We also cover the standard workflow for building predictive models, testing and evaluating model performance, improving model accuracy, and interpreting and reporting findings.

::: {.callout}
**{{< fa terminal >}} Lessons**

<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- 
- [ ] Update lesson name, 
- [ ] update lesson purpose
-->

**What**: [Supervised Learning](https://github.com/qtalr/lessons)\
**How**: In the R Console pane load `swirl`, run `swirl()`, and follow prompts to select the lesson.\
**Why**: ... {{< fa wrench >}} 
:::

## Orientation {#sec-pda-orientation}

In this section, I introduce the concept of supervised learning and provide an overview of the workflow for building and evaluating predictive models for text analysis. First we will discuss the research goals that are typically addressed using supervised learning, contrasting them with the goals of exploratory and inferential analysis. Next, we will discuss the approaches that are typically used to address these goals, including the types of data structures and algorithms that are used. Finally, we will discuss the workflow for building predictive models, including the steps for preparing data, training and testing models, and evaluating and reporting results.

### Research goal {#sec-pda-research-goal}

Predictive data analysis (PDA) is a powerful analysis method for linguists and other researchers interested in making predictions about new or future data based on patterns in existing data. As discussed in @sec-aa-predict and @sec-fr-plan, PDA is a type of supervised learning, which means that it involves training a model on a labeled dataset where the input data and desired output are both provided. The model is able to make predictions or classifications based on the input data by learning the relationships between the input and output data. Supervised machine learning is an important tool for linguists studying language and communication, as it allows us to analyze language data to identify patterns or trends in language use, verify hypotheses, and prescribe actions. 

In contrast to EDA, PDA does require that we have a particular goal in mind from the outset. This goal is to predict a fixed outcome variable based on a set of predictor variables. However, PDA can be applied with distinct aims in mind: exploratory-oriented or hypothesis-driven prediction. In exploratory-oriented prediction, the goal is to examine potential relationships between an outcome and a mutable set of predictor variables. The results of this type of analysis can be used to generate new insight and questions. In hypothesis-driven prediction, in contrast, the goal is to use predictive data analysis to test a hypothesis about the relationship between the outcome and a pre-defined set of predictor variables. In this case, the results of the analysis is used to explain or infer a relationship between the outcome and predictor variables.

::: {.callout}
**{{< fa medal >}} Dive deeper**

- MuPDAR approach (Multifactorial Prediction and Deviation Analysis with Regressions) [@Deshors2016; @Gries2014] and 
  - Training on group
  - Testing on another group (or groups)
- Discriminant analysis [@Baayen2011]

:::

It is important to establish which aim is being taken, as this will influence the approach that is taken.

### Approach {#sec-pda-approach}

<!-- Note:  
This section should cover the following:
- Workflow: Identify, Initial split, Integrate, Inspect, Interrogate, Interpret, (Iterate)
  - Add a few words on the common aspects with other research approaches
  - And point out the differences (use of data, results, interpretation, etc.)
-->

The approach to conducting predictive analysis shares some commonalities with exploratory data analysis (@sec-eda-approach) (as well as inferential analysis @sec-inference), but there are also some key differences. Let's look at the workflow in @tbl-pda-workflow and then discuss these commonalities and differences.

<!-- Workflow -->

| Step | Name | Description |
|------|-------------|-------------|
| 1 | Identify | Consider the research question and aim and identify relevant variables |
| 2 | Initial split | Split the data into representative training and testing sets |
| 3 | Integrate | Apply variable selection and engineering procedures |
| 4 | Inspect | Inspect the data to ensure that it is in the correct format and that the training and testing sets are representative of the data |
| 5 | Interrogate | Train and evaluate the model on the training set, adjusting models or hyperparameters as needed, to produce a final model |
| 6 | Interpret | Interpret the results of the final model in light of the research question or hypothesis |
| 7 | (Optional) Iterate | Repeat steps 3-6 to selecting new variables |

: Workflow for predictive data analysis {#tbl-pda-workflow tbl-colwidths="[10, 20, 70]" .striped}

Focusing on the overlap with other analysis methods, we can see some fundamentals steps such as identifying relevant variables, inspecting the data, interrogating the data, and interpreting the results. And if our research aim is exploratory in nature, iteration may also be a part of the workflow. These steps highlight the importance conducting methodologic and communicable research, as discussed in @sec-fr-frame. 

There are two main differences, however, between the PDA and the EDA workflow we discussed in @sec-exploration. The first, reflected the majority of the steps in the workflow, is that PDA requires partitioning the data into training and testing sets. As discussed in @sec-aa-predict, the training set is used to develop the model, and the testing set is used to evaluate the model's performance. This strategy is used to ensure that the model is robust and generalizes well to new data. It is well known, and makes intuitive sense, that using the same data to develop and evaluate a model likely will not produce a model that generalizes well to new data. This is because the model will have potentially conflated the nuances of the data ('the noise') with any real trends ('the signal') and therefore will not be able to generalize well to new data. This is called overfitting and by holding out a portion of the data for testing, we can evaluate the model's performance on data that it has not seen before and therefore get a more accurate estimate of the generalizable trends in the data. 

Another procedure to avoid the perils of overfitting, is to use resampling methods as part of the model evaluation on the training set. Resampling is the process of repeatedly drawing samples from the training set and evaluating the model on each sample. The two most common resampling methods are **bootstrapping** (resampling with replacement) and **cross-validation** (resampling without replacement). The performance of these multiple models are summarized and the error between them is assessed. The goal is to minimize the performance differences between the models while maximizing the overall performance. These measures go a long way to avoiding overfitting and therefore maximizing the chance that the training phase will produce a model which is robust at the testing phase.

The second difference, not reflected in the workflow but inherent in predictive analysis, is that PDA requires a fixed outcome variable. This means that the outcome variable must be defined from the outset and cannot be changed during the analysis. Furthermore, the informational nature of the outcome variable will dictate the what type of algorithm we choose to interrogate the data and how we will evaluate the model's performance. If the outcome is categorical in nature, we will use a **classification algorithm** (*e.g.* logistic regression, naive bayes, *etc.*). Classification evaluation metrics include accuracy, precision, recall, and F1 score which can be derived from and visualized in a cross-tabulation of the predicted and actual outcome values. 

<!-- [ ] consider a graphic/ and better explanation -->

To understand these measures it is helpful to consider a confusion matrix, which is a table that describes the performance of a classification model on data for which the true values are known. The confusion matrix is a two-by-two matrix that shows the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN), as seen in @tbl-pda-confusion-matrix-metrics.

```{r}
#| label: tbl-pda-confusion-matrix-metrics
#| tbl-cap: "A labeled confusion matrix"
#| echo: false

# Generate a confusion matrix where the cells are labeled with true positives, false positives, true negatives, and false negatives
confusion_matrix <- matrix(c("TP", "FP", "FN", "TN"), nrow = 2, ncol = 2, byrow = TRUE)
rownames(confusion_matrix) <- c("Predicted positive", "Predicted negative")
colnames(confusion_matrix) <- c("Actual positive", "Actual negative")
confusion_matrix |>
  kable() |> 
  kable_styling()
```

If the outcome is numeric in nature, we will use a **regression algorithm** (*e.g.* linear regression, support vector regression, *etc.*). Since the difference between prediction and actual values is numeric, metrics that quantify numerical differences, such as root mean square error (RMSE) or $R^2$, are used to evaluate the model's performance.

```{r}
#| label: fig-pda-regression-metrics
#| fig-cap: "A plot of the actual and predicted values for a regression model"
#| echo: false
#| fig-height: 4

# Generate a hypothetical results with a MSE of .75 for a supevised learning regression model with 50 observations

set.seed(123) # Set the seed for reproducibility
results <- tibble(
  actual = rnorm(100, mean = 10, sd = 1), # Generate 100 random numbers from a normal distribution
  predicted = actual + rnorm(100, mean = 1, sd = .5) # Add random noise to the actual values
)

# Fit a linear model to the `results` data
m1 <- lm(actual ~ predicted, data = results)

# Plot the actual and predicted values with the model error
results |>
  ggplot(aes(x = predicted, y = actual)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  geom_segment(aes(x = predicted, xend = predicted, y = fitted(m1), yend = fitted(m1) + residuals(m1)), color = "black", linetype = 2) + #
  labs(x = "Predicted", y = "Actual")
```

The evaluation of the model is quantitative on the one hand, but it is also qualitative in that we need to consider the implications of the model's performance in light of the research question or hypothesis. Furthermore, depending on our research question we may be interested in exploring the features that are most important to the model's performance. This is called **feature importance** and can be derived from the model's coefficients or weights. Notably, however, some of the most powerful models in use today, such as deep neural networks, are not easily interpretable and therefore feature importance is not easily derived. This is something to keep in mind when considering the research question and the type of model that will be used to address it.

## Analysis {#sec-pda-analysis}

<!-- Goals of this section -->

In this section we now turn to the practical application of predictive data analysis. The dicussion will be separated into classification and regression tasks, as model selection and evaluation procedures differ between the two. For each task, we will frame a research goal and work through the process of building a predictive model to address that goal. Along the way we will cover concepts and methods that are common to both classification and regression tasks and specific to each.

<!-- Research questions -->

To frame our analyses, we will posit research aimed at identifying language usage patterns in second language use, one for a classification task and one for a regression task. Our first research question will be to identify potential salient differences in Spanish language use between natives and L1 English learners (categorical). Our second research question will be to gauge the extent to which the the L1 English learners' Spanish language placement test scores (numeric) can be predicted based on their language use.

We will use data from the CEDEL2 corpus[^2]. We will include a subset of the variables from this data that are relevant to our research questions. The data dictionary for this dataset is seen in @tbl-pda-cedel2-data-dictionary.

[^2]: See [Appendix -@sec-data-cabnc] for more information on the CEDEL2 corpus.

```{r}
#| eval: false
#| label: pda-class-curate-data-run
#| message: false
#| echo: false

# Read in the datasets
cedel2_learners_df <- 
  read_csv("data/cedel2_learners.csv") |>   # read in the learners dataset outcome variable |> 
  mutate(subcorpus = "Learner") |> 
  select(subcorpus, placement_score = placement_test_score_percent, proficiency, text)                  # select variables

cedel2_natives_df <-
  read_csv("data/cedel2_natives.csv") |>    # read in the natives dataset
  mutate(placement_score = NA) |>                       # create an outcome variable
  mutate(proficiency = "Native") |>                   # create a proficiency variable
  select(subcorpus, placement_score, proficiency, text)                  # select the text column

# Combine the datasets by row
cedel2_df <- 
  bind_rows(                                          # combine the datasets by row
    cedel2_learners_df, 
    cedel2_natives_df
  ) |> 
  mutate(doc_id = row_number()) |>                    # create a document id
  select(doc_id, subcorpus, placement_score, proficiency, text)          # select variables

# Write to disk
cedel2_df |> 
  write_csv("data/cedel2_df.csv")

# Create data dictionary 
create_data_dictionary(
  data = cedel2_df,
  file_path = "data/cedel2_df_dd.csv",
  model = "gpt-3.5-turbo",
  grouping = "subcorpus"
)
```


```{r}
#| label: tbl-pda-cedel2-data-dictionary
#| tbl-cap: "Data dictionary for the CEDEL2 corpus"
#| message: false
#| echo: false

# Read in the data dictionary
read_csv("data/cedel2_df_dd.csv") |> 
  kable() |> 
  kable_styling()
```

We will be using the `tidymodels` framework in R to perform this analysis. `tidymodels` is a metapackage, much like `tidyverse`, that provides a consistent interface for modeling and machine learning. Some key packages unique to `tidymodels` are `recipes`, `parsnip`, `workflows`, and `tune`. `recipes` includes functions for preprocessing and engineering features. `parsnip` provides a consistent interface for specifying modeling algorithms. `worflows` allows us to combine recipes and models into a single pipeline. Finally, `tune` give us the ability to evaluate and tune hyperparameters of models.

Since we are using text data, we will also be using the `textrecipes` package which makes various functions available for preprocessing text and extracting and engineering features.

Let's go ahead and do the setup, loading the necessary packages and data. This is seen in @exm-pda-packages-data.

::: {#exm-pda-packages-data}
```{r}
#| label: pda-packages-data-show
#| eval: false

# Load packages
library(tidymodels)   # modeling metapackage
library(textrecipes)  # text preprocessing
library(janitor)      # data inspection

# Set global options
tidymodels_prefer()   # prefer tidymodels functions over other functions with the same name

# Read in the dataset
cedel2_df <- 
  read_csv("../data/cedel2/cedel2_df.csv")

# Preview
cedel2_df |> glimpse()
```

```{r}
#| label: pda-packages-data-run
#| message: false
#| echo: false

# Load packages
library(tidymodels)   # modeling metapackage
library(textrecipes)  # text preprocessing
library(janitor)      # data inspection

# Set global options
tidymodels_prefer()   # prefer tidymodels functions

# Read in the dataset
cedel2_df <- 
  read_csv("data/cedel2_df.csv")

# Preview
cedel2_df |> glimpse()
```
:::

The results from @exm-pda-packages-data show that we have five variables and `r nrow(cedel2_df)` observations. The primary variables for the classification task are the `subcorpus` variable and the `text` variable. The `placement_score` variable is the outcome variable for the regression task. 

<!-- After we have a baseline model, we will compare it to the null model to see if our model is better than the null model. Depending on the results, we will either move on to a more complex model or we will stop here. A more complex model may include changing the features, using a different model, or tuning the hyperparameters of the model. We will then evaluate the final model on the testing data to see how well it generalizes to new data. We will also analyze the predictions, errors, and feature importance of the final model. -->

### Text classification {#sec-pda-text-classification}

<!-- Research question: outcome and features -->

<!-- [ ] include a rationale? citations? -->

The goal of this analysis is to classify texts as either native or learner based on the writing samples. This is a binary classification problem. We will approach this problem from an exploratory perspective, and therefore our aim is to derive features from the text that best distinguish between the two classes.

<!-- FIXME adding `proficiency` in temp. -->

Let's modify the data frame to include only the variables we need for this analysis. In the process, we will rename the `subcorpus` variable to `outcome` to reflect that it is the outcome variable and convert it to a factor vector to meet requirements of the modeling functions we will use in our analysis. This is seen in @exm-pda-class-data.

::: {#exm-pda-class-data}
```{r}
#| label: pda-class-data

# Select variables and factor outcome
cedel2_cls_df <- 
  cedel2_df |> 
  select(outcome = subcorpus, proficiency, text) |> 
  mutate(outcome = factor(outcome))

# Preview
cedel2_cls_df |> glimpse()
```
:::

To view the distribution of the outcome variable between the two levels we can use the `tabyl()` function from the `janitor` package, as seen in @exm-pda-class-data-tabyl.

::: {#exm-pda-class-data-tabyl}
```{r}
#| label: pda-class-corpus-tabyl

# View the distribution of the outcome variable
cedel2_cls_df |> 
  tabyl(outcome) |> 
  adorn_pct_formatting(digits = 1)
```
:::

So a little less than two-thirds of the texts are from learners. It is important to gauge the distribution of the outcome variable to see if it is balanced or imbalanced. The classes need not be perfectly balanced, but if they are wildly imbalanced it can cause problems for the model. 

<!-- Step 1: identify features -->

So in step 1 of our workflow (from @tbl-pda-workflow), we need to identify the features that we will use to classify the texts. There may be many features that we could use. These could be features derived from raw text (*e.g.* characters, words, n-grams, *etc.*), feature vectors (*e.g.* word embeddings), or meta-linguistic features (*e.g.* part-of-speech tags, syntactic parses, or semantic features) that have been derived from these through manual or automatic annotation.

<!-- [ ] explain bag-of-words approach -->

Let's start simple and use a bag-of-words approach to classify texts where words are the features. This is a simple approach that is often used as a baseline for more complex models. If it doesn't work well, we can try something else.

This provides us the linguistic unit we will use but we still need to decide how to represent these words. Do we use raw token counts? Do we use normalized frequencies? Do we use some type of weighting scheme? These are questions that we need to consider as we embark on this analysis. Since we are exploring we can use trial-and-error or consider the implications of each approach and choose the one that best fits our research question --or both. 

As a first pass, let's use raw token counts with our word features. 

<!-- Step 2: Initial split -->

With our features identified, we can move on to step 2 of our workflow and split the data into training and testing sets. We make the splits to our data at this point to draw a line in the sand between the data we will use to train the model and the data we will use to test the model. A typical approach in supervised machine learning is to allocate around 75-80% of the data to the training set and the remaining 20-25% to the testing set, depending on the number of observations. We have `r nrow(cedel2_cls_df)` observations in our data set, so we can allocate 80% of the data to the training set and 20% of the data to the testing set. 

In @exm-pda-class-split, we will use the `initial_split()` function from the `rsample` package to split the data into training and testing sets. The `initial_split()` function takes a data frame and a proportion and returns a `split` object which contains the training and testing sets. We will use the `strata` argument to stratify the data by the `outcome` variable. This will ensure that the training and testing sets have the same proportion of native and learner texts. 

::: {#exm-pda-class-split}
```{r}
#| label: pda-class-split

# Split the data into training and testing sets
cedel2_cls_split <-
  initial_split(
    data = cedel2_cls_df, 
    prop = 0.8, 
    strata = outcome
    )

# Create training set
cedel2_cls_train <- training(cedel2_cls_split)  # 80% of data

# Create testing set
cedel2_cls_test <- testing(cedel2_cls_split)    # 20% of data
```
:::

A confirmation of the distribution of the data across the training and testing sets as well as a break down of the outcome variable can be seen in @exm-pda-class-split-tabyl.

::: {#exm-pda-class-split-tabyl}
```{r}
#| label: pda-class-split-tabyl

# View the distribution of the outcome variables
cedel2_cls_train |> 
  tabyl(outcome) |> 
  adorn_totals("row") |>
  adorn_pct_formatting(digits = 1)

cedel2_cls_test |>
  tabyl(outcome) |> 
  adorn_totals("row") |>
  adorn_pct_formatting(digits = 1)
```
:::

We can see that the split was successful. The training and testing sets have very similiar proportion of native and learner texts. 

<!-- Step 3: Integrate: plan to select and engineer features -->

We are now ready to create a 'recipe', step 3 in our analysis. A recipe is a set of instructions or blueprint which specify the outcome variable and the feature variable and determines how to preprocess and engineer the feature variables.

We will use the `recipe()` function from the `recipes` package to create the recipe. The `recipe()` function minimally takes a formula and a data frame and returns a `recipe` object. The formula specifies the outcome variable ($y$) and the feature variable(s) ($x_1 .. x_n$). For example `y ~ x` can be read as "y is a function of x". In our particular case, we will use the formula `outcome ~ text` to specify that the outcome variable is the `outcome` variable and the feature variable is the `text` variable. The code is seen in @exm-pda-class-recipe.

::: {#exm-pda-class-recipe}
```{r}
#| label: pda-class-recipe
#| results: asis

# Create a recipe
cedel2_cls_rec <- 
  recipe(
    outcome ~ text, 
    data = cedel2_cls_train
    )

# Preview
cedel2_cls_rec
```
:::

The recipe object at this moment contains just one instruction, what the variables are and what their relationship is.

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

R formulas are a powerful way to specify relationships between variables and are used extensively in data modeling including exploratory, predictive, and inferential analysis. The basic formula syntax is `y ~ x` where `y` is the outcome variable and `x` is the feature variable. The formula syntax can be extended to include multiple feature variables, interactions, and transformations. For more information on R formulas, see [R for Data Science](https://r4ds.github.io/bookclub-tmwr/r-formula-syntax.html).
:::

The `recipes` package provides a wide range of `step_*()` functions which can be applied to the recipe to specify how to engineer the variables in our recipe call. These include functions to scale (*e.g* `step_center()`, `step_scale()`, *etc.*) and transform (*e.g.* `step_log()`, `step_pca()`, *etc.*) numeric variables, and functions to encode (*e.g.* `step_dummy()`, `step_labelencode()`, *etc.*) categorical variables. 

These step functions are great when we have selected the variables we want to use in our model and we want to engineer them in a particular way. In our case, however, we need to derive features from the text in the `text` column of datasets before we engineer them. To ease this process, the `textrecipes` package provides a number of step functions for preprocessing text data. These include functions to tokenize (*e.g.* `step_tokenize()`), remove stop words (*e.g.* `step_stopwords()`), and to derive meta-features (*e.g.* `step_lemma()`, `step_stem()`, *etc.*) [^1]. Furthermore, there are functions to engineer features in ways that are particularly relevant to text data, such as feature frequencies and weights (*e.g.* `step_tf()`, `step_tfidf()`, *etc.*) and token filtering (*e.g.* `step_tokenfilter()`).

[^1]: Note that functions for meta-features require more sophisticated text analysis software to be installed on the computing environment (e.g. `spacyr` for `step_lemma()`, `step_pos()`, *etc.*). See the `textrecipes` package documentation for more information.

So let's build on our basic recipe `cedel2_cls_rec` by adding steps relevant to our task. To extract our features, we will use the `step_tokenize()` function to tokenize the text into words. The default behavior of the `step_tokenize()` function is to tokenize the text into words, but other token units can be derived and various options can be added to the function call (as the `tokenizers` package is used under the hood). Adding the `step_tokenize()` function to our recipe is seen in @exm-pda-class-recipe-tokenize.

::: {#exm-pda-class-recipe-tokenize}
```{r}
#| label: pda-class-recipe-tokenize

# Add step to tokenize the text
cedel2_cls_rec <- 
  cedel2_cls_rec |> 
  step_tokenize(text)

# Preview
cedel2_cls_rec
```
:::

The recipe object now contains two instructions, one for the outcome variable and one for the feature variable. The feature variable instruction specifies that the text should be tokenized into words.

We now need to consider how to engineer the word features. If we add `step_tf()` we will get a matrix of token counts by default. We also have the option to add `step_tfidf()` to get a matrix of term frequencies weighted by inverse document frequency, which effectively down weights words that are common across all documents. 

We decided in step 1 that we will start with raw token counts, so we will add `step_tf()` to our recipe. This is seen in @exm-pda-class-recipe-tf.

::: {#exm-pda-class-recipe-tf}
```{r}
#| label: pda-class-recipe-tf

# Add step to tokenize the text
cedel2_cls_rec <- 
  cedel2_cls_rec |> 
  step_tf(text)

# Preview
cedel2_cls_rec
```
:::

<!-- Step 4: Inspect:  -->

To make sure things are in order and that the recipe performs as expected, we can use the functions `prep()` and `bake()` to inspect the recipe. The `prep()` function takes a recipe object and a data frame and returns a `prep` object. The `prep` object contains the recipe and the data frame with the feature variables engineered according to the recipe. The `bake()` function takes a `prep` object and an optional new dataset to apply the recipe to. If we only want to see the application to the training set, we can use the `new_data = NULL` argument. 

In @exm-pda-class-recipe-prep, we use the `prep()` function to create a `prep` object and then use the `bake()` function to create a data frame with the feature variables. We can then inspect the data frame to see if the recipe performed as expected.

::: {#exm-pda-class-recipe-prep}
```{r}
#| label: pda-class-recipe-prep

# Create a prep object
cedel2_cls_prep <- 
  prep(
    cedel2_cls_rec, 
    training = cedel2_cls_train
    )

# Create a data frame with the feature variables
cedel2_cls_bake <- 
  bake(
    cedel2_cls_prep, 
    new_data = NULL
    )

# Preview
cedel2_cls_bake |> dim()

cedel2_cls_bake[
  c(1000:1001, 2000:2001),     # selected rows
  c(1, 1000:1001, 20000:20001) # selected columns
  ]
```
:::

The resulting engineered features data frame has `r nrow(cedel2_cls_bake)` observations and `r ncol(cedel2_cls_bake)` variables. The first variable is the outcome variable and the remaining variables are the engineered features. We can see that the recipe performed as expected and that the feature variables are the token counts for each word in the text.

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

When applying tokenization and feature engineering steps to text data the result is often contained in a matrix object. In the the `recipes` package a data frame with a matrix-like structure is returned. 
:::

But we should pause. We have a lot of features! One for every single word in the entire corpus. This is an unweildy number of features for a model and it is likely that many of these features are not useful for our classification task. Furthermore, the more features we have, the more chance these features capture the nuances of these particular writing samples, thus, we are the more likely we are to overfit the model. All in all, we need to reduce the number of features. 

Our domain knowledge about language can be of help us decide on how to approach reducing the feature set. On the one hand, we know that most tokens occur rarely in any sizable corpus. And the lower the frequency, the more likely that the token may reflect nuances of the speaker or the content of particular texts rather than generalizable trends. On the other hand, we know that a relatively small number of the most frequent words quickly account for a large proportion of the tokens in a corpus. 

So just with these two considerations, we can see that we can filter out many infrequent words and likely have quite a few viable features. Let's start with an arbitrary threshold of the top 1,000 words by frequency. We can use the `step_tokenfilter()` function to filter out the top 1,000 words by frequency. This particular step needs to be applied before the `step_tf()` step, so we will add it to our recipe before the `step_tf()` step. This is seen in @exm-pda-class-recipe-tokenfilter.

::: {#exm-pda-class-recipe-tokenfilter}
```{r}
#| label: pda-class-recipe-tokenfilter

# Rebuild recipe with tokenfilter step
cedel2_cls_rec <- 
  recipe(
    outcome ~ text, 
    data = cedel2_cls_train
    ) |> 
  step_tokenize(text) |> 
  step_tokenfilter(text, max_tokens = 1000) |> 
  step_tf(text)

# Prep and bake
cedel2_cls_prep <- 
  prep(
    cedel2_cls_rec, 
    training = cedel2_cls_train
    )

cedel2_cls_bake <-
  bake(
    cedel2_cls_prep, 
    new_data = NULL
    )

# Preview
cedel2_cls_bake |> dim()

cedel2_cls_bake[1:5, 1:10]
```
:::

We now have a manageable set of features. Only during the interrogation step will we know if they are useful.

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

The `prep()` and `bake()` functions are useful for inspecting the recipe and the engineered features, but they are not required to build a recipe. When a recipe is added to a workflow, the `prep()` and `bake()` functions are called automatically as part of the process.   
:::

There is one last step we should add to our recipe which concerns the skewed nature of word frequency distributions. The magnitude of the token counts will be highly skewed with a few words having very high counts and most words having relatively low counts, even for the top 1,000 words. This skew can be problematic for some models, so we will add a step to log normalize the token counts. This will not change the rank order, only the magnitude of the differences. Note that we use `all_predictors()` to apply the log transformation to all the word features, as seen in @exm-pda-class-recipe-log.

::: {#exm-pda-class-recipe-log}
```{r}
#| label: pda-class-recipe-log

# Add log step to recipe
cedel2_cls_rec <- 
  cedel2_cls_rec |> 
  step_log(all_predictors(), offset = 1) # add 1 to avoid log(0)

cedel2_cls_rec
```
:::

<!-- Step 5: Interrogate -->

We are now ready to turn our attention to step 5 of our workflow, interrogating the data. In this step, we will first select a classification algorithm, then add this algorithm and our recipe to a workflow object. We will then use the workflow object to train and evaluate the model on the training set.

<!-- Select a classification algorithm -->

There are many classification algorithms to choose from with their own strengths and shortcomings. In @tbl-pda-class-algorithms, we list some of the most common classification algorithms and their characteristics.

| Algorithm | Strengths | Shortcomings |
|-----------|-----------|--------------|
| Logistic regression | Simple, interpretable, fast | Assumes linear relationship between features and outcome |
| Naive Bayes | Simple, interpretable, fast | Assumes independence between features |
| Decision trees | Nonlinear relationships, interpretable | Prone to overfitting |
| Random forest | Nonlinear relationships, interpretable | Prone to overfitting |
| Support vector machines | Nonlinear relationships, interpretable | Prone to overfitting |
| Neural networks | Nonlinear relationships, fast | Prone to overfitting, difficult to interpret |

: Classification algorithms {#tbl-pda-class-algorithms .striped}


<!--  
[ ] Model descriptions: 

Logistic regression is a supervised machine learning method for classification. It is a parametric method, which means that it makes assumptions about the underlying distribution of the data. It is an iterative method, which means that it uses an iterative algorithm to find the optimal parameters. To avoid overfitting it uses regularization such as ridge regression or lasso regression. These regularization methods penalize the model for having too many parameters. However, how does one know what the optimal number of parameters is? This is where cross-validation comes in. 

Naive Bayes is a supervised machine learning method for classification. It is a parametric method, which means that it makes assumptions about the underlying distribution of the data. It is a probabilistic method, which means that it uses Bayes' theorem to calculate the probability of a class given the predictor variables. It is a generative method, which means that it learns the joint probability distribution of the predictor variables and the outcome variable. It is a simple method, which means that it makes the assumption that the predictor variables are independent of each other. This assumption is called the naive assumption. Now this assumption does not theoretically hold for language data as words are not independent of each other. However, in practice, Naive Bayes' models still perform well on many text classification tasks.

-->


<!-- Models: simple to complex  -->

In the process of selecting an algorithm, simple, computationally efficient, and interpretable models are preferred over complex, computationally expensive, and uninterpretable models, all things being equal. Only if the performance of the simple model is not good enough should we move on to a more complex model. 

With this end mind, we will start with a simple logistic regression model to see how well we can classify the texts in the training set with the features we have engineered. We will use the `logistic_reg()` function from the `parsnip` package to specify the logistic regression model. We then select the implementation engine (`glm` General Linear Model) and the mode of the model (`classification`). The implementation engine is the software that will be used to fit the model. The mode is the type of model, either classification or regression. The code is seen in @exm-pda-class-model-spec.

::: {#exm-pda-class-model-spec}
```{r}
#| label: pda-class-model-spec

# Create a model specification
cedel2_cls_spec <- 
  logistic_reg() |> 
  set_engine("glm") |>
  set_mode("classification")

# Preview
cedel2_cls_spec
```
:::

In the `parsnip` package, model specifications are separate from model fitting. This allows us to specify the model once and then fit the model with different data sets. Furthermore, different algorithms will have different hyperparameters that can be tuned. The code in @exm-pda-class-model-spec uses the default hyperparameters for the logistic regression model. 

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

You can retrieve the list of potential engines for a given model specification with the `show_engines()` function. For example, 

```r
show_engines(logistic_reg())
```

```
# A tibble: 7 × 2
  engine    mode          
  <chr>     <chr>         
1 glm       classification
2 glmnet    classification
3 LiblineaR classification
4 spark     classification
5 keras     classification
6 stan      classification
7 brulee    classification
```
:::

Now we will combine our recipe and model specification into a workflow object. The workflow object will allow us to train and evaluate the model on the training set. We will use the `workflow()` function from the `workflows` package to combine the recipe and model specification into a workflow object. The code is seen in @exm-pda-class-workflow.

::: {#exm-pda-class-workflow}
```{r}
#| label: pda-class-workflow

# Create a workflow
cedel2_cls_wf <- 
  workflow() |> 
  add_recipe(cedel2_cls_rec) |> 
  add_model(cedel2_cls_spec)

# Preview
cedel2_cls_wf
```
:::

The worflow contains all the preprocessing and the model-specific parameters we've selected. A workflow object at this stage in the analysis is not trained. It is a blueprint for a model. We can use the `fit()` function to apply the workflow to the training data to train the model and update our workflow object with the trained model. The `fit()` function takes a workflow and a data frame and returns an updated workflow object. The code is seen in @exm-pda-class-workflow-fit.

::: {#exm-pda-class-workflow-fit}
```{r}
#| label: pda-class-workflow-fit

# Fit the workflow
cedel2_cls_wf_fit <- 
  fit(
    cedel2_cls_wf, 
    data = cedel2_cls_train
    )
```
:::

The workflow object now contains the trained model. But things did not go off without a hitch. We receive two warning messages when fitting the workflow to the training data: 

```
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
```

Warning messages are a sign that something may be amiss with the features we have choosen, the model we have selected, or both! Warning messages can sometimes be cryptic and steeped in the jargon of the algorithm. When in doubt it is best to consult the documentation of the algorithm and/ or seek help from the R community.

The first warning, the 'algorithm did not converge', means that given the data and the model, the algorithm could not estimate a stable solution. The reason or reasons for this can be varied, but it typically due to extreme outliers, collinearity, or a small sample size. Collinearity is when two or more features are highly correlated. Given we are using words as features, it is likely that there is collinearity in the data. We will need to address this.

The second warning is 'fitted probabilities numerically 0 or 1 occurred'. This means that the model is overfitting the data, (i.e. there are one or more features that perfectly predict the outcome). This very well could be related to the first warning, as a small set of features may be perfectly predicting the outcome in a way that will likely not generalize to new data. 

The upshot is that this model specification and/ or the feature engineering needs to be changed. So we step back in the workflow and make changes that will cascade downstream. 

It turns out that a regularized logistic regression model often addresses the issues this model is experiencing. We will use the `glmnet` engine to fit a regularized logistic regression model. It is a more complex complex approach, but it can often mitigate the issues of collinearity and overfitting by penalizing features that are highly correlated and by shrinking the coefficients of features that are not useful for predicting the outcome. 

We will build a new model specification using the `logistic_reg()` function and the `glmnet` engine. This time we will use hyperparameters in our `logistic_reg()` call to specify the penality we want to use and the strength of the penality. The penalty we will use is the lasso (L1). We now need to decide what value to use for the strength of the penalty. We either experiment with different values one by one or we can implement a range of values in tandem and then select the best value. We will use the latter approach.

The `tune` package provides a number of functions for selecting, or 'tuning', hyperparameters. The first is the `tune()` function which we add as the argument of the hyperparameter we want to tune in the model specification, as seen in @exm-pda-class-model-spec-tune.

::: {#exm-pda-class-model-spec-tune}
```{r}
#| label: pda-class-model-spec-tune

# Create a model specification for tuning
cedel2_cls_spec_tune <- 
  logistic_reg(penalty = tune(), mixture = 1) |> 
  set_engine("glmnet") |>
  set_mode("classification")

# Create a workflow
cedel2_cls_wf_tune <- 
  workflow() |> 
  add_recipe(cedel2_cls_rec) |> 
  add_model(cedel2_cls_spec_tune)
```
:::

We now have a workflow that includes the `tune()` function as a placeholder for a range of values for the penalty hyperparameter. We use the `grid_regular()` function from the `dials` package to specify a grid of values for the penalty hyperparameter. Let's choose a random set of 10 values, as seen in @exm-pda-class-model-spec-tune-grid-values.

::: {.callout}
**{{< fa medal >}} Dive deeper**

The hyperparameters `penalty` and `mixture` control which type(s) of regularization to apply and if there is a mixing of the types. In the model specification in @exm-pda-class-model-spec-tune, the `penalty` is set to be tuned and the `mixture` is set to 1, which means that only a lasso penalty will be applied.

See the documentation for the `parsnip::logistic_reg()` function for more information.
:::

::: {#exm-pda-class-model-spec-tune-grid-values}
```{r}
#| label: pda-class-model-spec-tune-grid-values

# Create a grid of values for the penalty hyperparameter
cedel2_cls_grid <- 
  grid_regular(
    penalty(), 
    levels = 10
    )

# Preview
cedel2_cls_grid
```
:::

The 10 values chosen to be in the grid range from nearly zero to 1, where 0 indicates no penalty and 1 indicates a strong penalty. 

Now to perform the tuning and arrive at an optimal value for `penalty` we need to create a tuning workflow. We do this by calling the `tune_grid()` function using our tuning model specification workflow, a resampling object, and our hyperparameter grid and returns a `tune_grid` object.

<!-- [ ] explain cross-fold validation better -->

Now, a resampling object is not something we've seen yet. Resampling is a strategy that allows us to generate multiple training and testing sets from a single data set --in this case the training data we split at the outset. Each variation of the training and testing sets is called a fold. Which is why this type of resampling is called k-fold cross-validation. The `vfold_cv()` function from the `rsample` package takes a data frame and a number of folds and returns a `vfold_cv` object. We will use 10 folds and include the model specification and the hyperparameter grid in the `tune_grid()` function call. The code is seen in @exm-pda-class-model-spec-tune-grid-cv.

::: {#exm-pda-class-model-spec-tune-grid-cv}
```{r}
#| label: pda-class-model-spec-tune-grid-cv

# Set seed for reproducibility
set.seed(123)

# Create a resampling object
cedel2_cls_vfold <- 
  vfold_cv(
    cedel2_cls_train, 
    v = 10
    )

# Tune the model
cedel2_cls_tune <- 
  tune_grid(
    cedel2_cls_wf_tune, 
    resamples = cedel2_cls_vfold, 
    grid = cedel2_cls_grid
    )

# Preview
cedel2_cls_tune
```
:::

The `cedel2_cls_tune` object contains the results of the tuning for each fold. We can see the results of the tuning for each fold by calling the `collect_metrics()` function on the `cedel2_cls_tune` object, as seen in @exm-pda-class-model-spec-tune-grid-collect.

::: {#exm-pda-class-model-spec-tune-grid-collect}
```{r}
#| label: fig-pda-class-model-spec-tune-grid-collect
#| fig-cap: "Metrics for each fold of the tuning process."
#| fig-width: 8

# Collect the results of the tuning
cedel2_cls_tune_metrics <- 
  collect_metrics(cedel2_cls_tune)

# Visualize metrics
cedel2_cls_tune |> autoplot()
```
:::

The most common metrics for model performance in classification are accuracy and the area under the receiver operating characteristic curve (ROC-AUC). Accuracy is the proportion of correct predictions. The ROC-AUC is a measure of the trade-off between sensitivity and specificity. Sensitivity is the proportion of true positives that are correctly identified. Specificity is the proportion of true negatives that are correctly identified. The ROC-AUC is a measure of how well the model can distinguish between the two classes.

In the plot of the metrics, we can see that the many of the penalty values performed similarly, with a drop off in performance at the higher values. Conveniently, the `show_best()` function from the `tune` package takes a `tune_grid` object and returns the best performing hyperparameter values. The code is seen in @exm-pda-class-model-spec-tune-grid-collect-best.

::: {#exm-pda-class-model-spec-tune-grid-collect-best}
```{r}
#| label: pda-class-model-spec-tune-grid-collect-best

# Show the best performing hyperparameter value
cedel2_cls_tune |> 
  show_best(metric = "roc_auc")
```
:::

We can make this selection programmatically by using the `select_best()` function. This function needs a metric to select by. We will use the ROC-AUC and select the best value for the penalty hyperparameter. The code is seen in @exm-pda-class-model-spec-tune-grid-collect-select.

::: {#exm-pda-class-model-spec-tune-grid-collect-select}
```{r}
#| label: pda-class-model-spec-tune-grid-collect-select

# Select the best performing hyperparameter value
cedel2_cls_best <- 
  select_best(cedel2_cls_tune, metric = "roc_auc")

# Preview
cedel2_cls_best
```
:::

All of that to tune a hyperparameter! Now we can update the model specification and workflow with the best performing hyperparameter value using the previous `cedel2_cls_wf_tune` workflow and the `finalize_workflow()` function. The `finalize_workflow()` function takes a workflow and the selected parameters and returns an updated `workflow` object, as seen in @exm-pda-class-tune-hyperparameters-update-workflow.

::: {#exm-pda-class-tune-hyperparameters-update-workflow}
```{r}
#| label: pda-class-tune-hyperparameters-update-workflow

# Update model specification
cedel2_cls_wf_lasso <-
  cedel2_cls_wf_tune |>
  finalize_workflow(cedel2_cls_best)

cedel2_cls_wf_lasso
```
:::

Our model specification and the worflow are updated with the parameters. 

Let's now return to fitting the workflow to the training set as we did before for the vanilla logistic regression model. As a reminder we are still working in step 5 of our workflow, interrogating the data. We identified and addressed potential issues in the model specification, leaving the feature selection the same. 

We again fit the workflow to the training set, as seen in @exm-pda-class-tune-hyperparameters-fit.

::: {#exm-pda-class-tune-hyperparameters-fit}
```{r}
#| label: pda-class-tune-hyperparameters-fit

# Fit the workflow
cedel2_cls_wf_lasso_fit <- 
  fit(
    cedel2_cls_wf_lasso, 
    data = cedel2_cls_train
    )
```
:::

No warnings, so that is a good sign! --Or at least a better sign than before.

Let's evaluate the performance of the model on the training data. The `predict()` function takes a trained model specification and a data frame and returns a data frame with the predicted outcome. We can join these predicted outcomes with the actual outcomes in the training data. The `metrics()` function takes a data frame with the actual and predicted outcomes and returns a data frame with the metrics for the model. The code is seen in @exm-pda-class-tune-hyperparameters-evaluate.

::: {#exm-pda-class-tune-hyperparameters-evaluate}
```{r}
#| label: pda-class-tune-hyperparameters-evaluate

# Evaluate workflow
cedel2_cls_lasso_fit_preds <-
  bind_cols(
    cedel2_cls_train,
    predict(cedel2_cls_wf_lasso_fit, cedel2_cls_train)
  )

# Calculate accuracy
cedel2_cls_lasso_fit_preds |>
  metrics(truth = outcome, estimate = .pred_class)
```
:::

Again, no warnings, so we have a functioning model. It also has a high accuracy, but we know that this is not the most reliable metric for the robustness of the model on new data. Similar to what we did to tune the hyperparameters, we can use cross-validation to gauge the variability of the model. The `fit_resamples()` function takes a workflow and a resampling object and returns metrics for each fold. The code is seen in @exm-pda-class-tune-hyperparameters-evaluate-workflow-cv.

::: {#exm-pda-class-tune-hyperparameters-evaluate-workflow-cv}
```{r}
#| label: pda-class-tune-hyperparameters-evaluate-workflow-cv

# Cross-validate workflow
cedel2_cls_lasso_cv <-
  cedel2_cls_wf_lasso |>
  fit_resamples(
    resamples = cedel2_cls_vfold, 
    control = control_resamples(save_pred = TRUE)
  )
```
::: 

We want to aggregate the metrics across the folds to get a sense of the variability of the model. The `collect_metrics()` function takes the results of a cross-validation and returns a data frame with the metrics. 

::: {#exm-pda-class-tune-hyperparameters-evaluate-workflow-cv-collect}
```{r}
#| label: pda-class-tune-hyperparameters-evaluate-workflow-cv-collect

# Collect metrics
cedel2_cls_lasso_cv |> 
  collect_metrics()
```
:::

The accuracy has dropped, but is still very high. From these metrics it appears we have a good candidate model. In many cases, however, there may be further room for improvement. A good next step to in these cases is to evaluate the model errors and see if there are any patterns that can be addressed before evaluating the model on the test set.

For classification tasks, a good place to start is to visualize the confusion matrix to assess false negatives and false positives. The `conf_mat_resampled()` function takes a `fit_resamples` object and returns a table (`tidy = FALSE`) with the confusion matrix for the aggregated folds. We can pass this to the `autoplot()` function to plot as in @exm-pda-class-tune-hyperparameters-evaluate-workflow-cv-confusion.

::: {#exm-pda-class-tune-hyperparameters-evaluate-workflow-cv-confusion}
```{r}
#| label: fig-class-tune-hyperparameters-evaluate-workflow-cv-confusion
#| fig-cap: "Confusion matrix for the aggregated folds of the cross-validation."
#| fig-height: 4

# Plot confusion matrix
cedel2_cls_lasso_cv |> 
  conf_mat_resampled(tidy = FALSE) |> 
  autoplot(type = "heatmap")
```
:::

The top left to bottom right diagonal contains the true positives and true negatives. The top right to bottom left diagonal contains the false positives and false negatives. The convention is speak of one class being the positive class and the other class being the negative class. In our case, we will consider the positive class to be the 'learner' class and the negative class to be the 'natives' class.

- [ ] interpretation has changed, with log-transformation

We can see that there are more learners falsely predicted to be natives than the other way around. This may be due to the fact that there are simply more learners than natives in the data set or this could signal that there are some learners that are more similar to natives than other learners. Clearly this can't be the entire explanation as the model is not perfect, even some natives are classified falsely as learners! But may be an interesting avenue for further exploration. Maybe these are learners that are more advanced or have a particular style of writing that is more similar to natives.

<!--  
[ ] Null model?

Building a null model for classification we simply predict the most common class in the training data. In other words, we use central tendency measure for categorical data, the mode.

For regression, the central tendency is estimated by the mean. So a null model for regression is simply the mean of the outcome variable.
-->


Another perspective often applied to evaluate a model is the ROC curve. The ROC curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) for different classification thresholds. To produce the ROC curve, we pass the resampled fit to the `collect_preditions()` function. We then pass this result to `roc_curve()` function. But we want the results grouped by each fold, so we use `group_by(id)`. Finally, we can pass this to the `autoplot()` function to plot as in @exm-pda-class-tune-hyperparameters-evaluate-workflow-cv-roc.

::: {#exm-pda-class-tune-hyperparameters-evaluate-workflow-cv-roc}
```{r}
#| label: fig-pda-class-tune-hyperparameters-evaluate-workflow-cv-roc
#| fig-cap: "ROC curve for the aggregated folds of the cross-validation."
#| fig-height: 4

# Plot ROC curve
cedel2_cls_lasso_cv |> 
  collect_predictions() |>
  group_by(id) |>
  roc_curve(truth = outcome, .pred_Native) |> 
  autoplot()
```
:::

<!-- [ ] research roc-auc to understand what this plot means -->

All signs point to a robust model.

<!-- Step 6: Evaluate -->

We are now ready to move on to step 6, evaluating the model on the test set. We will use the `predict()` function to predict the outcome on the test set. The `metrics()` function takes a data frame with the actual and predicted outcomes and returns a data frame with the metrics for the model. The code is seen in @exm-pda-class-tune-hyperparameters-evaluate-test.

::: {#exm-pda-class-tune-hyperparameters-evaluate-test}
```{r}
#| label: pda-class-tune-hyperparameters-evaluate-test

# Evaluate model on test set
cedel2_cls_lasso_fit_preds_test <-
  bind_cols(
    cedel2_cls_test,
    predict(cedel2_cls_wf_lasso_fit, cedel2_cls_test)
  )

# Calculate accuracy
cedel2_cls_lasso_fit_preds_test |>
  metrics(truth = outcome, estimate = .pred_class)
```
:::

The accuracy is as good as the training set, even a tad higher. This is a good sign that the model is robust as it performs well on both training and test sets. We can evaluate the confusion matrix on the test set as well. The code is seen in @exm-pda-class-tune-hyperparameters-evaluate-test-confusion.

::: {#exm-pda-class-tune-hyperparameters-evaluate-test-confusion}
```{r}
#| label: fig-pda-class-tune-hyperparameters-evaluate-test-confusion
#| fig-cap: "Confusion matrix for the test set."
#| fig-height: 4

# Plot confusion matrix
cedel2_cls_lasso_fit_preds_test |> 
  conf_mat(truth = outcome, estimate = .pred_class) |> 
  autoplot(type = "heatmap")
```
:::

On the test set the false instances look similar to the distribution on the training set, with the exception that learners are more likely to be falsely predicted to be natives than the other way around. 

We can inspect the errors on the test set by filtering the data frame to only include the false instances. I will then select the columns with the actual outcome, the predicted outcome, the proficiency level, and the text and separate the predicted outcome to inspect them separately, as seen in @exm-pda-class-tune-hyperparameters-evaluate-test-errors.

::: {#exm-pda-class-tune-hyperparameters-evaluate-test-errors}
```{r}
#| label: pda-class-tune-hyperparameters-evaluate-test-errors

# Inspect errors
cedel2_cls_lasso_fit_preds_test |> 
  filter(outcome != .pred_class) |> 
  select(outcome, .pred_class, proficiency, text)

# Inspect learners falsely predicted to be natives
cedel2_cls_lasso_fit_preds_test |> 
  filter(outcome == "Learner", .pred_class == "Native") |> 
  select(outcome, .pred_class, proficiency, text) |> 
  count(proficiency, sort = TRUE)
```
:::

- Majority of misclassified learners are advanced, which could be expected as they are more similar to natives. There are some beginners that are misclassified as natives, which is interesting, and not expected. But all models are wrong, but some are useful --George Box. 

- Still an open question as to why some natives are classified as learners.

We can inspect the estimates for the features in the model to gain some insight into what features are most important for predicting the outcomes. The `extract_fit_parsnip()` function takes a trained model specification and returns a data frame with the estimated coefficients for each feature. The code is seen in @exm-pda-class-tune-hyperparameters-evaluate-test-estimates.

::: {#exm-pda-class-tune-hyperparameters-evaluate-test-estimates}
```{r}
#| label: pda-class-tune-hyperparameters-evaluate-test-estimates

# Extract estimates
cedel2_cls_wf_lasso_fit |> 
  extract_fit_parsnip() |>
  tidy()
```
:::


The estimates are the log odds of the outcome. In a binary classification task, the log odds of the outcome is the log of the probability of the outcome divided by the probability of the other outcome. In our case, the reference outcome is "Learner", so negative log-odds indicate that the feature is associated with the "Learner" outcome and positive log-odds indicate that the feature is associated with the "Native" outcome.

The estimates are in log-odds, so we need to exponentiate them to get the odds. The odds are the probability of the outcome divided by the probability of the other outcome. The probability of the outcome is the odds divided by the odds plus one. The code is seen in @exm-pda-class-tune-hyperparameters-evaluate-test-estimates-probability.

::: {#exm-pda-class-tune-hyperparameters-evaluate-test-estimates-probability}
```{r}
#| label: pda-class-tune-hyperparameters-evaluate-test-estimates-probability

# Calculate probability
cedel2_cls_wf_lasso_fit |> 
  extract_fit_parsnip() |>
  tidy() |> 
  mutate(probability = exp(estimate) / (exp(estimate) + 1))
```
:::

So just looking at the snippet of the features returned from @exm-pda-class-tune-hyperparameters-evaluate-test-estimates-probability, we can see that the features 'a' and 'abandonado' are slightly associated with the "Native" outcome nd the other features are neutral (`probability` = 0.5).

A quick way to extract the most important features for predicting the each outcome is to use the `vi()` function from the `vip` package. It takes a trained model specification and returns a data frame with the most important features. The code is seen in @exm-pda-class-tune-hyperparameters-evaluate-test-vip.

::: {#exm-pda-class-tune-hyperparameters-evaluate-test-vip}
```{r}
#| label: pda-class-tune-hyperparameters-evaluate-test-vip

# Load package
library(vip)

conflicted::conflicts_prefer(vip::vi)

# Extract important features
var_importance_df <- 
  cedel2_cls_wf_lasso_fit |> 
  vi()

# Preview
var_importance_df
```
:::

The `Variable` column contains each feature (with the feature type and corresponding variable `tf_text_`), `Importance` provides the absolute log-odds value, and the `Sign` column indicates whether the feature is associated with the "NEG" ("Learner") or the "POS" ("Native") outcome. We can recode the `Variable` and `Sign` columns to make them more interpretable and exponentiate the log-odds to get probabilites and then plot them using `ggplot()`, as in @exm-pda-class-tune-hyperparameters-evaluate-test-vip-plot.

::: {#exm-pda-class-tune-hyperparameters-evaluate-test-vip-plot}
```{r}
#| label: fig-pda-class-tune-hyperparameters-evaluate-test-vip-plot
#| fig-cap: "Most important features for predicting the outcome."
#| fig-height: 4
#| fig-width: 8

# Recode variable and sign
var_importance_df <- 
  var_importance_df |> 
  mutate(
    Feature = str_remove(Variable, "tf_text_"), 
    Outcome = case_when(Sign == "NEG" ~ "Learner", Sign == "POS" ~ "Native"),
    Importance = exp(Importance) / (exp(Importance) + 1)
    ) |> 
  select(Outcome, Feature, Importance)

# Plot
var_importance_df |> 
  slice_max(Importance, n = 50) |>
  ggplot(aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_point() +
  coord_flip() +
  facet_wrap(~ Outcome, scales = "free_y") +
  labs(x = NULL, y = "Importance", fill = NULL) +
  theme_minimal()
```
:::

We can inspect @fig-pda-class-tune-hyperparameters-evaluate-test-vip-plot, and qualitatively assess what these features may be telling us about the differences between the learners and the natives. 

<!--  
In a supervised text classification task, you can use parallel coordinate plots to visualize the distribution of class labels across different feature dimensions. This can help identify which features are most informative for distinguishing between classes and inform feature selection or dimensionality reduction techniques.
-->

In this section we've build a text classifier using a regularized logistic regression model. We've tuned the hyperparameters to arrive at a robust model that performs well on both the training and test sets. We've also evaluated the model errors and inspected the most important features for predicting the outcome.

The `tidymodels` package provides a framework for building and evaluating supervised machine learning models that is modular in nature. This means, that we can quickly and easily change the model specification, the features, feature engineering, and the hyperparameters to arrive at a robust model. If the model is not robust, we can change models in the model specification. If the features are not robust, we can change the recipe. If the model is overfitting, we can tune the hyperparameters. 

### Text regression {#sec-pda-text-regression}

We will now turn our attention to the second task in this section, text regression. In this task, we will use the same original dataset as in the classification task, but we will predict the placement score based on the learner writing samples. Let's start by extracting the observations (only learners) and the relevant variables from the original data set. The code is seen in @exm-pda-reg-data.

::: {#exm-pda-reg-data}
```{r}
#| label: pda-reg-data

# Extract observations and relevant variables
cedel2_reg <- 
  cedel2_df |> 
  filter(proficiency != "Native") |> 
  select(outcome = placement_score, proficiency, text)

# Preview
cedel2_reg |> glimpse()
```
:::

<!-- Step 1: Identify variables -->

In this task, our outcome variable is numeric so we do not need, or want, to convert it to a factor. Our predictor variable `text` is the same as before. We have already weighed the options for feature engineering and decided to use the term frequency method (raw counts) for the top 1,000. Since we are setting up the same recipe as before, essentially, we can use the same code as before. 

<!-- [ ] different features? bigrams? tf-idf weighting? -->

<!-- Step 2: Initial split -->

Let's first move to step 2, initial split. We will use the `initial_split()` function again, but this time we will not need to stratify the data as we are not predicting a categorical variable. The code is seen in @exm-pda-reg-initial-split.

::: {#exm-pda-reg-initial-split}
```{r}
#| label: pda-reg-initial-split

# Set seed for reproducibility
set.seed(123)

# Split data
cedel2_reg_split <- 
  initial_split(cedel2_reg, prop = 0.8)

# Training set
cedel2_reg_train <- 
  training(cedel2_reg_split)

# Test set
cedel2_reg_test <- 
  testing(cedel2_reg_split)
```
:::

The training set has `r nrow(cedel2_reg_train)` observations and the test set has `r nrow(cedel2_reg_test)` observations.

<!-- Step 3: Integrate -->

Now we can create the recipe to set up the variable relations, select the features, and engineer the features. The code is seen in @exm-pda-reg-recipe.

::: {#exm-pda-reg-recipe}
```{r}
#| label: pda-reg-recipe

# Create a recipe
cedel2_reg_rec <- 
  recipe(outcome ~ text, data = cedel2_reg_train) |> 
  step_tokenize(text) |> 
  step_tokenfilter(text, max_tokens = 1000) |>
  step_tf(text) |> 
  step_log(all_predictors(), offset = 1)

# Preview
cedel2_reg_rec
```
:::

<!-- Step 4: Inspect -->

At this point we would inspect our recipe to make sure that it looks as expected and gauge the number of features. But since are using the same recipe as before, we can skip this step.

<!-- Step 5: Interrogate -->

We can now proceed to interrogate the data. As before we will want to start with a simple model and then build up to more complex models. Let's consider some common algorithms for regression tasks in @tbl-pda-reg-algorithms.

| Algorithm | Strengths | Shortcomings |
|-----------|-----------|--------------|
| Linear regression | Simple, interpretable, fast | Assumes linear relationship between features and outcome |
| Decision trees | Nonlinear relationships, interpretable | Prone to overfitting |
| Random forest | Nonlinear relationships, interpretable | Prone to overfitting |
| Neural networks | Nonlinear relationships, fast | Prone to overfitting, difficult to interpret |

: Regression algorithms {#tbl-pda-reg-algorithms .striped}

<!--  
[ ] Model descriptions:

Linear regression can be used to predict a numeric outcome variable from a set of features. It is a parametric method, which means that it makes assumptions about the underlying distribution of the data. It is an iterative method, which means that it uses an iterative algorithm to find the optimal parameters. To avoid overfitting it uses regularization such as ridge regression or lasso regression. These regularization methods penalize the model for having too many parameters. However, how does one know what the optimal number of parameters is? This is where cross-validation comes in.

Decision trees for text classification are a supervised machine learning method for classification. They are a non-parametric method, which means that they do not make any assumptions about the underlying distribution of the data. They are a greedy method, which means that they use a greedy algorithm to find the optimal split of the predictor variables. They are a simple method, which means that they are easy to understand and implement. 

In practical terms using decision trees for text classification can be very useful as they are easy to interpret. For example, we can see which words are most important for the classification of a text. However, they are prone to overfitting the training data. 

To avoid this we can use a technique called **bagging**. Bagging is a resampling technique that builds and combines multiple decision trees to make a prediction. 

A **random forest** uses bagged decision trees, with the addition that each decision tree is trained on a random sample of the features. This helps to reduce the correlation between the decision trees and thus reduces the variance of the model.


Neural networks are a supervised machine learning method for regression and classification. They are a non-parametric method, which means that they do not make any assumptions about the underlying distribution of the data. They are an iterative method, which means that they use an iterative algorithm to find the optimal parameters. They are a complex method, which means that they are difficult to understand and implement. However, they are very powerful and can be used to solve a wide range of problems. However, they are expensive to train and require a lot of data. It is often the case that a simpler method will perform just as well as a neural network in certain contexts. 

See https://smltar.com/mlregression.html for more info on evaluation (and other aspects) of supervised machine learning regression models.  -->

Let's start with a with a linear regression model in mind for our model specification. But let's also consider what we learned in our first attempt to build a logistic regression model using word frequencies. We learned that the model was overfitting the training data and we needed to use a regularized model to reduce the variance of the model. So let's start with a regularized linear regression model. 

The `linear_reg()` function, just as the `logistic_reg()` function, provides arguments for the regularization hyperparameters when the `glmnet` computational engine is used. Let's tune the `penalty` hyperparameter in the same way as before. The code for the process is seen in @exm-pda-reg-model-spec-tune.

::: {#exm-pda-reg-model-spec-tune}
```{r}
#| label: pda-reg-model-spec-tune

# Create model specification
cedel2_reg_spec_lasso <-
  linear_reg(penalty = tune(), mixture = 1) |> 
  set_engine("glmnet") |> 
  set_mode("regression")

# Create workflow
cedel2_reg_wf_lasso <-
  workflow() |> 
  add_recipe(cedel2_reg_rec) |> 
  add_model(cedel2_reg_spec_lasso)

# Create tuning grid
cedel2_reg_grid <-
  grid_regular(penalty(), levels = 10)

# Set seed for reproducibility
set.seed(123)

# Create tuning workflow
cedel2_reg_tune <-
  tune_grid(
    cedel2_reg_wf_lasso,
    resamples = vfold_cv(cedel2_reg_train, v = 10),
    grid = cedel2_reg_grid, 
    control = control_resamples(save_pred = TRUE)
  )

# Select best parameter
chosen_penalty <-
  cedel2_reg_tune |>
  select_best(metric = "rmse")

# Update workflow
cedel2_reg_final_wf_lasso <-
  cedel2_reg_wf_lasso |>
  finalize_workflow(chosen_penalty)

cedel2_reg_final_wf_lasso
```
:::

Now we fit this model to the training data and evaluate the performance using cross-validation. The code is seen in @exm-pda-reg-model-spec-tune-fit-evaluate.

::: {#exm-pda-reg-model-spec-tune-fit-evaluate}
```{r}
#| label: pda-reg-model-spec-tune-fit-evaluate

# Cross-validated workflow
cedel2_reg_cv_lasso <-
  cedel2_reg_final_wf_lasso |>
  fit_resamples(
    resamples = vfold_cv(cedel2_reg_train, v = 10),
    control = control_resamples(save_pred = TRUE)
  )

# Collect metrics
cedel2_reg_cv_lasso |> 
  collect_metrics()
```
:::

```{r}
#| label: pda-reg-metrics-lasso-text-block-1
#| echo: false

# Collect metrics
estimates <- 
  cedel2_reg_cv_lasso |>
  collect_metrics() |>
  pull(3)
```

Now, the RMSE estimate is `r format(estimates[1])`. RMSE is expressed in the same units as the outcome variable. In this case, the outcome variable is the placement test score percent. So the RMSE is `r format(estimates[1])` percentage points. The $R^2$ (rsq) is `r format(estimates[2])`. This means that the model explains `r scales::percent(estimates[2])` of the variance in the outcome variable. Taken together, this isn't the best model. 

But how good or bad is it? This is where we can use the null model to compare the model to. The null model is a model that predicts the mean of the outcome variable. We can use the `null_model()` function to create a null model and submit it to cross-validation. The code is seen in @exm-pda-reg-model-spec-tune-fit-evaluate-null.

::: {#exm-pda-reg-model-spec-tune-fit-evaluate-null}
```{r}
#| label: pda-reg-model-spec-tune-fit-evaluate-null

# Create null model
null_model <- 
  null_model() |> 
  set_engine("parsnip") |> 
  set_mode("regression")

# Cross-validate null model
null_cv <- 
  workflow() |> 
  add_recipe(cedel2_reg_rec) |>
  add_model(null_model) |>
  fit_resamples(
    resamples = vfold_cv(cedel2_reg_train, v = 10),
    metrics = metric_set(rmse)
  )

# Collect metrics
null_cv |> 
  collect_metrics()
```
:::

Our the word features perform better than the null model which means that it is picking up on some signal in the data.

Let's visualize the distribution of the predictions and the errors from our word features model to see if there are any patterns of interest. We can use the `collect_predictions()` function to extract the predictions of the cross-validation and plot the true outcome agains the predicted outcome using `ggplot()`, as in @exm-pda-reg-lr-eval-rmse.

::: {#exm-pda-reg-lr-eval-rmse}
```{r}
#| label: fig-pda-reg-lr-eval-rmse
#| fig-cap: "Distribution of the RMSE for the cross-validated linear regression model."
#| fig-height: 4

# Visualize predictions
cedel2_reg_cv_lasso |> 
  collect_predictions() |> 
  ggplot(aes(outcome, .pred, shape = id)) +
  geom_point(alpha = 0.5, position = position_jitter(width = 0.5)) +
  geom_smooth(method = "lm", se = FALSE, linewidth = 0.5) + # trend for each fold
  labs(
    x = "Truth",
    y = "Predicted score",
    shape = "Fold"
  )
```
:::

From this plot, we see data points for each predicted and truth value pair for each of the ten folds. There is a trend line for each fold which shows the linear relationship between the predicted and truth values for each fold. The trend lines are more similar than different, which is a good sign that the model is not wildly overfitting the training data. Looking closer, however, we can see the errors. Some are noticeably distant from the linear trend lines, *i.e.* outliers, in particular for test scores in the higher and lower ranges. There also seems to be a pattern in that the model predicts lower scores for higher scores and higher scores for lower scores. We can see this by the overplotted points in the higher and lower ranges.

If the $R^2$ value is in the ballpark, this means that somewhere around 40% of the variation is not explained by the frequency of the top 1,000 words. This is not surprising, as there are many other factors that contribute to the proficiency level of a text.

We have a model that is performing better than the null model, but it is not performing well enough to be very useful. We will need to update the model specification and/ or the features to try to improve the model fit. Let's start with the model. There are many different model specifications we could try, but we will likely need to use a more complex model specification to capture the complexity that we observed in the errors from the previous model.

Let's try a decision tree model. **Decision trees** are non-linear models that are able to model non-linear relationships and interactions between the features and the outcome and tend to be less influenced by outliers. These are all desirable characteristics. Decision trees, however, can be prone to overfitting

Along the spectrum of model complexity, decision trees are more complex than linear regression models, but less complex than other models such as neural networks. Furthermore, decision trees are interpretable, which is a nice feature for an exploratory-oriented analysis.

To implement a new model in `tidymodels`, we need to create a new model specification and a new workflow. We will use the `decision_tree()` function from the `parsnip` package to create the model specification. The `decision_tree()` function takes no arguments and returns a `decision_tree` object. We create the new model specification in @exm-pda-reg-model-spec-decision-tree.

::: {#exm-pda-reg-model-spec-decision-tree}
```{r}
#| label: pda-reg-model-spec-decision-tree

# Create model specification
cedel2_reg_spec_tree <-
  decision_tree() |> 
  set_engine("rpart") |> 
  set_mode("regression")

cedel2_reg_spec_tree
```
:::

We now have a new model specification. We can now create a new workflow. We will use the same recipe as before, but we will change the model specification to `cedel2_prof_spec_tree`. We add this to a new workflow and fit the workflow to the training data. The code is seen in @exm-pda-reg-model-spec-decision-tree-workflow.

::: {#exm-pda-reg-model-spec-decision-tree-workflow}
```{r}
#| label: pda-reg-model-spec-decision-tree-workflow

# Create workflow
cedel2_reg_wf_tree <-
  workflow() |> 
  add_recipe(cedel2_reg_rec) |> 
  add_model(cedel2_reg_spec_tree)

# Set seed for reproducibility
set.seed(123)

# Cross-validated workflow
cedel2_reg_cv_tree <-
  cedel2_reg_wf_tree |>
  fit_resamples(
    resamples = vfold_cv(cedel2_reg_train, v = 10),
    control = control_resamples(save_pred = TRUE)
  )
```
:::

We can now evaluate the performance of the model using cross-validation. The code is seen in @exm-pda-reg-model-spec-decision-tree-workflow-evaluate.

::: {#exm-pda-reg-model-spec-decision-tree-workflow-evaluate}
```{r}
#| label: pda-reg-model-spec-decision-tree-workflow-evaluate

# Collect metrics
cedel2_reg_cv_tree |> 
  collect_metrics()
```
:::


```{r}
#| label: pda-reg-metrics-tree-text-block-1
#| echo: false

# Collect metrics
estimates <- 
  cedel2_reg_cv_tree |>
  collect_metrics() |>
  pull(3)
```

The metrics for the vanilla decision tree are similar to the regularized linear regression model. The RSME is `r format(estimates[1])` and the $R^2$ is `r format(estimates[2])`. Yet, if we compare the standard error between the two models, we can see that the decision tree model has a larger standard error. This means that the decision tree model is more prone to overfitting the training data.

To minimize overfitting, we can tune hyperparameters of the model. Regularization is one way to reduce overfitting, as we saw with the regularized linear regression model. Another way to reduce overfitting is to reduce the complexity of the model. This can be done by reducing the number of features or by reducing the number of splits in the decision tree, known as **pruning**.

Another approach is to implement a random forest model. A **random forest** is an ensemble model that combines multiple decision trees to make a prediction. A random forest is a type of ensemble model which means that it combines multiple models to make a prediction. In addition to multiple decision trees, random forests also perform random feature selection. This helps to reduce the correlation between the decision trees and thus reduces the variance of the model.

Let's try a random forest model. We will use the `rand_forest()` function from the `parsnip` package to create the model specification. The `rand_forest()` function takes no arguments and returns a `rand_forest` object. We will select the `ranger` engine and add the `importance` argument to ensure that we can extract feature importance if this model proves to be useful. We create the new model specification in @exm-pda-reg-model-spec-random-forest.

::: {#exm-pda-reg-model-spec-random-forest}
```{r}
#| label: pda-reg-model-spec-random-forest

# Create model specification
cedel2_reg_spec_rf <-
  rand_forest() |> 
  set_engine("ranger", importance = "impurity") |> 
  set_mode("regression")

cedel2_reg_spec_rf
```
:::

We can now update the workflow to use the new model specification. The code is seen in @exm-pda-reg-model-spec-random-forest-workflow.

::: {#exm-pda-reg-model-spec-random-forest-workflow}
```{r}
#| label: pda-reg-model-spec-random-forest-workflow

# Update workflow
cedel2_reg_wf_rf <-
  cedel2_reg_wf_tree |> 
  update_model(cedel2_reg_spec_rf)

# Set seed for reproducibility
set.seed(123)

# Cross-validated workflow
cedel2_reg_cv_rf <-
  cedel2_reg_wf_rf |>
  fit_resamples(
    resamples = vfold_cv(cedel2_reg_train, v = 10),
    control = control_resamples(save_pred = TRUE)
  )
```
:::

We can now evaluate the performance of the model using cross-validation. The code is seen in @exm-pda-reg-model-spec-random-forest-workflow-evaluate.

::: {#exm-pda-reg-model-spec-random-forest-workflow-evaluate}
```{r}
#| label: pda-reg-model-spec-random-forest-workflow-evaluate

# Collect metrics
cedel2_reg_cv_rf |> 
  collect_metrics()
```
:::

```{r}
#| label: pda-reg-metrics-rf-text-block-1
#| echo: false

# Collect metrics
estimates <- 
  cedel2_reg_cv_rf |>
  collect_metrics() |>
  pull(3)
```

The random forest model performs better than the decision tree model and the regularized linear regression model. The RSME is `r format(estimates[1])` and the $R^2$ is `r format(estimates[2])`. We also see that the standard error is the lowest of the models we have tried so far. This means that the random forest model is less prone to overfitting the training data.

Before we settle on this model, let's try one more model, a support vector machine (SVM). A **support vector machine** is a supervised machine learning model that can be used for both classification and regression. It is a non-parametric method, which means that it does not make any assumptions about the underlying distribution of the data. It is also a method which can be better suited for high-dimensional data where many values are zero, that is, sparse data, which is often the case with text data.

Let's again specify the model. We will use the `svm_linear()` function from the `parsnip` package to create the model specification and we will use the `LiblineaR` computational engine. We create the new model specification in @exm-pda-reg-model-spec-svm.

::: {#exm-pda-reg-model-spec-svm}
```{r}
#| label: pda-reg-model-spec-svm

# Create model specification
cedel2_reg_spec_svm <-
  svm_linear() |> 
  set_engine("LiblineaR") |> 
  set_mode("regression")

cedel2_reg_spec_svm
```
:::


Now let's update the workflow to use the new model specification. The code is seen in @exm-pda-reg-model-spec-svm-workflow.

::: {#exm-pda-reg-model-spec-svm-workflow}
```{r}
#| label: pda-reg-model-spec-svm-workflow

# Update workflow
cedel2_reg_wf_svm <-
  cedel2_reg_wf_rf |> 
  update_model(cedel2_reg_spec_svm)

# Set seed for reproducibility
set.seed(123)

# Cross-validated workflow
cedel2_reg_cv_svm <-
  cedel2_reg_wf_svm |>
  fit_resamples(
    resamples = vfold_cv(cedel2_reg_train, v = 10),
    control = control_resamples(save_pred = TRUE)
  )
```
:::

Let's evaluate the cross-validation performance of the model. The code is seen in @exm-pda-reg-model-spec-svm-workflow-evaluate.

::: {#exm-pda-reg-model-spec-svm-workflow-evaluate}
```{r}
#| label: pda-reg-model-spec-svm-workflow-evaluate

# Collect metrics
cedel2_reg_cv_svm |> 
  collect_metrics()
```
:::

This SVR is not performing better than the linear regression and random forest models and the standard error is not particularly low. So we will not pursue this model further.

So in summary, we've tried four different model specifications. The regularized linear regression model, the decision tree model, the random forest model, and the support vector machine model. The random forest model performed the best. But there stands to be improvement, but it looks as if we may need to update the features.

In the recipe we have used until this point we have limited the word features to 1,000 and used the raw counts of the words. The rationale for this was that we wanted to limit the number of features to reduce the complexity of the model while still capturing the most important features. But we may have limited the features too much, at least now that we are comparing features between the same population (learners). A potentially more informative feature would be the TF-IDF score. This score is a measure of how important a word is to a document in a collection or corpus. Specifically, it is the product of the term frequency and the inverse document frequency. The more dispersed a feature is across the corpus, the lower the TF-IDF score. The more concentrated a feature is across the corpus, the higher the TF-IDF score, and the more informative the feature is for distinguishing between documents.

So we can use the `step_tfidf()` function to update the recipe. The code is seen in @exm-pda-reg-recipe-tfidf.

::: {#exm-pda-reg-recipe-tfidf}
```{r}
#| label: pda-reg-recipe-tfidf

# Create a recipe
cedel2_reg_rec <- 
  recipe(outcome ~ text, data = cedel2_reg_train) |> 
  step_tokenize(text) |> 
  step_tokenfilter(text, max_tokens = 1000) |>
  step_tfidf(text)

# Preview
cedel2_reg_rec
```
:::

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

Note that the log-transformation is not necessary when using TF-IDF scores as normalization is built into the TF-IDF calculation.
:::

Another change we can explore is to increase the number of features. But how many features should we use? We can turn to the `tune()` function to help us answer this question. Let's add the `tune()` function to the recipe and tune the `max_tokens` hyperparameter. The code is seen in @exm-pda-reg-recipe-tune.

::: {#exm-pda-reg-recipe-tune}
```{r}
#| label: pda-reg-recipe-tune

# Create a recipe
cedel2_reg_rec <- 
  recipe(outcome ~ text, data = cedel2_reg_train) |> 
  step_tokenize(text) |> 
  step_tokenfilter(text, max_tokens = tune()) |>
  step_tfidf(text)

# Preview
cedel2_reg_rec
```
:::

Let's tune the `max_tokens` with the random forest model specification that we created earlier. We will create a tuning workflow, which will tune the `max_tokens` hyperparameter on the training data using cross-validation. The code is seen in @exm-pda-reg-model-spec-random-forest-workflow-tune.

::: {#exm-pda-reg-model-spec-random-forest-workflow-tune}
```{r}
#| label: pda-reg-model-spec-random-forest-workflow-tune

# Create tuning workflow
cedel2_tune_wf <- 
  workflow() |> 
  add_recipe(cedel2_reg_rec) |> 
  add_model(cedel2_reg_spec_rf)

cedel2_tune_wf
```
:::

The `tune_grid()` function takes a workflow, a resampling method, a tuning grid, and a set of metrics. The tuning grid is a set of hyperparameters and their values. The `tune_grid()` function will tune the hyperparameters on the training data using cross-validation. The code is seen in @exm-pda-reg-model-spec-random-forest-workflow-tune-grid. This is a computationally intensive process, as it will train a 10 folds, for each hyperparameter value. Each fold in the random forest is 500 trees, so this will train 5,000 trees for each hyperparameter value! So that is 30,000 trees in total. This code will take some time to run.

::: {#exm-pda-reg-model-spec-random-forest-workflow-tune-grid}
```{r}
#| label: pda-reg-model-spec-random-forest-workflow-tune-grid

# Create tuning grid
cedel2_tune_grid <- 
  grid_regular(max_tokens(range = c(500, 3500)), levels = 5)

cedel2_tune_grid

# Set seed for reproducibility
set.seed(123)

# Create tuning workflow
cedel2_tune <-
  tune_grid(
    cedel2_tune_wf,
    resamples = vfold_cv(cedel2_reg_train, v = 10),
    grid = cedel2_tune_grid, 
    metrics = metric_set(rmse, rsq),
    control = control_resamples(save_pred = TRUE)
  )

cedel2_tune
```
:::

Just as we did for the `penalty` hyperparameter, we can visualize the performance of the model as a function of the `max_tokens` hyperparameter. The code is seen in @exm-pda-reg-model-spec-random-forest-workflow-tune-plot.

::: {#exm-pda-reg-model-spec-random-forest-workflow-tune-plot}
```{r}
#| label: fig-pda-reg-model-spec-random-forest-workflow-tune-plot
#| fig-cap: "Performance of the random forest model as a function of the max_tokens hyperparameter."
#| fig-height: 4

# Visualize tuning results
cedel2_tune |>
  collect_metrics() |>
  ggplot(aes(max_tokens, mean)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(
    breaks = scales::pretty_breaks(n = 10),
    labels = scales::comma
  ) +
  facet_wrap(~ .metric, scales = "free_y", ncol = 1) +
  labs(
    x = "Max tokens"
  )
```
:::

We added a range of maximum token values to the tuning grid to see if there is a point of diminishing returns. We can see that the performance of the model actually appears to improve, not with more tokens, but instead with less. Our lowest value of 500 tokens appears to perform the best. This is interesting, as it suggests that there are a small number of words that are most informative for predicting the placement test score. It also suggests that individual words, on the whole, are not very informative for predicting the placement test score.

We can now select the best hyperparameter value. The code is seen in @exm-pda-reg-model-spec-random-forest-workflow-tune-select.

::: {#exm-pda-reg-model-spec-random-forest-workflow-tune-select}
```{r}
#| label: pda-reg-model-spec-random-forest-workflow-tune-select

# Select best parameter
chosen_max_tokens <-
  cedel2_tune |>
  select_best(metric = "rmse")

# Update workflow
cedel2_reg_final_rf <-
  cedel2_tune_wf |>
  finalize_workflow(chosen_max_tokens)

cedel2_reg_final_rf
```
:::

We can now fit the model to the training data and evaluate the performance using cross-validation. The code is seen in @exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate.

::: {#exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate}
```{r}
#| label: pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate

# Cross-validated workflow
cedel2_reg_cv_rf <-
  cedel2_reg_final_rf |>
  fit_resamples(
    resamples = vfold_cv(cedel2_reg_train, v = 10),
    control = control_resamples(save_pred = TRUE)
  )

# Collect metrics
cedel2_reg_cv_rf |> 
  collect_metrics()
```
:::

We've improved the model, but not very much. We can see that the RMSE is `r format(estimates[1])` and the $R^2$ is `r format(estimates[2])`. The standard error is `r format(estimates[3])`. This is the lowest standard error we have seen so far, but it is still not as low as we would like.

Let's now visualize the distribution of the predictions and the errors from our word features model to see if there are any patterns of interest. We can use the `collect_predictions()` function to extract the predictions of the cross-validation and plot the true outcome agains the predicted outcome using `ggplot()`, as in @exm-pda-reg-lr-eval-rmse.

::: {#exm-pda-reg-lr-eval-rmse}
```{r}
#| label: fig-pda-reg-rf-eval-rmse
#| fig-cap: "Distribution of the RMSE for the cross-validated linear regression model."
#| fig-height: 4

# Visualize predictions
cedel2_reg_cv_rf |> 
  collect_predictions() |> 
  ggplot(aes(outcome, .pred, shape = id)) +
  geom_point(alpha = 0.5, position = position_jitter(width = 0.5)) +
  geom_smooth(method = "lm", se = FALSE) + # trend for each fold
  labs(
    x = "Truth",
    y = "Predicted score",
    shape = "Fold"
  )
```
:::

There appears to be more cohesion in the predictions, overall. The errors however seem visually to be larger for the lower scores. 

At this point we can either consider this model to be good enough or we can try to improve it further. Let's take one more shot at improving the features by including bigrams. To include words (unigrams) and bigrams, we can modify the `step_tokenize()` function in our recipe. We add the `token = "ngrams"` argument and specify the `options` as `list(n = 2, n_min = 1)`. This will create unigrams and bigrams. The code is seen in @exm-pda-reg-recipe-tfidf-bigrams.

::: {#exm-pda-reg-recipe-tfidf-bigrams}
```{r}
#| label: pda-reg-recipe-tfidf-bigrams

# Create a recipe
cedel2_reg_rec <- 
  recipe(outcome ~ text, data = cedel2_reg_train) |> 
  step_tokenize(text, token = "ngrams", options = list(n = 2, n_min = 1)) |> 
  step_tokenfilter(text, max_tokens = tune()) |>
  step_tfidf(text)

# Preview
cedel2_reg_rec
```
:::

Since the features have changed, we won't assume that our tuning of the `max_tokens` is still valid. So we will tune the `max_tokens` hyperparameter again, as in @exm-pda-reg-recipe-tune through @exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate. For the sake of brevity, we will not show the code here. We will simply show the results.

```{r}
#| label: pda-reg-tune-bigrams

# Create workflow
cedel2_reg_wf_rf <-
  workflow() |> 
  add_recipe(cedel2_reg_rec) |> 
  add_model(cedel2_reg_spec_rf)

# Create tuning grid
cedel2_tune_grid <- 
  grid_regular(max_tokens(range = c(500, 3500)), levels = 5)

# Set seed for reproducibility
set.seed(123)

# Create tuning workflow
cedel2_tune <-
  tune_grid(
    cedel2_reg_wf_rf,
    resamples = vfold_cv(cedel2_reg_train, v = 10),
    grid = cedel2_tune_grid, 
    metrics = metric_set(rmse, rsq),
    control = control_resamples(save_pred = TRUE)
  )

# Select best parameter
chosen_max_tokens <-
  cedel2_tune |>
  select_best(metric = "rmse")

# Update workflow
cedel2_reg_final_rf <-
  cedel2_reg_wf_rf |>
  finalize_workflow(chosen_max_tokens)

# Cross-validated workflow
cedel2_reg_cv_rf <-
  cedel2_reg_final_rf |>
  fit_resamples(
    resamples = vfold_cv(cedel2_reg_train, v = 10),
    control = control_resamples(save_pred = TRUE)
  )

# Collect metrics
estimates <- 
  cedel2_reg_cv_rf |> 
  collect_metrics() |> 
  pull(3)
```

The tuning of `max_tokens` selected 2,000 features as the optimal number. The cross-validation of the training dataset produced an RMSE of `r format(estimates[1])` and an $R^2$ of `r format(estimates[2])`. The standard error is `r format(estimates[3])`. The upshot is that by including bigrams we have not improved, or worsened, the model. Including the unigrams and bigrams together may be more informative for the exploration of the features. 

We could continue to try to improve the model, but at this point we have a model that is performing better than the null model and is performing better than the other models we have tried. So we will consider this model to be good enough.

Let's now fit the 1-2 gram model to the testing data and evaluate the performance on the testing set. The code is seen in @exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate-test.

<!-- Step 6: Interpret -->

::: {#exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate-test}
```{r}
#| label: pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate-test

# Fit model on training data
cedel2_reg_final_rf_fit <- 
  cedel2_reg_final_rf |>
  fit(cedel2_reg_train)

# Predict on testing data
cedel2_reg_final_rf_pred <-
  bind_cols(
    cedel2_reg_test,
    predict(cedel2_reg_final_rf_fit, cedel2_reg_test)
  )

# Evaluate performance
cedel2_reg_final_rf_pred |> 
  metrics(truth = outcome, estimate = .pred)
```
:::

We can now visualize the feature importance of the model. The code is seen in @exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate-test-feature-importance.

::: {#exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate-test-feature-importance}
```{r}
#| label: fig-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate-test-feature-importance
#| fig-cap: "Feature importance of the random forest model."
#| fig-height: 4

# Extract predictions
cedel2_reg_final_rf_fit |> 
  vip::vi(scale = TRUE)  |> 
  mutate(Variable = str_replace(Variable, "^tfidf_text_", "")) |> 
  slice_max(Importance, n = 20) |>
  # reorder variables by importance
  ggplot(aes(reorder(Variable, Importance), Importance)) +
  geom_point() +
  coord_flip() +
  labs(
    x = "Feature",
    y = "Importance"
  ) 
```
:::


## Summary {#pda-summary}

In this chapter we have learned about supervised machine learning. We have learned about the different types of supervised machine learning methods and how they can be used to predict and classify. We have also learned about the different types of data structures that are used in supervised machine learning. Finally, we have learned about the different types of evaluation metrics that are used to evaluate the performance of supervised machine learning models.

## Activities {.unnumbered}

::: {.callout}
**{{< fa regular file-code >}} Recipe**

<!-- Understand, apply, and analyze verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

**What**: [Predictive models: prep, train, test, and evaluate](https://qtalr.github.io/qtalrkit/articles/recipe-8.html)\
**How**: Read Recipe 9 and participate in the Hypothes.is online social annotation.\
**Why**: To illustrate some common coding strategies for preparing datasets for inferential data analysis, as well as the steps conduct descriptive assessment, statistical interrogation, and evaluation and reporting of results.
:::

::: {.callout}
**{{< fa flask >}} Lab**

<!-- Analyze, evaluate, and create verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

**What**: [Predictive Data Analysis](https://github.com/qtalr/lab-9)\
**How**: Clone, fork, and complete the steps in Lab 10.\
**Why**: To gain experience working with coding strategies to prepare, feature engineer, train and test a predictive model, and evaluate results from a predictive data analysis, practice transforming datasets into new object formats and visualizing relationships, and implement organizational strategies for organizing and reporting results in a reproducible fashion.
:::

## Questions {.unnumbered}

::: {.callout}
{{< fa wrench >}} **Conceptual questions**

1. What is the difference between a numeric and a categorical variable?
2. What is the difference between a regression and a classification model?
3. What is the difference between a training set and a testing set?
4. What is the difference between a hyperparameter and a parameter?
5. What is the difference between a supervised and an unsupervised machine learning model?
6. What advantages and disadvantages do supervised machine learning models have over traditional methods of text analysis?
7. What are some potential applications of supervised machine learning in linguistics?

:::

::: {.callout}
{{< fa wrench >}} **Applied questions**

1. Write a program to build a classification model which uses a set of collected text features to predict a target variable.
2. Use the classification model to classify a series of documents and assess the accuracy of the model.
3. Develop a regression model which uses text features to predict a numeric target variable.
4. Create a text mining application to analyze a large body of text and discover correlations between variables.
5. Use a clustering algorithm to discover clusters in a large dataset, and create a visualization to present the identified clusters.
6. Analyze the structure of a text corpus and identify patterns in word usage and feature distributions.
7. Build a predictive model using text as an input and binary or categorical outcomes as the target.
8. Develop a natural language processing application which classifies text into predefined categories using a supervised learning algorithm.
9. Use a supervised learning algorithm to build a predictive model which classifies a set of unseen texts into predefined categories.
10. Develop a web application which allows users to easily explore a set of text documents, visualize the content of the documents, and generate predictive models from the text.
:::
