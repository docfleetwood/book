---
execute: 
  echo: true
---

# Transform datasets {#sec-transform-datasets}

```{r}
#| label: setup-options
#| child: "_common.qmd"
#| cache: false
```

::: {.callout-caution title="Caution"}
{{< fa route >}} In progress...
:::

<!--

Content:

Exercises:

- [ ] add concept questions to Activities
- [ ] add exercises to Activities
- [ ] add thought questions/ case studies to prose sections

Formatting:

- [ ] Add a description of the importance of the unit of analysis and unit of observation in the transformation process.
  - In the curation process, the original data and utility of the dataset is priority. In the transformation process, the focus is on the analysis and the unit of analysis and unit of observation are the primary considerations.
- [ ] Point out that some units of observation will require that the data have more textual context (e.g. syntactic parsing) than others (e.g. word frequencies). It is important that the presentation of the transformation steps are no necessarily in the order in which they are applied.

-->

> Nothing is lost. Everything is transformed.
>
> --- Michael Ende, The Neverending Story

::: {.callout}
**{{< fa regular list-alt >}} Outcomes**

<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- [ ] update to outcomes -->

- What is the role of data transformation in a text analysis project?
- What are the general processes for preparing datasets for analysis?
- How do each of these general processes transform datasets?
:::

```{r}
#| label: transform-data-packages
#| echo: false
#| message: false

# [ ] Load packages as part of the chapter

pacman::p_load(tidytext, cleanNLP) # load packages
```

<!-- 
- Transformation: goals
  - Prepare curated dataset for analysis
  - Manipulate dataset row-wise and/ or column-wise
  - Focus on particular analysis needs: unit of analysis, unit of observation, operationalization of variables, etc.
-->

In this chapter, we will focus on transforming a curated dataset to refine and possibly expand its relational characteristics to align with our research. The transformation process is divided into four categories: text normalization, text tokenization, variable recoding, variable generation, and observation/ variable merging. These categories are not sequential but may occur in any order based on the researcher's evaluation of the dataset characteristics and the desired outcome.

It is sometimes necessary to create several transformed datasets from one curated dataset in cases where there are multiple analyses to be performed. This why we start with a curated dataset rather than directly deriving a transformed dataset from the original data. It allows for various transformation methods to produce different formats for different analyses.

::: {.callout}
**{{< fa terminal >}} Swirl lesson**

<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- 
- [ ] Update lesson name, 
- [ ] update lesson purpose
-->

**What**: [Reshape dataset rows, Reshape dataset columns](https://github.com/qtalr/lessons)\
**How**: In the R Console pane load `swirl`, run `swirl()`, and follow prompts to select the lesson.\
**Why**: ...
:::

<!-- 
New outline: 2023-08-12


(See 'Understanding data' chapter for background @sec-ud-transformation)

text normalization, text tokenization, variable recoding, variable generation, and observation/ variable merging.

- Normalization (column-wise)
  - Artifact removal
- Recoding (column-wise)
  - Classify
  - Derive
- Tokenization (row-wise)
  - Words, n-grams, sentences
- Generation (row-wise and column-wise)
  - Lexical annotation
  - Syntactic annotation
- Merging (row-wise/ column-wise)
  - Joining datasets

---

- Summary
- Activities

-->


## Normalization {#sec-td-normalization}

<!--  

column-wise

-->

The process of normalizing datasets in essence is to sanitize the values of variable or set of variables such that there are no artifacts that will contaminate subsequent processing. It may be the case that non-linguistic metadata may require normalization but more often than not linguistic information is the most common target for normalization as text often includes artifacts from the acquisition process which will not be desired in the analysis. 

**Europarl Corpus**

The data dictionary for the Europarl Corpus appears in @tbl-td-europarl-dd.

```{r}
#| label: tbl-td-europarl-dd
#| tbl-cap: "Data dictionary for the Europarl Corpus."
#| message: false

read_csv(file = "data/cd-europarl_curated_dd.csv") |> 
  kable() |> 
  kable_styling()
```

```{r}
#| label: td-europarl-read-run
#| message: false

europarl_curated_tbl <- 
  read_csv(file = "data/europarl_curated.csv")
```



Consider the curated Europarl Corpus dataset. I will read in the dataset. Since the dataset is quite large, I have also subsetted the dataset keeping only the first 1,000 observations for each of value of `type` for demonstration purposes.

```{r}
#| eval: false
#| label: td-europarl-normalize-read-subset-show

europarl <- read_csv(file = "../data/derived/europarl/europarl_curated.csv") |> # read curated dataset
  filter(sentence_id < 1001) # keep first 1000 observations for each type

glimpse(europarl)
```

```{r}
#| eval: false
#| label: td-europarl-normalize-read-subset-run
#| echo: false

europarl <- read_csv(file = "data/transform-datasets/data/derived/europarl/europarl_curated.csv") |> # read curated dataset
  filter(sentence_id < 1001) # keep first 1000 observations for each type

glimpse(europarl)
```

Simply looking at the first 14 lines of this dataset, we can see that if our goal is to work with the transcribed ('Source') and translated ('Target') language, there are lines which do not appear to be of interest.

```{r}
#| eval: false
#| label: tbl-td-europarl-preview-1
#| tbl-cap: "europarl Corpus curated dataset preview."
#| echo: false

europarl |>
  slice_head(n = 14) |>
  knitr::kable(booktabs = TRUE)
```

`sentence_id` 1 appears to be title and `sentence_id` 7 reflects description of the parliamentary session. Both of these are artifacts that we would like to remove from the dataset. 

To remove these lines we can turn to the programming strategies we've previously worked with. Namely we will use `filter()` to filter observations in combination with `str_detect()` to detect matches for some pattern that is indicative of these lines that we want to remove and not of the other lines that we want to keep. 

Before we remove any lines, let's try craft a search pattern to identify these lines, and exclude the lines we will want to keep. Condition one is lines which start with an opening parenthesis `(`. Condition two is lines that do not end in standard sentence punctuation (`.`, `!`, or `?`). I've added both conditions to one `filter()` using the logical *OR* operator (`|`)  to ensure that either condition is matched in the output. 

```{r}
#| eval: false
#| label: tbl-td-europarl-search-non-speech
#| tbl-cap: "Non-speech lines in the europarl dataset."

# Identify non-speech lines
europarl |>
  filter(str_detect(sentence, "^\\(") | str_detect(sentence, "[^.!?]$")) |> # filter lines that detect a match for either condition 1 or 2
  slice_sample(n = 10) |> # random sample of 10 observations
  knitr::kable(booktabs = TRUE)
```

Since this search appears to match lines that we do not want to preserve, let's move now to eliminate these lines from the dataset. To do this we will use the same regular expression patterns, but now each condition will have it's own `filter()` call and the `str_detect()` will be negated with a prefixed `!`.

```{r}
#| eval: false
#| label: td-europarl-filter-non-speech

europarl <-
  europarl |> # dataset
  filter(!str_detect(sentence, pattern = "^\\(")) |> # remove lines starting with (
  filter(!str_detect(sentence, pattern = "[^.!?]$")) # remove lines not ending in ., !, or ?
```

Let's look at the first 14 lines again, now that we have eliminated these artifacts. 

```{r}
#| eval: false
#| label: tbl-td-europarl-preview-2
#| tbl-cap: "europarl Corpus non-speech lines removed."
#| echo: false

europarl |>
  slice_head(n = 14) |>
  knitr::kable(booktabs = TRUE)
```

One further issue that we may want to resolve concerns the fact that there are whitespaces between possessive forms (i.e. "minuteâ€™ s silence"). In this case we can employ `str_replace_all()` inside the `mutate()` function to overwrite the `sentence` values that match an apostrophe `'` with whitespace (`\\s`) before `s`.

```{r}
#| eval: false
#| label: td-europarl-remove-whitespace

europarl <-
  europarl |> # dataset
  mutate(sentence = str_replace_all(
    string = sentence,
    pattern = "'\\ss",
    replacement = "'s"
  )) # replace ' s with `s
```

Now we have normalized text in the `sentence` column in the europarl dataset. 

## Recoding {#sec-td-recoding}

<!--  

column-wise

Data:
  - SWDA
    - education (classify)
    - birth_year (derive)
    - utterance_text (extract, disfluencies)
-->

<!-- Consider:

The process of recoding aims to _recast_ the values of a variable or set of variables to a new variable or set of variables to enable more direct access. This may include extracting values from a variable, stemming or lemmatization of words, tokenization of linguistic forms (words, ngrams, sentences, etc.), calculating the lengths of linguistic units, calculating type-token ratios, syntactic complexity, and/ or removing variables that will not be used in the analysis, etc.
-->

Normalizing text can be seen as an extension of dataset curation to some extent in that the structure of the dataset is maintained. In the Europarl case, we saw this to be true. In the case of recoding, and other transformational steps, the aim will be to modify the dataset structure either by rows, columns, or both. Recoding processes can be characterized by the creation of structural changes which are derived from values in variables effectively recasting values as new variables to enable more direct access in our analyses.

**Switchboard Dialogue Act Corpus**

<!--  

Recode: 

- education (classify)
- birth_year (derive)
- utterance_text (extract, disfluencies) (derive)

-->

```{r}
#| label: tr-swda-dataset-read-run
#| include: false

swda_curated_tbl <- 
  read_csv("data/swda_curated.csv") |> 
  select(
    doc_id,
    speaker_id,
    sex,
    education,
    birth_year,
    utterance_id,
    damsl_tag,
    utterance_text
  )


swda_curated_dd <- 
  create_data_dictionary(
  data = swda_curated_tbl,
  file_path = "data/tr-swda_curated_dd.csv",
  model = "gpt-3.5-turbo"
)
```

Data dictionary. 

```{r}
#| label: tbl-tr-swda-curated-dd
#| tbl-cap: "Data dictionary for the Switchboard Dialogue Act Corpus."
#| message: false
#| echo: false

# SWDA data dictionary ----
read_csv("data/tr-swda_curated_dd.csv") |>
  kable() |>
  kable_styling()
```

Read in the curated dataset. 

```{r}
#| label: tr-swda-dataset-read-show
#| eval: false

swda_curated_tbl <- 
  read_csv("data/derived/swda/swda_curated.csv")
```

Preview the first 10 lines of the dataset. 

```{r}
#| label: tr-swda-curated-preview

swda_curated_tbl |>
  slice_head(n = 10)
```

---


The Switchboard Dialogue Act Corpus dataset that was //FIXME curated in the previous chapter contains a number of variables describing conversations between speakers of American English. 

Let's read in this dataset and take a closer look.

```{r}
#| eval: false
#| label: td-sdac-read-show

sdac <- read_csv(file = "../data/derived/sdac/sdac_curated.csv") # read curated dataset
```

```{r}
#| eval: false
#| label: td-sdac-read-run
#| echo: false

sdac <- read_csv(file = "data/transform-datasets/data/derived/sdac/sdac_curated.csv") # read curated dataset
```

Among a number of metadata variables, curated dataset includes the `utterance_text` column which contains dialogue from the conversations interleaved with a [disfluency annotation scheme](https://staff.fnwi.uva.nl/r.fernandezrovira/teaching/DM-materials/DFL-book.pdf). 

```{r}
#| eval: false
#| label: tbl-td-sdac-preview-curated-dataset
#| tbl-cap: "20 randomly sampled lines of the SDAC curated dataset."
#| echo: false

sdac |>
  slice_sample(n = 20) |>
  kable(booktabs = TRUE)
```

Let's drop a few variables from our dataset to rein in our focus. I will keep the `doc_id`, `speaker_id`, and `utterance_text`. 

```{r}
#| eval: false
#| label: td-sdac-simplified

sdac_simplified <-
  sdac |> # dataset
  select(doc_id, speaker_id, utterance_text) # columns to retain
```

```{r}
#| eval: false
#| label: tbl-td-sdac-simple-preview
#| tbl-cap: "First 10 lines of the simplified SDAC curated dataset."
#| echo: false

sdac_simplified |>
  slice_head(n = 10) |>
  kable(booktabs = TRUE)
```

In this disfluency annotation system, there are various conventions used for non-sentence elements. If say, for example, a researcher were to be interested in understanding the use of filled pauses ('uh' or 'uh'), the aim would be to identify those lines where the `{F ...}` annotation is used around the utterances 'uh' and 'um'.  

To do this we turn to the `str_count()` function. This function will count the number of matches found for a pattern. We can use a regular expression to identify the pattern of interest which is all the instances of `{F` followed by either `uh` or `um`. Since the disfluencies may start an utterance, and therefore be capitalized we need to formulate a regular expression which allows for either `U` or `u` for each disfluency type. The result from each disfluency match will be added to a new column. To create a new column we will wrap each `str_count()` with `mutate()` and give the new column a meaningful name. In this case I've opted for `uh` and `um`. 

```{r}
#| eval: false
#| label: td-sdac-count-disfluencies

sdac_disfluencies <-
  sdac_simplified |> # dataset
  mutate(uh = str_count(utterance_text, "\\{F [Uu]h")) |> # match {F Uh or {F uh}
  mutate(um = str_count(utterance_text, "\\{F [Uu]m")) # match {F Um or {F um}
```

```{r}
#| eval: false
#| label: tbl-td-sdac-count-disfluencies-show
#| tbl-cap: "First 20 lines of SDAC dataset with counts for the disfluencies 'uh' and 'um'."
#| echo: false

sdac_disfluencies |> # dataset
  slice_head(n = 20) |>
  kable(booktabs = TRUE)
```

Now we have two new columns, `uh` and `um` which indicate how many times the relevant pattern was matched for a given utterance. By choosing to focus on disfluencies, however, we have made a decision to change the unit of observation from the utterance to the use of filled pauses (`uh` and `um`). This means that as the dataset stands, it is not in tidy format --where each observation corresponds to the observational unit. When datasets are misaligned in this particular way, there are in what is known as 'wide' format. What we want to do, then, is to restructure our dataset such that each row corresponds to the unit of observation --in this case each filled pause type. 

To convert our current (wide) dataset to one where each filler type is listed and the counts are measured for each utterance we turn to the `pivot_longer()` function. This function creates two new columns, one in which the column names are listed and one for the values for each of the column names.  

```{r}
#| eval: false
#| label: td-sdac-disfluencies-longer

sdac_disfluencies <-
  sdac_disfluencies |> # dataset
  pivot_longer(
    cols = c("uh", "um"), # columns to convert
    names_to = "filler", # column for the column names (i.e. filler types)
    values_to = "count"
  ) # column for the column values (i.e. counts)
```

```{r}
#| eval: false
#| label: tbl-td-sdac-count-disfluencies-longer-show
#| tbl-cap: "First 20 lines of SDAC dataset with tidy format for `fillers` as the unit of observation."
#| echo: false

sdac_disfluencies |> # dataset
  slice_head(n = 20) |>
  kable(booktabs = TRUE)
```


## Tokenization {#sec-td-tokenization}

<!--  

row-wise

Data: 

- CABNC (expand)
  - tidytext::unnest_tokens()
- PELIC tokens (contract)
  - stringr::str_c() [w/ collapse = " "]

-->


The CABNC Corpus data dictionary. 

```{r}
#| label: tbl-td-cabnc-dd
#| tbl-cap: "Data dictionary for the CABNC Corpus."
#| message: false

# [ ] filter the data dictionary to only include the variables of interest

read_csv(file = "data/cd-cabnc_curated_dd.csv") |> 
  kable() |> 
  kable_styling()
```


Read the dataset. 
```{r}
#| label: td-cabnc-read-run
#| include: false
#| message: false

cabnc_curated_tbl <- 
  read_csv(file = "data/cabnc_curated.csv")
```

```{r}
#| label: td-cabnc-read-show
#| eval: false

cabnc_curated_tbl <- 
  read_csv(file = "../data/derived/cabnc_curated.csv")
```


<!-- [ ] Add PELIC data dictionary and read dataset. -->



## Generatation {#sec-td-generation}

<!--  

row- and column-wise

Data: 

- Europarl
  - English
  - Spanish

- Lexical annotation
  - POS, lemma, etc.
- Syntax annotation
  - Dependency parsing

Mention `rsyntax` package for extracting/ recoding patterns from syntactic annotations.

-->

The process of generating a dataset involves the addition of information to a dataset. Whereas recoding involves the transformation of information that is already explicit in a dataset, generation involves the transformation of information that is implicit in a dataset.

The most common type of operation involved in the generation process is the addition of linguistic annotation. This process can be accomplished manually by a researcher or research team or automatically through the use of pre-trained linguistic resources and/ or software. Ideally the annotation of linguistic information can be conducted automatically. 

There are important considerations, however, that need to be taken into account when choosing whether linguistic annotation can be conducted automatically. First and foremost has to do with the type of annotation desired. Information such as part of speech (grammatical category) and morpho-syntactic information are the the most common types of linguistic annotation that can be conducted automatically. Second the degree to which the resource that will be used to annotate the linguistic information is aligned with the language variety and/or register is also a key consideration. As noted, automatic linguistic annotation methods are contingent on pre-trained resources. The language and language variety used to develop these resources may not be available for the language under investigation, or if it does, the language variety and/ or register may not align. The degree to which a resource does not align with the linguistic information targeted for annotation is directly related to the quality of the final annotations. To be clear, no annotation method, whether manual or automatic is guaranteed to be perfectly accurate. 

Let's take a look at annotation some of the language from the Europarl dataset we normalized. 

```{r}
#| eval: false
#| label: tbl-td-europarl-en-preview
#| tbl-cap: "First 10 lines in English from the normalized SDAC dataset."

europarl |>
  filter(type == "Target") |>
  slice_head(n = 10) |>
  kable(booktabs = TRUE)
```

We will use the cleanNLP package to do our linguistic annotation. The annotation process depends on the pre-trained language models. There is [a list of the models available to access](https://github.com/bnosac/udpipe#pre-trained-models). The `load_model_udpipe()` custom function below downloads the specified language model and initialized the `udpipe` engine (`cnlp_init_udpipe()`) for conducting annotations. 

```{r}
#| eval: false
#| label: td-functions-load-udpipe-model

load_model_udpipe <- function(model_lang) {
  # Function
  # Download and load the specified udpipe language model
  
  cnlp_init_udpipe(model_lang) # to download the model, if not downloaded
base_path <- system.file("extdata", package = "cleanNLP") # get the base path
  model_name <- # extract the model_name
    base_path |> # extract the base path
    dir() |> # get the directory
    stringr::str_subset(pattern = paste0("^", model_lang)) # extract the name of the model
  
  model_path <- udpipe::udpipe_load_model(file = file.path(base_path, model_name, fsep = "/")) # create the path to the downloaded model stored on disk
    return(model_path)
}
```


In a test case, let's load the 'english' model to annotate a sentence line from the europarl dataset to illustrate the basic workflow. 

```{r}
#| eval: false
#| label: td-generation-europarl-en-example

eng_model <- load_model_udpipe("english") # load and initialize the language model, 'english' in this case.

eng_annotation <-
  europarl |> # dataset
  filter(type == "Target" & sentence_id == 6) |> # select English and sentence_id 6
  cnlp_annotate(
    text_name = "sentence", # input text (sentence)
    doc_name = "sentence_id"
  ) # specify the grouping column (sentence_id)

glimpse(eng_annotation) # preview structure
```

We see that the structure returned by the `cnlp_annotate()` function is a list. This list contains two data frames (tibbles). One for the tokens (and there annotation information) and the document (the metadata information). We can inspect the annotation characteristics for this one sentence by targetting the `$tokens` data frame. Let's take a look at the linguistic annotation information returned. 

```{r}
#| eval: false
#| label: tbl-td-generation-test-annotation-english
#| tbl-cap: "Annotation information for a single English sentence from the europarl dataset."
#| echo: false

eng_annotation$token |>
  kable(booktabs = TRUE)
```

There is quite a bit of information which is returned from `cnlp_annotate()`. First note that the input sentence has been tokenized by word. Each token includes the token, lemma, part of speech (`upos` and `xpos`), morphological features (`feats`), and syntactic relationships (`tid_source` and `relation`). It is also key to note that the `doc_id`, `sid` and `tid` maintain the relational attributes from the original dataset --and therefore maintains our annotated dataset in tidy format.

Let's now annotate the same sentence from the europarl corpus for the Source ('Spanish') and note the similarities and differences.

```{r}
#| eval: false
#| label: td-generation-europarl-es-example

spa_model <- load_model_udpipe("spanish") # load and initialize the language model, 'spanish' in this case.

spa_annotation <-
  europarl |> # dataset
  filter(type == "Source" & sentence_id == 6) |> # select Spanish and sentence_id 6
  cnlp_annotate(
    text_name = "sentence", # input text (sentence)
    doc_name = "sentence_id"
  ) # specify the grouping column (sentence_id)
```

```{r}
#| eval: false
#| label: tbl-td-generation-test-annotation-spanish
#| tbl-cap: "Annotation information for a single Spanish sentence from the europarl dataset."
#| echo: false

spa_annotation$token |>
  kable(booktabs = TRUE)
```

For the Spanish version of this sentence, we see the same variables. However, the `feats` variable has morphological information which is specific to Spanish --notably gender and mood. 

::: {.callout}
**{{< fa exclamation-triangle >}} Warning**

The `rsyntax` package [@R-rsyntax] can be used to recode and extract patterns from the output from automatic linguistic annotations using cleanNLP. [See the documentation for more information](https://github.com/vanatteveldt/rsyntax).
:::

<!-- Consider:


- europarl Corpus, syntactic annotation

- Syntactic parsing for last.fm music lyrics, phrasal verbs (compound:prt) a la [@Akbary2018], with automated approach
- SOTU Corpus, frequency weighting and scaling for clustering algorithm?
- ...


The process of generation aims to _augment_ a variable or set of variables. In essence this aims to make implicit attributes explicit to that they are directly accessible. This often targeted at the automatic generation of linguistic annotations such as grammatical category (part-of-speech) or syntactic structure. 


-->

## Merging {#sec-td-merging}

<!--

Data: 

- CEDEL2 (row-wise)
  - Natives
  - Learners

- PEDC (column-wise)
  - Wordbank: word-level `category`` and `item_kind`
  - eLexicon: word-level word frequency measures

Consider: 

- Linguistic information:
  - Sentiment lexicons
  - Stopword lists
  - MRC word information to merge
  - English Lexicon project RT times: https://elexicon.wustl.edu/index.html
  - `lingtypology` package: ENNTT country/ language properties (?)

- Non-linguistic information:
  - Demographic information
  - Geographic information
  - Time information
  - ...

---

Useful language: 

Consider the `get_sentiments()` function which returns words which have been classified as 'positive'- or 'negative'-biased, if the lexicon is set to 'bing' [@Hu2004]. This function can be used to generate a new variable which can be used to measure the sentiment of a text. 

-->

One final class of transformations that can be applied to curated datasets to enhance their informativeness for a research project is the process of merging two or more datasets. To merge datasets it is required that the datasets share one or more attributes. With a common attribute two datasets can be joined to coordinate the attributes of one dataset with the other effectively adding attributes and one dataset with extended information. Another approach is to join datasets with the goal of filtering one of the datasets given the matching attribute. 

Let's see this in practice. 

<!-- ADD.... -->

We can also merge datasets that we generate in our analysis or that we import from other sources. This can be useful when there are cases in which a corpus has associated metadata that is contained in files separate from the corpus itself. This is the case for the Switchboard Dialogue Act Corpus. 

Our existing, disfluency recoded, version includes the following variables. 

```{r}
#| eval: false
#| label: td-sdac-disfluencies

sdac_disfluencies |> # dataset
  slice_head(n = 10) # preview first 10 observations
```

The [online documentation page](https://catalog.ldc.upenn.edu/docs/LDC97S62/) provides a key file `caller_tab.csv` which contains speaker metadata information. Included in this `.csv` file is a column `caller_no` which contains the `speaker_id` we currently have in the `sdac_disfluencies` dataset. Let's read this file into our R session renaming `caller_no` to `speaker_id` to prepare to join these datasets. 

```{r}
#| eval: false
#| label: td-sdac-meta-read

sdac_speaker_meta <-
  read_csv(
    file = "https://catalog.ldc.upenn.edu/docs/LDC97S62/caller_tab.csv",
    col_names = c(
      "speaker_id", # changed from `caller_no`
      "pin",
      "target",
      "sex",
      "birth_year",
      "dialect_area",
      "education",
      "ti",
      "payment_type",
      "amt_pd",
      "con",
      "remarks",
      "calls_deleted",
      "speaker_partition"
    )
  )

glimpse(sdac_speaker_meta)
```

Now to join the `sdac_disfluencies` and `sdac_speaker_meta`. Let's turn to `left_join()` again as we want to retain all the observations (rows) from `sdac_disfluencies` and add the columns for `sdac_speaker_meta` where the `speaker_id` column values match. 

```{r}
#| eval: false
#| label: td-sdac-join-metadata

sdac_disfluencies <-
  left_join(sdac_disfluencies, sdac_speaker_meta) # join by ``speaker_id`

glimpse(sdac_disfluencies)
```

Now there are some metadata columns we may want to keep and others we may want to drop as they may not be of importance for our analysis. I'm going to assume that we want to keep `sex`, `birth_year`, `dialect_area`, and `education` and drop the rest. 

```{r}
#| eval: false
#| label: td-sdac-disfluencies-subset-cols

sdac_disfluencies <-
  sdac_disfluencies |> # dataset
  select(doc_id:count, sex:education) # subset key columns
```

```{r}
#| eval: false
#| label: tbl-td-sdac-disfluencies-meta-preview
#| tbl-cap: "First 10 observations for the `sdac_disfluencies` dataset with speaker metadata."
#| echo: false

sdac_disfluencies |> # dataset
  slice_head(n = 10) |> # first 10 observations
  kable(booktabs = TRUE)
```

```{r}
#| eval: false
#| label: td-sdac-disfluencies-write
#| include: false

write_csv(
  sdac_disfluencies, 
  file = "data/transform-datasets/data/derived/sdac/sdac_disfluencies.csv")
```

## Summary {-}

In this chapter we covered the process of transforming datasets. The goal is to manipulate the curated dataset to make it align better for analysis. 
There are four general types of transformation steps: normalization, recoding, tokenization, generation, and merging. In any given research project some or all of these steps will be employed --but not necessarily in the order presented in this chapter. Furthermore, there may also be various datasets generated in at this stage each with a distinct analysis focus in mind. In any case, it is important to write these datasets to disk and to document them according to reproducible research principles. 

This chapter concludes the section on data/ dataset preparation. The next section we turn to analyzing datasets. This is the stage where we interrogate the datasets to derive knowledge and insight either through exploratory, predictive, or inferential methods.

## Activities {.unnumbered}

::: {.callout}
**{{< fa regular file-code >}} Recipe**

<!-- Understand, apply, and analyze verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- [ ] update recipe -->

**What**: [Dataset manipulation: tokenization and joining datasets](https://lin380.github.io/tadr/articles/recipe_8.html)\
**How**: Read Recipe 8 and participate in the Hypothes.is online social annotation.\
**Why**: To work with to primary types of transformations, tokenization and joins. Tokenization is the process of recasting textual units as smaller textual units. The process of joining datasets aims to incorporate other datasets to augment or filter the dataset of interest.
:::

::: {.callout}
**{{< fa flask >}} Lab**

<!-- Analyze, evaluate, and create verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- [ ] update lab -->

**What**: [Dataset manipulation: tokenization and joining datasets](https://github.com/lin380/lab_8)\
**How**: Clone, fork, and complete the steps in Lab 8.\
**Why**: To gain experience working with coding strategies for transforming datasets using tidyverse functions and regular expressions, practice reading/ writing data from/ to disk, and implement organizational strategies for organizing and documenting a dataset in reproducible fashion.
:::

## Questions {.unnumbered}

::: {.callout}
{{< fa wrench >}} **Conceptual questions**

1. ...
2. ...

:::

::: {.callout}
{{< fa wrench >}} **Technical questions**

1. ...
2. ...
:::
