---
execute: 
  echo: true
---

# Transform datasets {#sec-transform-datasets}

```{r}
#| label: setup-options
#| child: "_common.qmd"
#| cache: false
```

::: {.callout-caution title="Caution"}
{{< fa route >}} In progress...
:::

<!--

Content:

- [x] Add normalization section
- [x] Add recoding section
- [ ] Add tokenization section
- [ ] Add generation section
- [ ] Add merging section

Exercises:

- [ ] add concept questions to Activities
- [ ] add exercises to Activities
- [ ] add thought questions/ case studies to prose sections

Formatting:

- [ ] Add a description of the importance of the unit of analysis and unit of observation in the transformation process.
  - In the curation process, the original data and utility of the dataset is priority. In the transformation process, the focus is on the analysis and the unit of analysis and unit of observation are the primary considerations.
- [ ] Point out that some units of observation will require that the data have more textual context (e.g. syntactic parsing) than others (e.g. word frequencies). It is important that the presentation of the transformation steps are no necessarily in the order in which they are applied.

-->

> Nothing is lost. Everything is transformed.
>
> --- Michael Ende, The Neverending Story

::: {.callout}
**{{< fa regular list-alt >}} Outcomes**

<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- [ ] update to outcomes -->

- What is the role of data transformation in a text analysis project?
- What are the general processes for preparing datasets for analysis?
- How do each of these general processes transform datasets?
:::

```{r}
#| label: transform-data-packages
#| echo: false
#| message: false

```

<!-- 
- Transformation: goals
  - Prepare curated dataset for analysis
  - Manipulate dataset row-wise and/ or column-wise
  - Focus on particular analysis needs: unit of analysis, unit of observation, operationalization of variables, etc.
-->

In this chapter, we will focus on transforming a curated dataset to refine and possibly expand its relational characteristics to align with our research. The transformation process is divided into four categories: text normalization, variable recoding, text tokenization, variable generation, and observation/ variable merging. These categories are not sequential but may occur in any order based on the researcher's evaluation of the dataset characteristics and the desired outcome.

It is sometimes necessary to create several transformed datasets from one curated dataset in cases where there are multiple analyses to be performed. This why we start with a curated dataset rather than directly deriving a transformed dataset from the original data. It allows for various transformation methods to produce different formats for different analyses.

::: {.callout}
**{{< fa terminal >}} Swirl lesson**

<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- 
- [ ] Update lesson name, 
- [ ] update lesson purpose
-->

**What**: [Reshape dataset rows, Reshape dataset columns](https://github.com/qtalr/lessons)\
**How**: In the R Console pane load `swirl`, run `swirl()`, and follow prompts to select the lesson.\
**Why**: ...
:::

<!-- 

- Normalization (column-wise):  to standardize or simplify the text to reduce variability
  - Artifact removal
- Recoding (column-wise)
  - Classify
  - Derive
- Tokenization (row-wise): to segment the text into meaningful components
  - Words, n-grams, sentences
- Generation (row-wise and column-wise)
  - Lexical annotation
  - Syntactic annotation
- Merging (row-wise/ column-wise)
  - Joining datasets

---

- Summary
- Activities

-->


## Normalization {#sec-td-normalization}

<!--  

column-wise: 

- Standardize text and remove unwanted variation
  - Artifact removal
  - Case conversion (lowercasing)
  - Punctuation and extraneous character removal
  - Adjustment of forms (e.g. contractions, numbers, abbreviations,  )

- Data:
  - Europarl

- R: 
  - stringr::str_replace_all()
  - stringr::str_remove_all()
-->

The process of normalizing datasets in essence is to sanitize the values of variable or set of variables such that there are no artifacts that will contaminate subsequent processing. It may be the case that non-linguistic metadata may require normalization but more often than not linguistic information is the most common target for normalization as text often includes artifacts from the acquisition process which will not be desired in the analysis. 

### Orientation {#sec-td-normalization-orientation}

To explore some of the strategies of normalization, we will look at the Europarl Corpus. As we are working working towards transforming a curated dataset, we will start by getting oriented to the dataset. Following the reproducible research principles, I will assume that the curated dataset and its associated data dictionary are in the *data/derived/* directory, as seen in @exm-td-europarl-curated-structure. 

::: {#exm-td-europarl-curated-structure}
```bash
data/
├── analysis/
├── derived/
│   ├── europarl_curated_dd.csv
│   └── europarl/
│       └── europarl_curated.csv
└── original/
```
:::

The contents of the data dictionary for this dataset appears in @tbl-td-europarl-dd.

```{r}
#| label: tbl-td-europarl-dd
#| tbl-cap: "Data dictionary for the Europarl Corpus."
#| echo: false
#| message: false

read_csv(file = "data/cd-europarl_curated_dd.csv") |>
  kable() |>
  kable_styling()
```

This dataset contains transcribed source language (Spanish) and translated target language (English) from the proceedings of the European Parliament. The unit of observation is the `lines` variable whose values are are lines of dialog. 

Let's read in the dataset CSV file with `read_csv()` and inspect the first lines of the dataset with `slice_head()` in @exm-tb-europarl-preview.

::: {#exm-tb-europarl-preview}
```{r}
#| label: td-europarl-read-show
#| eval: false

# Read in the dataset
europarl_curated_tbl <-
  read_csv(file = "../data/derived/europarl_curated.csv")

# Preview the first 10 lines
europarl_curated_tbl |>
  slice_head(n = 10)
```

```{r}
#| label: td-europarl-read-run
#| echo: false

# Read in the dataset
europarl_curated_tbl <- 
  read_csv(file = "data/europarl_curated.csv")

# Preview the first 10 lines
europarl_curated_tbl |>
  slice_head(n = 10)
```
:::

Simply looking at the first 10 lines of this dataset gives us a clearer sense of the dataset structure, but, in terms of normalization procedures we might apply, it is likely not sufficient. We want to get a sense of any potential inconsistencies in the dataset, in particular in the `lines` variable. Since this is a large dataset with `r nrow(europarl_curated_tbl)` observations, we will need to explore the dataset in manageable chunks. The `slice_sample()` function will allow us to randomly sample a subset of the dataset of a certain number of observations specified by the `n = ` argument, as seen in @exm-td-europarl-sample-1.

```{r}
#| label: set-seed-sample-1
#| echo: false
#| cache: false

set.seed(124)
```

::: {#exm-td-europarl-sample-1}
```{r}
#| label: td-europarl-sample-1
#| cache: false

# Randomly sample 5 observations
europarl_curated_tbl |>
  slice_sample(n = 5)
```
:::

We should run the code in @exm-td-europarl-sample-1 multiple times to get a sense of the variation in the dataset. 

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

R functions which return samples (*e.g.* `slice_sample()`) are generated using pseudo-random number generators. These generators are initialized with a seed value. You can control the seed value R uses to generate the random numbers by using the `set.seed()` function and setting the seed value to a number of your choice. It is important to note that setting a seed only affects the subsequent random number generation.

Using `set.seed()` is useful when you want to ensure that the same random numbers are generated each time you run the code. This is particularly helpful when you want to reproduce results in a report or other document.
:::

In the case of the Europarl corpus dataset, it may be useful to see the source and target lines in the same sample. Do do this, we can first sample from the `line_id` variable and then filter the `europarl_curated_tbl` wit the `filter()` function and the `%in%` operator to select the lines that match the sampled `line_id` values, as seen in @exm-td-europarl-sample-2.

```{r}
#| label: set-seed-sample-2
#| echo: false
#| cache: false

set.seed(124)
```

::: {#exm-td-europarl-sample-2}
```{r}
#| label: td-europarl-sample-2
#| cache: false

# Randomly sample 5 line_id values
line_id_sample_vec <- 
  europarl_curated_tbl |>
  distinct(line_id) |> 
  slice_sample(n = 5) |> 
  pull(line_id)

# Select the lines that match the sampled line_id values
europarl_curated_tbl |>
  filter(line_id %in% line_id_sample_vec)
```
:::

After running the code in @exm-td-europarl-sample-1 and @exm-td-europarl-sample-2 multiple times, I identified a number of artifacts that we will want to consider addressing. These are included in @tbl-td-europarl-normalization.

| Description | Examples | Concern |
|:---|:---|:---|
| Non-speech annotations | `(Abucheos)`, `(A4-0247/98)`, `(The sitting was opened at 09:00)` | Not of interest for our analysis |
| Inconsistent whitespace | `5 % ,`, <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code>, `Palacio' s` | May be problematic for tokenization |
| Non-sentence punctuation | ` - ` | May be problematic for tokenization |
| Abbreviations | `Mr.`, `Sr.`, `Mme.`, `Mr`, `Sr`, `Mme`, `Mister`, `Señor`, `Madam` | May be problematic for tokenization |
| Text case | `The`, `the`, `White`, `white` | May be problematic for tokenization |

: Characteristics of the Europarl Corpus dataset that may require normalization. {#tbl-td-europarl-normalization tbl-colwidths="[33, 33, 33]" .sm .striped}

The first three items in @tbl-td-europarl-normalization are relatively straightforward. Non-speech annotations most likely are not relevant for our research. Inconsistent whitespace and non-sentence punctuation may be problematic for tokenization that depends on whitespace and punctuation regularities. 

The other two considerations are more contingent on our research aims. The existence of various forms for the same word, abbreviated and unabreviated, introduces variability may not be of interest for our analysis. Secondly, common conventions for capitalization in prose can introduce unwanted variability. If we leave the text as is, the tokens `The` and `the` will be treated as distinct. If we convert the text to lowercase, the tokens `White` and `white` will be treated as the same, even if `White` corresponds to a proper noun (*e.g.* `White House`). 

These observations provide us a roadmap for the normalization process. For demonstration, let's focus only on a couple of these cases: removing parlimentary session descriptions and extra whitespace.

### Application {#sec-td-normalization-application}

Identifying our normalization goals is an important first step. The next step is to identify the procedures that will accomplish these goals. The majority of text normalization procedures can be accomplished with the `stringr` package [@R-stringr]. This package provides a number of functions for manipulating text. The workhorse functions we will use for our tasks are the `str_remove()` and `str_replace()` functions. As the these functions give us the ability to remove or replace text. Our task is to identify the patterns we want to remove or replace.

Before we modify any lines, let's try craft a search pattern to identify the text of interest. This is done to avoid over- or under-generalizing the search pattern. If we are too general, we may end up removing or replacing text that we want to keep. If we are too specific, we may not remove or replace all the text we want to remove or replace.

<!-- non-speech -->

Let's start by identifying non-parlimentary speech. Two functions from the `stringr` package come in handy here: `str_detect()` and `str_extract()`. `str_detect()` detects a pattern in a character vector and returns a logical vector, `TRUE` if the pattern is detected and `FALSE` if it is not. `str_extract()` extracts the text in a character vector that matches a pattern. 

`str_detect()` pairs well with the `filter()` function to return observations that match a pattern in a character vector. `str_extract()` pairs well with the `mutate()` function to create a new variable which contains character vector that match a pattern.

Let's start with the `str_detect()` function. We will use this function to identify the lines that contain the parliamentary session descriptions. From the examples above, we can see that these instances are wrapped with parentheses `(` and `)`. The text within the parentheses can vary, so we need a Regular Expression to do the heavy lifting. To start out we can match any one or multiple characters with `.+`. But it is important to recognize the `+` (and also the `*`) operators are 'greedy', meaning that if there are multiple matches, the longest match will be returned. In this case, we want to match the shortest match. To do this we can use the `?` operator to make the `+` operator 'lazy'. This will match the shortest match.

Our test code appears in @exm-td-europarl-search-non-speech.

```{r}
#| label: set-seed-sample-3
#| echo: false
#| cache: false

set.seed(111)
```

::: {#exm-td-europarl-search-non-speech}
```{r}
#| label: td-europarl-search-non-speech
#| cache: false

# Identify non-speech lines
europarl_curated_tbl |>
  filter(str_detect(lines, "\\(.+?\\)")) |>
  slice_sample(n = 10) 
```
:::

The results from @exm-td-europarl-search-non-speech show that we have identified the lines that contain at least one of the parliamentary session description annotations. A more targeted search to identify specific instances of the parliamentary session descriptions can be accomplished adding the `str_extract()` function as seen in @exm-td-europarl-search-non-speech-2.

```{r}
#| label: set-seed-sample-4
#| echo: false
#| cache: false

set.seed(111)
```

::: {#exm-td-europarl-search-non-speech-2}
```{r}
#| label: td-europarl-search-non-speech-2
#| cache: false

# Extract non-speech fragments
europarl_curated_tbl |>
  filter(str_detect(lines, "\\(.+?\\)")) |>
  mutate(non_speech = str_extract(lines, "\\(.+?\\)")) |>
  slice_sample(n = 10) 
```
:::

The results from @exm-td-europarl-search-non-speech-2 show that we have identified the lines that contain parliamentary session description annotations and extracted this text --or have we? What if a given line contains more than one parliamentary session description annotation? It turns out that `str_extract()` only returns the first match. To return all matches we can use the `str_extract_all()` function. Let's try again, in @exm-td-europarl-search-non-speech-3.


```{r}
#| label: set-seed-sample-5
#| echo: false
#| cache: false

set.seed(111)
```


::: {#exm-td-europarl-search-non-speech-3}
```{r}
#| label: td-europarl-search-non-speech-3
#| cache: false

# Extract non-speech fragments
europarl_curated_tbl |>
  filter(str_detect(lines, "\\(.+?\\)")) |>
  mutate(non_speech = str_extract_all(lines, "\\(.+?\\)")) |> 
  slice_sample(n = 10)
```
:::

OK, that might not be what you expected. The `str_extract_all()` function returns a list of character vectors. This is because for any given line in `lines` there may be a different number of matches. To maintain the data frame as rectangular, a list is returned for each value of `non_speech`. We could expand the list into a data frame with the `unnest()` function from the `tidyr` package if our goal were to work with these matches. But that is not our aim. Rather, we want to know if we have multiple matches per line. Note that the information provided for the `non_speech` column by the tibble object tells use that we have some lines with muliple matches, as we can see in line 6 of our small sample. So good thing we checked!

Let's now remove these parliamentary session description annotations from each line in the `lines` column. We turn to `str_remove_all()`, a variant of `str_remove()`, that, as you expect, will remove multiple matches in a single line. We will use the `mutate()` function to overwrite the `lines` column with the modified text. The code is seen in @exm-td-europarl-remove-non-speech.

::: {#exm-td-europarl-remove-non-speech}
```{r}
#| label: td-europarl-remove-non-speech

# Remove non-speech fragments
europarl_curated_tbl <- 
  europarl_curated_tbl |>
  mutate(lines = str_remove_all(lines, "\\(.+?\\)"))
```
:::

I recommend spot checking the results of this normalization step by running the code in @exm-td-europarl-search-non-speech again, if nothing appears we've done our job. 

<!-- extra whitespace -->

The second item of business to address is the extra whitespace we observed in @tbl-td-europarl-normalization. If we consider the extra whitespace cases, we can categorize them into two types: multiple spaces that should be a single space (*e.g.* <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code>) and single spaces that occur within a word (*e.g.* `5 % ,` or `Palacio' s`). 

To deal with muliple spaces, we can turn to the `str_replace_all()` function. This function will replace a pattern with a replacement string for every pattern match. In this case, we want to replace multiple spaces with a single space. We can use the `\\s+` pattern to match one or more spaces and then replace it with `\\s` or a single whitespace character `" "`. 

Before we apply this normalization step, let's assess how many instances of multiple spaces we have in the dataset. We can use the `str_count()` function to count the number of matches for a pattern. The pattern we want needs to be a bit more precise than `\\s+`, because this matches one or more. We want to match *two* or more. Using the regular expression operator `{,}` we can specify our pattern to be `\\s{2,}`, *i.e.* two or more continguous whitespaces. Let's count and sum all these matches. The code is seen in @exm-td-europarl-count-whitespace.

::: {#exm-td-europarl-count-whitespace}
```{r}
#| label: td-europarl-count-whitespace

# Count multiple spaces
europarl_curated_tbl |>
  mutate(multiple_spaces = str_count(lines, "\\s{2,}")) |>
  summarize(total_multiple_spaces = sum(multiple_spaces, na.rm = TRUE))
```
:::

::: {.callout}
**{{< fa exclamation-triangle >}} Warning**

You may be wondering what the extra parameter `na.rm = TRUE` is doing in the `sum()` function. This parameter tells R to ignore values that are `NA` (not available). This is important because if we don't ignore `NA` values, the `sum()` function will return `NA` if there are any `NA` values in the vector. The code in @exm-td-europarl-count-whitespace will return `NA` when the `\\s{2,}` doesn't match for a given line. This is because the `str_count()` function returns `NA` when there are no matches. If we don't ignore these `NA` values, the `sum()` function will return `NA` for the entire dataset.
:::

The results from @exm-td-europarl-count-whitespace show that we have over 130k instances of multiple spaces. Let's replace these with a single space. The code is seen in @exm-td-europarl-remove-multiple-whitespace.

::: {#exm-td-europarl-remove-multiple-whitespace}
```{r}
#| label: td-europarl-remove-multiple-whitespace

# Remove multiple spaces
europarl_curated_tbl <- 
  europarl_curated_tbl |>
  mutate(lines = str_replace_all(lines, "\\s{2,}", "\\s"))
```
:::

To check our work, we can run the code in @exm-td-europarl-count-whitespace again. We should see that there are no more instances of multiple spaces.

Now let's turn to the single spaces that occur within a word. We can use the `str_replace_all()` function again to replace these single spaces with no space --but is that what we want? Probably not, we want *most* of the single spaces to remain, otherwise we would have one very, very long string on each line. 

Instead, we want to narrow our scope and focus in on whitespace that occurs in particular contexts. One context we can focus on is the single quote `'`, as in `Palacio' s`. In this use, the single quote is an apostrophe in a possessive form, but we might want to see if other forms, such as contractions, that also may have this extra whitespace. Let's check using a regular expression that matches a character string `\\w+` with single quote `'` at the end followed by whitespace `\\s`. To give some more context I will add a character string `\\w+` after the whitespace. Using the `str_extract_all()` function we can extract all the matches for this pattern. It returns a list, so we `unnest()` the list. Then we can pull the vector of matches and list the unique matches to get a sense of the context we are working with. The code is seen in @exm-td-europarl-search-apostrophe.

```{r}
#| label: set-seed-sample-6
#| echo: false
#| cache: false

set.seed(111)
```

::: {#exm-td-europarl-search-apostrophe}
```{r}
#| label: td-europarl-search-apostrophe

# Match apostrophe followed by whitespace
europarl_curated_tbl |>
  slice_sample(n = 10000) |> 
  mutate(quote_whitespace = str_extract_all(lines, "\\w+'\\s\\w+")) |> 
  unnest(quote_whitespace) |>
  pull(quote_whitespace) |>
  unique()
```
:::

In @exm-td-europarl-search-apostrophe we can see that we have many singular possessive forms we want to ammend and other forms that we may want to keep --in particular when the single quote is part of a quote or a plural or irregular singular possessive. From various runs of this code, it looks safe to remove whitespace between the single quote and the `s` in the possessive form  We need to be careful not to remove whitespace in other contexts where the single quote is followed by ` s`. To do this we can use the word boundary pattern `\\b` after the `s` in our pattern to ensure that the `s` is not part of a following word. The code is seen in @exm-td-europarl-remove-apostrophe.

::: {#exm-td-europarl-remove-apostrophe}
```{r}
#| label: td-europarl-remove-apostrophe

# Remove whitespace after apostrophe
europarl_curated_tbl <- 
  europarl_curated_tbl |>
  mutate(lines = str_replace_all(lines, "'\\ss\\b", "'s"))
```
:::

To check our work, we can run the code in @exm-td-europarl-search-apostrophe again. We should see that there are no more instances of whitespace after the single quote in possessive forms.

Other normalization procedures often follow a similar line of attack. There are cases, however, in which normalization procedures are more easily accomplished after subsequent transformation steps or need to be post-poned to further the goals of other transformation steps. For example, standardizing abbreviated forms may be more easily accomplished after tokenization when each token is a word. Another example is the case of case conversion. Even if we are not directly interested in the case differences between words, certain generation procedures, Named Entity Recognition for example, may use case information to identify proper nouns. In these cases, it may be better to leave the case as is until after the generation step.

## Recoding {#sec-td-recoding}

Normalizing text can be seen as an extension of dataset curation to some extent in that the structure of the dataset is maintained. In the Europarl case, we saw this to be true. In the case of recoding, and other transformational steps, the aim will be to modify the dataset structure either by rows, columns, or both. Recoding processes can be characterized by the creation of structural changes which are derived from values in variables effectively recasting values as new variables to enable more direct access in our analyses.

### Orientation {#sec-td-recoding-orientation}

```{r}
#| label: tr-swda-dataset-curate-run
#| include: false

swda_curated_tbl <- 
  read_csv("data/swda_curated.csv") |> 
  select(
    doc_id,
    speaker_id,
    sex,
    education,
    birth_year,
    utt_id = utterance_id,
    utt_text = utterance_text,
    damsl_tag
  )

swda_curated_dd <- 
  create_data_dictionary(
  data = swda_curated_tbl,
  file_path = "data/tr-swda_curated_dd.csv",
  model = "gpt-3.5-turbo"
)
```

The Switchboard dialog Act Corpus corpus contains a number of variables describing conversations between speakers of American English. A subset of this dataset provides a good example of the recoding process. The data dictionary for this dataset appears in @tbl-tr-swda-curated-dd.

```{r}
#| label: tbl-tr-swda-curated-dd
#| tbl-cap: "Data dictionary for the Switchboard dialog Act Corpus."
#| message: false
#| echo: false

# SWDA data dictionary ----
read_csv("data/tr-swda_curated_dd.csv") |>
  kable() |>
  kable_styling()
```

The data dictionary gives us a sense of the variables in the dataset. Let's read in the dataset and preview the first 10 lines to get a sense of the values in the dataset, as in @exm-tr-swda-dataset-read-show.

::: {#exm-tr-swda-dataset-read-show}
```{r}
#| label: tr-swda-dataset-read-show
#| eval: false

# Read in the dataset
swda_curated_tbl <-
  read_csv("data/derived/swda/swda_curated.csv")

# Preview the first 10 lines
swda_curated_tbl |>
  slice_head(n = 10)
```

```{r}
#| label: tr-swda-dataset-read-run
#| echo: false

swda_curated_tbl |> 
  slice_head(n = 10)
```
:::

Considering the data dictionary and the preview of the `swda_curated_tbl` dataset, we observe a number of metadata variables, such as `doc_id`, `speaker_id`, `sex`, `education`, `birth_year`, `utt_id`, `utt_text`, and `damsl_tag`. 

Most of these variables and their values are readily interpretable. However, the `damsl_tag` variable and the annotation scheme that appears interleaved with the dialog in `utt_text` may require a bit more explanation. If we consult the data origin file and/ or the corpus website, we seee that the `damsl_tag` is a utterance-level annotation which indicates the dialog act type (*e.g.* statement, question, backchannel, *etc.*). The annotation interleaved with the dialog in `utt_text` is a [disfluency annotation scheme](https://staff.fnwi.uva.nl/r.fernandezrovira/teaching/DM-materials/DFL-book.pdf). This scheme includes annotation for non-sentence elements such as filled pauses (*e.g.* `{F uh}`), discourse markers (*e.g.* `{D well}`), repetitions/ restarts (*e.g.* `[I think + I believe]`), among others.

Let's assume that we are interested in understanding the use of filled pauses in the Switchboard dialog. @Tottie2011 investigates the relationship between speakers' use of filled pauses `uh` and `um` and their socio-demographic background (sex, socio-economic status, and age) in British English. An American English comparison would be insightful. To do this, however, we will need to recode some of the variables in the dataset.

In this case, we will use `education` as a proxy for socio-economic status. As is, the values are numeric and are not maximally transparent. We can recode these values to be more interpretable. In the corpus documentation, the values for `education` are described in @tbl-tr-swda-education.

| Value | Description                 |
|-----------|-------------------------|
| 0         | Less Than High School   |
| 1         | Less Than College       |
| 2         | College                 |
| 3         | More Than College       |
| 9         | Unknown                 |

: Values for the `education` variable in the Switchboard dialog Act Corpus. {#tbl-tr-swda-education .sm .striped}

To derive a variable which reflects the age of each speaker, we will use the `birth_year` variable. This variable is a numeric value which indicates the year of birth for each speaker. We can derive a new variable `age` by subtracting the `birth_year` from the year of recordings, 1992.

Together `sex`, `education`, and `age` will provide us with the socio-demographic information we need to investigate the relationship between speakers' use of filled pauses and their socio-demographic background. The last component we need to derive is the use of filled pauses. To do this we will need to extract the filled pauses from the `utt_text` variable. We also need to consider what we mean by 'use'. In this case, we will operationalize the use of filled pauses as the number of times a filled pause is used per utterance.

### Application {#sec-td-recoding-application}

The plan to transform the `swda_curated_tbl` dataset is established. Now we need to implement the plan. We will start by recoding the `education` variable. Specifically, we want to map the numeric values to the descriptions in @tbl-tr-swda-education.

To do this we will use the `case_when()` function from the `dplyr` package. This function allows us to specify a series of conditions and the values to return if the condition is met. `case_when()` evaluates the conditions and `mutate()` writes the variable, in this case overwrites it, as seen in @exm-tr-swda-recoding-education.

::: {#exm-tr-swda-recoding-education}
```{r}
#| label: tr-swda-recoding-education

# Recode education
swda_curated_tbl <- 
  swda_curated_tbl |>
  mutate(
    education = case_when(
      education == 0 ~ "Less Than High School",
      education == 1 ~ "Less Than College",
      education == 2 ~ "College",
      education == 3 ~ "More Than College",
      education == 9 ~ "Unknown"
    )
  )
# Preview the first 10 lines
swda_curated_tbl |>
  slice_head(n = 10)
```
:::

To create the `age` variable, all we need to do is subtract the `birth_year` from the year of recording, 1992. Again we will use `mutate()` to create the `age` variable. The values are created by a subtraction operation. Since we will not need the `birth_year` variable afterwards, we will drop it from the dataset. The code is seen in @exm-tr-swda-recoding-age.

::: {#exm-tr-swda-recoding-age}
```{r}
#| label: tr-swda-recoding-age

# Recode age
swda_curated_tbl <- 
  swda_curated_tbl |>
  mutate(age = 1992 - birth_year) |>
  select(-birth_year)

# Preview the first 10 lines
swda_curated_tbl |>
  slice_head(n = 10)
```
:::

Our final recoding step is to derive the frequency of filled pauses per utterance. In other words, we want to match the 'uh' and 'um' and return the number of matches for each utterance. There are a number of ways to do this. We could use an approach which applies the `str_extract_all()` function, which returns a list of matches, and then `unnest()` the list and count the number of matches. An alternative approach is to use the `str_count()` function to count the number of matches for a pattern. The later approach is more efficient for our purposes. 

In either case, a pattern to match these annotations is needed. As always with pattern matching, we need to craft an expression that is as specific as possible to avoid over- or under-matching. We know from our observation and the corpus documentation that all filled pauses are wrapped by the `{F ...}` annotation. We can use this to our advantage. Before we jump into counting the filled pauses, let's test a regular expression that matches the entire `{F ...}` annotation. An expression like `{F.*}` might be tempting, but this will be problematic for two reasons. First, since the `{` and `}` are regular expression operators we will need to escape them with the `\\` convention. Second, the `*` operator is greedy, meaning that it will match the longest possible string. So if in a given utterance there are multiple filled pauses, the `*` operator will match all of them at once, not individually. To avoid this, we can use the `?` operator to make the `*` operator lazy. With these considerations in mind, we can move forward with `\\{F.*?\\}` as our expression.

We will send each utterance to the `str_extract_all()` function to match the filled pauses. This function returns a list of matches, so we will need to `unnest()` the list to get a vector of matches. Afterwards we will apply the ` count()` function to summarize the number of matches for each match variation. The code is seen in @exm-tr-swda-recoding-filled-pauses-test.

::: {#exm-tr-swda-recoding-filled-pauses-test}
```{r}
#| label: tr-swda-recoding-filled-pauses-test

# Test filled pause pattern
swda_curated_tbl |>
  mutate(
    matches = str_extract_all(
      utt_text, 
      "\\{F.*?\\}")
  ) |>
  unnest(matches) |> 
  count(matches)
```
:::

The result from our test indicates that the `\\{F .*?\\}` pattern matches a wide variety of filled pause annotations, 120 to be exact! We can see that there are a number of filled pause annotations that we are not be interested in, *e.g.* `{F Oh}`. Furthermore, the 'uh' and 'um' we are interested in sometimes include more annotation structure, *e.g.* `{F + ] Um, }`. 

A more sophisticated pattern is needed. When faced with a pattern matching task such as this, I find it helpful to start with a simple pattern and then add complexity as needed. This is an iterative process. To speed up the process, I often extract the matches and use an online tool for developing regular expressions, such as [regex101.com](https://regex101.com/).

With a (monster) regular expression that matches each of the filled pauses we are interested in, and only those filled pauses, we can move forward with counting the number of matches for each utterance. 

To do this we will use the `str_count()` function from the `stringr` package. This function counts the number of matches for a pattern in a character vector. We will use `mutate()` to create new variables `uh` and `um` which will contain the counts for the filled pauses `uh` and `um`, respectively. The code is seen in @exm-tr-swda-recoding-filled-pauses.

::: {#exm-tr-swda-recoding-filled-pauses}
```{r}
#| label: tr-swda-recoding-filled-pauses

# Recode filled pauses 
swda_curated_tbl <- 
  swda_curated_tbl |>
  mutate(
    uh = str_count(
      utt_text, 
      "\\{F\\s+[\\(+\\s\\]]*(u|U)h[\\)\\s,\\.\\?-]*\\}"
    ),
    um = str_count(
      utt_text, 
      "\\{F\\s+[\\(+\\s\\]]*(u|U)m[\\)\\s,\\.\\?-]*\\}"
    )
  )
# Preview the first 10 lines 
swda_curated_tbl |>
  slice_head(n = 10)
```
:::

Now we have two new columns, `uh` and `um` which indicate how many times the relevant pattern was matched for a given utterance. By choosing to focus on disfluencies, however, we have made a decision to change the unit of observation from the utterance to the use of filled pauses (`uh` and `um`). This means that as the dataset stands, it is not in tidy format --where each observation corresponds to the observational unit. When datasets are misaligned in this particular way, there are in what is known as 'wide' format. What we want to do, then, is to restructure our dataset such that each row corresponds to the unit of observation --in this case each filled pause type. 

<!-- [ ] may need a graphic or more explanation here (pivot_longer) -->

To convert our current (wide) dataset to one where each filler type is listed and the counts are measured for each utterance we turn to the `pivot_longer()` function. This function creates two new columns, one in which the column names are listed and one for the values for each of the column names. The code is seen in @exm-tr-swda-recoding-filled-pauses-longer.

::: {#exm-tr-swda-recoding-filled-pauses-longer}
```{r}
#| label: tr-swda-recoding-filled-pauses-longer

# Tidy filled pauses
swda_curated_tbl <- 
  swda_curated_tbl |>
  pivot_longer(
    cols = c("uh", "um"), 
    names_to = "filler", 
    values_to = "count"
  )
# Preview the first 10 lines
swda_curated_tbl |>
  slice_head(n = 10)
```
:::

Now we have a transformed dataset that is in tidy format. Each row corresponds to a filled pause type and the number of times it was used in a given utterance. It also includes the key socio-demographic variables we are interested in.

::: {.callout}
**{{< fa regular lightbulb >}} Consider this**

As confident as we may be in our recoding process, it is always a good idea to perform some data checks to ensure that the recoding process was successful. In the process, we can also gain some insight into the data. Considering the structure in the transformed Switchboard dialog Act Corpus dataset, what are some data checks we might want to perform? What are some insights we might gain from these data checks?
:::

Finalize the transformation process by writing the dataset to a CSV file, create a data dictionary, and review and comment your code in the transformation script.

## Tokenization {#sec-td-tokenization}

<!--  

row-wise

The goal is to segment units of language into relevant components given the research question. This may include breaking text into smaller units, such as words, n-grams, sentences, etc. or combining text into larger units.

Data: 

- CABNC (expand)
  - tidytext::unnest_tokens()

- ?PELIC tokens (contract)
  - stringr::str_c() [w/ collapse = " "]

R: 
- start with `tokenizers` package for tokenization types (character, word, n-gram, sentence, etc.)
- introduce `tidytext` package, for tokenization and unnesting
- show how to collapse tokens with `stringr::str_c()`

Consider: 

- Adding lemmatization, stemming to this section but to make sure that it is clear that these are 'recoding' processes, they just happen to be more accessible to use after tokenization.

-->

Another common transformation process that is particularly relevant for text analysis is tokenization. Tokenization is the process of segmenting units of language into components relevant for the research question. This may include breaking text in curated datasets into smaller units, such as words, $n$-grams, sentences, *etc.* or combining text into larger units. 

The process of tokenization is fundamentally row-wise. By scaling the text units up or down, we change the unit of observation. In turn, it is much easier to perform subsequent processing and/ or analysis. For example, if we are interested in the use of particular words, we may want to segment the text into words. If we are interested in the use of particular phrases, we may want to segment the text into $n$-grams. If we are interested in the use of particular sentences, we may want to segment the text into sentences. Each of these levels, then, are more directly accessible than the original text units.

A key consideration in tokenization is the criteria for segmentation. While it may appear obvious to a human what 'word' or 'sentence' means, a computer needs a definition. This can prove tricker than it seems. For example, in English, we can segment text into words by splitting on whitespace. This works fairly well but there are some cases where this is not ideal. For example, in the case of contractions, such as `don't`, `won't`, `can't`, *etc.* the apostrophe is not a whitespace character. If we want to consider these contractions as separate words, then we need to consider a different tokenization strategy.

::: {.callout}
**{{< fa regular lightbulb >}} Consider this**

Consider the following paragraph: 

"As the sun dipped below the horizon, the sky was set ablaze with shades of orange-red, illuminating the landscape. It's a sight Mr. Johnson, a long-time observer, never tired of. On the lakeside, he'd watch with friends, enjoying the ever-changing hues—especially those around 6:30 p.m.—and reflecting on nature's grand display. Even in the half-light, the water's glimmer, coupled with the echo of distant laughter, created a timeless scene. The so-called 'magic hour' was indeed magical, yet fleeting, like a well-crafted poem; it was the essence of life itself."

What text conventions would prove insufficient for word tokenization based on a whitespace critieron?
:::

Furthermore, tokenization strategies can vary between languages. For German words are often compounded together, meaning many 'words' will not be captured by the whitespace convention. Whitespace may not even be relevant for word tokenization in written languages, such as Chinese. The take home message is there is no one-size-fits-all tokenization strategy.

### Orientation {#sec-td-tokenization-orientation}


```{r}
#| label: tr-cabnc-dataset-curate-run
#| include: false

cabnc_curated_tbl <- 
  read_csv("data/cabnc_curated.csv") |> 
  select(
    doc_id,
    part_id,
    sex = part_sex,
    age = part_age,
    utt_id = utt_num,
    utt_text = utterance
  )

cabnc_curated_dd <- 
  create_data_dictionary(
  data = cabnc_curated_tbl,
  file_path = "data/tr-cabnc_curated_dd.csv",
  model = "gpt-3.5-turbo"
)
```

Let's look at a curated dataset from the CABNC Corpus to explore tokenization. The data dictionary for this dataset appears in @tbl-td-cabnc-dd.

```{r}
#| label: tbl-td-cabnc-dd
#| tbl-cap: "Data dictionary for the CABNC Corpus."
#| message: false

read_csv(file = "data/tr-cabnc_curated_dd.csv") |>
  kable() |>
  kable_styling()
```

The CABNC dataset contains a number of variables describing conversations between speakers of British English. Now let's look at the dataset and preview the first 10 lines to get a sense of the values in the dataset, as in @exm-tr-cabnc-dataset-read-show.

::: {#exm-tr-swda-dataset-read-show}
```{r}
#| label: tr-cabnc-dataset-read-show
#| eval: false

# Read in the dataset
cabnc_curated_tbl <-
  read_csv("data/derived/cabnc/cabnc_curated.csv")

# Preview the first 10 lines
cabnc_curated_tbl |>
  slice_head(n = 10)
```

```{r}
#| label: tr-cabnc-dataset-read-run
#| echo: false

cabnc_curated_tbl |> 
  slice_head(n = 10)
```
:::

Considering the data dictionary and the preview of the `cabnc_curated_tbl` dataset, we observe a number of metadata variables, such as `doc_id`, `part_id`, `sex`, `age`, and `utt_id` and the utterances in `utt_text`.

Let's assume that we are performing an exploratory analysis with this dataset and would like to consider the use of words and word sequences ($n$-grams). In this case we will be deriving multiple datasets with different units of observation.   

### Application {#sec-td-tokenization-application}

In the `cabnc_curated_tbl` the `utt_text` variable contains the text we want to tokenize. We will start by tokenizing the text into words. Abstracting away from some of the metadata variables, if we envision what this should look like we might imagine something like @tbl-td-cabnc-tokenization-words-example.

|  doc_id   | utt_id | utt_word |
|----------|--------|----------|
| KB0RE000 |      0 |      You |
| KB0RE000 |      0 |  enjoyed |
| KB0RE000 |      0 | yourself |
| KB0RE000 |      0 |       in |
| KB0RE000 |      0 |  America |

: Example of tokenizing the `utt_text` variable into words. {#tbl-td-cabnc-tokenization-words-example .sm .striped}

Comparing @tbl-td-cabnc-tokenization-words-example to the first line of the output of @exm-tr-swda-dataset-read-show, we can see that we want to segment the words in the `utt_text` and then have each segment appear as a separate observation, retaining the relevant metadata variables.

Before we work with tokenizing text in a data frame, let's start with a character vector to get a sense of how tokenization works and what we will need to do to achieve the output in @tbl-td-cabnc-tokenization-words-example. Let's start with a character vector which contains the first three utterances from the `cabnc_curated_tbl` dataset. The code is seen in @exm-tr-cabnc-tokenization-words.

::: {#exm-tr-cabnc-tokenization-words-vector}
```{r}
#| label: tr-cabnc-tokenization-words-vector

# Pull a character vector
cabnc_utts_chr <- 
  cabnc_curated_tbl |>
  slice_head(n = 3) |>
  pull(utt_text)

# Preview the character vector
cabnc_utts_chr
```
:::

We have the first three utterances in `cbanc_utts_chr`. Now we can tokenize the utterances into words using the `tokenize_words()` function from the  `tokenizers` package [@R-tokenizers]. It's only required argument is a character vector, as seen in @exm-tr-cabnc-tokenization-words-tokenize.

::: {#exm-tr-cabnc-tokenization-words-tokenize}
```{r}
#| label: tr-cabnc-tokenization-words-tokenize

# Load package
library(tokenizers)

# Tokenize the utterances into words
cabnc_utts_chr |> 
  tokenize_words()
```
:::



To do this we will use the `unnest_tokens()` function from the `tidytext` package. This function takes a character vector and a tokenization type and returns a dataset with the tokenized text. The tokenization types available are `character`, `word`, `ngram`, `sentence`, `regex`, and `skip_ngram`. We will use the `word` tokenization type. The code is seen in @exm-tr-cabnc-tokenization-words.

<!-- 

- [ ] Add PELIC data dictionary and read dataset. 
  - [ ] ? I may just make reference to the fact that rows can be collapsed into a single string using `stringr::str_c()`. 

-->

## Generatation {#sec-td-generation}

<!--  

row- and column-wise

The goal is to augment the dataset with additional information based on the existing information, but not necessarily explicit in the dataset. This may include adding linguistic annotations, sentiment analysis, etc.

Data: 

- Europarl
  - English
  - Spanish

- Lexical annotation
  - POS, lemma, etc.
- Syntax annotation
  - Dependency parsing

Consider:

- UD models: note that there are different models for different languages and language varieties, some may be more in line with the language variety of interest than others.
- Mention `rsyntax` package for extracting/ recoding patterns from syntactic annotations.

-->

The process of generating a dataset involves the addition of information to a dataset. Whereas recoding involves the transformation of information that is already explicit in a dataset, generation involves the transformation of information that is implicit in a dataset.

The most common type of operation involved in the generation process is the addition of linguistic annotation. This process can be accomplished manually by a researcher or research team or automatically through the use of pre-trained linguistic resources and/ or software. Ideally the annotation of linguistic information can be conducted automatically. 

There are important considerations, however, that need to be taken into account when choosing whether linguistic annotation can be conducted automatically. First and foremost has to do with the type of annotation desired. Information such as part of speech (grammatical category) and morpho-syntactic information are the the most common types of linguistic annotation that can be conducted automatically. Second the degree to which the resource that will be used to annotate the linguistic information is aligned with the language variety and/or register is also a key consideration. As noted, automatic linguistic annotation methods are contingent on pre-trained resources. The language and language variety used to develop these resources may not be available for the language under investigation, or if it does, the language variety and/ or register may not align. The degree to which a resource does not align with the linguistic information targeted for annotation is directly related to the quality of the final annotations. To be clear, no annotation method, whether manual or automatic is guaranteed to be perfectly accurate. 

Let's take a look at annotation some of the language from the Europarl dataset we normalized. 

```{r}
#| eval: false
#| label: tbl-td-europarl-en-preview
#| tbl-cap: "First 10 lines in English from the normalized SDAC dataset."

europarl |>
  filter(type == "Target") |>
  slice_head(n = 10) |>
  kable(booktabs = TRUE)
```

We will use the cleanNLP package to do our linguistic annotation. The annotation process depends on the pre-trained language models. There is [a list of the models available to access](https://github.com/bnosac/udpipe#pre-trained-models). The `load_model_udpipe()` custom function below downloads the specified language model and initialized the `udpipe` engine (`cnlp_init_udpipe()`) for conducting annotations. 

```{r}
#| eval: false
#| label: td-functions-load-udpipe-model

load_model_udpipe <- function(model_lang) {
  # Function
  # Download and load the specified udpipe language model
  
  cnlp_init_udpipe(model_lang) # to download the model, if not downloaded
base_path <- system.file("extdata", package = "cleanNLP") # get the base path
  model_name <- # extract the model_name
    base_path |> # extract the base path
    dir() |> # get the directory
    stringr::str_subset(pattern = paste0("^", model_lang)) # extract the name of the model
  
  model_path <- udpipe::udpipe_load_model(file = file.path(base_path, model_name, fsep = "/")) # create the path to the downloaded model stored on disk
    return(model_path)
}
```


---
A test of `udpipe` annotation for English.

```{r}
#| eval: false
#| label: td-generation-udpipe-english-test

library(cleanNLP)

cnlp_init_udpipe("english") # initialize the udpipe engine for English

text_sample <- "As the sun dipped below the horizon, the sky was set ablaze with shades of orange-red, illuminating the landscape. It's a sight Mr. Johnson, a long-time observer, never tired of. On the lakeside, he'd watch with friends, enjoying the ever-changing hues—especially those around 6:30 p.m.—and reflecting on nature's grand display. Even in the half-light, the water's glimmer, coupled with the echo of distant laughter created a timeless scene. The so-called 'magic hour' was indeed magical, yet fleeting, like a well-crafted poem; it was the essence of life itself."

text_tbl <- tibble(doc_id = 1, text = text_sample) # create a tibble with the text to be annotated

text_anno <-
  text_tbl |> # specify the text to be annotated
  cnlp_annotate() # specify the name of the text column

text_anno$token |>
  print(n = Inf)
```

---

In a test case, let's load the 'english' model to annotate a sentence line from the europarl dataset to illustrate the basic workflow. 

```{r}
#| eval: false
#| label: td-generation-europarl-en-example

eng_model <- load_model_udpipe("english") # load and initialize the language model, 'english' in this case.

eng_annotation <-
  europarl |> # dataset
  filter(type == "Target" & sentence_id == 6) |> # select English and sentence_id 6
  cnlp_annotate(
    text_name = "sentence", # input text (sentence)
    doc_name = "sentence_id"
  ) # specify the grouping column (sentence_id)

glimpse(eng_annotation) # preview structure
```

We see that the structure returned by the `cnlp_annotate()` function is a list. This list contains two data frames (tibbles). One for the tokens (and there annotation information) and the document (the metadata information). We can inspect the annotation characteristics for this one sentence by targetting the `$tokens` data frame. Let's take a look at the linguistic annotation information returned. 

```{r}
#| eval: false
#| label: tbl-td-generation-test-annotation-english
#| tbl-cap: "Annotation information for a single English sentence from the europarl dataset."
#| echo: false

eng_annotation$token |>
  kable(booktabs = TRUE)
```

There is quite a bit of information which is returned from `cnlp_annotate()`. First note that the input sentence has been tokenized by word. Each token includes the token, lemma, part of speech (`upos` and `xpos`), morphological features (`feats`), and syntactic relationships (`tid_source` and `relation`). It is also key to note that the `doc_id`, `sid` and `tid` maintain the relational attributes from the original dataset --and therefore maintains our annotated dataset in tidy format.

Let's now annotate the same sentence from the europarl corpus for the Source ('Spanish') and note the similarities and differences.

```{r}
#| eval: false
#| label: td-generation-europarl-es-example

spa_model <- load_model_udpipe("spanish") # load and initialize the language model, 'spanish' in this case.

spa_annotation <-
  europarl |> # dataset
  filter(type == "Source" & sentence_id == 6) |> # select Spanish and sentence_id 6
  cnlp_annotate(
    text_name = "sentence", # input text (sentence)
    doc_name = "sentence_id"
  ) # specify the grouping column (sentence_id)
```

```{r}
#| eval: false
#| label: tbl-td-generation-test-annotation-spanish
#| tbl-cap: "Annotation information for a single Spanish sentence from the europarl dataset."
#| echo: false

spa_annotation$token |>
  kable(booktabs = TRUE)
```

For the Spanish version of this sentence, we see the same variables. However, the `feats` variable has morphological information which is specific to Spanish --notably gender and mood. 

::: {.callout}
**{{< fa exclamation-triangle >}} Warning**

The `rsyntax` package [@R-rsyntax] can be used to recode and extract patterns from the output from automatic linguistic annotations using cleanNLP. [See the documentation for more information](https://github.com/vanatteveldt/rsyntax).
:::


## Merging {#sec-td-merging}

<!--

Row-wise and column-wise

The goal to combine two or more datasets to create a new dataset. This may include concatenating datasets row-wise or joining datasets column-wise. The goal is to create a dataset that is more informative for the research question. In most cases this will involve the addition of information from one dataset to another. But in some cases it may be the case that the goal is to filter one dataset based on the information in another dataset.

The datasets to be merged must share attributes. In the case of concatenation, the datasets must share the same attributes. In the case of joining, the datasets must share at least one attribute, or key, to join on.

Data: 

- CEDEL2 (row-wise) "Concatenating"
  - Natives
  - Learners

- PEDC (column-wise) "Joining"
  - Wordbank: word-level `category`` and `item_kind`
  - eLexicon: word-level word frequency measures (https://elexicon.wustl.edu/index.html)

- PEDIC (column-wise) "Joining"
  - `lingtypology` package: ENNTT country/ language properties (?)

Consider: 

- Linguistic information:
  - Sentiment lexicons
  - Stopword lists
  - MRC word information to merge

---

Useful language: 

Consider the `get_sentiments()` function which returns words which have been classified as 'positive'- or 'negative'-biased, if the lexicon is set to 'bing' [@Hu2004]. This function can be used to generate a new variable which can be used to measure the sentiment of a text. 

-->

One final class of transformations that can be applied to curated datasets to enhance their informativeness for a research project is the process of merging two or more datasets. To merge datasets it is required that the datasets share one or more attributes. With a common attribute two datasets can be joined to coordinate the attributes of one dataset with the other effectively adding attributes and one dataset with extended information. Another approach is to join datasets with the goal of filtering one of the datasets given the matching attribute. 

Let's see this in practice. 

<!-- ADD.... -->

We can also merge datasets that we generate in our analysis or that we import from other sources. This can be useful when there are cases in which a corpus has associated metadata that is contained in files separate from the corpus itself. This is the case for the Switchboard dialog Act Corpus. 

Our existing, disfluency recoded, version includes the following variables. 

```{r}
#| eval: false
#| label: td-sdac-disfluencies

sdac_disfluencies |> # dataset
  slice_head(n = 10) # preview first 10 observations
```

The [online documentation page](https://catalog.ldc.upenn.edu/docs/LDC97S62/) provides a key file `caller_tab.csv` which contains speaker metadata information. Included in this `.csv` file is a column `caller_no` which contains the `speaker_id` we currently have in the `sdac_disfluencies` dataset. Let's read this file into our R session renaming `caller_no` to `speaker_id` to prepare to join these datasets. 

```{r}
#| eval: false
#| label: td-sdac-meta-read

sdac_speaker_meta <-
  read_csv(
    file = "https://catalog.ldc.upenn.edu/docs/LDC97S62/caller_tab.csv",
    col_names = c(
      "speaker_id", # changed from `caller_no`
      "pin",
      "target",
      "sex",
      "birth_year",
      "dialect_area",
      "education",
      "ti",
      "payment_type",
      "amt_pd",
      "con",
      "remarks",
      "calls_deleted",
      "speaker_partition"
    )
  )

glimpse(sdac_speaker_meta)
```

Now to join the `sdac_disfluencies` and `sdac_speaker_meta`. Let's turn to `left_join()` again as we want to retain all the observations (rows) from `sdac_disfluencies` and add the columns for `sdac_speaker_meta` where the `speaker_id` column values match. 

```{r}
#| eval: false
#| label: td-sdac-join-metadata

sdac_disfluencies <-
  left_join(sdac_disfluencies, sdac_speaker_meta) # join by ``speaker_id`

glimpse(sdac_disfluencies)
```

Now there are some metadata columns we may want to keep and others we may want to drop as they may not be of importance for our analysis. I'm going to assume that we want to keep `sex`, `birth_year`, `dialect_area`, and `education` and drop the rest. 

```{r}
#| eval: false
#| label: td-sdac-disfluencies-subset-cols

sdac_disfluencies <-
  sdac_disfluencies |> # dataset
  select(doc_id:count, sex:education) # subset key columns
```

```{r}
#| eval: false
#| label: tbl-td-sdac-disfluencies-meta-preview
#| tbl-cap: "First 10 observations for the `sdac_disfluencies` dataset with speaker metadata."
#| echo: false

sdac_disfluencies |> # dataset
  slice_head(n = 10) |> # first 10 observations
  kable(booktabs = TRUE)
```

```{r}
#| eval: false
#| label: td-sdac-disfluencies-write
#| include: false

write_csv(
  sdac_disfluencies,
  file = "data/transform-datasets/data/derived/sdac/sdac_disfluencies.csv"
)
```


## Summary {-}

In this chapter we covered the process of transforming datasets. The goal is to manipulate the curated dataset to make it align better for analysis. 
There are four general types of transformation steps: normalization, recoding, tokenization, generation, and merging. In any given research project some or all of these steps will be employed --but not necessarily in the order presented in this chapter. Furthermore, there may also be various datasets generated in at this stage each with a distinct analysis focus in mind. 

<!--  

?? Add at the end of the chapter?

[x] Add data checks
[ ] Reminder on writing the dataset and data dictionary to *data/derived/* directory
  [ ] There may be multiple transformed datasets, so it is important to di  stinguish them from one another and document them separately.
[ ] Reminder to review and document code added to the *code/* directory, within the *3-transform-dataset.qmd* file.

-->

In any case, it is important to write these datasets to disk and to document them according to reproducible research principles. 

This chapter concludes the section on data/ dataset preparation. The next section we turn to analyzing datasets. This is the stage where we interrogate the datasets to derive knowledge and insight either through exploratory, predictive, or inferential methods.

## Activities {.unnumbered}

::: {.callout}
**{{< fa regular file-code >}} Recipe**

<!-- Understand, apply, and analyze verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- [ ] update recipe -->

**What**: [Dataset manipulation: tokenization and joining datasets](https://lin380.github.io/tadr/articles/recipe_8.html)\
**How**: Read Recipe 8 and participate in the Hypothes.is online social annotation.\
**Why**: To work with to primary types of transformations, tokenization and joins. Tokenization is the process of recasting textual units as smaller textual units. The process of joining datasets aims to incorporate other datasets to augment or filter the dataset of interest.
:::

::: {.callout}
**{{< fa flask >}} Lab**

<!-- Analyze, evaluate, and create verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- [ ] update lab -->

**What**: [Dataset manipulation: tokenization and joining datasets](https://github.com/lin380/lab_8)\
**How**: Clone, fork, and complete the steps in Lab 8.\
**Why**: To gain experience working with coding strategies for transforming datasets using tidyverse functions and regular expressions, practice reading/ writing data from/ to disk, and implement organizational strategies for organizing and documenting a dataset in reproducible fashion.
:::

## Questions {.unnumbered}

::: {.callout}
{{< fa wrench >}} **Conceptual questions**

1. ...
2. ...

:::

::: {.callout}
{{< fa wrench >}} **Technical questions**

1. ...
2. ...
:::
