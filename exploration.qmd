---
execute: 
  echo: true
--- 

# Exploration {#sec-exploration}

```{r}
#| label: setup-options
#| child: "_common.qmd"
#| cache: false
```

::: {.callout-caution title="Caution"}
Under development.
:::

<!--

Content:

Bxploratory Data Analysis (EDA) is a set of methods used for analyzing and exploring text data in the areas of text analysis and corpus linguistics. Descriptive methods commonly used in EDA include data visualization, word frequency and distribution analysis, and collocation analysis. These methods help identify common words and phrases, as well as relationships between different elements in the text. 

Additionally, unsupervised learning methods such as clustering, topic modeling, semantic analysis, and word embedding are used in EDA. These techniques help group similar items, extract topics, understand content, and represent words as numerical vectors for various tasks.



Descriptive analysis:
- `tidytext` package
	- `unnest_tokens()` function
	- `count()` function
	- `bind_tf_idf()` function
	- `widyr` package for co-occurrence analysis
	- `tidy()`
  
- `quanteda` package
  - `dfm()` objects
    - matrix subsetting `[, ]`
    - `dfm_select()`, `dfm_trim()`, `dfm_remove()`, `dfm_keep()`, `dfm_sample()

Unsupervised learning:
- `cluster()`, `factoextra` package
- `kmeans()`, `factoextra` package
- `hclust()`, `factoextra` package
- `topicmodels` package
- `text2vec` package

Exercises:

- [ ] add concept questions to Activities
- [ ] add exercises to Activities
- [ ] add thought questions/ case studies to prose sections

Formatting:

-->

<!---
> The real voyage of discovery consists not in seeking new landscapes, but in having new eyes.
> 
> --- Marcel Proust 
--->


> The data speaks for itself, but only if we are willing to listen. 
> 
> --- Nate Silver

::: {.callout}
**{{< fa regular list-alt >}} Outcomes**

<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

- Identify when an exploratory data analysis approach is the best fit for a given research project.
- Describe the fundamental methods of descriptive analysis and unsupervised learning, recognizing their strengths in revealing patterns and summarizing data.
- Interpret the basic insights gained from data summarization and pattern recognition, considering how these insights could guide further questions or research.
:::

```{r}
#| label: exploration-data-packages
#| echo: false
#| message: false

```

In this chapter, we examine a wide range of strategies for deriving insight from data in cases where the researcher does not start with a preconceived hypothesis or prediction, but rather the researcher aims to uncover patterns and associations from data allowing the data to guide the trajectory of the analysis. The chapter outlines two main branches of exploratory data analysis: 1) descriptive analysis which statistically and/ or visually summarizes a dataset and 2) unsupervised learning which is a machine learning approach that does not assume any particular relationship between variables in a dataset. Either through descriptive or unsupervised learning methods, exploratory data analysis employs quantitative methods to summarize, reduce, and sort complex datasets and statistically and visually interrogate a dataset in order to provide the researcher novel perspective to be qualitatively assessed.

::: {.callout}
**{{< fa terminal >}} Lessons**

<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- [ ] lesson: update the why -->

**What**: [Matrices, Exploratory Visualization](https://github.com/qtalr/lessons)\
**How**: In the R Console pane load `swirl`, run `swirl()`, and follow prompts to select the lesson.\
**Why**: Learn how to work with matrices to store and analyze numeric data using `quanteda` and to further your understanding of graphically representing data using `ggplot2` and other packages for more advanced plotting.
:::

## Orientation {#sec-eda-orientation}

The aim of this section is to provide an overview of exploratory data analysis (EDA). We will delve into various descriptive methods, such as frequency analysis and co-occurrence analysis, which are fundamental tools in linguistic research. However, our exploration won't stop there. We will also integrate modern exploratory methods from unsupervised learning approaches, including clustering, topic modeling, and vector space modeling. This may sound overwhelming, but I will strive to keep explanations clear and concise, ensuring their practicality and relevance to your linguistic inquiries is apparent. To this end, we will provide real-world examples to exemplify the applicability of these methodologies. 

### Research goal {#sec-eda-research-goal}

As discussed in @sec-aa-explore and @sec-fr-aim, the goal of exploratory data analysis is to discover, describe, and posit new hypotheses. The researcher does not start with a preconceived hypothesis or prediction, but rather the researcher aims to uncover patterns and associations from data allowing the data to guide the trajectory of the analysis. This analysis approach is best-suited for research where the literature on a research question is limited, or where the researcher is interested in exploring a new research question. 

Since the researcher does not start with a preconceived hypothesis, the researcher is not able to test a hypothesis and generalize to a population, but rather the researcher is able to describe the data and provide a new perspective to be qualitatively assessed. This is achieved through an iterative and inductive process of data exploration, where the researcher uses quantitative methods to summarize, reduce, and sort complex datasets and statistically and visually interrogate a dataset letting the data guide the analysis. 

### Approach {#sec-eda-approach}

<!-- Note:  

This section should cover the following:
- Workflow: Identify, Inspect, Interrogate, Interpret
  - Add a few words on the common aspects with other research approaches
  - And point out the differences (use of data, results, interpretation, etc.)

Drop the independent sections in favor of a more integrated approach.
-->

The approach to exploratory data analysis is iterative and inductive. To reign in the analysis, however, it is important to have a research question to guide the analysis. The research question will often be broad and exploratory in nature, but it will provide a framework for the analysis including the unit of analysis and sometimes the units of observation. Yet the units of observation can be modified as needed to address the research question. Furthermore, the methods applied to the data can evolve as the research unfolds. The researcher may start with a descriptive analysis and then move to an unsupervised learning approach, or vice versa. The researcher may also pivot the approach to explore new questions and new variables. Ultimately, the researcher is guided by the data and the research question, but the researcher is not bound by a preconceived hypothesis or prediction.

<!-- Workflow -->

With a research question and relevant data in hand, we can look to conduct the analysis. The workflow for exploratory data analysis is as follows:

1. Identify and extract the variables of interest in the dataset
2. Inspect the dataset to ensure the quality of the data and understand its characteristics
3. Interrogate the dataset using descriptive analysis and/ or unsupervised learning
4. Interpret the results of the analysis to determine if they are meaningful and if they provide a new perspective on the research question
5. (Optional) Pivot and repeat steps 1-4 to explore new questions and new variables

Let's elaborate on each of these steps. First, we want to consider our research question and identify the variables of potential interest to provide insight to our question. Starting with a transformed dataset means that much of the data preparation has already been done, but we may need to further transform the data, either up front or as we explore the data. In text analysis, this often includes identifying and extracting the linguistic variables of interest, such as words, $n$-grams, sentences, *etc*. Depending on the annotation scheme, other linguistic variables may be of interest, such as part-of-speech tags, syntactic dependencies, semantic roles, *etc*.

We may also want to consider the operational measures of the variables derived from the text, such as frequency, dispersion, co-occurrence, keyness, *etc*.  We may also want to consider the other variables in the dataset that may be target for grouping or filtering the dataset, such as speaker information, document information, linguistic unit information, *etc*. 

During or after extracting and operationalizing the variables of interest, we want to inspect the dataset to ensure the quality of the data and understand its characteristics. This may include checking for missing data, checking for outliers, checking for errors, checking for inconsistencies, *etc*. We may also want to inspect the distribution of the variables of interest to understand their characteristics. Summary statistics and visualizations, such as those covered in @sec-aa-diagnose, are useful for inspecting the dataset and also provide a foundation for interrogating the dataset.

Once we have identified the variables of interest and inspected the dataset, we can interrogate the dataset using descriptive analysis and/ or unsupervised learning. Descriptive analysis is a set of methods that statistically and/ or visually summarizes a dataset. Descriptive analysis can be used to describe a dataset and to identify linguistic units (frequency analysis) or co-occuring (co-occurrence analysis) units that are distinctive to a particular group or sub-group in the dataset. Unsupervised learning is a machine learning approach that does not assume any particular relationship between variables in a dataset. It can be used to identify groupings (clustering) in the data including patterning of linguistic units, identifying semantically similar topics (topic modeling), and estimating word context relationships (vector space modeling). 

<!-- Interpret -->

Exploratory methods will produce a set of statistical and/ or visual results. The researcher must interpret these results to determine if they are meaningful and if they provide a new perspective on the research question. Many times the results from one method will lead to new questions which can be explored with other methods. In some cases, the results may not be meaningful and the researcher may need to return to the data preparation stage to modify the dataset or the variables of interest. As the aim of exploratory analysis is just that, to explore, the researcher can pivot the approach to explore new questions and new variables. Ultimately, what is meaningful is determined by the researcher in the light of the research question and the potential insight obtained from the results.

## Analysis {#sec-eda-analysis}

In this section will discuss exploratory data analysis (EDA) for linguists, with a focus on descriptive methods such as frequency analysis and co-occurence analysis, as well as unsupervised learning approaches such as clustering, topic modelling, and word embedding. To ground the discussion, we will use the the Manually Annotated Sub-Corpus (MASC) of the American National Corpus. The data dictionary for the `masc_transformed` dataset is shown in @tbl-eda-masc-dd-show.

<!-- Show data dictionary -->

```{r}
#| label: tbl-eda-masc-dd-show
#| tbl-cap: "Data dictionary for the MASC dataset."
#| message: false
#| echo: false

# Read in the data dictionary
read_csv("data/masc_transformed_dd.csv") |> 
  kable() |> 
  kable_styling()
```

<!-- Load the MASC dataset/ preview -->

We will work with the MASC as our dataset to approach a task, more than a question. The task will be to identify relevant materials for an English Language Learner (ELL) textbook. This will involve multiple research questions and allow us to illustrate some very fundamental concepts that will emerge across text analysis research. 

First, I'll read in the dataset and only keep the variables that will pertain to our task, dropping the `description` and `domain` variables, and preview the dataset in @exm-eda-masc-read.

::: {#exm-eda-masc-read}

```{r}
#| label: exm-eda-masc-read-show
#| eval: false

# Read and subset the MASC dataset
masc_tbl <- 
  read_csv("../data/masc/masc_transformed.csv") |> 
  select(-description, -domain)

# Preview the MASC dataset
masc_tbl |> 
  slice_head(n = 5)
```

```{r}
#| label: exm-eda-masc-read-run
#| message: false
#| echo: false

# Read and subset the MASC dataset
masc_tbl <- 
  read_csv("data/masc_transformed.csv") |> 
  select(-description, -domain) 

# Preview the MASC dataset
masc_tbl |> 
  slice_head(n = 5)
```
:::

From the output in @exm-eda-masc-read, we should note a couple of things. First the `doc_id` is treated as numeric `<dbl>` and it is not a quantitative variable --we should change this vector type to `<chr>`. Second, at some point in our analysis we may need to recode some of the character variables to factor variables as analysis methods may require this.

::: {#exm-eda-masc-doc-id}
```{r}
#| label: exm-eda-masc-doc-id

# Change doc_id to character
masc_tbl <- 
  masc_tbl |> 
  mutate(doc_id = as.character(doc_id))
```
:::

To get a better sense of distribution of the dataset, let's use `skim()` from the `skimr` package to generate a summary of the dataset. In particular, let's just focus on the character variables by using `yank("character")`, as seen in @exm-eda-masc-skim.

::: {#exm-eda-masc-skim}
```{r}
#| label: tbl-exm-eda-masc-skim
#| tbl-cap: "Summary of the MASC dataset."

# Load package
library(skimr)

# Generate summary of the MASC dataset
masc_tbl_skm <- 
  masc_tbl |> 
  skim()

# Pull character variables
masc_tbl_skm |> 
  yank("character") |> 
  kable()
```
:::

Looking at @tbl-exm-eda-masc-skim, we see that there are 392 documents, two modalities, 18 genres, over 30k unique terms (which are words), over 28k lemmas (word base forms), and 39 distinct part-of-speech tags.

<!--  
[ ] the skim shows that there are some 'words' up to 99 characters long, which are likely errors
[ ] there are 25 missing tokens and 4 missing lemmas
-->

### Descriptive analysis {#sec-eda-descriptive} 

Descriptive analysis includes common techniques such as frequency analysis to determine the most frequent words or phrases, dispersion analysis to see how terms or topics are distributed throughout a document or corpus, keyword analysis to identify distinctive terms, and/ or co-occurrence analysis to see what terms tend to appear together. 

Using the MASC dataset, we will entertain questions such as: 

- What are the most common terms a beginning ELL should learn?
- Are there term differences between spoken and written discourses that should be emphasized?
- What are the most common verb particle constructions?

Along the way, we will introduce some fundamental concepts in text analysis such as tokens and types and frequency, dispersion, and co-occurrence measures. In addition, we will apply various descriptive analysis techniques and visualizations to explore the dataset and identify new questions and new variables of interest.

#### Frequency analysis {#sec-eda-frequency}

<!-- 4 I's: identify, inspect, interrogate, interpret -->

At its core, frequency analysis is a descriptive method that counts the number of times a linguistic unit, or term, (*i.e.* word, $n$-gram, sentence, *etc*.) occurs in a dataset. The results of frequency analysis can be used to describe the dataset and to identify terms that are linguistically distinctive or distinctive to a particular group or sub-group in the dataset.

<!--- Raw frequency (counting) --->

##### Raw frequency {#sec-eda-frequency-raw}

Let's consider what the most common words in the MASC dataset are as a starting point to making inroads on our task by identifying relevant vocabulary for an ELL textbook.

In the `masc_tbl` data frame we have the linguistic unit `term` which corresponds to the word-level annotation of the MASC. The `lemma` corresponds to the base form of each term, for words with inflectional morphology, the lemma is the word sans the inflection (*e.g.* is - be, are - be). For other words, the `term` and the `lemma` will be the same (*e.g.* the - the, in - in). These two variables pose a choice point for us: do we consider words to be the actual forms or the base forms? There is an argument to be made for both. In this case I will operationalize our linguistic unit as the `lemma` variable, as this will allow us to group words with inflectional morphology together. 

To perform a basic word frequency analysis, we start by using the `count()` function from the `dplyr` package to count the number of times each lemma occurs in the dataset. We'll sort by the most frequent lemmas, as seen in @exm-eda-masc-count.

::: {#exm-eda-masc-count}
```{r}
#| label: eda-masc-count

# Lemma count, sorted
masc_tbl |> 
  count(lemma, sort = TRUE)
```
:::

The output of this frequency tabulation in @exm-eda-masc-count is a data frame with two columns: `lemma` and `n`. The `lemma` column contains the unique lemmas in the dataset, and the `n` column contains the frequency of each lemma. The data frame is sorted in descending order by the frequency of lemmas. Now the result includes over 28,000 rows --which corresponds to the number of unique lemmas in the dataset. 

- [ ] re-write this to make tokens-types make more sense with 'term' and 'lemma'

At this point, it is important to define two key concepts that are fundamental to working with text. First, a **term** is a defined linguistic unit extracted from a corpus. In our dataset, the terms are words, such as 'the', 'houses', 'are'. A lemma is an annotated recoding of words which represent the uninflected base form of a word. In either case, the term or lemma is an instance of a linguistic unit. These instances are called **tokens**. When we count the number of times a term or lemma occurs in a dataset, we are counting the number of tokens (`n`), such as in @exm-eda-masc-count. Now, the list of unique linguistic units is a list of **types** (`lemma`). By definition, then, there will always be at least as many tokens as types, but more often than not (many) more tokens than types.

Our first pass a calculating lemma frequency in @exm-eda-masc-count should bring something else to our attention. As we can see among the most frequent lemmas are non-words such as `,`, and `.`. As you can imagine, given the conventions of written and transcriptional language, these types are very frequent. For a frequency analysis focusing on words, however, we should probably remove them. Thinking ahead, there may also be other non-words that we want to remove, such as symbols, numbers, *etc*. Let's take a look at @tbl-eda-masc-pos, where I've counted the part-of-speech tags `pos` in the dataset to see what other non-words we might want to remove.

::: {#exm-eda-masc-pos}
```{r}
#| label: tbl-eda-masc-pos
#| tbl-cap: "Part-of-speech tags in the MASC dataset."
#| message: false

# [ ] consider how to present this better, more concisely

# Part-of-speech tags
masc_tbl |> 
  count(pos) |> 
  arrange(pos) |> 
  kable()
```
:::

Consulting the [PENN Tagset online](https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html), we can see that the `pos` variable includes a number of non-words or other elements to exclude including: 

- 'CD' - Cardinal number
- 'FW' - Foreign word 
- 'LS' - List item marker
- 'SYM' - Symbol 

This modified tagset has grouped the punctuation tags into a single tag, 'PUNCT'.

We can use this information to remove lemmas that are tagged with either of these values. We can do this by filtering the data frame to only include lemmas that are not tagged with the `pos` values listed above, as seen in @exm-eda-masc-count-filter.

- [ ] think about how to name, rename objects: `masc_words_tbl`?

::: {#exm-eda-masc-count-filter}
```{r}
#| label: eda-masc-count-filter

# Filter out lemmas with PUNCT or SYM for pos
masc_tbl <- 
  masc_tbl |> 
  filter(!(pos %in% c("CD", "FW", "LS", "SYM", "PUNCT")))

# Lemma count, sorted (again)
masc_tbl |> 
  count(lemma, sort = TRUE)
```
:::

Now we are only viewing the most frequent words in the dataset, which reduces the number of observations to around 26k. Let's now explore the frequency distribution of the tokens. In @fig-eda-masc-count-plots, I've created three plots which include: 1) all the types, 2) the top 100 types, and 3) the top 10 types in the dataset.

```{r}
#| label: fig-eda-masc-count-plots
#| fig-cap: "Frequency plots of tokens in the MASC dataset"
#| fig-subcap:
#|  - "All types"
#|  - "Top 100 types"
#|  - "Top 10 types"
#| layout-ncol: 3
#| fig-height: 4

# [ ] consider how to present the 'all types' plot better, more concisely

# Plot lemma count for all types
masc_tbl |> 
  count(lemma) |>
  arrange(desc(n)) |>
  ggplot(aes(x = reorder(lemma, desc(n)), y = n)) +
  geom_col() +
  labs(x = "Types", y = "Token frequency") +
  theme(axis.text.x = element_blank())

# Plot lemma count for top 100 types
masc_tbl |>
  count(lemma) |>
  arrange(desc(n)) |>
  slice_head(n = 100) |>
  ggplot(aes(x = reorder(lemma, desc(n)), y = n)) +
  geom_col() +
  labs(x = "Types", y = "Token frequency") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1.3))

# Plot lemma count for top 10 types
masc_tbl |> 
  count(lemma) |>
  arrange(desc(n)) |>
  slice_head(n = 10) |>
  ggplot(aes(x = reorder(lemma, desc(n)), y = n)) +
  geom_col() +
  labs(x = "Types", y = "Token frequency") +
  theme(axis.text.x = element_text(angle = 65, hjust = 1.3))
```

The distributions we see in @fig-eda-masc-count-plots are highly right-skewed (in @fig-eda-masc-count-plots-1 in a very extreme way!). This is typical of natural language distributions, notably documented by George Kingsley Zipf [@Zipf1949]. This type of distribution approaches the theoretical Zipf distribution. A Zipf (or Zipfian) distribution is characterized by the fact that the frequency of any word is inversely proportional to its rank in the frequency table. In other words, the most frequent word occurs approximately twice as often as the second most frequent word, three times as often as the third most frequent word, and so on.

As we can see, our distribuions to not follow the Zipf distribution exactly. This is because the Zipf distribution is a theoretical distribution, and the actual distribution of words in a corpus is affected by various sampling factors, including the size of the corpus. The larger the corpus, the closer the distribution will be to the Zipf distribution.

::: {.callout}
**{{< fa medal >}} Dive deeper**

As stated above, Zipfian distributions are typical of natural language and are observed a various linguistic levels. This is because natural language is a complex system, and complex systems tend to exhibit Zipfian distributions. Other examples of complex systems that exhibit Zipfian distributions include the size of cities, the frequency of species in ecological communities, the frequency of links in the World Wide Web, *etc.*
:::

The observation captured in the Zipf distribution is key to understanding quantitative text analysis. It demonstrates that most of the types in a corpus occur (relatively) infrequently, while a small number of types occur very frequently. In fact, if we calculate the cumulative frequency of the lemmas in the `masc_tbl` data frame, we can see that the top 10 types account for over 20% of the lemmas used in the dataset --by 100 types that increases to over 40%, as seen in @exm-eda-masc-count-cumulative.

::: {#exm-eda-masc-count-cumulative}
```{r}
#| label: fig-eda-masc-count-cumulative
#| fig-cap: "Cumulative frequency of lemmas in the MASC dataset"
#| fig-height: 4
#| fig-width: 8

# Calculate cumulative frequency
lemma_cumul_freq <- 
  masc_tbl |> 
  count(lemma) |> 
  arrange(desc(n)) |> 
  mutate(cumulative = cumsum(n)) |> 
  mutate(percent = cumulative / sum(n))

lemma_cumul_freq |>
  slice_head(n = 2000) |> 
  ggplot(aes(x = reorder(lemma, desc(n)), y = percent)) +
  geom_col() +
  geom_vline(xintercept = 10, linetype = "dashed") +
  geom_vline(xintercept = 100, linetype = "dashed") +
  # annotate("text", x = 10+10, y = 0.5, label = "10 lemmas") +
  # annotate("text", x = 100+10, y = 0.5, label = "100 lemmas") +
  scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
  labs(x = "Types", y = "Cumulative frequency percent") +
  theme(axis.text.x = element_blank())
```
:::

If we look at the types that appear within the first 100 most frequent, you can likely also appreciate another thing about language use. Let's list the top 100 types in @exm-eda-masc-count-top-100.

<!---
  [ ] work on text output
--->
::: {#exm-eda-masc-count-top-100}
```{r}
#| label: tbl-eda-masc-count-top-100
#| tbl-cap: "Top 100 lemma types in the MASC dataset."

# Top 100 types
lemma_cumul_freq |> 
  slice_head(n = 100) |> 
  pull(lemma) |> 
  matrix(ncol = 10, byrow = TRUE) |>
  kable(col.names = NULL)
```
:::


For the most part, the most frequent words are not content words, but rather function words (*e.g.* determiners, prepositions, pronouns, auxiliary verbs). Function words include a closed class of relatively few words that are used to express grammatical relationships between content words. It then is no surprise that they are the comprise many of the most frequent words in a corpus. 

Another key observation is that among the most frequency content words (*e.g.* nouns, verbs, adjectives, adverbs) are words that are quite semantically generic --that is, they are words that are used in a wide range of contexts and take a wide range of meanings. Take for example the adjective 'good'. It can be used to describe a wide range of nouns, such as 'good food', 'good people', 'good times', *etc*. A sometimes near-synonym of 'good', for example 'good student', is the word 'studious'. Yet, 'studious' is not as frequent as 'good' as it is used to describe a narrower range of nouns, such as 'studious student', 'studious scholar', 'studious researcher', *etc*. In this way, 'studious' is more semantically specific than 'good'.

::: {.callout}
**{{< fa regular lightbulb >}} Consider this**

Based on what you now know about the expected distribution of words in a corpus, what if your were asked to predict what the most frequency English word used is in each U.S. State? What would you predict? How confident would you be in your prediction? What if you were asked to predict what the most frequency word used is in the language of a given country? What would you want to know before making your prediction?
:::

So common across corpus samples, in some analyses these usual suspects of the most common words are considered irrelvant and are filtered out. In our ELL materials task, however, we might exclude them for this simple fact that it will be a given that we will teach these words given their grammatical importance. If we want to focus on the most common content words, we can filter out the function words.

One approach to filtering out these words is to use a pre-determined list of **stopwords**. The `tidytext` package includes a data frame `stop_words` of stopword lexicons for English. We can select a lexicon from `stop_words` and use `anti_join()` to filter out the words that appear in the `word` variable from the `lemma` variable in the `masc_tbl` data frame. In @exm-eda-masc-count-stop-words, I perform this filtering and then re-run the frequency analysis for the top 100 lemmas.

::: {#exm-eda-masc-count-stop-words}
```{r}
#| label: tbl-eda-masc-count-stop-words
#| tbl-cap: "Frequency of tokens in the MASC dataset after filtering out stopwords"

# Load package
library(tidytext)

# Select stopword lexicon
stopwords <- 
  stop_words |> 
  filter(lexicon == "SMART")

# Filter out stop words
anti_join(
  x = masc_tbl,
  y = stopwords,
  by = c("lemma" = "word")
  ) |>
  count(lemma, sort = TRUE) |> 
  slice_head(n = 100) |> 
  pull(lemma) |> 
  matrix(ncol = 10, byrow = TRUE) |>
  as_tibble() |> 
  kable(col.names = NULL)
```
:::

The resulting list in @tbl-eda-masc-count-stop-words paints a different picture of the most frequent words in the dataset. The most frequent words are now content words, and included in most frequent words are more semantically specific words.

Eliminating words in this fashion, however, may not always be the best approach. Available lists of stopwords vary in their contents and are determined by other researchers for other potential uses. We may instead opt to create our own stopword list that is tailored to the task, or we may opt to use a statistical approach based on their distribution in the dataset using a combination of frequency and dispersion measures, as we will see in [the next section.]

For our case, however, we have another strategy to apply. Since our task is to identify relevant vocabulary, beyond the fundamental function words in English, we can use the part-of-speech tags to reduce our dataset to just the content words, that is nouns, verbs, adjectives, and adverbs. We need to consult the Penn Tagset again, to ensure we are selecting the correct tags. I will assign this data frame to `masc_content_tbl` to keep it separate from our main data frame `masc_tbl`, seen in @exm-eda-masc-filter-pos.

::: {#exm-eda-masc-filter-pos}
```{r}
#| label: eda-masc-filter-pos

# Penn Tagset for content words
# Nouns: NN, NNS,
# Verbs: VB, VBD, VBG, VBN, VBP, VBZ
# Adjectives: JJ, JJR, JJS
# Adverbs: RB, RBR, RBS

content_pos <- c("NN", "NNS", "VB", "VBD", "VBG", "VBN", "VBP", "VBZ", "JJ", "JJR", "JJS", "RB", "RBR", "RBS")

# Select content words
masc_content_tbl <- 
  masc_tbl |> 
  filter(pos %in% content_pos)
```
:::

We now have reduced the number of observations by `r label_percent()(1-nrow(masc_content_tbl) / nrow(masc_tbl))` focusing on the content words. We are getting closer to identifying the vocabulary that we want to include in our ELL materials, but we will need some more tools to help us identify the most relevant vocabulary.

##### Dispersion {#sec-eda-frequency-dispersion}

**Dispersion** is a measure of how evenly distributed a linguistic unit is across a dataset. This is a key concept in text analysis, as important as frequency. It is important to recognize that frequency and dispersion are measures of different characteristics. We can have two words that occur with the same frequency, but one word may be more evenly distributed across a dataset than the other. Depending on the researcher's aims, this may be an important distinction to make. For our task, it is likely the case that we want to capture words that are well-dispersed across the dataset as words that have a high frequency and a low dispersion tend to be connected to a particular context, whether that be a particular genre, a particular speaker, a particular topic, *etc*. In other research, aim may be the reverse; to identify words that are highly frequent and highly concentrated in a particular context to identify words that are distinctive to that context.

This a wide variety of measures that can be used to estimate the distribution of types across a dataset. Let's focus on three measures: document frequency ($df$), inverse document frequency ($idf$), and Gries' Deviation of Proportions ($dp$).

The most basic measure is **document frequency** ($df$). This is the number of documents in which a type appears at least once. For example, if a type appears in 10 documents, then the document frequency is 10. This is a very basic measure, but it is a good starting point.

A nuanced version of document frequency is **inverse document frequency** ($idf$). This measure takes the total number of documents and divides it by the document frequency. This results in a measure that is inversely proportional to the document frequency. That is, the higher the document frequency, the lower the inverse document frequency. This measure is often log-transformed to spread out the values.

One thing to consider about $df$ and $idf$ is that niether takes into account the length of the documents in which the type appears nor the spread of the type within each document. To take these factors into account, we can use Gries' Deviation of Proportions ($dp$) measure [@Gries2023, pp. 87-88]. The $dp$ measure is calculated as the difference between the proportion of a tokens in a document and tokens in the corpus. The metric can be subtracted from 1 to create a normalized measure of dispersion ranging between 0 and 1, with lower values being more dispersed.

Let's consider how these measures differ with three scenarios: 

Imagine a type with a token frequency of 100 appears in each of the 10 documents in a corpus.

A. Each of the documents is 100 words long. The type appears 10 times in each document. 
B. Each of the documents is 100 words long. But now the type appears once in 9 documents and 91 times in 1 document. 
C. Nine of the documents constitute 99% of the corpus. The type appears once in each of the 9 documents and 91 times in the 10th document.

Scenario A is the most dispersed, scenario B is less dispersed, and scenario C is the least dispersed. Yet, the type's $df$ and $idf$ scores will be the same. But the $dp$ score will reflect increasing concentration of the type from A to B to C. You may wonder why we would want to use $df$ or $idf$ at all. The answer is some combination of the fact that they are computationally less expensive to calculate, they are widely used (especially $idf$), and/ or in many practical situations they often highly correlated with $dp$.

So for our task we will use $dp$ as our measure of dispersion. The `qtalrkit` package includes the `calc_type_metrics()` function which calculates, among other metrics, the dispersion metrics $df$, $idf$, and/ or $dp$. Let's select `dp` and assign the result to `masc_lemma_disp`, as seen in @exm-eda-masc-dp.

::: {#exm-eda-masc-dp}
```{r}
#| label: eda-masc-dp

# Load package
library(qtalrkit)

# Calculate deviance of proportions (DP)
masc_lemma_disp <- 
  masc_content_tbl |> 
  calc_type_metrics(
    type = lemma, 
    documents = doc_id, 
    dispersion = "dp"
  ) |> 
  arrange(dp)

# Preview
masc_lemma_disp |> 
  slice_head(n = 10)
```
:::

We would like to identify lemmas that are frequent and well-dispersed. But an important question arises, what is the threshold for frequency and dispersion that we should use to identify the lemmas that we want to include in our ELL materials?

::: {.callout}
**{{< fa regular lightbulb >}} Consider this**

You may be wondering why the Inverse Document Frequency is, in fact, the inverse of the document counts, instead of just a count of the documents that each type appears in. The $idf$ is a very common measure in machine learning that is used in combination with (term) frequency to calculate the $tf-idf$ (term frequency-inverse document frequency) measure. That is, the product of the frequency of a term and the inverse document frequency of the term. This serves as a weighting measure that lowers the $tf-idf$ score for terms that are frequent across documents and increases the $tf-idf$ score for terms that are infrequent across documents. 

Consider what types will end up with a high or a low $tf-idf$ score. What use(s) could this measure have for distinguishing between types in a corpus? 

Hint: consider the earlier discussion of stopword lists.
:::

There are statistical approaches to identifying natural breakpoints, including clustering, but a visual inspection is often good enough for practical purposes. Let's create a density plot to see if there is a natural break in the distribution of our dispersion measure, as seen in @fig-eda-masc-dp-density.

::: {#exm-eda-masc-dp-density}
```{r}
#| label: fig-eda-masc-dp-density
#| fig-cap: "Density plot of Deviation of Proportions for lemmas in the MASC dataset"
#| fig-width: 8
#| fig-height: 4

# Density plot of dp
masc_lemma_disp |> 
  ggplot(aes(x = dp)) +
  geom_density() +
  scale_x_continuous(breaks = seq(0, 1, .1)) +
  labs(x = "Deviation of Proportions")
```
:::

What we are looking for is an elbow in the distribution of dispersion measures. In @fig-eda-masc-dp-density, we can see that there is distinctive bend in the distribution between .85 and .97. We can split the difference and use this as a threshold to filter out lemmas that are less dispersed. In @exm-eda-masc-dp-filter, I filter out lemmas that have a dispersion measure less than .91. Then in @tbl-eda-masc-dp-filter, I preview the top and bottom 50 lemmas in the dataset.

::: {#exm-eda-masc-dp-filter}
```{r}
#| label: tbl-eda-masc-dp-filter
#| tbl-cap: "Frequency of tokens in the MASC dataset after filtering out lemmas with a Deviation of Proportions less than .91"
#| tbl-subcap: 
#| - "Top 50 lemmas"
#| - "Bottom 50 lemmas"
#| layout-nrow: 2

# Filter for lemmas with dp <= .91
masc_lemma_disp_thr <- 
  masc_lemma_disp |> 
  filter(dp <= .91) |> 
  arrange(desc(n))

# Preview top
masc_lemma_disp_thr |> 
  slice_head(n = 50) |> 
  pull(type) |>
  matrix(ncol = 10, byrow = TRUE) |> 
  kable(col.names = NULL)

# Preview bottom
masc_lemma_disp_thr |>
  slice_tail(n = 50) |> 
  pull(type) |>
  matrix(ncol = 10) |> 
  kable(col.names = NULL)
```
:::

We now have a good candidate list of common vocabulary that is spread well across the corpus. 

##### Relative frequency {#sec-eda-frequency-relative}

Gauging frequency and dispersion across the entire corpus is a good starting point for any frequency analysis, but it is often the case that we want to compare the frequency and dispersion of linguistic units across corpora or sub-corpora. 

In the case of the MASC dataset, for example, we may want to compare metrics across the two modalities or the various genres. Simply comparing frequency counts across these sub-corpora is not a good approach, and can be misleading, as the sub-corpora may vary in size. For example, if one sub-corpus is twice as large as another sub-corpus, then, all else being equal, the frequency counts will be twice as large in the larger sub-corpus. This is why we use relative frequency measures, which are normalized by the size of the sub-corpus.

::: {.callout}
**{{< fa regular lightbulb >}} Consider this**

A variable in the MASC dataset that has yet to be used is the `pos` part-of-speech variable. How could we use this variable to refine our frequency and dispersion analysis of lemma types?

Hint: consider lemma forms that may be tagged with different parts-of-speech.
:::

To normalize the frequency of linguistic units across sub-corpora, we can use the **relative frequency** measure. This is the frequency of a linguistic unit divided by the total number of linguistic units in the sub-corpus. This bakes in the size of the sub-corpus into the measure. The notion of relative frequency is key to all research working with text, as it is the basis for the statistical approach to text analysis where comparisons are made. 

There are some field-specific terms that are used to refer to relative frequency measures. For example, in information retrieval and Natural Language Processing, the relative frequency measure is often referred to as the **term frequency**. In corpus linguistics, the relative frequency measure is often modified slightly to include a constant (*e.g.* $rf * 100$) which is known as the **observed relative frequency**. Athough the observed relative frequency per number of tokens is not strictly necessary, it is often used to make the values more interpretable as we can now talk about an observed relative frequency of 1.5 as a linguistic unit that occurs 1.5 times per 100 linguistic units.

Let's consider how we might compare the frequency and dispersion of lemmas across the two modalities in the MASC dataset, spoken and written. To make this a bit more interesting and more relevant, let's add the `pos` variable to our analysis. The intent, then, will be to identify lemmas tagged with particular parts of speech that are particularly indicative of each of the modaliites. 

We can do this by collapsing the `lemma` and `pos` variables into a single variable, `lemma_pos`, with the `str_c()` function, as seen in @exm-eda-masc-type.

::: {#exm-eda-masc-type}
```{r}
#| label: eda-masc-type

# Collapse lemma and pos into type
masc_content_tbl <- 
  masc_content_tbl |> 
  mutate(lemma_pos = str_c(lemma, pos, sep = "_"))

# Preview
masc_content_tbl |> 
  slice_head(n = 10)
```
:::

Now this will increase the number of lemma types in the dataset as we are now considering lemmas where the same lemma form is tagged with different parts-of-speech. 

Getting back to calculating the frequency and dispersion of lemmas in each modality, we can use the `calc_type_metrics()` function with `lemma_pos` as our type argument. We will, however, need to apply this function to each sub-corpus independently and then concatenate the two data frames. This function returns a (raw) frequency measure by default, but we can specify the`frequency` argument to `rf` to calculate the relative frequency of the linguistic units as in @exm-eda-masc-metrics-modality.

::: {#exm-eda-masc-metrics-modality}
```{r}
#| label: eda-masc-metrics-modality

# Calculate relative frequency
# Spoken
masc_spoken_metrics <- 
  masc_content_tbl |> 
  filter(modality == "Spoken") |> 
  calc_type_metrics(
    type = lemma_pos, 
    documents = doc_id, 
    frequency = "rf",
    dispersion = "dp"
  ) |> 
  mutate(modality = "Spoken") |>
  arrange(desc(n))

# Written 
masc_written_metrics <- 
  masc_content_tbl |> 
  filter(modality == "Written") |> 
  calc_type_metrics(
    type = lemma_pos, 
    documents = doc_id, 
    frequency = "rf",
    dispersion = "dp"
  ) |> 
  mutate(modality = "Written") |> 
  arrange(desc(n))

# Concatenate spoken and written metrics
masc_metrics <-
  bind_rows(masc_spoken_metrics, masc_written_metrics)

# Preview
masc_metrics |> 
  slice_head(n = 10)
```
:::

With the `rf` measure, we are now in a position to compare 'apples to apples', as you might say. We can now compare the relative frequency of lemmas across the two modalities. Let's preview the top 10 lemmas in each modality, as seen in @exm-eda-masc-relative-frequency-top.

::: {#exm-eda-masc-relative-frequency-top}
```{r}
#| label: eda-masc-relative-frequency-top

# Preview top 10 lemmas in each modality
masc_metrics |> 
  group_by(modality) |> 
  slice_max(n = 10, order_by = rf) |>
  ungroup()
```
:::

We can appreciate, now, that there are similarities and a few differences between the most frequent lemmas for each modality. First, there are similar lemmas in written and spoken modalities, such as 'be', 'have', and 'not'. Second, the top 10 include verbs and adverbs. Now we are looking at the most frequent types, so it is not surprising that we see more in common than not. However, looking close we can see that contracted forms are more frequent in the spoken modality, such as 'isn't', 'don't', and 'can't' and that ordering of the verb tenses differs to some degree. Whether these are important distinctions for our task is something we will need to consider.

We can further cull our results by filtering out lemmas that are not well-dispersed across the sub-corpora. Although it may be tempting to use the threshold we used earlier, we should consider that the size of the sub-corpora are different and the distribution of the dispersion measure may be different. With this in mind, we need to visualize the distribution of the dispersion measure for each modality, as seen in @fig-eda-masc-dispersion-threshold.

```{r}
#| label: fig-eda-masc-dispersion-threshold
#| fig-cap: "Density plot of Deviation of Proportions for lemmas in the MASC dataset by modality"
#| fig-width: 8
#| fig-height: 4

# Density plot of dp by modality
masc_metrics |> 
  ggplot(aes(x = dp)) +
  geom_density(alpha = .5) +
  scale_x_continuous(breaks = seq(0, 1, .1)) +
  labs(x = "Deviation of Proportions", y = "Density") +
  facet_wrap(~ modality, ncol = 2, scales = "free_x")
```

As expected, the density plots point to different thresholds for each modality.The written subcorpus follows closely with the previous distribution, but the spoken subcorpus has more than one bend in the distribution. Why are there multiple peaks in the density plot? It points to some level of inconsistency in the spoken data, either potentially some level of context-dependent language use (genres, topics, speech styles) or it could be due to the fact that the spoken subcorpus' size is too small to provide a reliable distribution.

In any case, we can estimate the threshold for the spoken corpus making use of the largest peak in the distribution as the reference point. With this approach in mind, lets maintain the $.91$ threshold for the written subcorpus and use a $.79$ threshold for the spoken subcorpus. Let's filter out lemmas that have a dispersion measure less than .91 for the written subcorpus and less than .79 for the spoken subcorpus, as seen in @exm-eda-masc-subcorpora-filtered.

::: {#exm-eda-masc-subcorpora-filtered}
```{r}
#| label: eda-masc-subcorpora-filtered

# Filter for lemmas with
# dp <= .91 for written and 
# dp <= .79 for spoken
masc_metrics_thr <- 
  masc_metrics |> 
  filter(
    (modality == "Written" & dp <= .91) | 
    (modality == "Spoken" & dp <= .79)
  ) |> 
  arrange(desc(rf))
```
:::

Filtering the less-dispersed types reduces the dataset from `r nrow(masc_metrics)` to `r nrow(masc_metrics_thr)` observations. This will provide us with a more succinct list of common and well-dispersed lemmas that are used in each modality. 

As much as the frequency and dispersion measures can provide us with a good starting point, it does not provide an understanding of what types are more indicative of a particular sub-corpus, modality subcorpora in our case. We can do this by calculating the log odds ratio of each lemma in each modality. 

The **log odds ratio** is a measure that quantifies the difference between the frequencies of a type in two corpora or sub-corpora. In spirit and in name, it compares the odds of a type occurring in one corpus versus the other. The values range from negative to positive infinity, with negative values indicating that the type is more frequent in the first corpus and positive values indicating that the lemma is more frequent in the second corpus. The magnitude of the value indicates the strength of the association.

The `tidylo` package provides a convenient function `bind_log_odds()` to calculate the log odds ratio, and a weighed variant, for each type in each sub-corpus. Let's use this function to calculate the log odds ratio for each lemma in each modality, as seen in @exm-eda-masc-log-odds.

::: {#exm-eda-masc-log-odds}
```{r}
#| label: eda-masc-log-odds

# Load package
library(tidylo)

# Calculate log odds ratio
masc_metrics_thr <- 
  masc_metrics_thr |> 
  bind_log_odds(
    set = modality,
    feature = type,
    n = n, 
    unweighted = TRUE
  )

# Preview (ordered by log_odds)
# Spoken
masc_metrics_thr |> 
  slice_max(n = 10, order_by = log_odds)

# Written
masc_metrics_thr |> 
  slice_min(n = 10, order_by = log_odds)
```
:::


- [ ]  not clear to me what the log_odds are doing. I thought they would run from 1 to -1 with the more negative, the more indicative of writing (as it is alphabetically second)

[description of the results...]

The second measure produced by `bind_log_odds()` function, is the weighted log odds ratio. This measure provides a more robust and interpretable measure for comparing term frequencies across corpora, especially when term frequencies are low or when corpora are of different sizes. The weighting (or standardization) also makes it easier to identify terms that are particularly distinctive or characteristic of one corpus over another. Note that the weighted measure's interpretation is slightly different that the log odds's. The larger positive values in each corpus indicate that the type is more indicative of that (sub-)corpus, and the larger negative values indicate that the type is less indicative. 

Let's imagine we would like to extract the most indicative verbs for each modality using the weighted log odds as our measure. We can do this with a little regular expression magic. Let's use the `str_subset()` function to filter for lemmas that start with 'V' and then use `slice_max()` to extract the top 10 most indicative verb lemmas, as seen in @exm-eda-masc-log-odds-weighted-verbs.

::: {#exm-eda-masc-log-odds-weighted-verbs}
```{r}
#| label: eda-masc-log-odds-weighted-verbs

# Preview (ordered by log_odds_weighted)
# Spoken and written
masc_metrics_thr |> 
  group_by(modality) |> 
  filter(str_detect(type, "_V")) |> 
  slice_max(n = 10, order_by = log_odds_weighted) |>
  select(-n, -log_odds) |>
  ungroup() 
```
:::

Note that the log odds are larger for the spoken modality than the written modality. This indicates that theses types are more strongly indicative of the spoken modality than the types in the written modality are indicative of the written modality. This is not surprising, as the written modality is typically more diverse in terms of lexical usage than the spoken modality, where the terms tend to be repeated more often, including verbs.

#### Co-occurrence analysis {#sec-eda-co-occurrence}

Moving forward on our task, we have a good idea of the general vocabulary that we want to include in our ELL materials and can identify lemma types that are particularly indicative of each modality. Another useful approach to complement our analysis is to identify words that co-occur with our target lemmas --in particular verbs. In English it is common for verbs to appear with a preposition or adverb, such as 'give up', 'look after'. These 'phrasal verbs' form a semantic unit that is distinct from the verb alone.

In cases such as this, we are aiming to do a co-occurrence analysis. Co-occurrence analysis is a set of methods that are used to identify words that appear in close proximity to a target type. 

<!-- Concordances -->

An exploratory, primarily qualitative, approach is to display the co-occurrence of words in a Keyword in Context (KWIC). This is a table that displays the target word in the center of the table and the words that appear before and after the target word. This is a useful approach for spot identifying collocations of a target word or phrase. 

The `quanteda` package includes a function `kwic()` that can be used to create a KWIC table. It does require some transformation to the data, however. We need to collapse the `lemma` column into a single string for each document from the original transformed dataset, `masc_tbl`.  Then we can apply the `corpus()` and then `tokens` function to create a quanteda tokens object. Then we can apply the `kwic()` function to create a KWIC table.

::: {#exm-eda-masc-kwic}
```{r}
#| label: eda-masc-kwic


# Collapse lemma, pos into a single string
masc_text_tbl <- 
  masc_tbl |> 
  mutate(lemma_pos = str_c(lemma, pos, sep = "_")) |>
  group_by(doc_id, modality, genre) |> 
  summarize(text = str_c(lemma_pos, collapse = " ")) |> 
  ungroup()

# Load package
library(quanteda)

masc_corpus <- 
  masc_text_tbl |> 
  corpus(
    text_field = "text",
    docid_field = "doc_id"
  )

summary(masc_corpus)

masc_corpus |> 
  tokens() |> 
  kwic(
    pattern = phrase("*_V* *_IN*"),
    window = 3
  ) |> 
  slice_sample(n = 10)
``` 
:::

<!-- N-grams -->

A straightforward quantitative way to explore co-occurrence is to set the unit of observation to an $n-gram$ of terms. An $n-gram$ is a sequence of $n$ words. For example, a 2-gram is a sequence of two words, a 3-gram is a sequence of three words, and so on. Then, the frequency and dispersion metrics can be calculated for each $n-gram$.

In general, deriving $n-grams$ from a corpus is a straightforward process. The `tidytext` package includes a function `unnest_tokens()` that can be used to create $n-grams$ from a corpus. The function can take a single column of untokenized text or a tokenized column in combination with a variable to use as the grouping variable. In the `masc_tbl` dataset, we have tokenized text in the `lemma` column and a grouping variable in the `doc_id` column. We can use the `unnest_tokens()` function to create a new data frame with a row for each $n-gram$ in each document, as seen in @exm-eda-masc-bigrams-tidytext.

::: {#exm-eda-masc-bigrams-tidytext}
```{r}
#| label: eda-masc-bigrams-tidytext

# Load package
library(tidytext)

# Create bigrams
masc_tbl |> 
  unnest_tokens(
    output = bigrams, 
    input = lemma, 
    token = "ngrams", 
    n = 2, 
    collapse = "doc_id"
    ) |> 
  slice_head(n = 10)
```
:::

The result of this operation can be joined with the original dataset using the `doc_id` as the key variable. Then we can calculate the frequency and dispersion metrics for each $n-gram$ in each modality. However, this approach is not ideal for our task. The reason is that we are interested in identifying $n-grams$ that include verbs. The `unnest_tokens()` function does not allow us to filter the $n-grams$ by part-of-speech.

Another, more informative approach is to create a new variable that combines the lemma and part-of-speech for each observation before using `unnest_tokens()` to generate the bigrams. We can use the `str_c()` function from the `stringr` package to join the `lemma` and `pos` columns into a single string, so that we have a variable `lemma_pos` with the lemma and part-of-speech joined by an underscore. 

One consideration that we need to take for our goal to identify verb particle constructions, is how we ultimately want to group our `lemma_pos` values. This is particularly important given the fact that our `pos` tags for verbs include information about the verb's tense and person. This means that a verb in a verb particle bigram, such as 'look after', will be represented by multiple `lemma_pos` values, such as 'look_VB', 'look_VBP', 'look_VBD', and 'look_VBG'. If we want this level of detail, we just proceed as described above. However, if we want to group the verb particle bigrams by a single verb value, we need to recode the `pos` values for verbs. We can do this with the `case_match()` function from the `dplyr` package.

In @exm-eda-masc-lemma-pos, I recode the `pos` values for verbs to 'V' and then join the `lemma` and `pos` columns into a single string.

::: {#exm-eda-masc-lemma-pos}
```{r}
#| label: eda-masc-lemma-pos

# Collapse lemma into a single string
masc_lemma_pos_tbl <- 
  masc_tbl |> 
  mutate(pos = case_when(
    str_detect(pos, "^V") ~ "V",
    TRUE ~ pos
  )) |> 
  group_by(doc_id) |> 
  mutate(lemma_pos = str_c(lemma, pos, sep = "_")) |>
  ungroup()

# Preview
masc_lemma_pos_tbl |> 
  slice_head(n = 10)

masc_lemma_pos_tbl |> 
  unnest_tokens(
    output = bigrams, 
    input = lemma_pos, 
    token = "ngrams", 
    n = 2, 
    to_lower = FALSE,
    collapse = "doc_id"
  ) |> 
  filter(str_detect(bigrams, "_V.*_IN")) |> 
  calc_type_metrics(
    type = bigrams, 
    documents = doc_id, 
    frequency = "rf",
    dispersion = "dp"
  ) |> 
  arrange(desc(rf))
```
:::

We have identified and derived frequency and dispersion metrics for $n-grams$ that include verb particle construction candidates. Yet, there is a problem with this approach. The problem is that the $n-grams$ are not necessarily verb particle constructions in the sense that they form a semantic unit. Second, frequency and dispersion metrics are not necessarily the best measures for identifying the co-occurrence relationship between the verb and the particle. In other words, just because a two-word sequence is frequent and well-dispersed does not mean that the two words form a semantic unit. 

<!-- Collocation -->

To address these issues, we can use a statistical measures to estimate collocational strength between two words. A **collocation** is a sequence of words that co-occur more often than would be expected by chance. The most common measure of collocation is the **pointwise mutual information** (PMI) measure. The PMI measure is calculated as the log ratio of the observed frequency of two words co-occurring to the expected frequency of the two words co-occurring. The expected frequency is calculated as the product of the frequency of each word. The PMI measure is a log ratio, so the values range from negative to positive infinity, with negative values indicating that the two words co-occur less often than would be expected by chance and positive values indicating that the two words co-occur more often than would be expected by chance. The magnitude of the value indicates the strength of the association.

Let's calculate the PMI for all the bigrams in the MASC dataset. We can use the `calc_assoc_metrics()` function from `qtalrkit`. We need to specify the `association` argument to `pmi` and the `type` argument to `bigrams`, as seen in @exm-eda-masc-bigrams-pmi.

::: {#exm-eda-masc-bigrams-pmi}
```{r}
#| label: eda-masc-bigrams-pmi

masc_lemma_pos_assoc <- 
  masc_lemma_pos_tbl |> 
  calc_assoc_metrics(
    doc_index = doc_id, 
    token_index = term_num, 
    type = lemma_pos, 
    association = "pmi"
  )

# Preview 
masc_lemma_pos_assoc |> 
  arrange(desc(pmi)) |>
  slice_head(n = 10)
```
:::

One caveat to using the PMI measure is that it is sensitive to the frequency of the words. If the words in a bigram pair are infrequent, and especially if they only occur once, then the PMI measure will be inflated. To mitigate this issue, we can apply a frequency threshold to the bigrams before calculating the PMI measure. Let's filter out bigrams that occur less than 10 times, as seen in @exm-eda-masc-bigrams-pmi-filtered.

::: {#exm-eda-masc-bigrams-pmi-filtered}
```{r}
#| label: eda-masc-bigrams-pmi-filtered

# Filter for bigrams that occur >= 10 times
masc_lemma_pos_assoc_thr <- 
  masc_lemma_pos_assoc |> 
  filter(n >= 10) |> 
  arrange(desc(pmi))

# Preview
masc_lemma_pos_assoc_thr |> 
  slice_head(n = 10)
```
:::

Now we are in a position to identify verb particle constructions. We can filter for bigrams that include a verb and a particle and that have a PMI measure greater than 0, as seen in @exm-eda-masc-bigrams-pmi-filtered-vpc.

::: {#exm-eda-masc-bigrams-pmi-filtered-vpc}
```{r}
#| label: eda-masc-bigrams-pmi-filtered-vpc

# Filter for bigrams that include a verb and a particle
# and that have a PMI measure greater than 0
masc_verb_part_assoc <- 
  masc_lemma_pos_assoc_thr |> 
  filter(str_detect(x, "_V")) |> 
  filter(str_detect(y, "_IN")) |>
  filter(pmi > 0) |> 
  arrange(x)

# Preview 
masc_verb_part_assoc |> 
  slice_head(n = 10)
```
:::

We can clean up the results a bit by removing the part-of-speech tags from the `x` and `y` variables, up our minimum PMI value, and create a network plot to visualize the results, as seen in @fig-eda-masc-verb-part-network.

```{r}
#| label: fig-eda-masc-verb-part-network
#| fig-cap: "Network plot of verb particle constructions in the MASC dataset"
#| fig-width: 8

# Clean up results
masc_verb_part_assoc_plot <-
  masc_verb_part_assoc |>
  filter(pmi >= 2) |>
  mutate(
    x = str_remove(x, "_V.*"),
    y = str_remove(y, "_IN")
  )

# Create an association network plot
# `x` and `y` are the nodes
# `pmi` is the edge weight

library(igraph)
library(ggraph)

masc_verb_part_assoc_plot |>
  graph_from_data_frame() |>
  ggraph(layout = "nicely") +
  geom_edge_link(aes(color = pmi),
    alpha = 0.8,
    edge_width = 0.8,
    arrow = grid::arrow()
  ) +
  geom_node_point(color = "black") +
  geom_node_text(aes(label = name), repel = TRUE) +
  scale_edge_color_gradient(low = "grey90", high = "grey20") +
  theme_void()
```

From this plot, and from the underlying data, we can explore verb particle constructions. We could go further and apply our co-occurrence methods to each modality separately, if we wanted to identify verb particle constructions that are distinctive to each modality. We could also apply our co-occurrence methods to other parts-of-speech, such as adjectives and nouns, to identify collocations of these parts-of-speech. There is much more to explore with co-occurrence analysis, but this should give you a good idea of the types of questions that can be addressed with co-occurrence analysis.

### Unsupervised learning {#sec-eda-unsupervised}

Aligned in purpose, unsupervised learning approaches to exploratory data analysis are used to identify patterns in the data from an algorithmic perspective. Common methods in text analysis include clustering, topic modeling, and word embedding.

We will continue to use the MASC dataset as we develop materials for our ELL textbook to illustrate unsupervised learning methods. In the process, we will explore the following questions:

- Can we identify genres based on linguistic features or co-occurrence patterns of the data itself?
- Are there discernible topics that separate spoken from written discourses based on linguistic features or co-occurrence patterns?
- Do certain words or phrases have different meanings or uses depending on the modality in which they appear?

Through these questions we will build on our knowledge of frequency, dispersion, and co-occurrence analysis and introduce concepts and methods associated with machine learning.  

#### Clustering {#sec-eda-clustering}

**Clustering** is a unsupervised learning technique that can be used to group similar items in the text data, helping to organize the data into distinct categories and discover relationships between different elements in the text. The main steps in the procedure includes identifying the relevant linguistic features to use for clustering, representing the features in a way that can be used for clustering, and applying a clustering algorithm to the data. However, it is important to consider the strengths and weaknesses of the clustering algorithm for a particular task and how the results will be evaluated.

In our ELL textbook task, we may very well want to identify terms that are particularly indicative of each genre. This will help us design a textbook with relevant communicative context in which terms are used most naturally. However,we may have questions about how well the labeled genres in the dataset map to real distinctions in language use. For example, are the genres 'Letters' and 'Journal' really distinct genres? Or, are they more similar than different? If they are more similar than different, then we may want to consider combining them into a single genre. 

Enter clustering. Instead of relying on the labels in the MASC dataset, we can let the data itself say something about how related the genres are. Yet, a pivotal question is what features should we use, otherwise known as **feature selection**. We could use terms or lemmas, but we may want to consider other features, such as parts-of-speech or some co-occurrence patterns. We are not locked into using one criterion, and we can perform clustering multiple times with different features, but we should consider the implications of our feature selection for our interpretation of the results. 

Another key question is what clustering algorithm to use. Again, we are not married to one algorithm, and we can perform clustering multiple times with different algorithms, but not all algorithms are created equal. Some algorithms are better suited for certain types of data and certain types of tasks. For example, **Hierarchical clustering** is a good choice when we are not sure how many clusters we want to identify, as it does not require us to specify the number of clusters from the outset. However, it is not a good choice when we have a large dataset, as it can be computationally expensive compared to some other algorithms. **K-means clustering**, on the other hand, is a good choice when we want to identify a pre-defined number of clusters, and the aim is to gauge how well the data fit the clusters. These two clustering techniques, therefore complement each other with Hierarchical clustering being a good choice for initial exploration and K-means clustering being a good choice for targeted evaluation.

With these considerations in mind, let's start by identifying the linguistic features that we want to use for clustering. Imagine that among the various features that we are interested in associating with genres, adjective use is one of them. The MASC dataset, `masc_tbl` includes the `pos` column and we can use this to isolate adjectives in the dataset. But first, we need to consider how we are going to operationalize 'adjective use'. 

In machine learning, operationalizing a concept is often referred to as feature engineering. **Feature engineering** is the process of transforming raw data into features that can be used for machine learning. In our case, adjective use could refer to raw frequency, relative frequency, or dispersion of individual adjectives, or even some co-occurrence pattern that includes adjectives. Each of these measures of use bakes in a different assumption about what adjective use means. Furthermore, some measures may be more useful for the clustering purposes at hand than others. For example, dispersion, while useful for identifying how spread or concentrated a type is across a corpus, may not be as useful for identifying genres based on adjective use since we will likely want to identify adjective-genre associations regardless of how spread or concentrated the adjective is across the corpus. Or maybe it is. For this reason, it is crucial to understand the implications of the feature engineering decisions that we make in the context of the task at hand.

Let's consider the raw frequency distribution of individual adjectives in the MASC dataset by genre. I will use the `tabyl()` function from the `janitor` package to create a frequency table of adjectives in the dataset, as seen in @exm-eda-masc-adjective-frequency.

::: {#exm-eda-masc-adjective-frequency}
```{r}
#| label: fig-eda-masc-adjective-frequency
#| fig-cap: "Barplot of adjective frequency in the MASC dataset"
#| fig-width: 8
#| fig-height: 4

# Load packages
library(janitor) # for tabyl()

# Barplot of adjective frequency
masc_tbl |> 
  filter(pos == "JJ") |> # "JJ" Adjective in the Penn tagset
  tabyl(genre) |> 
  adorn_pct_formatting(digits = 1) |> 
  ggplot(aes(x = genre, y = percent)) +
  geom_col() +
  labs(x = "Genre", y = "Percent") +
  coord_flip()
```
:::

From the perspective of raw frequency, we can see that the distribution of adjectives is not even across genres. This is not surprising, as the genres are not evenly distributed across the dataset. However, this means that we cannot compare the raw frequencies of adjectives across genres. Instead, we need to normalize the frequencies. But normalize them how? Normalizing by genre will not work, as we will be back to comparing apples to oranges. Instead, we need to normalize the frequencies by the total number of adjectives in each document. This will give us the relative frequency of adjectives in each document and allow the documents to be compared without bias. 

The next question to address in any analysis is how to represent the features. In our case, we want to represent the relative frequency of adjectives in each document. In machine learning, the most common way to represent features is in a matrix. In our case, we want to create a matrix with the documents in the rows and the types in the columns. The values in the matrix will be the relative frequency of each adjective in each document. This configuration is known as a **document-term matrix** (DTM).

To recast a data frame into a DTM, we can use the `cast_dtm()` function from the `tidytext` package. This function takes a data frame with a document identifier, a feature identifier, and a value for each observation and casts it into a matrix. Operations such as normalization are easily and efficiently performed in R on matrices, so initially we can cast a frequency table of adjectives into a matrix and then normalize the matrix by documents. 

Let's add raw frequency counts for adjectives in each document into a TDM, as seen in @exm-eda-masc-adjective-matrix.

::: {#exm-eda-masc-adjective-matrix}
```{r}
#| label: eda-masc-adjective-matrix

# Load package
library(tidytext) # for cast_sparse()
# library(Matrix) # for sparse matrices

# Create a dtm of adjective frequencies
masc_adj_dtm <- 
  masc_tbl |> 
  filter(pos == "JJ") |> 
  count(doc_id, lemma) |> 
  cast_dtm(
    document = doc_id,
    term = lemma,
    value = n
  ) |> 
  as.matrix()

# Preview
masc_adj_dtm[1:10, 1:5]
```
:::

Note preview the a subset of the contents of a matrix, such as in @exm-eda-masc-adjective-matrix, we use bracket syntax `[]` instead of the `head()` function.

Now we can normalize the matrix by documents. We can do this by dividing each adjective count by the total number of adjectives in each document. This is a row-wise transformation, so we can use the `rowSums()` function from base R to calculate the total number of adjectives in each document and then divide each adjective frequency by its row's total adjective count, as seen in @exm-eda-masc-adjective-matrix-normalized.

::: {#exm-eda-masc-adjective-matrix-normalized}
```{r}
#| label: eda-masc-adjective-matrix-normalized

# Normalize the matrix by documents
masc_adj_norm_dtm <- masc_adj_dtm / rowSums(masc_adj_dtm)

# Preview
masc_adj_norm_dtm[1:10, 1:5]
```
:::

- clustering algorithms
  - Hierarchical clustering
  - K-means clustering
    - Random seed
    - Number of clusters
- evaluation metrics
  - Dendrogram
  - Silhouette coefficient
  - Elbow method
- interpretability
  - Visualize clusters
  - Identify cluster features


- Hierarchical clustering (dendrogram)

Now that we have a TDM and we have normalized the matrix by documents, we can apply a clustering algorithm to the data. Let's start with Hierarchical clustering. Hierarchical clustering is a good choice when we are not sure how many clusters we want to identify. To apply Hierarchical clustering, we need to calculate the distance between each pair of documents. The distance measure that we use will depend on the type of data that we have. Given the skewed nature of word frequencies, we will use the Manhattan distance measure. The Manhattan method is often used in text analysis because it measures the absolute differences between the coordinates of two points. This means that it is not affected by the direction of the differences, only their magnitude.

Let's calculate the Manhattan distance between each pair of documents using the `dist()` function from base R, as seen in @exm-eda-masc-adjective-dist. The `dist()` function takes an argument `method = "manhattan"` to specify the Manhattan distance measure.

```{r}
#| label: eda-masc-adjective-dist

# Calculate the Manhattan distance matrix
masc_adj_dist <- 
  masc_adj_norm_dtm |> 
  dist(method = "manhattan")

# Preview 
masc_adj_dist |> head()
```

Now we can apply the clustering algorithm. For Hiearchical clustering, we can use the `hclust()` function from base R. The `hclust()` function takes the distance matrix as its argument and an argument `method = "average"` to specify the average linkage method. The average linkage method takes the average of the dissimilarities between all pairs in two clusters. It is less sensitive to outliers compared to other methods.

```{r}
#| label: eda-masc-adjective-hclust

# Apply the clustering algorithm
masc_adj_hclust <- 
  masc_adj_dist |> 
  hclust()

```

We can now visualize the results of the clustering using a dendrogram. The `ggdendro` package includes a function `ggdendrogram()` that can be used to create a dendrogram plot. The `ggdendrogram()` function takes the output of the `hclust()` function as its argument. We can use the `theme_dendro()` function from the `ggdendro` package to customize the appearance of the dendrogram, as seen in @fig-eda-masc-adjective-hclust.

```{r}
#| label: fig-eda-masc-adjective-hclust
#| fig-cap: "Dendrogram of adjective use in the MASC dataset"
#| fig-width: 8
#| fig-height: 4

# Load package
library(ggdendro) # for dendrogram plots
library(factoextra) 

# Create a dendrogram plot
masc_adj_hclust |> 
  fviz_dend(
    k = 18,
    # rect = TRUE,
    # rect_fill = TRUE,
    # rect_border = "grey50",
    # rect_border_size = 0.5,
    # rect_fill_alpha = 0.1,
    xlab = "Documents",
    ylab = "Distance"
  ) +
  theme_minimal()

# Create a dendrogram plot
# masc_adj_hclust |> 
#   ggdendrogram() +
#   theme_dendro()
```

Distance measures: 

- Euclidean distance: The straight-line distance between two points in Euclidean space. This is the most common distance measure and is used by default in the `dist()` function. It is a good choice when the data is normally distributed and the features are on the same scale. However, it is sensitive to outliers and does not work well when the data is not normally distributed or the features are on different scales.
- Manhattan distance: particularly useful for count data, such as word frequency, because it measures the absolute differences between the coordinates of two points. This means that it is not affected by the direction of the differences, only their magnitude.
- Cosine distance: Cosine similarity is often used in text mining because it measures the angle between two vectors, rather than their absolute distances. This makes it more robust against documents of different lengths. It is less sensitive to outliers than Euclidean distance. 


Clustering methods:

- Average linkage takes the average of the dissimilarities between all pairs in two clusters. It is less sensitive to outliers compared to other methods.
- Ward's method (specifically ward.D2) minimizes the total within-cluster variance. It tends to produce more compact, spherical clusters.


- K-means (pre-defined number of clusters)

Working with K-means clustering is similar to Hierarchical clustering. The main difference is that we need to specify the number of clusters that we want to identify. Since we are exploring the usefulness of the 18 genre labels used in the MASC dataset we have a good idea of how many clusters we want to start with. Our goal, then, will be to assess how well this number of clusters fits the data. If it does not fit the data well, we can try a different number of clusters. We can then compare the results of the clustering with the genre labels to see how well the clusters map to the labels and make ajustments to the way we group the labels as necessary. 

Let's start by applying the K-means clustering algorithm to the data. We can use the `kmeans()` function from base R to apply the K-means clustering algorithm. The `kmeans()` function takes the matrix of features as its first argument and the number of clusters as its second argument. We can specify the number of clusters with the `centers` argument. The `kmeans()` function also takes an argument `nstart` to specify the number of random starts. The K-means algorithm is sensitive to the initial starting points, so it is a good idea to run the algorithm multiple times with different starting points. The `nstart` argument specifies the number of random starts. The default value is 1, but we can increase this to 10 or 20 to increase the likelihood of finding a good solution. 

```{r}
#| label: eda-masc-adjective-kmeans

# Apply the clustering algorithm
masc_adj_kmeans <- 
  # masc_adj_norm_dtm |> 
  masc_adj_norm_dtm[-c(304, 215), ] |> 
  kmeans(
    centers = 2,
    nstart = 20
  )

# Preview
masc_adj_kmeans
```

We can plot the clusters with the `factoextra` package's `fviz_cluster()` function, as seen in @fig-eda-masc-adjective-kmeans. The `fviz_cluster()` function takes the output of the `kmeans()` function as its first argument and the matrix of features as its second argument. The `ggdendro` package's `ggdendrogram()` function can also be used to plot the clusters, as seen in @fig-eda-masc-adjective-kmeans-dendro.

```{r}

fviz_cluster(masc_adj_kmeans, masc_adj_norm_dtm[-c(304, 215), ])
```

#### Topic modeling {#sec-eda-topic-modeling}

ELL: we want to identify themes or topics that are distinctive to genres or a particular genre. This will help us design a textbook with relevant topics for each genre and the most relevant vocabulary for each topic.

<!--  

Topic modeling: Another unsupervised learning technique used to identify and extract topics from a set of documents. This can be useful for summarizing and understanding the content in a corpus.


- Topic modeling
	- Are there discernible topics that separate spoken from written discourses based on linguistic features or co-occurrence patterns?
		- Use LDA to identify topics of linguistic features or co-occurrence patterns
-->

This section will discuss topic modeling techniques, which are used to identify and group semantically similar topics in unstructured data. It will discuss various approaches to topic modelling, such as Latent Dirichlet Allocation (LDA), and discuss their applications in linguistics.

- LDA (latent Dirichlet allocation)
- LSA (latent semantic analysis)

#### Word embedding {#sec-eda-word-embedding}

ELL: we want to meaning similarities and potential differences between spoken and written discourses. This will help provide students with a more nuanced understanding of potential synonyms within and differences between spoken and written discourses.

<!--  

Word embedding: An unsupervised learning technique used for representing words as numerical vectors. This can be used for creating a vector space model of the text data, which can be used for tasks such as language translation or text classification.

- Word embeddings
  - Do certain words have different nearest neighbors in the embedding space when appear in spoken versus written discourses? What could this tell us about the differences between spoken and written discourses?
    - Use word2vec to identify nearest neighbors of words in spoken and written discourses by training a model on each modality. Compare the nearest neighbors of words in each modality.
  - If I consider hedges to be a semantic class, what are the nearest neighbors of hedges in the embedding space? What could this tell us about the semantic class of hedges?
    - Use word2vec to identify nearest neighbors of hedges by training a model on the entire dataset. Compare the nearest neighbors of hedges to the nearest neighbors of other words.
-->

This section will discuss word embedding techniques, which are used to represent words in a vector space. It will discuss various approaches to word embedding, such as Word2Vec and GloVe, and discuss their applications in linguistics.

- Word2vec (skip-gram)
- GloVe (global vectors for word representation)

## Summary 

Exploratory data analysis is a set of methods that can be used to explore a dataset and to identify new questions and new variables of interest. The methods can be used to describe a dataset and to identify linguistic units that are distinctive to a particular group or sub-group in the dataset. The methods can also be used to identify semantically similar topics in unstructured data. The results of exploratory analysis can be used to inform the development of a hypothesis or to inform the design of a machine learning model. 

## Activities {.unnumbered}

<!-- [ ] Add description of the activites -->

::: {.callout}
**{{< fa regular file-code >}} Recipe**

**What**: [Exploratory methods: descriptive and unsupervised learning analysis methods](https://lin380.github.io/tadr/articles/recipe_11.html)\
**How**: Read Recipe 10 and participate in the Hypothes.is online social annotation.\
**Why**: To illustrate how to prepare a dataset for descriptive and unsupervised machine learning methods and evaluate the results for exploratory data analysis.
:::

::: {.callout}
**{{< fa flask >}} Lab**

<!-- Analyze, evaluate, and create verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- [ ] update lab -->

**What**: [Exploratory Data Analysis](https://github.com/lin380/lab_11)\
**How**: Clone, fork, and complete the steps in Lab 9.\
**Why**: To gain experience working with coding strategies to prepare, feature engineer, explore, and evaluate results from exploratory data analyses, practice transforming datasets into new object formats and visualizing relationships, and implement organizational strategies for organizing and reporting results in a reproducible fashion.
:::

## Questions {.unnumbered}

::: {.callout}
{{< fa wrench >}} **Conceptual questions**

1. What is exploratory data analysis?
2. How can exploratory data analysis be used to uncover patterns and associations?
3. Describe the workflow of exploratory data analysis?
4. What are the advantages and disadvantages of descriptive analysis?
5. What are the advantages and disadvantages of unsupervised learning?
6. What is the difference between supervised and unsupervised learning?
7. How does exploratory data analysis differ from traditional hypothesis testing?

:::

::: {.callout}
{{< fa wrench >}} **Technical questions**

1. Write a function in R to conduct a hierarchical cluster analysis on a dataset.
2. Implement a k-means algorithm in R to identify clusters within a dataset.
3. Implement a Principal Component Analysis (PCA) algorithm in R to identify patterns and associations within a dataset.
4. Write a function in R to produce a descriptive summary of a dataset.
5. Conduct a correlation analysis in R to identify relationships between variables in a dataset.
6. Load a dataset into R and conduct a frequency analysis on the dataset.
7. Load a dataset into R and conduct a keyword in context analysis on the dataset.
8. Load a dataset into R and conduct a keyword analysis on the dataset.
9. Load a dataset into R and conduct a sentiment analysis on the dataset.
10. Load a dataset into R and conduct a topic modelling analysis on the dataset.
:::
