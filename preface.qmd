# Preface {#sec-preface .unnumbered}

```{r}
#| label: setup-options
#| child: "_common.qmd"
#| cache: false
```

<!--

Content:
- [ ] Add approach section information.
  - [ ] Add information on the 'tidyverse' and why it is used in this textbook.
  - [ ] Add information on the 'tidy' approach to data.
- [ ] Consider revamping the Aims section to reflect my current student learning outcomes.
- [ ] Consider whether to advocate for RStudio Desktop/ Cloud or include VS Code as an alternative.
- [ ] Refine the 'To the instructor' section.

Exercises:
- [ ] Update resources (Text as Data to Quantitative Text Analysis Resources `qtalr` site and package)
  - [ ] Change GitHub organization name to `qtalr`
- [ ] Add activity questions to this preface chapter

Formatting:


-->

> The journey of a thousand miles begins with one step.
>
> --- [Lao Tzu](https://en.wikipedia.org/wiki/Laozi)

::: callout-note
## Keys

-   What is the rationale for this textbook?
-   What are the aims and the approach taken in this textbook?
-   How is the textbook designed to support attaining these aims?
-   What is needed to get started?
:::

This chapter aims to provide a brief summary of current research trends that form the context for the rationale for this textbook. It also provides instructors and readers an overview of the aims and approach of the textbook, a description of the main components of each section and chapter, a guide to conventions used in the book, and a summary of the supporting resources available. Additionally information on setting up your computing environment and where to seek support is included.

## Rationale {.unnumbered}

Data science has emerged as an exciting and rapidly growing field in recent years, driven in large part by the increase in computing power available to the average individual and the abundance of electronic data now available through the internet. This has led to a growing interest in quantitative data analysis methods across various fields, including linguistics subfields and language-informed disciplines. 

This textbook aims to meet this growing demand by providing an introduction to the fundamental concepts and practical programming skills from data science applied to the task of quantitative text analysis. It is intended primarily for undergraduate readers, but may also be useful for graduate readers and researchers seeking to expand their methodological toolbox. The textbook takes a pedagogical approach to ensure accessibility and assumes no prior experience with statistics or programming, making it an attractive resource for novices in quantitative text analysis methods.



<!-- previous rationale

In recent years there has been a growing buzz around the term 'Data Science' and related terms; data analytics, data mining, *etc*. In a nutshell data science is the process by which an investigator leverages statistical methods and computational power to uncover insight from machine-readable data sources. Driven in large part by the increase in computing power available to the average individual and the increasing amount of electronic data that is now available through the internet, interest in data science has expanded to virtually all fields in academia and areas in the public sector.

This textbook is an introduction to the fundamental concepts and practical programming skills from data science applied to the task of quantitative text analysis. The intended readership for this textbook is primarily undergraduate readers but may also be applicable to graduate readers and researchers looking to expand their methodological toolbox. This textbook aims to meet the growing interest in quantitative data analysis methods taking hold across linguistics subfields and language-informed disciplines. To ensure that this resource is accessible to a wide variety of readers and researchers it does not assume a strong background in linguistics. Additionally, some readers interested in the aforementioned disciplines may lack experience with and/ or feel hesitant towards statistical methods and/ or programming. To make this textbook attractive to novices in quantitative text analysis methods, I will make no assumptions about the reader's experience with quantitative data analysis or programming, in general, or programming with the statistical programming language R, in particular.


-->

## Aims {.unnumbered}

This textbook aims to develop the reader's proficiency in three main areas:

**Data literacy**: the ability to interpret, assess, and contextualize findings based on data. Throughout this textbook we will explore topics which will help you understand how data analysis methods derive insight from data. In this process you will be encouraged to critically evaluate connections across linguistic and language-related disciplines using data analysis knowledge and skills. Data literacy is an invaluable skillset for academics and professionals but also is an indispensable aptitude for in the 21st century citizens to navigate and actively participate in the 'Information Age' in which we live [@Carmi2020].

**Research skills**: the ability to conduct original research, communicate findings, and make meaningful connections with findings in the literature of the field. This target area does not differ significantly, in spirit, from common learning outcomes in a research methods course: identify an area of investigation, develop a viable research question or hypothesis, collect relevant data, analyze data with relevant statistical methods, and interpret and communicate findings. However, working with text will incur a series of key steps in the selection, collection, and preparation of the data that are unique to text analysis projects. In addition, I will stress the importance of research documentation and creating reproducible research as an integral part of modern scientific inquiry [@Buckheit1995].

**Programming skills**: the ability to implement research skills programmatically and produce research that is replicable and collaborative. Modern data analysis, and by extension, text analysis is conducted using programming. There are various key reasons for this: (1) programming affords researchers unlimited research freedom --if you can envision it, you can program it. The same cannot be said for off-the-shelf software which is either proprietary or unmaintained --or both. (2) Programming underlies well-documented and reproducible research --documenting button clicks and menu option selections leads to research which is not readily reproduced, either by some other researcher or by your future self! (3) Programming invites researchers to engage more intimately with the data and the methods for analysis. The more familiar you are with the data and the methods the more likely you are to produce higher quality work.

<!--
What the aims of this textbook are:

The aim of this textbook is to provide readers with foundational knowledge and practical skills in quantitative text analysis using the R programming language. By the end of this textbook, readers will be able to identify, interpret and evaluate data analysis procedures and results to support research questions within language science. Additionally, readers will gain experience in designing and implementing research projects that involve processing and analyzing textual data using automatic processing techniques. This textbook aims to instill a strong sense of reproducible research practices, which are critical for promoting transparency, verification, and sharing of research findings.

How the textbook meets these aims:

The textbook is structured to guide readers through the necessary steps involved in quantitative text analysis. This begins with Part I, which sets the context for text analysis by discussing research trends and providing an overview of ~~the book's approach, as well as instructions for the instructor and a summary of supporting resources available~~. Part II of the book covers the foundation of text analysis including how to understand data, approaching analysis, and framing research, while Parts III and IV get into more specific technical aspects such as acquiring, curating, and transforming datasets in addition to exploratory, predictive, and inferential data analysis. Finally, Part V covers important communication aspects of research, including reporting and collaborating.

Throughout the book, readers will learn how to conduct text analysis and automate processes using the R programming language. They will develop detailed protocols that clearly outline research questions, methodologies, and analysis plans that align with reproducible research practices. Furthermore, instructors will encourage readers to collaborate with their peers and share their research projects publicly to facilitate learning and promote good research habits. These activities help readers gain a better understanding of the full research process, from gathering and organizing data to communicating findings effectively.

Why these aims are significant: 

The aims of this textbook are important for linguistics students because they provide a foundation in the skills required to succeed in the rapidly evolving landscape of 21st-century research. Moreover, these skills go beyond linguistics research; they are widely applicable across many disciplines where quantitative data analysis and programming are becoming increasingly important. The capacity to identify, design, implement, and communicate research using textual data is an essential skill for linguistics students as it enables them to conduct high-quality empirical investigations on numerous topics. Furthermore, the ability to develop reproducible research projects using computational tools and techniques is vital for students entering the world of academia or industry, where data-driven insights are central to decision-making. Thus, this textbook provides students with a comprehensive introduction to quantitative text analysis that is relevant to linguistics research and that equips them with valuable skills for their future careers.

-->

## Approach {.unnumbered}

The approach taken in this textbook is designed to balance the conceptual and practical aspects of quantitative text analysis in both linguistics research and language-related applications. Each chapter will begin with a brief introduction to the topic, followed by a list of key learning objectives. Interactive R programming lessons will be included that will introduce readers to R programming techniques through hands-on experience. The content of each chapter will interleave conceptual discussions with authentic examples, graphical representations, and case studies from relevant literature to provide a deeper understanding of the concepts. The book will also emphasize reproducible research practices, including developing detailed protocols that outline research questions, methodologies, and analysis plans with clear documentation of data sources, data preparation strategies, and statistical analyses. To continue to expand the reader's knowledge and skills, each chapter will include a step-by-step programming demonstration (recipe) and a lab exercise which will provide an opportunity for the reader to apply the concepts and techniques applicable to the chapter. The lab exercises will also provide an opportunity for the reader to collaborate with their peers and share their research projects publicly to facilitate learning and promote good research habits.

The approach of this textbook is grounded in the belief that statistical concepts and practical programming skills for quantitative text analysis can be taught in a way that prioritizes intuitive understanding over in-depth mathematical explanations. To achieve this goal, each chapter will provide an introduction to key concepts with a focus on real-world examples and case studies from relevant literature. Through interactive R programming lessons and programming lab exercises, readers will learn how to apply these concepts to textual data in a way that emphasizes the development of the practical coding and analytical skills needed to conduct reproducible research.

To help facilitate this learning process, the textbook will adopt the Tidyverse approach to programming in R. This approach provides a consistent syntax across different packages and is known for its legibility, making it easier for readers to understand and write code. Using Tidyverse enables readers to quickly and flexibly manipulate their datasets using common tools for data wrangling tasks like filtering, grouping, mutating, summarizing, and visualizing data within a number of powerful libraries made available by Tidyverse. Furthermore, the tidyverse family of R packages also makes use of the pipe operator to chain commands together to create readable expressions that make code more digestible and even enjoyably readable.

## Resources {.unnumbered}

There are three resources that support the aims and approach of this textbook: 1) the textbook itself which includes prose discussion, figures/ tables, R code, case studies, and thought and practical exercises, 2) a companion R package `qtalr` which includes functions for accessing data and datasets and provides various useful functions and a corresponding package website[Quantitative Text Analysis for Linguists Resources](https://lin380.github.io/tadr/) which includes programming tutorials and demonstrations to develop and augment the reader's recognition of how programming strategies are implemented, and 3) a [GitHub repository](https://github.com/lin380) which contains both a set of interactive R programming lessons ([Swirl](https://github.com/lin380/swirl)) and [lab exercises](https://github.com/stars/francojc/lists/labs) which guide the reader through practical hands-on programming applications.


<!--
There are three main resources available to support the aims and approach of this textbook. Firstly, the textbook itself provides prose discussion, figures/ tables, R code, case studies, and thought and practical exercises. Secondly, there is a companion R package called `qtalr`, which includes functions for accessing data and datasets, as well as various useful functions developed specifically for this textbook. In addition, there is a comprehensive website [Quantitative Text Analysis for Linguists Resources]() that includes programming tutorials and demonstrations to enhance the reader's recognition of how programming strategies are implemented. Finally, a [GitHub repository]() is provided which contains both a set of interactive R programming lessons (Swirl) and lab exercises designed to guide the reader through practical hands-on programming applications. The companion R package qtalr and the GitHub repository are both under active development and will be updated regularly to ensure that supplementary materials remain relevant to the content of the text.
-->

## Conventions {.unnumbered}

<!--# Add: package name, function name, variable name conventions -->

This textbook is about the concepts for understanding and the techniques for doing quantitative text analysis with R. Therefore there will be an intermingling of prose and code presented. As such, an attempt to establish consistent conventions throughout the text has been made to signal the reader's attention as appropriate.

In terms of prose, key concepts will be signaled using **bold**, package names will appear in title case (Tidyverse), function names will appear as verbatim text with parentheses (`read_csv()`) and object names as verbatim text without parentheses (`variable_1`).

As we explore concepts, R code itself will be incorporated into the text. For example, the code block in @lst-code-block shows actual R code and the results that are generated when running this code. Note that the hashtag `#` to the right of `1 + 1` signals the beginning of a **code comment**. Everything right of the `#` is not run as code. In this textbook you will see code comments above code on a separate line and sometimes to the right of code on the same line. The code follows within the same code block and a subsequent code blocks display the output of the code.

``` {#lst-code-block .r lst-cap="Example code block"}
1 + 1 # Add 1 plus 1
```

```{r}
#| label: code-block
#| echo: false

1 + 1 # Add 1 plus 1
```

Code blocks which make use of functions will be hyperlinked to the function's online documentation.

```{r}
#| label: code-block-function

paste("Hello world!") # simple 'hello world' message
```

Inline code will be used when code blocks are short and the results are not needed for display. For example, the same code as above will sometimes appear as `1 + 1`.

At times meta-description of code or code chunk options will appear. This is particularly relevant for descriptions on authoring Quarto documents.

``` {{r}}
#| label: test-code 
#| include: false
1 + 1 # Add 1 plus 1
```

There is a series of text blocks that will be used to signal the reader's attention.

Key points summarize the main points to be covered in a chapter or a subsection of the text.

::: callout-note
## Keys

-   What is the rationale for this textbook?
-   What are the aims and the approach taken in this textbook?
-   How is the textbook designed to support attaining these aims?
-   What is needed to get started?
:::

From time to time there will be points for you to consider and questions to explore.

::: callout-important
## Consider {.unnumbered}

Consider the objectives in this course: what ways can the knowledge and skills you will learn benefit you in your academic studies and/ or professional and personal life?
:::

Case studies are provided in-line which highlight key concepts and/ or methodological approaches relevant to the current topic or section.

::: callout-caution
## Case study {.unnumbered}

@Eisenstein2012 track the geographic spread of neologisms from city to city in the United States using Twitter data collected between 6/2009 and 5/2011. They only used tweets with geolocation data and then associated each tweet with a zipcode using the US Census. The most populous metropolitan areas were used. They also used the demographics from these areas to make associations between lexical innovations and demographic attributes. From this analysis they are able to reconstruct a network of linguistic influence. One of the main findings is that demographically-similar cities are more likely to share linguistic influence. At the individual level, there is a strong, potentially stronger role of demographics than geographical location.
:::

Swirl interactive R programming lessons appear at the beginning of each chapter. The lessons provide a guided environment to experience running code in the R console. The instructions to install the `swirl` package and the textbook lessons can be found on the "[Text as Data Resources](https://lin380.github.io/tadr)" site or directly from [GitHub](https://github.com/lin380/swirl).

::: callout-tip
## Swirl {.unnumbered}

**What**: [Intro to Swirl](https://github.com/lin380/swirl)\
**How**: In the R Console pane load `swirl`, run `swirl()`, and follow prompts to select the lesson.\
**Why**: To familiarize you with navigating, selecting, and completing swirl lessons.
:::

At the end of each chapter, a text block will provide readers a cue to explore the applied programming demonstrations called "[Recipes](https://lin380.github.io/tadr/articles/)" on the "[Text as Data Resources](https://lin380.github.io/tadr)" site. Readers may add online annotations using the built-in social annotation tool [hypothes.is](https://web.hypothes.is/education/annotated/research/). *Note: Instructors may opt to [create their own private Hypothes.is annotation group](https://web.hypothes.is/creating-groups/).*

::: callout-tip
## Recipe {.unnumbered}

**What**: [Literate programming I](https://lin380.github.io/tadr/articles/recipe_1.html)\
**How**: Read Recipe 1 and participate in the Hypothes.is online social annotation.\
**Why**: To introduce the concept of Literate Programming using R, RStudio, and R Markdown.
:::

Hands-on lab activities to implement and extend programming strategies round out each chapter. These labs are found on [GitHub](https://github.com/lin380?q=lab&type=all&language=&sort=name) and can be downloaded and/ or cloned to any RStudio instance (either your computer or on the web [RStudio Cloud](https://www.rstudio.com/products/cloud/)).

::: callout-tip
## Lab {.unnumbered}

**What**: [Literate programming I](https://github.com/lin380/lab_1)\
***How***: Clone, fork, and complete the steps in Lab 1.\
**Why**: To put literate programming techniques covered in Recipe 1 into practice. Specifically, you will create and edit an R Markdown document and render a report in PDF format.
:::

Tips are used to signal helpful tips and warnings that might otherwise be overlooked.

::: callout-warning
## Tip

During a the course of an exploratory work session, many R objects are often created to test ideas. At some point inspecting the workspace becomes difficult due to the number of objects displayed using `ls()`.

To remove all objects from the workspace, use `rm(list = ls())`.
:::

<!-- Errors will be an inevitable part of learning to code, but some errors can be avoided. The text will used the warning text block to highlight common pitfalls and errors. -->

Although this is not intended to be a in-depth introduction to statistical techniques, mathematical formulas will at times be included in the text. These formulas will appear either inline $1 + 1 = 2$ or as block equations as in @eq-example-formula.

$$
\hat{c} = \underset{c \in C} {\mathrm{argmax}} ~\hat{P}(c) \prod_i \hat{P}(w_i|c)
$$ {#eq-example-formula}

Data analysis leans heavily on graphical representations. Figures will appear numbered, as in @fig-test-fig.

```{r}
#| label: fig-test-fig
#| fig-cap: 'Test plot from mtcars dataset'
#| echo: false
ggplot(mtcars, aes(x = hp, y = mpg)) + # map 'hp' and 'mpg' to coordinate space
  geom_point() + # add points
  geom_smooth(method = "lm") + # draw linear trend line
  labs(x = "Horsepower", # label x axis
       y = "Miles per gallon", # label y axis
       title = "Test plot", # add title
       subtitle = "From mtcars dataset") # add subtitle
```

Tables, such as @tbl-test-table will be numbered separately from figures.

```{r}
#| label: tbl-test-table
#| tbl-cap: 'Here is a nice table!'
#| echo: false

iris |> 
  slice_head(n = 10) |> 
  kable()
```


<!--

Key Concepts
Key concepts will be introduced in bold. In instances where a key concept is initially defined, it will be followed by an italicized explanation to provide additional context.

Example: Tidy Data: A framework for organizing data into rows and columns that can be manipulated with common data manipulation verbs.

R Packages
The first occurrence of an R package will always be in bold and accompanied by the full package name followed by its abbreviation (in parentheses). Subsequent mentions of the same package will be in regular font style.

Example: The tidytext R package provides tools for transforming raw unstructured text data into tidy data frames (i.e., objects with rows and columns) that are amenable to data analysis workflows. We will use tidytext throughout the book.

R Functions
R functions will be presented in code formatting as follows: function_name(). Arguments that are used with the function will follow in parentheses.

Example: We will use the unnest_tokens() function from the tidytext package to split text into individual words or tokens.

R Variables
R variables will be presented in code formatting as follows: variable_name.

Example: After splitting text into individual tokens using unnest_tokens(), we will store the result in a variable named tokens.

Code Chunks
Code chunks will be presented using the Quarto literate programming format. Each code block will have a title that describes the purpose of the code. All code chunks will be numbered consecutively throughout the book.

Example:

```{r}
#| label: code-chunk-1
#| eval: false

library(tidytext) # load tidytext package

# Split text into individual words
tokens <- unnest_tokens(data = text_data, input = text_column, output = "token")
```


Inline Code
Inline code will be presented using the Quarto literate programming format. It will be presented in line with the flow of a paragraph, wrapped within backticks.

Example: We can use the count() function from the dplyr package to obtain the frequency of each token.

Mathematical Formulas
Mathematical formulas will be presented in LaTeX format.

Example: $$\frac{x+y}{2}$$

Figures and Tables
Figures and Tables will have titles that describe their content. They will be numbered separately, consecutively throughout the book, and will be referred to by their respective number.

Example (Table):

Token	Frequency
word1	10
word2	5
Example (Figure):

plot{#fig:plot}

Quarto Callouts for Notes, Warnings, and Tips
Quarto callouts will be used to highlight important points such as notes, warnings, tips, among others.

Example (Tip):

?> Use the str_trim() function from the stringr package to remove white spaces from text data.

Quarto Callouts for Exercises
Quarto callouts will also be used to indicate exercises for the reader. Exercise numbers will be consecutive throughout the book.

Example (Exercise):

?> ## Exercise 2.4
?>
?> Modify the tokens data frame obtained in Exercise 2.3 to exclude tokens with less than three letters.


-->


## Getting started {.unnumbered}

Before jumping in to this and subsequent chapter's textbook activities, it is important to prepare your computing environment and understand how to take advantage of the resources available, both those directly and indirectly associated with the textbook.

### R and RStudio {.unnumbered}

Programming is the backbone for modern quantitative research. R is a popular programming language with statisticians and was adopted by many other fields in natural and social sciences. It is freely downloadable from [The R Project for Statistical Programming](https://www.r-project.org/) website and is available for [macOS, Linux, and Windows](https://cloud.r-project.org/) operating systems.

While R code can be written and executed in many different environments, [RStudio](https://www.rstudio.com/products/rstudio/) provides a very powerful interface that has been widely adopted by R programmers. RStudio is an IDE (Integrated Development Environment) and serves as a dashboard for working with R --therefore you must download and install R before installing RStudio. You may choose to run RStudio on your own computer ([RStudio Desktop](https://www.rstudio.com/products/rstudio/)) or use RStudio on the web ([RStudio Cloud](https://www.rstudio.com/products/cloud/)). There are advantages to both approaches. Either approach will be compatible with this textbook but if you plan to continue to work with R/RStudio in the future at some point you will most likely want to install the desktop version and maintain your own R and RStudio environment.

For more details to install R and RStudio consult the [RStudio Education](https://education.rstudio.com/learn/beginner/) page.

### R packages {.unnumbered}

Throughout your R programming journey you will take advantage of code created by other R users in the form of packages. A package is a downloadable set of functions and/ or datasets which aim to accomplish a given cohesive set of related tasks. There are official R package repositories such as [CRAN](https://cran.r-project.org/) (Comprehensive R Archive Network) and other packages are available on code-sharing repositories such as [GitHub](https://github.com/).

::: callout-note
## Consider {.unnumbered}

The Comprehensive R Archive Network (CRAN) includes groupings of popular packages related to a given applied programming task called [Task Views](https://cran.r-project.org/web/views/). Explore the available CRAN Task Views listings. Note the variety of areas (tasks) that are covered in this listing. Now explore in more detail one of the following task views which are directly related to topics covered in this textbook noting the associated packages and their descriptions: (1) Cluster, (2) MachineLearning, (3) NaturalLanguageProcessing, or (4) ReproducibleResearch.
:::

You will download a number of packages at different stages of this textbook, but there is a set of packages that will be key to have from the get go. Once you have access to a working R/ RStudio environment, you can proceed to install the following packages.

Install the following packages from CRAN.

-   `tidyverse`
-   `rmarkdown`
-   `tinytex`
-   `devtools`
-   `usethis`
-   `swirl`

You can do this by running the following code in the RStudio Console pane.

``` r
 # install key packages from CRAN
install.packages(c("tidyverse", "rmarkdown", "tinytex", "devtools", "usethis", "swirl"))
```

Or you can use the RStudio Packages pane and click 'Install' and type the names of the packages.

This textbook includes a support package `tadr` which is available on GitHub ([source code](https://github.com/lin380/tadr)). To install this package from a GitHub repository, you run the following code in the RStudio Console pane:

``` r
# install the tadr package from GitHub
devtools::install_github("lin380/tadr") 
```

Finally, although not a package we will need to download the interactive R programming lessons for this textbook that will be accessed with the `swirl` package. Download these lessons by running the following code in the RStudio Console pane.

``` r
# install the swirl lessons for this textbook
swirl::install_course_github("lin380", "swirl")
```

Later in this Preface and then at the beginning of each subsequent chapter there will be swirl lessons to complete. To load and choose a lesson to start, you will run the following code in the RStudio Console pane.

``` r
# load the swirl package
library(swirl) 
# run swirl
swirl()
```

You will then follow the prompts to select and complete the desired lesson.

### Git and GitHub {.unnumbered}

[GitHub](https://github.com/) is a code sharing website. Modern computing is highly collaborative and GitHub is a very popular platform for sharing and collaborating on coding projects. The [lab exercises for this textbook](https://github.com/stars/francojc/lists/labs) are shared on GitHub. To access and complete these exercises you will need to [sign up for a (free) GitHub account](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F&source=header-home) and then set up the version control software `git` on your computing environment. `git` is the conduit to interfacing GitHub and for many `git` will already be installed on your computer (or cloud computing environment). To verify your installation (or for installation instructions) and to set up your `git` configuration, consult the very useful [Happy Git and GitHub for the useR](https://happygitwithr.com/) chapter [Install Git](https://happygitwithr.com/install-git.html).

### Getting help {.unnumbered}

The technologies employed in this approach to text analysis will include a somewhat steep learning curve. And in all honesty, the learning never stops! Experienced programmers and novices alike require support. Fortunately there is a very large community of programmers who have developed many official support resources and who actively contribute to unofficial discussion forums. Together these resources provide ample methods for overcoming any challenge.

The easiest and most convenient place to get help with either R or RStudio is through the RStudio "Help" toolbar menu. There you will find links to help resources, guides, and manuals. R packages often include "Vignettes" (long-form documentation and demonstrations). These can be accessed either by running `browseVignettes()` in the RStudio Console pane or by searching for the package using a search engine in your web browser and consulting the package documentation there (e.g. [`usethis`](https://usethis.r-lib.org/)). For some of the more common packages you can find [cheatsheets](https://www.rstudio.com/resources/cheatsheets/) on the RStudio website.

For Git and GitHub I recommend [Happy Git and GitHub for the useR](https://happygitwithr.com/) but the official [Git](https://git-scm.com/doc) and [GitHub](https://docs.github.com/en) documentation pages are great resources as well.

There are a number of very popular discussion forum websites where the programming community asks and answers questions to real-world issues. These sites often have subsections dedicated to particular programming languages or software. Here is a list of some of the most useful in my experience:

-   StackOverflow: [R](https://stackoverflow.com/questions/tagged/r), [Git](https://stackoverflow.com/questions/tagged/git), [RStudio](https://stackoverflow.com/questions/tagged/rstudio), [GitHub](https://stackoverflow.com/questions/tagged/github)
-   Reddit: [R](https://www.reddit.com/r/rstats/), [Git](https://www.reddit.com/r/git/), [RStudio](https://www.reddit.com/r/RStudio/), [Github](https://www.reddit.com/r/github/)
-   [RStudio Community](https://community.rstudio.com/)

If you post a question on one of these communities ensure that if your question involves some coding issue or error that you provide enough background such that the community will be able to help you. This is often referred to as a "reproducible example" or "reprex". A reprex is a minimal piece of code that demonstrates the issue you are having. It is a very useful tool for both asking and answering questions. Here is a great resource for learning how to create a reprex:

-   https://reprex.tidyverse.org/

-   https://github.com/MilesMcBain/datapasta

<!-- 
- [ ] Work on reprex example with/without RStudio?
- [ ] Example of a reprex on StackOverflow: https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
 -->

```{r}
#| label: datapasta-example-5
#| eval: false

# paste the data frame into a reprex
reprex::reprex()
```


```{r}
#| label: reprex-example-1
#| eval: false

install.packages("reprex") # install reprex package
```

```{r}
#| label: reprex-example-2
#| eval: false
# Create a vector with 100 random values from the normal distribution
set.seed(123) # set seed for reproducibility
my_vec <- rnorm(100) # random normal vector

summary(my_vec) # 5 number summary
```

```{r}
#| label: reprex-example-3
#| eval: false

reprex::reprex() # run reprex
```

Datapasta is a package that allows you to copy and paste data frames from RStudio into a reprex. This is a very useful tool for creating reproducible examples. Here is an example of how to use datapasta to create a reprex. 

```{r}
#| label: datapasta-example-1
#| eval: false

# install the datapasta package
install.packages("datapasta")
```

```{r}
#| label: datapasta-example-2
#| eval: false

# load the datapasta package
library(datapasta)
```

```{r}
#| label: datapasta-example-3
#| eval: false

# Create a data frame with 100 random values from the normal distribution
set.seed(123) # set seed for reproducibility
my_df <- data.frame(x = rnorm(100)) # random normal data frame
```

```{r}
#| label: datapasta-example-4
#| eval: false

# copy the data frame to the clipboard
dpasta(my_df)
```

The take-home message here is that you are not alone. There are many people world-wide that are learning to program and/ or contribute to the learning of others. The more you engage with these resources and communities the more successful your learning will be. As soon as you are able, pay it forward. Posting questions and offering answers helps the community and engages and refines your skills --a win-win.

## To the instructor {.unnumbered}
<!-- 
Depending on the previous experience, level, or expecations of your readers, you may want to consider the following:

- Basic introduction: 
  - Cover chapters 1-5 and culminate the course with a research proposal.
  - If your readers have little to no experience with R, you may want to consider using the [RStudio Cloud](https://rstudio.cloud/) platform to host the course.
- Intermediate introduction:
  - Cover chapters 1, 5-10 and culminate the course with a research project.
  - You may consider using the [RStudio Cloud](https://rstudio.cloud/) platform to host the course or have your readers install R and RStudio on their own computers.
- Advanced introduction:
  - Cover chapters 1, 5-12 and culminate the course with a collaborative research project.
  - Have readers install R and RStudio on their own computers. -->

Depending on the experience level and expectations of your readers, you may want to consider adopting one of the following course designs for using this textbook.

**Basic Introduction:**

Cover chapters 1-5 in sequence to give your readers a foundational understanding of quantitative text analysis.
Culminate the course with a research proposal assignment that requires them to identify an interesting linguistic problem, propose ways of solving it using the methods covered in class, and identify potential data sources.
If your readers have little to no experience with R, you may want to consider using the RStudio Cloud platform to host the course. This will provide them with a pre-installed R environment and allow them to focus on learning the material rather than troubleshooting.

**Intermediate Introduction:**

Cover chapters 1, 5-10 in sequence to give your readers a deeper understanding of quantitative text analysis methods.
Explore additional case studies or dataset examples throughout the course if you wish to supplement your lectures.
Culminate the course with a research project assignment that allows your readers to apply what they've learned to linguistic content of their choice.
You may consider using the RStudio Cloud platform to host the course, but ensure that your readers have access to R and RStudio on their own computers as well.

**Advanced Introduction:**

Cover all 12 chapters to give your readers a thorough understanding of quantitative text analysis concepts and techniques.
Devote more time to demonstrations of how to approach different problems and evaluating alternative approaches.
Culminate the course with a collaborative research project that requires your readers to work in groups to conduct a comprehensive analysis of a given dataset.
Ensure that your readers install R and RStudio on their own computers as they will need full control over their coding environment.
At each level of instruction, we strongly recommend that you evaluate the students' success in understanding the material by providing a combination of quizzes, lab assignments, programming exercises, and written reports. Additionally, encourage your readers to ask questions, collaborate with peers, and seek help from the ample resources available online when they encounter scope-limited programming problems.

## Summary {.unnumbered}

<!-- In this preface I've provided the rationale and aims of this textbook. The structure of the texbook and the associated resources work to scaffold your learning and proficiency in the areas of Data literacy, Research skills, and Programming skills. The textbook include a series of conventions to signal important concepts, questions to explore, and resources available. As in the area of Data Science in general, quantitative text analysis is most effectively conducted using programmatic approaches. The process will not be without challenges but the gains are well worth the effort. I've outlined key resources to obtain support that are invaluable for the novice as well as the seasoned practitioner. -->

This preface introduces readers to quantitative text analysis in the context of data science. It outlines the aims and approach of the textbook, emphasizing accessibility and reproducibility of research practices through the use of R programming and tidyverse approach. The chapter also provides resources, including a companion R package, GitHub repository, and conventions that will be used for key elements such as figures, tables, formulas, code blocks, and exercises. Finally, it offers suggestions for instructors, based on different levels of experience, for covering chapters and culminating projects. By following the guidelines set out in this preface, readers can expect to learn the practical skills and conceptual understanding necessary to conduct research with textual data.

## Activities {.unnumbered}

::: callout-note
## Swirl {.unnumbered}

**What**: [Intro to Swirl](https://github.com/lin380/swirl)\
**How**: In the R Console pane load `swirl`, run `swirl()`, and follow prompts to select the lesson.\
**Why**: To familiarize you with navigating, selecting, and completing swirl lessons.
:::

::: callout-note
## Recipe {.unnumbered}

**What**: [Literate programming I](https://lin380.github.io/tadr/articles/recipe_1.html)\
**How**: Read Recipe 1 and participate in the Hypothes.is online social annotation.\
**Why**: To introduce the concept of Literate Programming using R, RStudio, and R Markdown.
:::

::: callout-note
## Lab {.unnumbered}

**What**: [Literate programming I](https://github.com/lin380/lab_1)\
**How**: Clone, fork, and complete the steps in Lab 1.\
**Why**: To put literate programming techniques covered in Recipe 1 into practice. Specifically, you will create and edit an R Markdown document and render a report in PDF format.
:::

## Questions {.unnumbered}

::: callout-note
## Conceptual questions

1. What is the purpose of the textbook and what are the three skills it aims to scaffold?
2. What are the key components of quantitative text analysis?
3. What is the role of programmatic approaches in quantitative text analysis?
4. What are the potential challenges involved in conducting quantitative text analysis and why are the gains worth the effort?
5. What are some key resources available to support learning and conducting quantitative text analysis effectively?
6. How does the structure of the textbook and associated resources work to support learning and proficiency in the areas of data literacy, research skills, and programming skills?
7. What are the conventions used in the textbook to signal important concepts and questions to explore?
8. How is the textbook designed to be accessible for both novice and seasoned practitioners in the area of quantitative text analysis?
9. What are some key resources available to obtain support for learning and conducting quantitative text analysis effectively?
10. What is the relationship between R and an IDE (e.g. RStudio, VS Code)?
11. What is the relationship between R and a version control system (e.g. Git, GitHub)?
12. What is the relationship between R and a package manager (e.g. CRAN, Bioconductor)?

---

* What is the goal of data science, and how has it become popular in recent years?
* Who is the intended readership for this textbook, and why is it designed to be accessible to a wide audience?
* What are the main components of each chapter, and how are they structured to support learning outcomes?
* Why is programming emphasized as an important skill for implementing text analysis techniques, and what language is used in this textbook?
* What resources are available to support the aims and approach of the textbook, and how are they helpful for readers?

:::

::: callout-note
## Technical exercises

1. Install the latest version of R by following the instructions for your operating system. <https://cran.r-project.org/>
2. Install an IDE
  - RStudio: a popular Integrated Development Environment (IDE) for R. This will provide a user-friendly interface for writing, testing, and debugging R code. <https://rstudio.com/products/rstudio/download/>
  - VS Code: a popular IDE for many programming languages. This will provide a user-friendly interface for writing, testing, and debugging R code. <https://code.visualstudio.com/download>
3. Install Git, a version control system that allows you to track changes to files and collaborate with others. <https://git-scm.com/downloads>
4. Create a GitHub account. <...> <!-- # Add link to GitHub account setup -->
5. Install the `tidyverse` package by running `install.packages("tidyverse")` in the R Console pane.
6. Install the `swirl` package by running `install.packages("swirl")` in the R Console pane.
7. Open RStudio and create a new project for this textbook. This will help you keep your code and files organized.
8. Fork the textbook repository to own GitHub repository and then clone it to your local machine. This will create a local copy of the textbook on your computer. <!-- # Add instructions for cloning a repo -->
9. Open the textbook in RStudio and explore the structure of the project. <!-- # Add instructions for opening a project -->
10. Open the `index.qmd` file and knit the document. This will render the textbook in HTML format. <!-- # Add instructions for knitting a document -->
11. Add, commit, and push your changes to the textbook to your GitHub repository. <!-- # Add instructions for adding, committing, and pushing changes -->
:::
