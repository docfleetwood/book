% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  letterpaper,
]{latex/krantz}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{3}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{255,255,255}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}
\usepackage{makeidx}
\makeindex

% wrap table contents
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Block}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
% Make links footnotes instead of hotlinks:
\DeclareRobustCommand{\href}[2]{#2\footnote{\url{#1}}}
\hypersetup{
  pdftitle={Quantitative Text Analysis for Linguistics},
  pdfauthor={Jerid Francom},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Quantitative Text Analysis for Linguistics}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Reproducible Research using R}
\author{Jerid Francom}
\date{May 31, 2023}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[breakable, interior hidden, borderline west={3pt}{0pt}{shadecolor}, enhanced, frame hidden, sharp corners, boxrule=0pt]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{welcome}{%
\chapter*{Welcome}\label{welcome}}

\markboth{Welcome}{Welcome}

This textbook is an introduction to the fundamental concepts and
practical programming skills from Data Science that are increasingly
employed in a variety of language-centered fields and sub-fields applied
to the task of quantitative text analysis. It is geared towards advanced
undergraduates, graduate students, and researchers looking to expand
their methodological toolbox.

\textbf{About the author}

Dr.~Jerid Francom is Associate Professor of Spanish and Linguistics at
Wake Forest University. His research focuses on the use of large-scale
language archives (corpora) from a variety of sources (news, social
media, and other internet sources) to better understand the linguistic
and cultural similarities and differences between language varieties for
both scholarly and pedagogical projects. He has published on topics
including the development, annotation, and evaluation of linguistic
corpora and analyzed corpora through corpus, psycholinguistic, and
computational methodologies. He also has experience working with and
teaching statistical programming with R.

\hypertarget{license}{%
\section*{License}\label{license}}
\addcontentsline{toc}{section}{License}

\markright{License}

This work by \href{https://francojc.github.io/}{Jerid C. Francom} is
licensed under a Creative Commons
Attribution-NonCommercial-NoDerivatives 4.0 International License.

\hypertarget{credits}{%
\section*{Credits}\label{credits}}
\addcontentsline{toc}{section}{Credits}

\markright{Credits}

Icons made from Icon Fonts are licensed by CC-BY-3.0

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

\markright{Acknowledgements}

TAD has been reviewed by and suggestions and changes incorporated based
on the feedback through
\href{https://hypothes.is/groups/Q3o92MJg/tad}{the TAD Hypothes.is
group} by the following people: Andrea Bowling, Caroline Brady, Declan
Golsen, Asya Little, Claudia Valdez, \ldots{}

\hypertarget{build-information}{%
\section*{Build information}\label{build-information}}
\addcontentsline{toc}{section}{Build information}

\markright{Build information}

This version of the textbook was built with R version 4.3.0 (2023-04-21)
on macOS Ventura 13.3.1 with the following packages:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1042}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.8125}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
package
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
version
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
source
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
dplyr & 1.1.2 & CRAN (R 4.3.0) \\
forcats & 1.0.0 & CRAN (R 4.3.0) \\
ggplot2 & 3.4.2 & CRAN (R 4.3.0) \\
here & 1.0.1 & CRAN (R 4.3.0) \\
knitr & 1.43 & CRAN (R 4.3.0) \\
lubridate & 1.9.2 & CRAN (R 4.3.0) \\
purrr & 1.0.1 & CRAN (R 4.3.0) \\
readr & 2.1.4 & CRAN (R 4.3.0) \\
rmarkdown & 2.21 & CRAN (R 4.3.0) \\
stringr & 1.5.0 & CRAN (R 4.3.0) \\
tadr & 0.1.2 & local
(/Users/francojc/Documents/Academic/Research/Projects/1Active/tad/tadr) \\
tibble & 3.2.1 & CRAN (R 4.3.0) \\
tidyr & 1.3.0 & CRAN (R 4.3.0) \\
tidytext & 0.4.1 & CRAN (R 4.3.0) \\
tidyverse & 2.0.0 & CRAN (R 4.3.0) \\
webshot & 0.5.4 & CRAN (R 4.3.0) \\
\end{longtable}

\bookmarksetup{startatroot}

\hypertarget{sec-preface}{%
\chapter*{Preface}\label{sec-preface}}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

\begin{quote}
The journey of a thousand miles begins with one step.

--- \href{https://en.wikipedia.org/wiki/Laozi}{Lao Tzu}
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, rightrule=.15mm, opacityback=0, arc=.35mm, leftrule=.75mm, toprule=.15mm, bottomrule=.15mm, colback=white]

\textbf{\faIcon{list-alt} Outcomes}\vspace{2mm}

\begin{itemize}
\tightlist
\item
  Comprehend the book's rationale, learning goals, and pedagogical
  approach.
\item
  Navigate and engage with the book's structure and content effectively.
\item
  Interpret conventions used in the book reliably.
\item
  Set up the computing environment and utilize textbook and support
  resources for an optimal learning experience.
\end{itemize}

\end{tcolorbox}

The purpose of this chapter is to present the rationale behind this
textbook, outline the key learning objectives, describe the pedagogical
approach, and identify the intended audience. Additionally, this chapter
will provide readers with a guide to the book's structure and the scope
of its content, as well as instructions for the instructor and a summary
of supporting resources available. Finally, this chapter will provide
readers with information on setting up their computing environment and
where to seek support.

\hypertarget{rationale}{%
\section*{Rationale}\label{rationale}}
\addcontentsline{toc}{section}{Rationale}

\markright{Rationale}

Data science, an interdisciplinary field that combines knownledge and
skills from statistics, computer science, and domain-specific expertise
to extract meaningful insight from structured and unstructured data, has
emerged as an exciting and rapidly growing field in recent years, driven
in large part by the increase in computing power available to the
average individual and the abundance of electronic data now available
through the internet. These advances have become an integral part of the
modern scientific landscape, with data-driven insights now being used to
inform decision-making in a wide variety of academic fields, including
linguistics and language-related disciplines.

This textbook aims to meet this growing demand by providing an
introduction to the fundamental concepts and practical programming
skills from data science applied to the task of quantitative text
analysis. It is intended primarily for undergraduate students, but may
also be useful for graduates and researchers seeking to expand their
methodological toolbox. The textbook takes a pedagogical approach which
assumes no prior experience with statistics or programming, making it an
accessible resource for novices beginning their exploration of
quantitative text analysis methods.

\hypertarget{aims}{%
\section*{Aims}\label{aims}}
\addcontentsline{toc}{section}{Aims}

\markright{Aims}

The aim of this textbook is to provide readers with foundational
knowledge and practical skills in quantitative text analysis using the R
programming language and other open source tools and technologies. By
the end of this textbook, readers will be able to:

\begin{itemize}
\tightlist
\item
  Identify, interpret and evaluate data analysis procedures and results
\item
  Design, implement, and communicate research
\item
  Apply programmatic strategies to develop and collaborate on
  reproducible research projects
\end{itemize}

These aims are important for linguistics students because they provide a
foundation for concepts and in the skills required to succeed in the
rapidly evolving landscape of 21st-century research. Moreover, these
skills go beyond linguistics research; they are widely applicable across
many disciplines where quantitative data analysis and programming are
becoming increasingly important. The capacity to identify, design,
implement, and communicate research using textual data is an essential
skill for linguistics students as it enables them to conduct
high-quality empirical investigation across linguistic fields on a wide
variety of topics. Furthermore, the ability to develop reproducible
research projects using computational tools and techniques is vital for
students entering the world of academia or industry, where data-driven
insights are central to decision-making. Thus, this textbook provides
students with a comprehensive introduction to quantitative text analysis
that is relevant to linguistics research and that equips them with
valuable skills for their future careers.

\begin{itemize}
\tightlist
\item[$\square$]
  **Continue: integrate Approach and Structure into the Preface.
\end{itemize}

See the Table~\ref{tbl-structure-approach} for a general overview of the
structure of each chapter, the resources that are available to support
learning, and the learning stages.

\hypertarget{tbl-structure-approach}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}@{}}
\caption{\label{tbl-structure-approach}The general structure of a
chapter including: the component, its purpose, where to find the
resource, and the target learning stage.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Resource
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Stage
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Resource
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Stage
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Outcomes & Identify the learning objectives for the chapter & Textbook &
Introduction \\
Overview & Provide a brief introduction to the chapter topic & Textbook
& Introduction \\
Coding Lessons & Teach programming techniques with hands-on interactive
exercises & GitHub & Skills \\
Content & Combine conceptual discussions and programming skills,
incorporating thought-provoking questions, relevant studies, and
advanced topic references & Textbook & Knowledge \\
Recipes & Offer step-by-step programming examples related to the chapter
& Resources website & Comprehension \\
Labs & Allow readers to apply chapter-specific concepts and techniques &
GitHub & Application \\
Summary & Review the key concepts and skills covered in the chapter &
Textbook & Review \\
Questions & Assess and expand the reader's knowledge and abilities &
Textbook & Assessment \\
\end{longtable}

The following sections provide a more detailed description of the
structure and approach of this textbook.

\hypertarget{approach}{%
\section*{Approach}\label{approach}}
\addcontentsline{toc}{section}{Approach}

\markright{Approach}

The approach taken in this textbook is designed to balance the
conceptual and technical aspects of quantitative text analysis in both
linguistics research and language-related applications. This is grounded
in the belief that statistical concepts and practical programming skills
for quantitative text analysis can be taught in a way that prioritizes
intuitive understanding over in-depth mathematical explanations.

Each chapter will begin with a brief introduction to the topic, followed
by a list of key learning objectives. Interactive R programming lessons
will be included that will introduce readers to R programming techniques
through hands-on experience.

The content of each chapter will interleave conceptual discussions with
authentic examples, graphical representations, and case studies from
relevant literature to provide a deeper understanding of the concepts.

The book will also emphasize reproducible research practices, including
developing detailed protocols that outline research questions,
methodologies, and analysis plans with clear documentation of data
sources, data preparation strategies, and statistical analyses.

To continue to expand the reader's knowledge and skills, each chapter
will include a step-by-step programming demonstration (recipe) and a lab
exercise which will provide an opportunity for the reader to apply the
concepts and techniques applicable to the chapter.

The lab exercises will also provide an opportunity for the reader to
collaborate with their peers and share their research projects publicly
to facilitate learning and promote good research habits.

The approach of this textbook is grounded in the belief that statistical
concepts and practical programming skills for quantitative text analysis
can be taught in a way that prioritizes intuitive understanding over
in-depth mathematical explanations. To achieve this goal, each chapter
will provide an introduction to key concepts with a focus on real-world
examples and case studies from relevant literature. Through interactive
R programming lessons and programming lab exercises, readers will learn
how to apply these concepts to textual data in a way that emphasizes the
development of the practical coding and analytical skills needed to
conduct reproducible research.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

How the textbook meets these aims:

The textbook is structured to guide readers through the necessary steps
involved in quantitative text analysis. This begins with Part I, which
sets the context for text analysis by \ldots. Part II of the book covers
the foundation of text analysis including how to understand data,
approaching analysis, and framing research, while Parts III and IV get
into more specific technical aspects such as acquiring, curating, and
transforming datasets in addition to exploratory, predictive, and
inferential data analysis. Finally, Part V covers important
communication aspects of research, including reporting and
collaborating.

Throughout the book, readers will learn how to conduct text analysis and
automate processes using the R programming language. They will develop
detailed protocols that clearly outline research questions,
methodologies, and analysis plans that align with reproducible research
practices. Furthermore, instructors will encourage readers to
collaborate with their peers and share their research projects publicly
to facilitate learning and promote good research habits. These
activities help readers gain a better understanding of the full research
process, from gathering and organizing data to communicating findings
effectively.

\hypertarget{structure}{%
\section*{Structure}\label{structure}}
\addcontentsline{toc}{section}{Structure}

\markright{Structure}

\hypertarget{resources}{%
\section*{Resources}\label{resources}}
\addcontentsline{toc}{section}{Resources}

\markright{Resources}

There are three resources that support the aims and approach of this
textbook:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the textbook itself which includes prose discussion, figures/ tables,
  R code, case studies, and thought and practical exercises,
\item
  a companion R package \texttt{qtalr} which includes functions for
  accessing data and datasets and provides various useful functions and
  a corresponding package
  website\href{https://lin380.github.io/tadr/}{Quantitative Text
  Analysis for Linguists Resources} which includes programming tutorials
  and demonstrations to develop and augment the reader's recognition of
  how programming strategies are implemented, and
\item
  a \href{https://github.com/lin380}{GitHub repository} which contains
  both a set of interactive R programming lessons
  (\href{https://github.com/lin380/swirl}{Swirl}) and
  \href{https://github.com/stars/francojc/lists/labs}{lab exercises}
  which guide the reader through practical hands-on programming
  applications.
\end{enumerate}

\hypertarget{conventions}{%
\section*{Conventions}\label{conventions}}
\addcontentsline{toc}{section}{Conventions}

\markright{Conventions}

This textbook is about the concepts for understanding and the techniques
for doing quantitative text analysis with R. Therefore there will be an
intermingling of prose and code presented. As such, an attempt to
establish consistent conventions throughout the text has been made to
signal the reader's attention as appropriate.

In terms of prose, key concepts will be signaled using \textbf{bold},
package names will appear in title case (Tidyverse), function names will
appear as verbatim text with parentheses (\texttt{read\_csv()}) and
object names as verbatim text without parentheses
(\texttt{variable\_1}).

As we explore concepts, R code itself will be incorporated into the
text. For example, the code block in Block~\ref{lst-code-block} shows
actual R code and the results that are generated when running this code.
Note that the hashtag \texttt{\#} to the right of \texttt{1\ +\ 1}
signals the beginning of a \textbf{code comment}. Everything right of
the \texttt{\#} is not run as code. In this textbook you will see code
comments above code on a separate line and sometimes to the right of
code on the same line. The code follows within the same code block and a
subsequent code blocks display the output of the code.

\begin{codelisting}

\caption{Example code block}

\hypertarget{lst-code-block}{%
\label{lst-code-block}}%
\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1} \CommentTok{\# Add 1 plus 1}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\begin{verbatim}
#> [1] 2
\end{verbatim}

Code blocks which make use of functions will be hyperlinked to the
function's online documentation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{paste}\NormalTok{(}\StringTok{"Hello world!"}\NormalTok{) }\CommentTok{\# simple \textquotesingle{}hello world\textquotesingle{} message}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Hello world!"
\end{verbatim}

Inline code will be used when code blocks are short and the results are
not needed for display. For example, the same code as above will
sometimes appear as \texttt{1\ +\ 1}.

At times meta-description of code or code chunk options will appear.
This is particularly relevant for descriptions on authoring Quarto
documents.

\begin{Shaded}
\begin{Highlighting}[]
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\CommentTok{\#| label: test{-}code }
\CommentTok{\#| include: false}
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1} \CommentTok{\# Add 1 plus 1}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

There is a series of text blocks that will be used to signal the
reader's attention.

Key points summarize the main points to be covered in a chapter or a
subsection of the text.

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, rightrule=.15mm, opacityback=0, arc=.35mm, leftrule=.75mm, toprule=.15mm, bottomrule=.15mm, colback=white]

\textbf{\faIcon{list-alt} Outcomes}\vspace{2mm}

\begin{itemize}
\tightlist
\item
  What is the rationale for this textbook?
\item
  What are the aims and the approach taken in this textbook?
\item
  How is the textbook designed to support attaining these aims?
\item
  What is needed to get started?
\end{itemize}

\end{tcolorbox}

From time to time there will be points for you to consider and questions
to explore.

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, rightrule=.15mm, opacityback=0, arc=.35mm, leftrule=.75mm, toprule=.15mm, bottomrule=.15mm, colback=white]

\textbf{\faIcon{question-circle} Consider}\vspace{2mm}

Consider the objectives in this course: what ways can the knowledge and
skills you will learn benefit you in your academic studies and/ or
professional and personal life?

\end{tcolorbox}

Case studies are provided in-line which highlight key concepts and/ or
methodological approaches relevant to the current topic or section.

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, rightrule=.15mm, opacityback=0, arc=.35mm, leftrule=.75mm, toprule=.15mm, bottomrule=.15mm, colback=white]

\textbf{\faIcon{file-alt} Case study}\vspace{2mm}

Eisenstein et al. (2012) track the geographic spread of neologisms from
city to city in the United States using Twitter data collected between
6/2009 and 5/2011. They only used tweets with geolocation data and then
associated each tweet with a zipcode using the US Census. The most
populous metropolitan areas were used. They also used the demographics
from these areas to make associations between lexical innovations and
demographic attributes. From this analysis they are able to reconstruct
a network of linguistic influence. One of the main findings is that
demographically-similar cities are more likely to share linguistic
influence. At the individual level, there is a strong, potentially
stronger role of demographics than geographical location.

\end{tcolorbox}

Swirl interactive R programming lessons appear at the beginning of each
chapter. The lessons provide a guided environment to experience running
code in the R console. The instructions to install the \texttt{swirl}
package and the textbook lessons can be found on the
``\href{https://lin380.github.io/tadr}{Text as Data Resources}'' site or
directly from \href{https://github.com/lin380/swirl}{GitHub}.

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, rightrule=.15mm, opacityback=0, arc=.35mm, leftrule=.75mm, toprule=.15mm, bottomrule=.15mm, colback=white]

\textbf{\faIcon{terminal} Swirl}\vspace{2mm}

\textbf{What}: \href{https://github.com/lin380/swirl}{Intro to Swirl}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To familiarize you with navigating, selecting, and
completing swirl lessons.

\end{tcolorbox}

At the end of each chapter, a text block will provide readers a cue to
explore the applied programming demonstrations called
``\href{https://lin380.github.io/tadr/articles/}{Recipes}'' on the
``\href{https://lin380.github.io/tadr}{Text as Data Resources}'' site.
Readers may add online annotations using the built-in social annotation
tool
\href{https://web.hypothes.is/education/annotated/research/}{hypothes.is}.
\emph{Note: Instructors may opt to
\href{https://web.hypothes.is/creating-groups/}{create their own private
Hypothes.is annotation group}.}

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, rightrule=.15mm, opacityback=0, arc=.35mm, leftrule=.75mm, toprule=.15mm, bottomrule=.15mm, colback=white]

\textbf{\faIcon{file-code} Recipe}\vspace{2mm}

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_1.html}{Literate
programming I}\\
\textbf{How}: Read Recipe 1 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To introduce the concept of Literate Programming using R,
RStudio, and R Markdown.

\end{tcolorbox}

Hands-on lab activities to implement and extend programming strategies
round out each chapter. These labs are found on
\href{https://github.com/lin380?q=lab\&type=all\&language=\&sort=name}{GitHub}
and can be downloaded and/ or cloned to any RStudio instance (either
your computer or on the web
\href{https://www.rstudio.com/products/cloud/}{RStudio Cloud}).

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, rightrule=.15mm, opacityback=0, arc=.35mm, leftrule=.75mm, toprule=.15mm, bottomrule=.15mm, colback=white]

\textbf{\faIcon{flask} Lab}\vspace{2mm}

\textbf{What}: \href{https://github.com/lin380/lab_1}{Literate
programming I}\\
\textbf{\emph{How}}: Clone, fork, and complete the steps in Lab 1.\\
\textbf{Why}: To put literate programming techniques covered in Recipe 1
into practice. Specifically, you will create and edit an R Markdown
document and render a report in PDF format.

\end{tcolorbox}

Tips are used to signal helpful tips and warnings that might otherwise
be overlooked.

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, rightrule=.15mm, opacityback=0, arc=.35mm, leftrule=.75mm, toprule=.15mm, bottomrule=.15mm, colback=white]

\textbf{\faIcon{hand-point-up} Tip}\vspace{2mm}

During a the course of an exploratory work session, many R objects are
often created to test ideas. At some point inspecting the workspace
becomes difficult due to the number of objects displayed using
\texttt{ls()}.

To remove all objects from the workspace, use
\texttt{rm(list\ =\ ls())}.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, rightrule=.15mm, opacityback=0, arc=.35mm, leftrule=.75mm, toprule=.15mm, bottomrule=.15mm, colback=white]

\textbf{\faIcon{exclamation-triangle} Warning}\vspace{2mm}

Errors will be an inevitable part of learning to code, but some errors
can be avoided. The text will used the warning text block to highlight
common pitfalls and errors.

\end{tcolorbox}

Although this is not intended to be a in-depth introduction to
statistical techniques, mathematical formulas will at times be included
in the text. These formulas will appear either inline \(1 + 1 = 2\) or
as block equations as in Equation~\ref{eq-example-formula}.

\begin{equation}\protect\hypertarget{eq-example-formula}{}{
\hat{c} = \underset{c \in C} {\mathrm{argmax}} ~\hat{P}(c) \prod_i \hat{P}(w_i|c)
}\label{eq-example-formula}\end{equation}

Data analysis leans heavily on graphical representations. Figures will
appear numbered, as in Figure~\ref{fig-test-fig}.

\begin{figure}[h]

{\centering \includegraphics{preface_files/figure-pdf/fig-test-fig-1.pdf}

}

\caption{\label{fig-test-fig}Test plot from mtcars dataset}

\end{figure}

Tables, such as Table~\ref{tbl-test-table} will be numbered separately
from figures.

\hypertarget{tbl-test-table}{}
\begin{table}
\caption{\label{tbl-test-table}Here is a nice table! }\tabularnewline

\centering
\begin{tabular}{r|r|r|r|l}
\hline
Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\
\hline
5.1 & 3.5 & 1.4 & 0.2 & setosa\\
\hline
4.9 & 3.0 & 1.4 & 0.2 & setosa\\
\hline
4.7 & 3.2 & 1.3 & 0.2 & setosa\\
\hline
4.6 & 3.1 & 1.5 & 0.2 & setosa\\
\hline
5.0 & 3.6 & 1.4 & 0.2 & setosa\\
\hline
5.4 & 3.9 & 1.7 & 0.4 & setosa\\
\hline
4.6 & 3.4 & 1.4 & 0.3 & setosa\\
\hline
5.0 & 3.4 & 1.5 & 0.2 & setosa\\
\hline
4.4 & 2.9 & 1.4 & 0.2 & setosa\\
\hline
4.9 & 3.1 & 1.5 & 0.1 & setosa\\
\hline
\end{tabular}
\end{table}

\hypertarget{getting-started}{%
\section*{Getting started}\label{getting-started}}
\addcontentsline{toc}{section}{Getting started}

\markright{Getting started}

Before jumping in to this and subsequent chapter's textbook activities,
it is important to prepare your computing environment and understand how
to take advantage of the resources available, both those directly and
indirectly associated with the textbook.

\hypertarget{r-and-rstudio}{%
\subsection*{R and RStudio}\label{r-and-rstudio}}
\addcontentsline{toc}{subsection}{R and RStudio}

Programming is the backbone for modern quantitative research. R is a
popular programming language with statisticians and was adopted by many
other fields in natural and social sciences. It is freely downloadable
from \href{https://www.r-project.org/}{The R Project for Statistical
Programming} website and is available for
\href{https://cloud.r-project.org/}{macOS, Linux, and Windows} operating
systems.

While R code can be written and executed in many different environments,
\href{https://www.rstudio.com/products/rstudio/}{RStudio} provides a
very powerful interface that has been widely adopted by R programmers.
RStudio is an IDE (Integrated Development Environment) and serves as a
dashboard for working with R --therefore you must download and install R
before installing RStudio. You may choose to run RStudio on your own
computer (\href{https://www.rstudio.com/products/rstudio/}{RStudio
Desktop}) or use RStudio on the web
(\href{https://www.rstudio.com/products/cloud/}{RStudio Cloud}). There
are advantages to both approaches. Either approach will be compatible
with this textbook but if you plan to continue to work with R/RStudio in
the future at some point you will most likely want to install the
desktop version and maintain your own R and RStudio environment.

For more details to install R and RStudio consult the
\href{https://education.rstudio.com/learn/beginner/}{RStudio Education}
page.

\hypertarget{r-packages}{%
\subsection*{R packages}\label{r-packages}}
\addcontentsline{toc}{subsection}{R packages}

Throughout your R programming journey you will take advantage of code
created by other R users in the form of packages. A package is a
downloadable set of functions and/ or datasets which aim to accomplish a
given cohesive set of related tasks. There are official R package
repositories such as \href{https://cran.r-project.org/}{CRAN}
(Comprehensive R Archive Network) and other packages are available on
code-sharing repositories such as \href{https://github.com/}{GitHub}.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Consider}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

The Comprehensive R Archive Network (CRAN) includes groupings of popular
packages related to a given applied programming task called
\href{https://cran.r-project.org/web/views/}{Task Views}. Explore the
available CRAN Task Views listings. Note the variety of areas (tasks)
that are covered in this listing. Now explore in more detail one of the
following task views which are directly related to topics covered in
this textbook noting the associated packages and their descriptions: (1)
Cluster, (2) MachineLearning, (3) NaturalLanguageProcessing, or (4)
ReproducibleResearch.

\end{tcolorbox}

You will download a number of packages at different stages of this
textbook, but there is a set of packages that will be key to have from
the get go. Once you have access to a working R/ RStudio environment,
you can proceed to install the following packages.

Install the following packages from CRAN.

\begin{itemize}
\tightlist
\item
  \texttt{tidyverse}
\item
  \texttt{rmarkdown}
\item
  \texttt{tinytex}
\item
  \texttt{devtools}
\item
  \texttt{usethis}
\item
  \texttt{swirl}
\end{itemize}

You can do this by running the following code in the RStudio Console
pane.

\begin{Shaded}
\begin{Highlighting}[]
 \CommentTok{\# install key packages from CRAN}
\FunctionTok{install.packages}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{, }\StringTok{"rmarkdown"}\NormalTok{, }\StringTok{"tinytex"}\NormalTok{, }\StringTok{"devtools"}\NormalTok{, }\StringTok{"usethis"}\NormalTok{, }\StringTok{"swirl"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Or you can use the RStudio Packages pane and click `Install' and type
the names of the packages.

This textbook includes a support package \texttt{tadr} which is
available on GitHub (\href{https://github.com/lin380/tadr}{source
code}). To install this package from a GitHub repository, you run the
following code in the RStudio Console pane:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install the tadr package from GitHub}
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"lin380/tadr"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Finally, although not a package we will need to download the interactive
R programming lessons for this textbook that will be accessed with the
\texttt{swirl} package. Download these lessons by running the following
code in the RStudio Console pane.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install the swirl lessons for this textbook}
\NormalTok{swirl}\SpecialCharTok{::}\FunctionTok{install\_course\_github}\NormalTok{(}\StringTok{"lin380"}\NormalTok{, }\StringTok{"swirl"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Later in this Preface and then at the beginning of each subsequent
chapter there will be swirl lessons to complete. To load and choose a
lesson to start, you will run the following code in the RStudio Console
pane.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load the swirl package}
\FunctionTok{library}\NormalTok{(swirl) }
\CommentTok{\# run swirl}
\FunctionTok{swirl}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

You will then follow the prompts to select and complete the desired
lesson.

\hypertarget{git-and-github}{%
\subsection*{Git and GitHub}\label{git-and-github}}
\addcontentsline{toc}{subsection}{Git and GitHub}

\href{https://github.com/}{GitHub} is a code sharing website. Modern
computing is highly collaborative and GitHub is a very popular platform
for sharing and collaborating on coding projects. The
\href{https://github.com/stars/francojc/lists/labs}{lab exercises for
this textbook} are shared on GitHub. To access and complete these
exercises you will need to
\href{https://github.com/signup?ref_cta=Sign+up\&ref_loc=header+logged+out\&ref_page=\%2F\&source=header-home}{sign
up for a (free) GitHub account} and then set up the version control
software \texttt{git} on your computing environment. \texttt{git} is the
conduit to interfacing GitHub and for many \texttt{git} will already be
installed on your computer (or cloud computing environment). To verify
your installation (or for installation instructions) and to set up your
\texttt{git} configuration, consult the very useful
\href{https://happygitwithr.com/}{Happy Git and GitHub for the useR}
chapter \href{https://happygitwithr.com/install-git.html}{Install Git}.

\hypertarget{getting-help}{%
\subsection*{Getting help}\label{getting-help}}
\addcontentsline{toc}{subsection}{Getting help}

The technologies employed in this approach to text analysis will include
a somewhat steep learning curve. And in all honesty, the learning never
stops! Experienced programmers and novices alike require support.
Fortunately there is a very large community of programmers who have
developed many official support resources and who actively contribute to
unofficial discussion forums. Together these resources provide ample
methods for overcoming any challenge.

The easiest and most convenient place to get help with either R or
RStudio is through the RStudio ``Help'' toolbar menu. There you will
find links to help resources, guides, and manuals. R packages often
include ``Vignettes'' (long-form documentation and demonstrations).
These can be accessed either by running \texttt{browseVignettes()} in
the RStudio Console pane or by searching for the package using a search
engine in your web browser and consulting the package documentation
there (e.g.~\href{https://usethis.r-lib.org/}{\texttt{usethis}}). For
some of the more common packages you can find
\href{https://www.rstudio.com/resources/cheatsheets/}{cheatsheets} on
the RStudio website.

For Git and GitHub I recommend \href{https://happygitwithr.com/}{Happy
Git and GitHub for the useR} but the official
\href{https://git-scm.com/doc}{Git} and
\href{https://docs.github.com/en}{GitHub} documentation pages are great
resources as well.

There are a number of very popular discussion forum websites where the
programming community asks and answers questions to real-world issues.
These sites often have subsections dedicated to particular programming
languages or software. Here is a list of some of the most useful in my
experience:

\begin{itemize}
\tightlist
\item
  StackOverflow: \href{https://stackoverflow.com/questions/tagged/r}{R},
  \href{https://stackoverflow.com/questions/tagged/git}{Git},
  \href{https://stackoverflow.com/questions/tagged/rstudio}{RStudio},
  \href{https://stackoverflow.com/questions/tagged/github}{GitHub}
\item
  Reddit: \href{https://www.reddit.com/r/rstats/}{R},
  \href{https://www.reddit.com/r/git/}{Git},
  \href{https://www.reddit.com/r/RStudio/}{RStudio},
  \href{https://www.reddit.com/r/github/}{Github}
\item
  \href{https://community.rstudio.com/}{RStudio Community}
\end{itemize}

If you post a question on one of these communities ensure that if your
question involves some coding issue or error that you provide enough
background such that the community will be able to help you. This is
often referred to as a ``reproducible example'' or ``reprex''. A reprex
is a minimal piece of code that demonstrates the issue you are having.
It is a very useful tool for both asking and answering questions. Here
is a great resource for learning how to create a reprex:

\begin{itemize}
\item
  https://reprex.tidyverse.org/
\item
  https://github.com/MilesMcBain/datapasta
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# paste the data frame into a reprex}
\NormalTok{reprex}\SpecialCharTok{::}\FunctionTok{reprex}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"reprex"}\NormalTok{) }\CommentTok{\# install reprex package}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a vector with 100 random values from the normal distribution}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# set seed for reproducibility}
\NormalTok{my\_vec }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{) }\CommentTok{\# random normal vector}

\FunctionTok{summary}\NormalTok{(my\_vec) }\CommentTok{\# 5 number summary}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reprex}\SpecialCharTok{::}\FunctionTok{reprex}\NormalTok{() }\CommentTok{\# run reprex}
\end{Highlighting}
\end{Shaded}

Datapasta is a package that allows you to copy and paste data frames
from RStudio into a reprex. This is a very useful tool for creating
reproducible examples. Here is an example of how to use datapasta to
create a reprex.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install the datapasta package}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"datapasta"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load the datapasta package}
\FunctionTok{library}\NormalTok{(datapasta)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a data frame with 100 random values from the normal distribution}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# set seed for reproducibility}
\NormalTok{my\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)) }\CommentTok{\# random normal data frame}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# copy the data frame to the clipboard}
\FunctionTok{dpasta}\NormalTok{(my\_df)}
\end{Highlighting}
\end{Shaded}

The take-home message here is that you are not alone. There are many
people world-wide that are learning to program and/ or contribute to the
learning of others. The more you engage with these resources and
communities the more successful your learning will be. As soon as you
are able, pay it forward. Posting questions and offering answers helps
the community and engages and refines your skills --a win-win.

\hypertarget{to-the-instructor}{%
\section*{To the instructor}\label{to-the-instructor}}
\addcontentsline{toc}{section}{To the instructor}

\markright{To the instructor}

Depending on the experience level and expectations of your readers, you
may want to consider adopting one of the following course designs for
using this textbook.

\textbf{Basic Introduction:}

Cover chapters 1-5 in sequence to give your readers a foundational
understanding of quantitative text analysis. Culminate the course with a
research proposal assignment that requires them to identify an
interesting linguistic problem, propose ways of solving it using the
methods covered in class, and identify potential data sources. If your
readers have little to no experience with R, you may want to consider
using the RStudio Cloud platform to host the course. This will provide
them with a pre-installed R environment and allow them to focus on
learning the material rather than troubleshooting.

\textbf{Intermediate Introduction:}

Cover chapters 1, 5-10 in sequence to give your readers a deeper
understanding of quantitative text analysis methods. Explore additional
case studies or dataset examples throughout the course if you wish to
supplement your lectures. Culminate the course with a research project
assignment that allows your readers to apply what they've learned to
linguistic content of their choice. You may consider using the RStudio
Cloud platform to host the course, but ensure that your readers have
access to R and RStudio on their own computers as well.

\textbf{Advanced Introduction:}

Cover all 12 chapters to give your readers a thorough understanding of
quantitative text analysis concepts and techniques. Devote more time to
demonstrations of how to approach different problems and evaluating
alternative approaches. Culminate the course with a collaborative
research project that requires your readers to work in groups to conduct
a comprehensive analysis of a given dataset. Ensure that your readers
install R and RStudio on their own computers as they will need full
control over their coding environment. At each level of instruction, we
strongly recommend that you evaluate the students' success in
understanding the material by providing a combination of quizzes, lab
assignments, programming exercises, and written reports. Additionally,
encourage your readers to ask questions, collaborate with peers, and
seek help from the ample resources available online when they encounter
scope-limited programming problems.

\hypertarget{summary}{%
\section*{Summary}\label{summary}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

This preface introduces readers to quantitative text analysis in the
context of data science. It outlines the aims and approach of the
textbook, emphasizing accessibility and reproducibility of research
practices through the use of R programming and tidyverse approach. The
chapter also provides resources, including a companion R package, GitHub
repository, and conventions that will be used for key elements such as
figures, tables, formulas, code blocks, and exercises. Finally, it
offers suggestions for instructors, based on different levels of
experience, for covering chapters and culminating projects. By following
the guidelines set out in this preface, readers can expect to learn the
practical skills and conceptual understanding necessary to conduct
research with textual data.

\hypertarget{activities}{%
\section*{Activities}\label{activities}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Swirl}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/swirl}{Intro to Swirl}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To familiarize you with navigating, selecting, and
completing swirl lessons.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Recipe}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_1.html}{Literate
programming I}\\
\textbf{How}: Read Recipe 1 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To introduce the concept of Literate Programming using R,
RStudio, and R Markdown.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Lab}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/lab_1}{Literate
programming I}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 1.\\
\textbf{Why}: To put literate programming techniques covered in Recipe 1
into practice. Specifically, you will create and edit an R Markdown
document and render a report in PDF format.

\end{tcolorbox}

\hypertarget{questions}{%
\section*{Questions}\label{questions}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Conceptual questions}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the purpose of the textbook and what are the three skills it
  aims to scaffold?
\item
  What are the key components of quantitative text analysis?
\item
  What is the role of programmatic approaches in quantitative text
  analysis?
\item
  What are the potential challenges involved in conducting quantitative
  text analysis and why are the gains worth the effort?
\item
  What are some key resources available to support learning and
  conducting quantitative text analysis effectively?
\item
  How does the structure of the textbook and associated resources work
  to support learning and proficiency in the areas of data literacy,
  research skills, and programming skills?
\item
  What are the conventions used in the textbook to signal important
  concepts and questions to explore?
\item
  How is the textbook designed to be accessible for both novice and
  seasoned practitioners in the area of quantitative text analysis?
\item
  What are some key resources available to obtain support for learning
  and conducting quantitative text analysis effectively?
\item
  What is the relationship between R and an IDE (e.g.~RStudio, VS Code)?
\item
  What is the relationship between R and a version control system
  (e.g.~Git, GitHub)?
\item
  What is the relationship between R and a package manager (e.g.~CRAN,
  Bioconductor)?
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  What is the goal of data science, and how has it become popular in
  recent years?
\item
  Who is the intended readership for this textbook, and why is it
  designed to be accessible to a wide audience?
\item
  What are the main components of each chapter, and how are they
  structured to support learning outcomes?
\item
  Why is programming emphasized as an important skill for implementing
  text analysis techniques, and what language is used in this textbook?
\item
  What resources are available to support the aims and approach of the
  textbook, and how are they helpful for readers?
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Technical exercises}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Install the latest version of R by following the instructions for your
  operating system. \url{https://cran.r-project.org/}
\item
  Install an IDE
\end{enumerate}

\begin{itemize}
\tightlist
\item
  RStudio: a popular Integrated Development Environment (IDE) for R.
  This will provide a user-friendly interface for writing, testing, and
  debugging R code. \url{https://rstudio.com/products/rstudio/download/}
\item
  VS Code: a popular IDE for many programming languages. This will
  provide a user-friendly interface for writing, testing, and debugging
  R code. \url{https://code.visualstudio.com/download}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Install Git, a version control system that allows you to track changes
  to files and collaborate with others.
  \url{https://git-scm.com/downloads}
\item
  Create a GitHub account. \textless\ldots\textgreater{}
\item
  Install the \texttt{tidyverse} package by running
  \texttt{install.packages("tidyverse")} in the R Console pane.
\item
  Install the \texttt{swirl} package by running
  \texttt{install.packages("swirl")} in the R Console pane.
\item
  Open RStudio and create a new project for this textbook. This will
  help you keep your code and files organized.
\item
  Fork the textbook repository to own GitHub repository and then clone
  it to your local machine. This will create a local copy of the
  textbook on your computer.
\item
  Open the textbook in RStudio and explore the structure of the project.
\item
  Open the \texttt{index.qmd} file and knit the document. This will
  render the textbook in HTML format.
\item
  Add, commit, and push your changes to the textbook to your GitHub
  repository.
\end{enumerate}

\end{tcolorbox}

\part{Orientation}

In this section the aims are to: 1) provide an overview of quantitative
research and their applications, by both highlighting visible
applications and notable research in various fields, 2) consider how
quantitative research contributes to language research, and 3) layout
the main types of research and situate quantitative text analysis inside
these.

\hypertarget{sec-text-analysis-in-context}{%
\chapter{Text analysis in context}\label{sec-text-analysis-in-context}}

\begin{quote}
Science walks forward on two feet, namely theory and
experiment\ldots Sometimes it is one foot which is put forward first,
sometimes the other, but continuous progress is only made by the use of
both.

---
\href{https://www.nobelprize.org/uploads/2018/06/millikan-lecture.pdf}{Robert
A. Millikan} (1923)
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  What is the role and goals of data analysis in and outside of
  academia?
\item
  In what ways is quantitative language research approached?
\item
  What are some of the applications of text analysis?
\end{itemize}

\end{tcolorbox}

In this chapter I will aim to introduce the topic of text analysis The
aim is to introduce the context needed to understand how text analysis
fits in a larger universe of data analysis and see the commonalities in
the ever-ubiquitous field of data analysis, with attention to how
linguistics and language-related studies employ data analysis down to
the particular area of text analysis.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Swirl}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/swirl}{Variables and
vectors, Workspace}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To explore some key building blocks of the R programming
language and to examine your local workspace in R and understand the
relationship between your R workspace and the file system of your
machine.

\end{tcolorbox}

\hypertarget{text-making-sense-of-a-complex-world}{%
\section{Making sense of a complex
world}\label{text-making-sense-of-a-complex-world}}

\hypertarget{heuristic-understanding}{%
\subsection{Heuristic Understanding}\label{heuristic-understanding}}

The world around us is full of actions and interactions so numerous that
it is difficult to really comprehend. Through this lens each individual
sees and experiences this world. We gain knowledge about this world and
build up heuristic knowledge about how it works and how we do and can
interact with it. This happens regardless of your educational
background. As humans we are built for this. Our minds process countless
sensory inputs many of which never make it to our conscious mind. They
underlie skills and abilities that we take for granted like being able
to predict what will happen if you see someone about to knock a wine
glass off a table and onto a concrete floor. You've never seen this
object before and this is the first time you've been to this winery, but
somehow and from somewhere you `instinctively' make an effort to warn
the would-be-glass-breaker before it is too late. You most likely have
not stopped to consider where this predictive knowledge has come from,
or if you have, you may have just chalked it up to `common sense'. As
common as it may be, it is an incredible display of the brain's capacity
to monitor your environment, relate the events and observations that
take place, and store that information all the time not making a big
fuss to tell your conscious mind what it's up to.

So wait, this is a textbook on text analysis, right? So what does all
this have to do with that? Well, there are two points to make that are
relevant for framing our journey: (1) the world is full of countless
information which unfold in real-time at a scale that is daunting and
(2) for all the power of the brain that works so efficiently behind the
scene making sense of the world, we are one individual living one life
that has a limited view of the world at large. Let me expand on these
two points a little more.

First let's be clear. There is no way for anyone to experience all
things at all times. But even extremely reduced slices of reality are
still vastly outside of our experiential capacity, at least in
real-time. One can make the point that since the inception of the
internet an individual's ability to experience larger slices of the
world has increased. But could you imagine reading, watching, and
listening to every file that is currently accessible on the web? Or has
been? (See the \href{https://web.archive.org/}{Wayback Machine}.) Scale
this down even further; let's take Wikipedia, the world's largest
encyclopedia. Can you imagine reading every wiki entry? As large as a
resource such as Wikipedia is \footnote{As of 22 July 2021, there are
  6,341,359 articles in the
  \href{https://en.wikipedia.org/wiki/English_Wikipedia}{English
  Wikipedia} containing over 3.9 billion words occupying around 19
  gigabytes of information.}, it is still a small fragment of the
written language that is produced on the web, just the web \footnote{For
  reference, \href{https://commoncrawl.org/big-picture/}{Common Crawl}
  has millions of gigabytes collected since 2008}. Consider that for a
moment.

To my second framing point, which is actually two points in one. I
underscored the efficiency of our brain's capacity to make sense of the
world. That efficiency comes from some clever evolutionary twists that
lead our brain to take in the world but it makes some shortcuts that
compress the raw experience into heuristic understanding. What that
means is that the brain is not a supercomputer. It does not store every
experience in raw form, we do not have access to the records of our
experience like we would imagine a computer would have access to the
records logged in a database. Where our brains do excel is in making
associations and predictions that help us (most of the time) navigate
the complex world we inhabit. This point is key --our brains are doing
some amazing work, but that work can give us the impression that we
understand the world in more detail that we actually do. Let's do a
little thought experiment. Close your eyes and think about the last time
you saw your best friend. What were they wearing? Can you remember the
colors? If your like me, or any other human, you probably will have a
pretty confident feeling that you know the answers to these questions
and there is a chance you a right. But it has been demonstrated in
numerous experiments on human memory that our confidence does not
correlate with accuracy (Talarico and Rubin 2003; Roediger and McDermott
2000). You've experienced an event, but there is no real reason that we
should bet our lives on what we experienced. It's a little bit scary,
for sure, but the magic is that it works `good enough' for practical
purposes.

So here's the deal: as humans we are (1) clearly unable to experience
large swaths of experience by the simple fact that we are individuals
living individual lives and (2) the experiences we do live are not
recorded with precision and therefore we cannot `trust' our intuitions,
at least in an absolute sense.

\hypertarget{advancing-understanding}{%
\subsection{Advancing understanding}\label{advancing-understanding}}

What does that mean for our human curiosity about the world around us
and our ability to reliably make sense of it? In short it means that we
need to approach understanding our world with the tools of science.
Science is so powerful because it makes strides to overcome our inherit
limitations as humans (breadth of our experience and recall and
relational abilities) and bring a complex world into a more digestible
perspective. Science starts with question, identifies and collects data,
careful selected slices of the complex world, submits this data to
analysis through clearly defined and reproducible procedures, and
reports the results for others to evaluate. This process is repeated,
modifying, and manipulating the procedures, asking new questions and
positing new explanations, all in an effort to make inroads to bring the
complex into tangible view.

In essence what science does is attempt to subvert our inherent
limitations in understanding by drawing on carefully and purposefully
collected slices of observable experience and letting the analysis of
these observations speak, even if it goes against our intuitions (those
powerful but sometime spurious heuristics that our brains use to make
sense of the world).

\hypertarget{data-analysis}{%
\section{Data analysis}\label{data-analysis}}

\hypertarget{science-2.0-data-science}{%
\subsection{Science 2.0: Data Science}\label{science-2.0-data-science}}

At this point I've sketched an outline strengths and limitations of
humans' ability to make sense of the world and why science is used to
address these limitations. This science I've described is the one you
are familiar with and it has been an indispensable tool to make sense of
the world. If you are like me, this description of science may be
associated with visions of white coats, labs, and petri dishes. While
science's foundation still stands strong in the 21st century, a series
of intellectual and technological events mid-20th century set in motion
changes that have changed aspects about how science is done, not why it
is done. We could call this Science 2.0, but let's use the more
popularized term \index{Data Science}{\textbf{Data Science}}. The
recognized beginnings of Data Science are attributed to work in the
``Statistics and Data Analysis Research'' department at Bell Labs during
the 1960s. Although primarily conceptual and theoretic at the time, a
framework for quantitative data analysis took shape that would
anticipate what would come: sizable datasets which would ``{[}\ldots{]}
require advanced statistical and computational techniques {[}\ldots{]}
and the software to implement them.'' (Chambers 2020) This framework
emphasized both the inference-based research of traditional science, but
also embraced exploratory research and recognized the need to address
practical considerations that would arise when working with and deriving
insight from an abundance of machine-readable data.

Fast-forward to the 21st century a world in which machine-readable data
is truly in abundance. With increased computing power and innovative
uses of this technology the world wide web took flight. To put this in
perspective, in 2019 it was estimated that every minute 511 thousand
tweets were posted, 18.1 million text messages were sent, and 188
million emails were sent ({``Data Never Sleeps 7.0 Infographic''} 2019).
The data flood has not been limited to language, there are more sensors
and recording devices than ever before which capture evermore swaths of
the world we live in (Desjardins 2019). Where increased computing power
gave rise to the influx of data, it is also one of the primary methods
for gathering, preparing, transforming, analyzing, and communicating
insight derived from this data (Donoho 2017). The vision laid out in the
1960s at Bell Labs had come to fruition.

\hypertarget{triad-of-skills-needed-for-data-science}{%
\subsection{Triad of skills needed for Data
Science}\label{triad-of-skills-needed-for-data-science}}

The interest in deriving insight from the available data is now almost
ubiquitous. The science of data has now reached deep into all aspects of
life where making sense of the world is sought. Predicting whether a
loan applicant will get a loan (Bao, Lianju, and Yue 2019), whether a
lump is cancerous (Saxena and Gyanchandani 2020), what films to
recommend based on your previous viewing history (Gomez-Uribe and Hunt
2015), what players a sports team should sign (Lewis 2004) all now
incorporate a common set of data analysis tools.

These advances, however, are not predicated on data alone. As envisioned
by researchers at Bell Labs, turning data into insight it takes
\textbf{computing skills} (i.e.~programming), \textbf{statistical
knowledge}, and, importantly, \textbf{domain expertise}. This triad has
been popularly represented in a Venn diagram
Figure~\ref{fig-intro-data-science-venn}.

\begin{figure}[h]

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{figures/text-analysis/data-science-venn-paper.png}

}

\caption{\label{fig-intro-data-science-venn}Data Science Venn Diagram
adapted from
\href{http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram}{Drew
Conway}.}

\end{figure}

This same toolbelt underlies well-known public-facing language
applications. From the language-capable personal assistant applications,
plagiarism detection software, machine translation, and search engines,
tangible results of quantitative approaches to language are becoming
standard fixtures in our lives.

\begin{figure}[h]

{\centering \includegraphics[width=4.5in,height=\textheight]{figures/text-analysis/well-known-language-applications.png}

}

\caption{\label{fig-intro-language-applications}Well-known language
applications}

\end{figure}

The spread of quantitative data analysis too has taken root in academia.
Even in areas that on first blush don't appear to be approachable in a
quantitative manner such as fields in the social sciences and
humanities, data science is making important and sometimes disciplinary
changes to the way that academic research is conducted. This textbook
focuses in on a domain that cuts across many of these fields; namely
language. At this point let's turn to quantitative approaches to
language.

\hypertarget{language-analysis}{%
\section{Language analysis}\label{language-analysis}}

Language is a defining characteristic of our species. As such, the study
of language is of key concern to a wide variety of fields, not just
linguistics. The goals of various fields, however, and as such
approaches to language research, vary. On the one hand some language
research traditions within linguistics, namely those closely associated
with Noam Chomsky, eschewed quantitative approaches to language research
during the later half of the 20th century and instead turned to
qualitative assessment of language structure through introspective
methods. On the other hand many language research programs, in and
outside linguistics, turned to and/or developed quantitative research
methods either by necessity or through theoretical principles. These
quantitative research trajectories share much of the common data
analysis toolbox described in the previous section. This means to a
large extent language analysis projects share a common research language
with other language research but also with research outside of
linguistics.

However, there is never a one-size-fits all approach to anything --much
less data analysis. And in quantitative analysis there is a key
distinction in data collection that has downstream effects in terms of
procedure but also in terms of interpretation. The key distinction, that
we need to make at this point, which will provide context for our
exploration of text analysis, comes down to the approach to collecting
language data and the nature of that data. This distinction is between
\textbf{experimental data}\index{experimental data} and
\textbf{observational data}\index{observational data}.

Experimental approaches start with a intentionally designed hypothesis
and lay out a research methodology with appropriate instruments and a
plan to collect data that shows promise for shedding light on the
validity of the hypothesis. Experimental approaches are conducted under
controlled contexts, usually a lab environment, in which participants
are recruited to perform a language related task with stimuli that have
been carefully curated by researchers to elicit some aspect of language
behavior of interest. Experimental approaches to language research are
heavily influenced by procedures adapted from psychology. This link is
logical as language is a central area of study in cognitive psychology.
This approach looks a much like the white-coat science that we made
reference to earlier but, as in most quantitative research, has now
taken advantage of the data analysis toolbelt to collect and organize
much larger quantities of data and conduct statistically more robust
analysis procedures and communicate findings more efficiently.

Observational approaches are a bit more of a mixed bag in terms of the
rationale for the study; they may either start with a testable
hypothesis or in other cases may start with a more open-ended research
question to explore. But a more fundamental distinction between the two
is drawn in the amount of control the researcher has on contexts and
conditions in which the language behavior data to be collected is
produced. Observational approaches seek out records of language behavior
that is produced by language speakers for communicative purposes in
natural(istic) contexts. This may take place in labs (language
development, language disorders, \emph{etc.}), but more often than not,
language is collected from sources where speakers are performing
language as part of their daily lives --whether that be posting on
social media, speaking on the telephone, making political speeches,
writing class essays, reporting the latest news for a newspaper, or
crafting the next novel destined to be a New York Times best-seller.
What is more, data collected from the `wild' varies more in structure
relative to data collected in experimental approaches and requires a
number of steps to prepare the data to sync up with the data analysis
toolbelt.

I liken this distinction between experimental and observational data
collection to the difference between farming and foraging. Experimental
approaches are like farming; the groundwork for a research plan is
designed, much as a field is prepared for seeding, then the researcher
performs as series of tasks to produce data, just as a farmer waters and
cares for the crops, the results of the process bear fruit, data in our
case, and this data is harvested. Observational approaches are like
foraging; the researcher scans the available environmental landscape for
viable sources of data from all the naturally existing sources, these
sources are assessed as to their usefulness and value to address the
research question, the most viable is selected, and then the data is
collected.

The data acquired from both of these approaches have their trade-offs,
just as farming and foraging. Experimental approaches directly elicit
language behavior in highly controlled conditions. This directness and
level of control has the benefit of allowing researchers to precisely
track how particular experimental conditions effect language behavior.
As these conditions are an explicit part of the design and therefore the
resulting language behavior can be more precisely attributed to the
experimental manipulation. The primary shortcoming of experimental
approaches is that there is a level of artificialness to this directness
and control. Whether it is the language materials used in the task, the
task itself, or the fact that the procedure takes place under
supervision the language behavior elicited can diverge quite
significantly from language behavior performed in natural communicative
settings. Observational approaches show complementary strengths and
shortcomings. Whereas experimental approaches may diverge from natural
language use, observational approaches strive to identify and collected
language behavior data in natural, uncontrolled, and unmonitored
contexts. In this way observational approaches do not have to question
to what extent the language behavior data is or is not performed as a
natural communicative act. On the flipside, the contexts in which
natural language communication take place are complex relative to
experimental contexts. Language collected from natural contexts are
nested within the complex workings of a complex world and as such
inevitably include a host of factors and conditions which can prove
challenging to disentangle from the language phenomenon of interest but
must be addressed in order to draw reliable associations and
conclusions.

\begin{figure}

{\centering 

\begin{figure}[H]

{\centering \includegraphics[width=9.05in,height=3.33in]{text-analysis_files/figure-latex/mermaid-figure-1.png}

}

\end{figure}

}

\caption{\label{fig-data-colletion-methods}Experimental and
observational data collection methods.}

\end{figure}

The upshot, then, is twofold: (1) data collection methods matter for
research design and interpretation and (2) there is no single best
approach to data collection, each have their strengths and shortcomings.
In the ideal, a robust science of language will include insight from
both experimental and observational approaches (Gilquin and Gries 2009).
And evermore there is greater appreciation for the complementary nature
of experimental and observational approaches and a growing body of
research which highlights this recognition. Given their particular
trade-offs observational data is often used as an exploratory starting
point to help build insight and form predictions that can then be
submitted to experimental conditions. In this way studies based on
observational data serve as an exploratory tool to gather a better and
more externally valid view of language use which can then serve to make
prediction that can be explored with more precision in an experimental
paradigm. However, this is not always the case; observational data is
also often used in hypothesis-testing contexts as well. And furthermore,
some in some language-related fields, a hypothesis-testing is not the
ultimate goal for deriving knowledge and insight.

\hypertarget{text-analysis}{%
\section{Text analysis}\label{text-analysis}}

\textbf{Text analysis} is the application of data analysis procedures
from data science to derive insight from textual data collected through
observational methods. I have deliberately chosen the term `text
analysis' to avoid what I see are the pitfalls of using some other
common terms in the literature such as Corpus Linguistics, Computational
Linguistics, or Digital Humanities. In
Figure~\ref{fig-academic-language-applications} I have highlighted some
of the academic fields that employ text analysis.

\begin{figure}[h]

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{figures/text-analysis/academic-language-applications.png}

}

\caption{\label{fig-academic-language-applications}Some academic
applications of text analysis.}

\end{figure}

There are plenty of learning resources that focus specifically on one of
these three fields when discussing the quantitative analysis of text.
But from my perspective what is missing is a resource which underscores
the fact that text analysis research and the methods employed span
across a wide variety of academic fields (and applications in industry).
This textbook aims to introduce you to these areas and provides a wider
view of the potential applications of using text as data and inspires
you to either employ quantitative text analysis in your research and/ or
to raise your awareness of the advantages of text analysis for making
sense of language-related and linguistic-based phenomenon.

So what are some applications of text analysis? The most public facing
applications stem from Computational Linguistic research, often known as
Natural Language Processing (NLP) by practitioners. Whether it be using
search engines, online translators, submitting your paper to plagiarism
detection software, \emph{etc.} the text analysis methods we will cover
are at play. These uses of text analysis are production-level
applications and there is big money behind developing evermore robust
text analysis methods.

In academia the use of quantitative text analysis is even more
widespread, despite the lack of public fanfare. Let's run through some
select studies to give you an idea of some areas that employ text
analysis, to highlight a range of topics researchers address with text
analysis, and to whet your interest for conducting your own text
analysis project.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-caution-color!10!white, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, colback=white]

Eisenstein et al. (2012) track the geographic spread of neologisms from
city to city in the United States using Twitter data collected between
6/2009 and 5/2011. They only used tweets with geolocation data and then
associated each tweet with a zip code using the US Census. The most
populous metropolitan areas were used. They also used the demographics
from these areas to make associations between lexical innovations and
demographic attributes. From this analysis they are able to reconstruct
a network of linguistic influence. One of the main findings is that
demographically-similar cities are more likely to share linguistic
influence. At the individual level, there is a strong, potentially
stronger role of demographics than geographical location.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-caution-color!10!white, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, colback=white]

Voigt et al. (2017) explore potential racial disparities in officer
respect in police body camera footage. The dataset is based on body
camera footage from the Oakland Police Department during April 2014. At
total of 981 stops by 245 different officers were included (black 682,
white 299) and resulted in 36,738 officer utterances. The authors found
evidence for racial disparities in respect but not formality of
utterances, with less respectful language used with the black community
members.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-caution-color!10!white, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, colback=white]

Conway et al. (2012) investigate whether the established drop in
language complexity of rhetoric in election seasons is associated with
election outcomes. The authors used US Democratic Primary Debates from
2004. The results suggest that although there was no overall difference
in complexity between winners and losers, their pattern differed over
time. Winners tended to drop the complexity of their language closer to
the upcoming election.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-caution-color!10!white, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, colback=white]

Kloumann et al. (2012) explore the extent to which languages are
positively, neutrally, or negatively biased. Using Twitter, Google Books
(1520-2008), NY Times newspaper (1987-2007), and music lyrics
(1960-2007) the authors extract the top 5,000 most frequent words from
each source and have participants rate each word for happiness (9-point
scale). The results show that positive words strongly outnumber negative
words overall suggesting English is positive-, and pro-social- biased.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-caution-color!10!white, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, colback=white]

Bychkovska and Lee (2017) investigates possible differences between
L1-English and L1-Chinese undergraduate students' use of lexical
bundles, multiword sequences which are extended collocations (i.e.~as
the result of), in argumentative essays. The authors used the Michigan
Corpus of Upper-Level Student Papers (MICUSP) corpus using the
argumentative essay section for L1-English and the Corpus of Ohio
Learner and Teacher English (COLTE) for the L1-Chinese English essays.
They found that L1-Chinese writers used more than 2 times as many bundle
types than L1-English peers which they attribute to L1-Chinese writers
attempt to avoid uncommon expressions and/or due to their lack of
register awareness (conversation has more bundles than writing
generally).

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-caution-color!10!white, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, colback=white]

Jaeger and Snider (2007) use a corpus study to investigate the
phenomenon of syntactic persistence, the increased tendency for speakers
to use a particular syntactic form over an alternate when the syntactic
form is recently processed. The authors attempt to distinguish between
two competing explanations for the phenomenon: (1) transient activation,
where the increased tendency is short-lived and time-bound and (2)
implicit learning, where the increased tendency is a reflect of learning
mechanisms. The use of a speech corpora (Switchboard and spoken BNC)
were used to avoid the artificialness that typically occurs in
experimental settings. The authors investigated the ditransitive
alternation (NP PP/ NP NP), voice alternation (active/ passive), and
complementizer/ relativizer omission. In these alternations structural
bias was established by measuring the probability for a verb form to
appear in one of the two syntactic forms. Then the probability that that
form (target) would change given previous exposure to the alternative
form (prime) was calculated; what the authors called surprisal. Distance
between the prime structure and the target verb were considered in the
analysis. In these alternations, the less common structure was used in
the target more often when the when it corresponded to the prime form
(higher surprisal) suggesting that implicit learning underlies syntactic
persistence effects.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-caution-color!10!white, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, colback=white]

Wulff, Stefanowitsch, and Gries (2007) explore differences between
British and American English at the lexico-syntactic level in the
\emph{into}-causative construction (ex. `He tricked me into employing
him.'). The analysis uses newspaper text (The Guardian and LA Times) and
the findings suggest that American English uses this construction in
verbal persuasion verbs whereas British English uses physical force
verbs.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-caution-color!10!white, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, colback=white]

Mosteller and Wallace (1963) provide a method for solving the authorship
debate surrounding The Federalist papers \footnotemark{}. They employ a
probabilistic approach using the word frequency profiles of the articles
with known authors to predict authorship of the disputed 12 papers. The
results suggest that the disputed papers were most likely authored by
Madison.

\end{tcolorbox}

\footnotetext{https://guides.loc.gov/federalist-papers/full-text}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-caution-color!10!white, coltitle=black, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, colback=white]

Olohan (2008) investigate the extent to which translated texts differ
from native texts. In particular the author explores the notion of
explicitation in translated texts (the tendency to make information in
the source text explicit in the target translation). The study makes use
of the Translational English Corpus (TEC) for translation samples and
comparable sections of the British National Corpus (BNC) for the native
samples. The results suggest that there is a tendency for syntactic
explicitation in the translational corpus (TEC) which is assumed to be a
subconscious process employed unwittingly by translators.

\end{tcolorbox}

This sample of studies include research from areas such as translation,
stylistics, language variation, dialectology, psychology,
psycholinguistics, political science, and sociolinguistics which
highlights the diversity of fields and subareas which employ
quantitative text analysis. Text analysis is at the center of these
studies as they share a set of common goals:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To detect and retrieve patterns from text too subtle or too numerous
  to be done by hand
\item
  To challenge assumptions and/ or provide other views from textual
  sources
\item
  To explore new questions and/ or provide novel insight
\end{enumerate}

\hypertarget{structure-1}{%
\section{Structure}\label{structure-1}}

Let's now turn to the last section of this chapter which will provide an
overview of the rationale for learning to do text analysis, the
structure of the content covered, and a justification for the approach
we will take to perform text analysis.

In this section I will provide a general overview of the rest of the
textbook motivating the general structure and sequencing as well as
setting the foundation for programmatic approaches to data analysis. Let
me highlight why I think this is a valuable area of study, what I hope
you gain from this textbook, and how the structure of this textbook is
configured to help scaffold your conceptual and practical knowledge of
text analysis.

In \textbf{Part I ``Orientation''} the aims are to: 1) provide an
overview of quantitative research and their applications, by both
highlighting visible applications and notable research in various
fields, 2) consider how quantitative research contributes to language
research, and 3) layout the main types of research and situate
quantitative text analysis inside these.

In \textbf{Part II ``Foundations''} we will build up a framework to
contextualize quantitative data analysis using the Data to Insight
(DIKI) Hierarchy in Figure~\ref{fig-diki-hierarchy-bw} \footnote{Adapted
  from Ackoff (1989)}.

\begin{figure}[h]

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{figures/text-analysis/diki-hierarchy-paper.png}

}

\caption{\label{fig-diki-hierarchy}Data to Insight Hierarchy (DIKI)}

\end{figure}

\begin{figure}[h]

{\centering \includegraphics[width=5in,height=\textheight]{figures/text-analysis/diki-hierarchy-bw.png}

}

\caption{\label{fig-diki-hierarchy-bw}Data to Insight Hierarchy (DIKI)}

\end{figure}

The DIKI Hierarchy highlights the stages and intermediate steps required
to derive insight from data.
\protect\hyperlink{sec-understanding-data}{Chapter 2 ``Understanding
data''} will cover both Data and Information covering the conceptual
topics of populations versus samples and how language data samples are
converted to information and the forms that they can take. In
\protect\hyperlink{sec-approaching-analysis}{Chapter 3 ``Approaching
analysis''} I will discuss the distinction between descriptive and
analytic statistics. In brief they are both important for conducting
data analysis, but descriptive statistics serve as a sanity check on the
dataset before submitting it to interrogation --which is the goal of
analytic statistics. We will also cover some of the main distinctions
between analytics approaches including inference-, exploration-, and
prediction-based methods. With a fundamental understanding of data,
information, and knowledge we will then move to
\protect\hyperlink{sec-framing-research}{Chapter 4 ``Framing research''}
where we will discuss how to develop a research plan, or what I will
call a `research blueprint'. At this point we will directly address
Research Skills and elaborate on how research really comes together; how
to bring yourself up to speed with the literature on a topic, how to
develop a research goal or hypothesis, how to select data which is
viable to address the research goal or hypothesis, how to determine the
necessary information and appropriate measures to prepare for analysis,
how to perform diagnostic statistics on the data and make adjustments
before analysis, how to select and perform the relevant analytic
statistics given the research goals, how to report your findings, and
finally, how to structure your project so that it is well-documented and
reproducible.

\textbf{Part III ``Preparation''} and \textbf{Part IV ``Analysis''}
serve as practical and more detailed guides to the R programming
strategies to conduct text analysis research and as such develop your
Programming Skills. In \protect\hyperlink{sec-acquire-data}{Chapter 5
``Acquire data''} I will discuss three main strategies for accessing
data: direct downloads, Automatic Programming Interfaces (APIs), and web
scraping. In \protect\hyperlink{sec-curate-datasets}{Chapter 6 ``Curate
data(sets)''} I will outline the process for converting or augmenting
the acquired data or dataset into a (more) structured format, therefore
creating information. This will include organizing linguistic and
non-linguistic metadata into one dataset. In
\protect\hyperlink{sec-transform-datasets}{Chapter 7 ``Transform
datasets''} I describe how to work with a curated dataset to derive more
detailed information and appropriate dataset structures that are
appropriate for the subsequent analysis.

\protect\hyperlink{sec-exploration}{Chapters 8 ``Exploration''},
\protect\hyperlink{sec-prediction}{9 ``Prediction''}, and
\protect\hyperlink{sec-inference}{10 ``Inference''} focus on different
categories of statistical analysis each associated with distinct
research goals. Inference deals with analysis methods associated with
standard hypothesis-testing. This will include some common statistical
models employed in text analysis: chi-squared, logistic regression, and
linear regression. Prediction covers methods for modeling associations
in data with the aim to accurately predict outcomes using new textual
data. I will cover some standard methods for text classification
including Näive Bayes, \emph{k}-nearest neighbors (\emph{k}-NN), and
decisions tree and random forest models. Exploration covers a variety of
analysis methods such as association measures, clustering, topic
modeling, and vector-space models. These methods are aligned with
research goals that aim to interpret patterns that arise in from the
data itself.

\textbf{Part V ``Communication''} covers the steps in presenting the
findings of the research both as a research document and as a
reproducible research project. Both research documents and reproducible
projects are fundamental components of modern scientific inquiry. On the
one hand a research document, covered in Chapter
\protect\hyperlink{sec-reporting}{11 ``Reporting''}, provides readers a
detailed summary of the main import of the research study. On the other
hand making the research project available to interested readers,
covered in Chapter \protect\hyperlink{sec-collaboration}{12
``Collaboration''}, ensures that the scientific community can gain
insight into the process implemented in the research and thus enables
researchers to vet and extend this research to build a more robust and
verifiable research base.

\hypertarget{summary-1}{%
\section*{Summary}\label{summary-1}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter I started with some general observations about the
difficulty of making sense of a complex world. The standard approach to
overcoming inherent human limitations in sense making is science. In the
21st century the toolbelt for doing scientific research and exploration
has grown in terms of the amount of data available, the statistical
methods for analyzing the data, and the computational power to manage,
store, and share the data, methods, and results from quantitative
research. The methods and tools for deriving insight from data have made
significant inroads in and outside academia, and increasingly figure in
the quantitative investigation of language. Text analysis is a
particular branch of this enterprise based on observational data from
real-world language and is used in a wide variety of fields.

This textbook aims to develop your knowledge and skills in three
fundamental areas: Data Literacy, Research Skills, and Programming
Skills. (\ldots{} add more on how the structure leads to developing this
knowledge and skills\ldots)

In the end I hope that you enjoy this exploration into text analysis.
Although the learning curve at times may seem steep --the experience you
will gain will not only improve your data literacy, research skills, and
programmings skills but also enhance your appreciation for the richness
of human language and its important role in our everyday lives.

\hypertarget{actitivies}{%
\section*{Actitivies}\label{actitivies}}
\addcontentsline{toc}{section}{Actitivies}

\markright{Actitivies}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_2.html}{Literate
programming II}\\
\textbf{How}: Read Recipe 2 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To explore additional functionality in R Markdown:
numbered sections, table of contents, in-line citations and a
document-final references list, and cross-referenced tables and figures.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/lab_2}{Literate
programming II}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 2.\\
\textbf{Why}: To put into practice R Markdown functionality to
communicate the aim(s) and main finding(s) from a primary research
article and to interpret a related plot.

\end{tcolorbox}

\hypertarget{questions-1}{%
\section*{Questions}\label{questions-1}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Conceptual questions}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Discriminate between quantitative and non-quantitative research.
\item
  Identify research in an area of interest in linguistics that has taken
  a quantitative approach to text analysis.
\item
  What are the benefits of reproducible research in data science?
\item
  In your own words, define literate programming?
\item
  What are the benefits of literate programming?
\item
  What are the benefits of using R and Quarto for literate programming?
\end{enumerate}

\begin{itemize}
\tightlist
\item
  What is the importance of science in making sense of a complex world?
\item
  How has the tool belt for scientific research and exploration changed
  in the 21st century?
\item
  What is text analysis and how is it used in various fields?
\item
  What are the three fundamental areas that this textbook aims to
  develop knowledge and skills in?
\item
  What are the benefits of learning text analysis beyond just improving
  data literacy, research skills, and programming skills?
\item
  How does the structure of the textbook lead to the development of
  knowledge and skills in data literacy, research skills, and
  programming skills?
\item
  How does text analysis contribute to our understanding of human
  language and its role in our everyday lives?
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Technical exercises}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  Create various data types in R (e.g.~vectors, matrices, data frames,
  lists, etc.) and explore their properties.
\item
  Create a literate programming document in Quarto. Edit the yaml header
  to reflect details of the work and add your work with the data types
  in R to code chunks. Add, commit, and push the project to GitHub.
\item
  Fork a quantitative text analysis project on GitHub and clone it to
  your local machine. Run the code and explore the results.
\item
  \ldots{}
\item
  Explore the following resources and identify a quantitative text
  analysis project. \href{https://rpubs.com/}{Rpubs},
  \href{https://github.com}{GitHub},
  \href{https://datacamp.com}{DataCamp},
  \href{https://kaggle.com}{Kaggle},
  \href{https://r-bloggers.com}{R-bloggers}.
\item
  The repository ``Text Mining and Sentiment Analysis of Twitter Data''
  located at
  https://github.com/anantavijay/Text-Mining-and-Sentiment-Analysis-of-Twitter-Data
  contains a real-world quantitative text analysis study with
  reproducible code in R. The repository contains code for sentiment
  analysis of Twitter data and for text-mining techniques such as
  tokenization, stemming, and sentiment analysis. The repository also
  contains an R script that can be used to reproduce the results of the
  analysis.
\end{itemize}

\end{tcolorbox}

\part{Foundations}

Before working on the specifics of a data project, it is important to
establish a fundamental understanding of the characteristics of each of
the levels in the ``Data, Information, Knowledge, and Insight Hierarchy
(DIKI)'' (see Figure~\ref{fig-diki-hierarchy}) and the roles each of
these levels have in deriving insight from data. In
\protect\hyperlink{understanding-data-chapter}{Chapter 2} we will
explore the Data and Information levels drawing a distinction between
two main types of data (populations and samples) and then cover how data
is structured and transformed to generate information (datasets) that is
fit for statistical analysis. In
\protect\hyperlink{approaching-analysis-chapter}{Chapter 3} I will
outline the importance and distinct types of statistical procedures
(descriptive and analytic) that are commonly used in text analysis.
\protect\hyperlink{framing-research-chapter}{Chapter 4} aims to tie
these concepts together and cover the required steps for preparing a
research blueprint to conduct an original text analysis project.

\hypertarget{sec-understanding-data}{%
\chapter{Understanding data}\label{sec-understanding-data}}

\begin{quote}
The plural of anecdote is not data.

--- Marc Bekoff
\end{quote}

\begin{quote}
The goal is to turn data into information, and information into insight.

--- Carly Fiorina
\end{quote}

\begin{quote}
Without data, you're just another person with an opinion.

--- W. Edwards Deming
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  What are the distinct types of data and how do they differ?
\item
  What is information and what form does it take?
\item
  What is the importance of documentation in reproducible research?
\end{itemize}

\end{tcolorbox}

In this chapter I cover the starting concepts in our journey to
understand how to derive insight from data, illustrated in the DIKI
Hierarchy (Figure~\ref{fig-diki-hierarchy}), focusing specifically on
the first two levels: Data and Information. We will see that what is
commonly referred to as `data' everyday uses is broken into three
distinct categories, two of which are referred to as data and the third
is known as information. We will introduce the key concepts and
structural characteristics which define information. We will also cover
the importance of documentation of data and datasets in quantitative
research.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Swirl}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/swirl}{Objects, Packages
and functions}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To introduce you to the main types of objects in R and to
understand the role and use of functions and packages in R programming.

\end{tcolorbox}

\hypertarget{data}{%
\section{Data}\label{data}}

Data is data, right? The term `data' is so common in popular vernacular
it is easy to assume we know what we mean when we say `data'. But as in
most things, where there are common assumptions there are important
details the require more careful consideration. Let's turn to the first
key distinction that we need to make to start to break down the term
`data': the difference between populations and samples.

\hypertarget{population-sampling}{%
\subsection{Population sampling}\label{population-sampling}}

The first thing that comes to many people's mind when the term
population is used is human populations (derived from Latin `populus').
Say for example we pose the question --What's the population of
Milwuakee? When we speak of a population in these terms we are talking
about the total sum of individuals living within the geographical
boundaries of Milwaukee. In concrete terms, a
\index{population}\textbf{population} is the objective make up of an
idealized set of objects and events in reality. Key terms here are
objective and idealized. Although we can look up the US Census report
for Milwaukee and retrieve a figure for the population, this cannot
truly be the population. Why is that? Well, whatever method that was
used to derive this numerical figure was surely incomplete. If not
incomplete, by the time someone recorded the figure some number of
residents of Milwaukee moved out, moved in, were born, or passed away
--the figure is no longer the true population.

Likewise when we talk about populations in terms of language we dealing
with an objective and idealized aspect of reality. Let's take the words
of the English language as an analog to our previous example population.
In this case the words are the people and English is the bounding
characteristic. Just as people, words move out, move in, are born, and
pass away. Any compendium of the words of English at any moment is
almost instananeously incomplete. This is true for all populations, save
those relatively rare cases in which the bounding characteristics select
a narrow slice of reality which is objectively measurable and whose
membership is fixed (the complete works of Shakespeare, for example).

In sum, (most) populations are amorphous moving targets. We objectively
hold them to exist, but in practical terms we often cannot nail down the
specifics of populations. So how do researchers go about studying
populations if they are theoretically impossible to access directly? The
strategy employed is called sampling.

A \index{sample}\textbf{sample} is the product of a subjective process
of selecting a finite set of observations from an objective population
with the goal of capturing the relevant characteristics of the target
population. Although there are strategies to minimize the mismatch
between the characteristics of the subjective sample and objective
population, it is important to note that it is almost certainly true
that any given sample diverges from the population it aims to represent
to some degree. The aim, however, is to employ a series of sampling
decisions, which are collectively known as a sampling frame, that
maximize the chance of representing the population.

What are the most common sampling strategies? First
\index{sample size}\textbf{sample size}. A larger sample will always be
more representative than a smaller sample. Sample size, however, is not
enough. It is not hard to imagine a large sample which by chance
captures only a subset of the features of the population. A next step to
enhance sample representativeness is apply \textbf{random sampling}.
Together a large random sample has an even better chance of reflecting
the main characteristics of the population better than a large or random
sample. But, random as random is, we still run the risk of acquiring a
skewed sample (i.e a sample which does not mirror the target
population).

To help mitigate these issues, there are two more strategies that can be
applied to improve sample representativeness. Note, however, that while
size and random samples can be applied to any sample with little
information about internal characteristics of the population, these next
two strategies require decisions depend on the presumed internal
characteristics of the population. The first of these more informed
sampling strategies is called \textbf{stratified sampling}. Stratified
samples make (educated) assumptions about sub-components within the
population of interest. With these sub-populations in mind, large random
samples are acquired for each sub-population, or strata. At a minimum,
stratified samples can be no less representative than random sampling
alone, but the chances that the sample is better increases. Can there be
problems in the approach? Yes, and on two fronts. First knowledge of the
internal components of a population are often based on a limited or
incomplete knowledge of the population. In other words, strata are
selected subjectively by researchers using various heuristics some of
which are based on some sense of `common knowledge'. The second front on
which stratified sampling can err concerns the relative sizes of the
sub-components relative to the whole population. Even if the relevant
sub-components are identified, their relative size adds another
challenge which researchers must address in order to maximize the
representativeness of a sample. To attempt to align, or
\textbf{balance}, the relative sizes of the samples for the strata is
the second population-informed sampling strategy.

\hypertarget{corpora}{%
\subsection{Corpora}\label{corpora}}

A key feature of a sample is that it is purposely selected. Samples are
not simply a collection or set of data from the population. Samples are
rigorously selected with an explicit target population in mind. In text
analysis a purposely sampled collection of texts, of the type defined
here, is known as a \textbf{corpus.} For this same reason a set of texts
or documents which have not been selected along a purposely selected
sampling frame is not a corpus. The sampling frame, and therefore the
populations modeled, in any given corpus most likely will vary and for
this reason it is not a safe assumption that any given corpus is equally
applicable for any and every research question. Corpus development
(\emph{i.e.} language sampling) is purposeful, and the characteristics
of the corpus development process should be made explicit through
documentation. Therefore vetting a corpus sample for its applicability
to a research goal is a key step in that a research must take to ensure
the integrity of the research findings.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-important-color!10!white, coltitle=black, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Consider}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-important-color-frame, bottomrule=.15mm, colback=white]

The Brown Corpus is widely recognized as one of the first large,
machine-readable corpora. It was compiled by Kucera and Francis (1967).
Consult the
\href{http://korpus.uib.no/icame/brown/bcm.html}{documentation for this
corpus}. Can you determine what language population this corpus aims to
represent? Given the sampling frame for this corpus (in the
documentation and summarized in Figure~\ref{fig-brown-distribution}),
what types of research might this corpus support or not support?

\end{tcolorbox}

\begin{figure}[h]

{\centering \includegraphics{understanding-data_files/figure-pdf/fig-brown-distribution-1.pdf}

}

\caption{\label{fig-brown-distribution}Brown Corpus of Written American
English}

\end{figure}

\hypertarget{types}{%
\subsubsection{Types}\label{types}}

With the notion of sampling frames in mind, some corpora are compiled
with the aim to be of general purpose (general or \textbf{reference
corpora}), and some with much more specialized sampling frames
(\textbf{specialized corpora}). For example, the
\href{https://www.anc.org/}{American National Corpus (ANC)} or the
\href{http://www.natcorp.ox.ac.uk/}{British National Corpus (BNC)} are
corpora which aim to model (represent/ reflect) the general
characteristics of a variety of the English language, the former of
American English and the later British English. These are ambitious
projects, and require significant investments of time in corpus design
and then in implementation (and continued development) that are usually
undertaken by research teams (Ädel 2020).

Specialized corpora aim to represent more specific populations. The
\href{https://www.linguistics.ucsb.edu/research/santa-barbara-corpus}{Santa
Barbara Corpus of Spoken American English (SBCSAE)}, as you can imagine
from the name of the resource, aims to model spoken American English. No
claim to written English is included. There are even more specific types
of corpora which attempt to model other types of sub-populations such as
\href{https://www.coventry.ac.uk/research/research-directories/current-projects/2015/british-academic-written-english-corpus-bawe/}{academic
writing},
\href{https://www.clarin.eu/resource-families/cmc-corpora}{computer-mediated
communication (CMC)}, language use in specific
\href{http://ice-corpora.net/ice/index.html}{regions of the world}, a
\href{https://www.wgtn.ac.nz/lals/resources/corpora-default/corpora-wsc}{country},
a \href{https://cesa.arizona.edu}{region of a country}, \emph{etc}.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-important-color!10!white, coltitle=black, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Consider}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-important-color-frame, bottomrule=.15mm, colback=white]

Grieve, Nini, and Guo (2018) compiled a 8.9 billion-word corpus of
geotagged posts from Twitter between 2013-2014 in the United States. The
authors provide a \href{https://isogloss.shinyapps.io/isogloss/}{search
interface} to explore relationship between lexical usage and geographic
location. Explore this corpus searching for terms related to slang
(``hella'', ``wicked''), geographical (``mountain'', ``river''),
meteorological (``snow'', ``rain''), and/ or any other term types. What
types of patterns do you find? What are the benefits and/ or limitations
of this type of data and/ or interface?

\end{tcolorbox}

Another set of specialized corpora are resources which aim to compile
texts from different languages or different language varieties for
direct or indirect comparison. Corpora that are directly comparable,
that is they include source and translated texts, are called
\textbf{parallel corpora}. Parallel corpora include different languages
or language varieties that are indexed and aligned at some linguistic
level (\emph{i.e.} word, phrase, sentence, paragraph, or document), see
\href{https://opus.nlpl.eu/}{OPUS}. Corpora that are compiled with
different languages or language varieties but are not directly aligned
are called \textbf{comparable corpora}. The comparable language or
language varieties are sampled with the same or similar sampling frame,
for example
\href{https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/0402}{Brown}
and
\href{https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/0167}{LOB}
corpora.

The aim of the quantitative text researcher is to select the corpus or
corpora (plural of corpus) which best aligns with the purpose of the
research. For example, a general corpus such as the American National
Corpus may be better suited to address a question dealing with the way
American English works, but this general resource may lack detail in
certain areas, such as \href{https://mtsamples.com/index.asp}{medical
language}, that may be vital for a research project aimed at
understanding changes in medical terminology. Furthermore, a researcher
studying spoken language might collect a corpus of transcribed
conversations from a particular community or region, such as the SBCSAE.
While this would not include every possible spoken utterance produced by
members of that group, it could be considered a representative sample of
the population of speech in that context.

\hypertarget{sources}{%
\subsubsection{Sources}\label{sources}}

The most common source of data used in contemporary quantitative
research is the internet. On the web an investigator can access corpora
published for research purposes and language used in natural settings
that can be compiled by investigators into a corpus. Many organizations
exist around the globe that provide access to corpora in browsable
catalogs, or \textbf{repositories}. There are repositories dedicated to
language research, in general, such as the
\href{https://www.ldc.upenn.edu/}{Language Data Consortium} or for
specific language domains, such as the language acquisition repository
\href{http://talkbank.org/}{TalkBank}. It is always advisable to start
looking for the available language data in a repository. The advantage
of beginning your data search in repositories is that a repository,
especially those geared towards the linguistic community, will make
identifying language corpora faster than through a general web search.
Furthermore, repositories often require certain standards for corpus
format and documentation for publication. A standardized resource many
times will be easier to interpret and evaluate for its appropriateness
for a particular research project.

In the table below I've compiled a list of some corpus repositories to
help you get started.

\begin{table}

\caption{A list of some corpus repositories}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="https://corpus.byu.edu/">BYU corpora</a> & A repository of corpora that includes billions of words of data.\\
\hline
<a href="http://corporafromtheweb.org/">COW (COrpora from the Web)</a> & A collection of linguistically processed gigatoken web corpora\\
\hline
<a href="http://wortschatz.uni-leipzig.de/en/download/">Leipzig Corpora Collection</a> & Corpora in different languages using the same format and comparable sources.\\
\hline
<a href="https://www.ldc.upenn.edu/">Linguistic Data Consortium</a> & Repository of language corpora\\
\hline
<a href="http://www.resourcebook.eu/searchll.php">LRE Map</a> & Repository of language resources collected during the submission process for the Language Resource and Evaluation Conference (LREC).\\
\hline
<a href="http://www.nltk.org/nltk\_data/">NLTK language data</a> & Repository of corpora and language datasets included with the Python package NLTK.\\
\hline
<a href="http://opus.lingfil.uu.se/">OPUS - an open source parallel corpus</a> & Repository of translated texts from the web.\\
\hline
<a href="http://talkbank.org/">TalkBank</a> & Repository of language collections dealing with conversation, acquisition, multilingualism, and clinical contexts.\\
\hline
<a href="https://corpus1.mpi.nl/ds/asv/?4">The Language Archive</a> & Various corpora and language datasets\\
\hline
<a href="http://ota.ox.ac.uk/">The Oxford Text Archive (OTA)</a> & A collection of thousands of texts in more than 25 different languages.\\
\hline
\end{tabular}
\end{table}

Repositories are by no means the only source of corpora on the web.
Researchers from around the world provide access to corpora and other
data sources on their own sites or through data sharing platforms.
Corpora of various sizes and scopes will often be accessible on a
dedicated homepage or appear on the homepage of a sponsoring
institution. Finding these resources is a matter of doing a web search
with the word `corpus' and a list of desired attributes, including
language, modality, register, \emph{etc}. As part of a general movement
towards reproducibility more corpora are available on the web than ever
before. Therefore data sharing platforms supporting reproducible
research, such as \href{https://github.com/}{GitHub},
\href{https://zenodo.org/}{Zenodo},
\href{http://www.re3data.org/}{Re3data}, \href{https://osf.io/}{OSF},
\emph{etc}., are a good place to look as well, if searching repositories
and targeted web searches do not yield results.

In the table below you will find a list of corpus resources and
datasets.

\begin{table}

\caption{Corpora and language datasets.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="http://www.socsci.uci.edu/\textasciitilde{}lpearl/CoLaLab/CHILDESTreebank/childestreebank.html">CHILDES Treebank</a> & A corpus derived from several corpora from the American English section of CHILDES with the goal to annotate child-directed speech utterance transcriptions with phrase structure tree information.\\
\hline
<a href="http://www.cs.cornell.edu/\textasciitilde{}cristian/Cornell\_Movie-Dialogs\_Corpus.html">Cornell Movie-Dialogs Corpus</a> & A corpus containing a large metadata-rich collection of fictional conversations extracted from raw movie scripts.\\
\hline
<a href="http://www.lllf.uam.es/\textasciitilde{}fmarcos/informes/corpus/coarginl.html">Corpus Argentino</a> & Corpus of Argentine Spanish\\
\hline
<a href="https://cesa.arizona.edu/">Corpus of Spanish in Southern Arizona</a> & Spanish varieties spoken in Arizona.\\
\hline
<a href="https://www.statmt.org/europarl/">Europarl Parallel Corpus</a> & A parallel corpus extracted from the proceedings of the European Parliament Proceedings between 1996-2011.\\
\hline
<a href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html">Google Ngram Viewer</a> & Google web corpus\\
\hline
<a href="http://ice-corpora.net/ice/">International Corpus of English (ICE)</a> & The International Corpus of English (ICE) began in 1990 with the primary aim of collecting material for comparative studies of English worldwide.\\
\hline
<a href="http://opus.lingfil.uu.se/OpenSubtitles\_v2.php">OpenSubtitles2011</a> & A collection of documents from http://www.opensubtitles.org/.\\
\hline
<a href="http://www.ruscorpora.ru/en/">Russian National Corpus</a> & A corpus of modern Russian language incorporating over 300 million words.\\
\hline
<a href="https://quantumstat.com//dataset">The Big Bad NLP Database - Quantum Stat</a> & NLP datasets\\
\hline
<a href="https://catalog.ldc.upenn.edu/docs/LDC97S62/">The Switchboard Dialog Act Corpus</a> & A corpus of 1155 5-minute conversations in American English, comprising 205,000 utterances and 1.4 million words, from the Switchboard corpus of telephone conversations.\\
\hline
<a href="http://langsnap.soton.ac.uk/">Welcome to LANGSNAP - LANGSNAP</a> & The aim of this repository is to promote research on the learning of French and Spanish as L2, by making parallel learner corpora for each language freely available to the research community.\\
\hline
<a href="http://www.psych.ualberta.ca/\textasciitilde{}westburylab/downloads/usenetcorpus.download.html">Westbury Lab Web Site: Usenet Corpus Download</a> & This corpus is a collection of public USENET postings. This corpus was collected between Oct 2005 and Jan 2011, and covers 47,860 English language, non-binary-file news groups (see list of newsgroups included with the corpus for details)\\
\hline
\end{tabular}
\end{table}

Language corpora prepared by researchers and research groups listed on
repositories or hosted by the researchers themselves is often the first
place to look for data. The web, however, contains a wealth of language
and language-related data that can be accessed by researcher to compile
their own corpus. There are two primary ways to attain language data
from the web. The first is through the process of web scraping.
\textbf{Web scraping} is the process of harvesting data from the web
either manually or (semi-)automatically from the actual public-facing
web. The second way to acquire data from the web is through an
\textbf{Application Programming Interface} (API). APIs are, as the title
suggests, programming interfaces which allow access, under certain
conditions, to information that a website or database accessible via the
web contains.

The table below lists some R packages that serve to interface language
data directly through R.

\begin{table}

\caption{R Package interfaces to language corpora and datasets.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="https://ropensci.org/tutorials/arxiv\_tutorial.html">aRxiv</a> & R package interface to query arXiv, a repository of electronic preprints for computer science, mathematics, physics, quantitative biology, quantitative finance, and statistics.\\
\hline
<a href="https://github.com/ropensci/crminer">crminer</a> & R package interface focusing on getting the user full text via the Crossref search API.\\
\hline
<a href="https://github.com/ropensci/dvn">dvn</a> & R package interface to access to the Dataverse Network APIs.\\
\hline
<a href="https://ropensci.org/tutorials/fulltext\_tutorial.html">fulltext</a> & R package interface to query open access journals, such as PLOS.\\
\hline
<a href="https://ropensci.org/tutorials/gutenbergr\_tutorial.html">gutenbergr</a> & R package interface to download and process public domain works from the Project Gutenberg collection.\\
\hline
<a href="https://ropensci.org/tutorials/internetarchive\_tutorial.html">internetarchive</a> & R package interface to query the Internet Archive.\\
\hline
<a href="https://github.com/hrbrmstr/newsflash">newsflash</a> & R package interface to query the Internet Archive and GDELT Television Explorer\\
\hline
<a href="https://github.com/ropensci/oai">oai</a> & R package interface to query any OAI-PMH repository, including Zenodo.\\
\hline
<a href="https://github.com/ropensci/rfigshare">rfigshare</a> & R package interface to query the data sharing platform FigShare.\\
\hline
<a href="https://github.com/ropensci/rtweet">rtweet</a> & R client for interacting with Twitter's APIs\\
\hline
\end{tabular}
\end{table}

Data for language research is not limited to (primary) text sources.
Other sources may include processed data from previous research; word
lists, linguistic features, \emph{etc}.. Alone or in combination with
text sources this data can be a rich and viable source of data for a
research project.

Below I've included some processed language resources.

\begin{table}

\caption{Language data from previous research and meta-studies.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="http://elexicon.wustl.edu/WordStart.asp">English Lexicon Project</a> & Access to a large set of lexical characteristics, along with behavioral data from visual lexical decision and naming studies.\\
\hline
<a href="https://github.com/ropensci/lingtypology">lingtypology</a> & R package interface to connect with the Glottolog database and provides additional functionality for linguistic mapping.\\
\hline
<a href="https://nyu-mll.github.io/CoLA/">The Corpus of Linguistic Acceptability (CoLA)</a> & A corpus that consists of 10657 sentences from 23 linguistics publications, expertly annotated for acceptability (grammaticality) by their original authors.\\
\hline
<a href="http://icon.shef.ac.uk/Moby/">The Moby lexicon project</a> & Language wordlists and resources from the Moby project.\\
\hline
\end{tabular}
\end{table}

The list of data available for language research is constantly growing.
I've document very few of the wide variety of resources. Below I've
included attempts by others to provide a summary of the corpus data and
language resources available.

\begin{table}

\caption{Lists of corpus resources.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="https://uclouvain.be/en/research-institutes/ilc/cecl/learner-corpora-around-the-world.html">Learner corpora around the world</a> & A listing of learner corpora around the world\\
\hline
<a href="https://paperswithcode.com/datasets">Machine Learning Datasets | Papers With Code</a> & A free and open resource with Machine Learning papers, code, and evaluation tables.\\
\hline
<a href="http://nlp.stanford.edu/links/statnlp.html\#Corpora">Stanford NLP corpora</a> & Listing of corpora and language resources aimed at the NLP community.\\
\hline
<a href="https://makingnoiseandhearingthings.com/2017/09/20/where-can-you-find-language-data-on-the-web/">Where can you find language data on the web?</a> & Listing of various corpora and language datasets.\\
\hline
\end{tabular}
\end{table}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-important-color!10!white, coltitle=black, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Consider}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-important-color-frame, bottomrule=.15mm, colback=white]

Explore some of the resources listed above and consider their sampling
frames. Can you think of a research question or questions that this
resource may be well-suited to support research into? What types of
questions would be less-than-adequate for a given resource?

\end{tcolorbox}

\hypertarget{formats}{%
\subsubsection{Formats}\label{formats}}

A corpus will often include various types of non-linguistic attributes,
or \index{meta-data}\textbf{meta-data}, as well. Ideally this will
include information regarding the source(s) of the data, dates when it
was acquired or published, and other author or speaker information. It
may also include any number of other attributes that were identified as
potentially important in order to appropriately document the target
population. Again, it is key to match the available meta-data with the
goals of your research. In some cases a corpus may be ideal in some
aspects but not contain all the key information to address your research
question. This may mean you will need to compile your own corpus if
there are fundamental attributes missing. Before you consider compiling
your own corpus, however, it is worth investigating the possibility of
augmenting an available corpus to bring it inline with your particular
goals. This may include adding new language sources, harnessing software
for linguistic annotation (part-of-speech, syntactic structure, named
entities, \emph{etc}.), or linking available corpus meta-data to other
resources, linguistic or non-linguistic.

Corpora come in various formats, the main three being: running text,
structured documents, and databases. The format of a corpus is often
influenced by characteristics of the data but may also reflect an
author's individual preferences as well. It is typical for corpora with
few meta-data characteristics to take the form of running text.

Running text sample from the
\href{https://www.statmt.org/europarl/}{Europarle Parallel Corpus}.

\begin{verbatim}
> Resumption of the session
> I declare resumed the session of the European Parliament adjourned on
Friday 17 December 1999, and I would like once again to wish you a
happy new year in the hope that you enjoyed a pleasant festive period.
> Although, as you will have seen, the dreaded 'millennium bug' failed
to materialise, still the people in a number of countries suffered a
series of natural disasters that truly were dreadful.
> You have requested a debate on this subject in the course of the next
few days, during this part-session.
> In the meantime, I should like to observe a minute' s silence, as a
number of Members have requested, on behalf of all the victims
concerned, particularly those of the terrible storms, in the various
countries of the European Union.
> Please rise, then, for this minute' s silence.
> (The House rose and observed a minute' s silence)
> Madam President, on a point of order.
> You will be aware from the press and television that there have been
a number of bomb explosions and killings in Sri Lanka.
> One of the people assassinated very recently in Sri Lanka was Mr
Kumar Ponnambalam, who had visited the European Parliament just a few
months ago.
\end{verbatim}

In corpora with more meta-data, a header may be appended to the top of
each running text document or the meta-data may be contained in a
separate file with appropriate coding to coordinate meta-data attributes
with each text in the corpus.

Meta-data header sample from the \href{}{Switchboard Dialog Act Corpus}.

\begin{verbatim}
> FILENAME: 4325_1632_1519
> TOPIC#: 323
> DATE: 920323
> TRANSCRIBER: glp
> UTT_CODER: tc
> DIFFICULTY: 1
> TOPICALITY: 3
> NATURALNESS: 2
> ECHO_FROM_B: 1
> ECHO_FROM_A: 4
> STATIC_ON_A: 1
> STATIC_ON_B: 1
> BACKGROUND_A: 1
> BACKGROUND_B: 2
> REMARKS: None.
>
>
=========================================================================
>
>
> o A.1 utt1: Okay.  /
> qw A.1 utt2: {D So, }
>
> qy^d B.2 utt1: [ [ I guess, +
>
> + A.3 utt1: What kind of experience [ do you, + do you ] have, then
with child care? /
>
> + B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /
>
> qy A.5 utt1: Does it say something? /
>
> sd B.6 utt1: I think it usually does.  /
> ad B.6 utt2: You might try, {F uh, } /
> h B.6 utt3: I don't know, /
> ad B.6 utt4: hold it down a little longer, /
> ad B.6 utt5: {C and } see if it, {F uh, } -/
\end{verbatim}

When meta-data and/ or linguistic annotation increases in complexity it
is common to structure each corpus document more explicitly with a
markup language such as XML (Extensible Markup Language) or organize
relationships between language and meta-data attributes in a database.

XML format for meta-data (and linguistic annotation) from the
\href{http://www.nltk.org/nltk_data/}{Brown Corpus}.

\begin{verbatim}
> <TEI
xmlns="http://www.tei-c.org/ns/1.0"><teiHeader><fileDesc><titleStmt><title>Sample
A01 from The Atlanta Constitution</title><title type="sub"> November 4,
1961, p.1 "Atlanta Primary ..."
> "Hartsfield Files"
> August 17, 1961, "Urged strongly ..."
> "Sam Caldwell Joins"
> March 6,1961, p.1 "Legislators Are Moving" by Reg Murphy
> "Legislator to fight" by Richard Ashworth
> "House Due Bid..."
> p.18 "Harry Miller Wins..."
> </title></titleStmt><editionStmt><edition>A part of the XML version
of the Brown Corpus</edition></editionStmt><extent>1,988 words 431
(21.7%) quotes 2
symbols</extent><publicationStmt><idno>A01</idno><availability><p>Used
by permission of The Atlanta ConstitutionState News Service (H), and
Reg Murphy (E).</p></availability></publicationStmt><sourceDesc><bibl>
The Atlanta
Constitution</bibl></sourceDesc></fileDesc><encodingDesc><p>Arbitrary
Hyphen: multi-million [0520]</p></encodingDesc><revisionDesc><change
when="2008-04-27">Header auto-generated for TEI
version</change></revisionDesc></teiHeader>
> <text xml:id="A01" decls="A">
> <body><p><s n="1"><w type="AT">The</w> <w type="NP"
subtype="TL">Fulton</w> <w type="NN" subtype="TL">County</w> <w
type="JJ" subtype="TL">Grand</w> <w type="NN" subtype="TL">Jury</w> <w
type="VBD">said</w> <w type="NR">Friday</w> <w type="AT">an</w> <w
type="NN">investigation</w> <w type="IN">of</w> <w
type="NPg">Atlanta's</w> <w type="JJ">recent</w> <w
type="NN">primary</w> <w type="NN">election</w> <w
type="VBD">produced</w> <c type="pct">``</c> <w type="AT">no</w> <w
type="NN">evidence</w> <c type="pct">''</c> <w type="CS">that</w> <w
type="DTI">any</w> <w type="NNS">irregularities</w> <w
type="VBD">took</w> <w type="NN">place</w> <c type="pct">.</c> </s>
> </p>
\end{verbatim}

Although there has been a push towards standardization of corpus
formats, most available resources display some degree of idiosyncrasy.
Being able to parse the structure of a corpus is a skill that will
develop with time. With more experience working with corpora you will
become more adept at identifying how the data is stored and whether its
content and format will serve the needs of your analysis.

\hypertarget{information}{%
\section{Information}\label{information}}

Identifying an adequate corpus resource for the target research question
is the first step in moving a quantitative text research project
forward. The next step is to select the components or characteristics of
this resource that are relevant for the research and then move to
organize the attributes of this data into a more useful and informative
format. This is the process of converting a corpus into a
\index{dataset}\textbf{dataset} --a tabular representation of the
information to be leveraged in the analysis.

\hypertarget{structure-2}{%
\subsection{Structure}\label{structure-2}}

Data alone is not informative. Only through explicit organization of the
data in a way that makes relationships explicit does the data become
information. In this form, our data is called a dataset. This is a
particularly salient hurdle in text analysis research. Many textual
sources are \index{unstructured data}\textbf{unstructured data} --that
is, the relationships that will be used in the analysis have yet to be
purposefully drawn and organized from the text to make the relationships
meaningful and useful for analysis.

For the running text in the Europarle Corpus, we know that there are
files which are the source text (original) and files that correspond to
the target text (translation). In Table~\ref{tbl-structure-europarle} we
see that this text has been organized so that there are columns
corresponding to the \texttt{type} and \texttt{sentence} with an
additional \texttt{sentence\_id} column to keep an index of how the
sentences are aligned.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

It is conventional to work with column names for datasets in R using the
same conventions that are used for naming objects. It is a matter of
taste which convention is used, but I have adopted
\href{https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/names.html\#snake_case}{snake
case} as my personal preference. There are also
\href{https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/names.html}{alternatives}.
Regardless of the convention you choose, it is good practice to be
consistent.

It is also of note that the column names should be balanced for
meaningfulness and brevity. This brevity is of practical concern but can
be somewhat opaque. For questions into the meaning of the column and is
values consult the resource's dataset documentation.

\end{tcolorbox}

\hypertarget{tbl-structure-europarle}{}
\begin{table}
\caption{\label{tbl-structure-europarle}First 10 source and target sentences in the Europarle Corpus. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 1 & Resumption of the session\\
Target & 1 & Reanudación del período de sesiones\\
Source & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Target & 2 & Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\\
Source & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
\addlinespace
Target & 3 & Como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\
Source & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
Target & 4 & Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\
Source & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Target & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\\
\addlinespace
Source & 6 & Please rise, then, for this minute' s silence.\\
Target & 6 & Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
Source & 7 & (The House rose and observed a minute' s silence)\\
Target & 7 & (El Parlamento, de pie, guarda un minuto de silencio)\\
Source & 8 & Madam President, on a point of order.\\
\addlinespace
Target & 8 & Señora Presidenta, una cuestión de procedimiento.\\
Source & 9 & You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\\
Target & 9 & Sabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en Sri Lanka.\\
Source & 10 & One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\\
Target & 10 & Una de las personas que recientemente han asesinado en Sri Lanka ha sido al Sr. Kumar Ponnambalam, quien hace pocos meses visitó el Parlamento Europeo.\\
\bottomrule
\end{tabular}
\end{table}

Other corpus resources are \textbf{semi-structured data} --that is,
there are some characteristics which are structured, but other which are
not.

The Switchboard Dialog Act Corpus is an example of a semi-structured
resource. It has meta-data associated with each of the 1,155
conversations in the corpus. In Table~\ref{tbl-structure-swda} a
language-relevant sub-set of the meta-data is associated with each
utterance.

\hypertarget{tbl-structure-swda}{}
\begin{table}
\caption{\label{tbl-structure-swda}First 5 utterances from the Switchboard Dialog Act Corpus. }\tabularnewline

\centering
\begin{tabular}{lrrllllll}
\toprule
doc\_id & speaker\_id & topic\_num & topicality & naturalness & damsl\_tag & speaker & utterance\_num & utterance\_text\\
\midrule
4325 & 1632 & 323 & 3 & 2 & o & A & 1 & Okay.  /\\
4325 & 1632 & 323 & 3 & 2 & qw & A & 2 & \{D So, \}\\
4325 & 1519 & 323 & 3 & 2 & qy\textasciicircum{}d & B & 1 & {}[ [ I guess, +\\
4325 & 1632 & 323 & 3 & 2 & + & A & 1 & What kind of experience [ do you, + do you ] have, then with child care? /\\
4325 & 1519 & 323 & 3 & 2 & + & B & 1 & I think, ] + \{F uh, \} I wonder ] if that worked. /\\
\bottomrule
\end{tabular}
\end{table}

Relatively fewer resources are \textbf{structured datasets}. In these
cases a high amount of meta-data and/ or linguistic annotation is
included in the corpus. The format convention, however, varies from
resource to resource. Some of the formats are programming general (.csv,
.xml, .json, \emph{etc}.) and others are resource specific (.cha, .utt,
.prd, \emph{etc}.). In Table~\ref{tbl-structure-brown} the XML version
of the Brown Corpus is represented in tabular format. Note that along
with other meta-data variables, it also contains a variable with
linguistic annotation for grammatical category (\texttt{pos}
part-of-speech) of each word.

\hypertarget{tbl-structure-brown}{}
\begin{table}
\caption{\label{tbl-structure-brown}First 10 words from the Brown Corpus. }\tabularnewline

\centering
\begin{tabular}{llll}
\toprule
document\_id & category & words & pos\\
\midrule
01 & A & The & AT\\
01 & A & Fulton & NP\\
01 & A & County & NN\\
01 & A & Grand & JJ\\
01 & A & Jury & NN\\
\addlinespace
01 & A & said & VBD\\
01 & A & Friday & NR\\
01 & A & an & AT\\
01 & A & investigation & NN\\
01 & A & of & IN\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{ud-tidy-data}{%
\subsubsection{Tidy Data}\label{ud-tidy-data}}

The selection of the attributes from a corpus and the juxtaposition of
these attributes in a relational format, or dataset, that converts data
into information is known as \textbf{data curation}. The process of data
curation minimally involves creating a base dataset, or \emph{derived
dataset}, which establishes the main informational associations
according to philosophical approach outlined by Wickham (2014).

In this work, a \textbf{tidy dataset} refers both to the structural
(physical) and informational (semantic) organization of the dataset.
Physically, a tidy dataset is a tabular data structure where each
\emph{row} is an observation and each \emph{column} is a variable that
contains measures of a feature or attribute of each observation. Each
cell where a given row-column intersect contains a \emph{value} which is
a particular attribute of a particular observation for the particular
observation-feature pair also known as a \emph{data point}.

\begin{figure}[h]

{\centering \includegraphics[width=7.2in,height=\textheight]{figures/understanding-data/tidy-format-paper.png}

}

\caption{\label{fig-tidy-format-image}Visual summary of the tidy
format.}

\end{figure}

\hypertarget{ud-informational-values}{%
\subsubsection{Informational values}\label{ud-informational-values}}

Each column in a tidy dataset is a \textbf{variable} which reflects
measures for a particular attribute that can take on different values.
These values are grouped into two main categories: \textbf{categorical}
and \textbf{numeric}.

Qualitative variables are \textbf{categorical} and can be further broken
down into \textbf{nominal} and \textbf{ordinal}. Quantitative variables
are \textbf{Numeric} and can be further broken down into
\textbf{interval} and \textbf{ratio} as seen in
Figure~\ref{fig-ud-informational-values}.

\begin{figure}

{\centering 

\begin{figure}[H]

{\centering \includegraphics[width=7.77in,height=2.8in]{understanding-data_files/figure-latex/mermaid-figure-1.png}

}

\end{figure}

}

\caption{\label{fig-ud-informational-values}Informational values of
variables.}

\end{figure}

\ldots. examples of levels of measurement \ldots.

\hypertarget{ud-semantic-value}{%
\subsubsection{Semantic value}\label{ud-semantic-value}}

Semantic value in a tidy dataset is derived from the association of this
physical structure along the two dimensions of this rectangular format.
First, each column is a \textbf{variable} which reflects measures for a
particular attribute that can take on different values. In this way it
`varies'.

In the Europarle Corpus dataset, in Table
Table~\ref{tbl-structure-europarle}, for example, the \texttt{type}
column measures the type of text, either \texttt{Source} or
\texttt{Target}. Futhermore variables can contain measures which are
either qualitative or quantitative, that is category- or scale-based.

Second, each row is an \textbf{observation} that contains all of the
variables associated with the primary \textbf{unit of observation}. The
primary unit of observation the variable that is the essential focus of
the informational structure. In this same dataset the first observation
contains the \texttt{type}, \texttt{sentence\_id}, and the
\texttt{sentence}. As this dataset is currently structured the primary
unit of investigation is the \texttt{sentence} as each of the other
variables have measures that characterize each value of
\texttt{sentence}.

The decision as to what the primary unit of observation is is
fundamentally guided by the research question, and therefore highly
specific to the particular research project. Say instead we wanted to
focus on words instead of sentences. The dataset would need to be
transformed such that a new variable (\texttt{words}) would be created
to contain each word in the corpus.

\hypertarget{tbl-tidy-words-europarle}{}
\begin{table}
\caption{\label{tbl-tidy-words-europarle}Europarle Paralle Corpus with \texttt{words} as primary unit of
investigation. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & words\\
\midrule
Source & 1 & Resumption\\
Source & 1 & of\\
Source & 1 & the\\
Source & 1 & session\\
Target & 1 & Reanudación\\
\addlinespace
Target & 1 & del\\
Target & 1 & período\\
Target & 1 & de\\
Target & 1 & sesiones\\
\bottomrule
\end{tabular}
\end{table}

The values for the variables \texttt{type} and \texttt{sentence\_id}
maintain the necessary description for each \texttt{word} to ensure the
required semantic relationships to identify the particular attributes
for each word observation. This dataset may seem redundant in that the
values for \texttt{type} and \texttt{sentence\_id} are repeated numerous
times but this `redundancy' makes the relationship between each variable
associated with the primary unit of investigation explicit. This format
makes a tidy dataset a versatile format for researchers to conduct
analyses in a powerful and flexible way, as we will see throughout this
textbook.

It is important to make clear that data in tabular format in itself does
not constitute a dataset, in the tidy sense we will be using. Data can
be organized in many ways which do not make relationships between
variables and observations explicit.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-important-color!10!white, coltitle=black, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Consider}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-important-color-frame, bottomrule=.15mm, colback=white]

All tabular data does not have the `tidy' format that I have described
here. Can you think of examples of tabular information that would not be
in a tidy format?

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-important-color!10!white, coltitle=black, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Consider}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-important-color-frame, bottomrule=.15mm, colback=white]

Consider the following dataset:

What is the primary unit of observation in this dataset? What are the
variables? What are the values for each variable? What is the semantic
value of this dataset?

\end{tcolorbox}

\hypertarget{transformation}{%
\subsection{Transformation}\label{transformation}}

At this point have introduced the first step in data curation in which
the original data is converted into a relational dataset (derived
dataset) and highlighted the importance of this informational structure
for setting the stage for data analysis. However, the primary derived
dataset is often not the final organizational step before proceeding to
statistical analysis. Many times, if not always, the derived dataset
requires some manipulation or transformation to prepare the dataset for
the specific analysis approach to be taken. This is another level of
human intervention and informational organization, and therefore another
step forward in our journey from data to insight and as such a step up
in the DIKI hierarchy. Common types of transformations include cleaning
variables (normalization), separating or eliminating variables
(recoding), creating new variables (generation), or incorporating others
datasets which integrate with the existing variables (merging). The
results of these transformations build on and manipulate the derived
dataset and produce an \emph{analysis dataset}. Let's now turn to
provide a select set of examples of each of these transformations using
the datasets we have introduced in this chapter.

\hypertarget{normalization}{%
\subsubsection{Normalization}\label{normalization}}

The process of normalization aims to \emph{sanitize} the values within a
variable or set of variables. This may include removing whitespace,
punctuation, numerals, or special characters or substituting uppercase
for lowercase characters, numerals for word versions, acronyms for their
full forms, irregular or incorrect spelling for accepted forms, or
removing common words (\textbf{stopwords}), \emph{etc}.

On inspecting the Europarle dataset
(Table~\ref{tbl-structure-europarle}) we will see that there are
sentence lines which do not represent actual parliament speeches. In
Table~\ref{tbl-normalize-non-speech-identify-europarle} we see these
lines.

\hypertarget{tbl-normalize-non-speech-identify-europarle}{}
\begin{table}
\caption{\label{tbl-normalize-non-speech-identify-europarle}Non-speech lines in the Europarle dataset. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 1 & Resumption of the session\\
Target & 1 & Reanudación del período de sesiones\\
Source & 7 & (The House rose and observed a minute' s silence)\\
Target & 7 & (El Parlamento, de pie, guarda un minuto de silencio)\\
\bottomrule
\end{tabular}
\end{table}

A research project aiming to analyze speech would want to normalize this
dataset removing these lines, as seen in
Table~\ref{tbl-normalize-non-speech-remove-europarle}.

\hypertarget{tbl-normalize-non-speech-remove-europarle}{}
\begin{table}
\caption{\label{tbl-normalize-non-speech-remove-europarle}The Europarle dataset with non-speech lines removed. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Target & 2 & Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\\
Source & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Target & 3 & Como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\
Source & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
\addlinespace
Target & 4 & Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\
Source & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Target & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\\
Source & 6 & Please rise, then, for this minute' s silence.\\
Target & 6 & Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
\addlinespace
Source & 8 & Madam President, on a point of order.\\
Target & 8 & Señora Presidenta, una cuestión de procedimiento.\\
Source & 9 & You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\\
Target & 9 & Sabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en Sri Lanka.\\
Source & 10 & One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\\
\addlinespace
Target & 10 & Una de las personas que recientemente han asesinado en Sri Lanka ha sido al Sr. Kumar Ponnambalam, quien hace pocos meses visitó el Parlamento Europeo.\\
\bottomrule
\end{tabular}
\end{table}

Another feature of this dataset which may require attention is the fact
that the English lines include whitespace between possessive nouns.

\hypertarget{tbl-normalize-whitespace-identify-europarle}{}
\begin{table}
\caption{\label{tbl-normalize-whitespace-identify-europarle}Lines with possessives with extra whitespace in the Europarle dataset. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Source & 6 & Please rise, then, for this minute' s silence.\\
\bottomrule
\end{tabular}
\end{table}

This may affect another transformation process or subsequent analysis,
so it may be a good idea to normalize these forms by removing the extra
whitespace.

\hypertarget{tbl-normalize-whitespace-remove-europarle}{}
\begin{table}
\caption{\label{tbl-normalize-whitespace-remove-europarle}The Europarle dataset with whitespace from possessives removed. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 5 & In the meantime, I should like to observe a minute's silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Source & 6 & Please rise, then, for this minute's silence.\\
\bottomrule
\end{tabular}
\end{table}

A final normalization case scenario involves changing converting all the
text to lowercase. If the goal for the research is to count words at
some point the fact that a word starts a sentence and by convention the
first letter is capitalized will result distinct counts for words that
are in essence the same (\emph{i.e.} ``In'' vs.~``in'').

\hypertarget{tbl-normalize-lowercase-europarle}{}
\begin{table}
\caption{\label{tbl-normalize-lowercase-europarle}The Europarle dataset with lowercasing applied. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 2 & i declare resumed the session of the european parliament adjourned on friday 17 december 1999, and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Target & 2 & declaro reanudado el período de sesiones del parlamento europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a sus señorías mi deseo de que hayan tenido unas buenas vacaciones.\\
Source & 3 & although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Target & 3 & como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. en cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\
Source & 4 & you have requested a debate on this subject in the course of the next few days, during this part-session.\\
\addlinespace
Target & 4 & sus señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\
Source & 5 & in the meantime, i should like to observe a minute's silence, as a number of members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the european union.\\
Target & 5 & a la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la unión europea afectados.\\
Source & 6 & please rise, then, for this minute's silence.\\
Target & 6 & invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
\addlinespace
Source & 8 & madam president, on a point of order.\\
Target & 8 & señora presidenta, una cuestión de procedimiento.\\
Source & 9 & you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka.\\
Target & 9 & sabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en sri lanka.\\
Source & 10 & one of the people assassinated very recently in sri lanka was mr kumar ponnambalam, who had visited the european parliament just a few months ago.\\
\addlinespace
Target & 10 & una de las personas que recientemente han asesinado en sri lanka ha sido al sr. kumar ponnambalam, quien hace pocos meses visitó el parlamento europeo.\\
\bottomrule
\end{tabular}
\end{table}

Note that lowercasing text, and normalization steps in general, can come
at a cost. For example, lowercasing the Europarle dataset sentences
means we lose potentially valuable information; namely the ability to
identify proper names (\emph{i.e.} ``Mr Kumar Ponnambalam'') and titles
(\emph{i.e.} ``European Parliament'') directly from the orthographic
forms. There are, however, transformation steps that can be applied
which aim to recover `lost' information in situations such as this and
others.

\hypertarget{recoding}{%
\subsubsection{Recoding}\label{recoding}}

The process of recoding aims to \emph{recast} the values of a variable
or set of variables to a new variable or set of variables to enable more
direct access. This may include extracting values from a variable,
stemming or lemmatization of words, tokenization of linguistic forms
(words, ngrams, sentences, \emph{etc}.), calculating the lengths of
linguistic units, removing variables that will not be used in the
analysis, \emph{etc}.

Words that we intuitively associate with a `base' word can take many
forms in language use. For example the word forms `investigation',
`investigation', `investigate', `investigated', \emph{etc}. are
intuitively linked. There are two common methods that can be applied to
create a new variable to facilitate the identification of these
associations. The first is stemming. \textbf{Stemming} is a rule-based
heuristic to reduce word forms to their stem or root form.

\hypertarget{tbl-recoding-stemming-brown-example}{}
\begin{table}
\caption{\label{tbl-recoding-stemming-brown-example}Results for stemming the first words in the Brown Corpus. }\tabularnewline

\centering
\begin{tabular}{lllll}
\toprule
document\_id & category & words & pos & word\_stems\\
\midrule
01 & A & The & AT & The\\
01 & A & Fulton & NP & Fulton\\
01 & A & County & NN & Counti\\
01 & A & Grand & JJ & Grand\\
01 & A & Jury & NN & Juri\\
\addlinespace
01 & A & said & VBD & said\\
01 & A & Friday & NR & Fridai\\
01 & A & an & AT & an\\
01 & A & investigation & NN & investig\\
01 & A & of & IN & of\\
\bottomrule
\end{tabular}
\end{table}

A few things to note here. First there are a number of stemming
algorithms both for individual languages and distinct languages
\footnote{https://snowballstem.org/algorithms/}. Second not all words
can be stemmed as they do not have alternate morphological forms
(\emph{i.e.} ``The'', ``of'', \emph{etc}.). This generally related to
the distinction between closed-class (articles, prepositions,
conjunctions, \emph{etc}.) and open-class (nouns, verbs, adjectives,
\emph{etc}.) grammatical categories. Third the stem generated for those
words that can be stemmed result in forms that are not words themselves.
Nonetheless, stems can be very useful for more easily extracting a set
of related word forms.

As an example, let's identify all the word forms for the stem
`investig'.

\hypertarget{tbl-recoding-stemming-brown-search}{}
\begin{table}
\caption{\label{tbl-recoding-stemming-brown-search}Results for filter word stems for ``investig'' in the Brown Corpus. }\tabularnewline

\centering
\begin{tabular}{lllll}
\toprule
document\_id & category & words & pos & word\_stems\\
\midrule
01 & A & investigation & NN & investig\\
01 & A & investigate & VB & investig\\
03 & A & investigation & NN & investig\\
03 & A & investigation & NN & investig\\
07 & A & investigations & NNS & investig\\
\addlinespace
07 & A & investigate & VB & investig\\
08 & A & investigation & NN & investig\\
09 & A & investigation & NN & investig\\
09 & A & investigating & VBG & investig\\
09 & A & investigation & NN & investig\\
\bottomrule
\end{tabular}
\end{table}

We can see from the results in
Table~\ref{tbl-recoding-stemming-brown-search} that searching for
\texttt{word\_stems} that match `investig' returns a set of stem-related
forms. But it is worth noting that these forms cut across a number of
grammatical categories. If instead you want to draw a distinction
between grammatical categories, we can apply \textbf{lemmatization.}
This process is distinct from stemming in two important ways: (1)
inflectional forms are grouped by grammatical category and (2) the
resulting forms are lemmas or `base' forms of words.

\hypertarget{tbl-recoding-lemmatization-brown-example}{}
\begin{table}
\caption{\label{tbl-recoding-lemmatization-brown-example}Results for lemmatization of the first words in the Brown Corpus. }\tabularnewline

\centering
\begin{tabular}{lllll}
\toprule
document\_id & category & words & pos & word\_lemmas\\
\midrule
01 & A & The & AT & The\\
01 & A & Fulton & NP & Fulton\\
01 & A & County & NN & County\\
01 & A & Grand & JJ & Grand\\
01 & A & Jury & NN & Jury\\
\addlinespace
01 & A & said & VBD & say\\
01 & A & Friday & NR & Friday\\
01 & A & an & AT & a\\
01 & A & investigation & NN & investigation\\
01 & A & of & IN & of\\
\bottomrule
\end{tabular}
\end{table}

To appreciate the difference between stemming and lemmatization, let's
compare a filter for \texttt{word\_lemmas} which match `investigation'.

\hypertarget{tbl-recoding-lemmatization-brown-investigation}{}
\begin{table}
\caption{\label{tbl-recoding-lemmatization-brown-investigation}Results for filter word stems for ``investigation'' in the Brown Corpus. }\tabularnewline

\centering
\begin{tabular}{lllll}
\toprule
document\_id & category & words & pos & word\_lemmas\\
\midrule
01 & A & investigation & NN & investigation\\
03 & A & investigation & NN & investigation\\
03 & A & investigation & NN & investigation\\
07 & A & investigations & NNS & investigation\\
08 & A & investigation & NN & investigation\\
\addlinespace
09 & A & investigation & NN & investigation\\
09 & A & investigation & NN & investigation\\
23 & A & investigation & NN & investigation\\
25 & A & investigation & NN & investigation\\
41 & A & investigation & NN & investigation\\
\bottomrule
\end{tabular}
\end{table}

Only lemma forms of `investigate' which are nouns appear. Let's run a
similar search but for the lemma `be'.

\hypertarget{tbl-recoding-lemmatization-brown-be}{}
\begin{table}
\caption{\label{tbl-recoding-lemmatization-brown-be}Results for filter word stems for ``be'' in the Brown Corpus. }\tabularnewline

\centering
\begin{tabular}{lllll}
\toprule
document\_id & category & words & pos & word\_lemmas\\
\midrule
01 & A & was & BEDZ & be\\
01 & A & been & BEN & be\\
01 & A & was & BEDZ & be\\
01 & A & was & BEDZ & be\\
01 & A & are & BER & be\\
\addlinespace
01 & A & are & BER & be\\
01 & A & be & BE & be\\
01 & A & is & BEZ & be\\
01 & A & was & BEDZ & be\\
01 & A & be & BE & be\\
\bottomrule
\end{tabular}
\end{table}

Again only words of the same grammatical category are returned. In this
case the verb `be' has many more inflectional forms than `investigate'.

Another form of recoding is to detect a pattern in the values of an
existing variable and create a new variable whose values are the
extracted pattern or register that the pattern occurs and/ or how many
times it occurs. As an example, let's count the number of disfluencies
(`uh' or `um') that occur in each utterance in \texttt{utterance\_text}
from the Switchboard Dialog Act Corpus. \emph{Note I've simplified the
dataset dropping the non-relevant variables for this example.}

\hypertarget{tbl-recoding-extract-switchboard}{}
\begin{table}
\caption{\label{tbl-recoding-extract-switchboard}Disfluency counts in the first 10 utterance text values from the
Switchboard Corpus. }\tabularnewline

\centering
\begin{tabular}{lr}
\toprule
utterance\_text & disfluency\_count\\
\midrule
Okay.  / & 0\\
\{D So, \} & 0\\
{}[ [ I guess, + & 0\\
What kind of experience [ do you, + do you ] have, then with child care? / & 0\\
I think, ] + \{F uh, \} I wonder ] if that worked. / & 1\\
\addlinespace
Does it say something? / & 0\\
I think it usually does.  / & 0\\
You might try, \{F uh, \}  / & 1\\
I don't know,  / & 0\\
hold it down a little longer,  / & 0\\
\bottomrule
\end{tabular}
\end{table}

One of the most common forms of recoding in text analysis is
tokenization. \textbf{Tokenization} is the process of recasting the text
into smaller linguistic units. When working with text that has not been
linguistically annotated, the most feasible linguistic tokens are words,
ngrams, and sentences. While word and sentence tokens are easily
understandable, ngram tokens need some explanation. An \textbf{ngram} is
a sequence of either characters or words where \emph{n} is the length of
this sequence. The ngram sequences are drawn incrementally, so the
bigrams (two-word sequences) for the sentence ``This is an input
sentence.'' are:

this is, is an, an input, input sentence

We've already seen word tokenization exemplified with the Europarle
Corpus in subsection \protect\hyperlink{structure}{Structure} in
Table~\ref{tbl-tidy-words-europarle}, so let's create (word) bigram
tokens for this corpus.

\hypertarget{tbl-recoding-tokenization-europarle-bigram-words}{}
\begin{table}
\caption{\label{tbl-recoding-tokenization-europarle-bigram-words}The first 10 word bigrams of the Europarle Corpus. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & word\_bigrams\\
\midrule
Source & 2 & i declare\\
Source & 2 & declare resumed\\
Source & 2 & resumed the\\
Source & 2 & the session\\
Source & 2 & session of\\
\addlinespace
Source & 2 & of the\\
Source & 2 & the european\\
Source & 2 & european parliament\\
Source & 2 & parliament adjourned\\
Source & 2 & adjourned on\\
\bottomrule
\end{tabular}
\end{table}

As I just mentioned, ngrams sequences can be formed of characters as
well. Here are character trigram (three-character) sequences.

\hypertarget{tbl-recoding-tokenization-europarle-trigram-chars}{}
\begin{table}
\caption{\label{tbl-recoding-tokenization-europarle-trigram-chars}The first 10 character trigrams of the Europarle Corpus. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & char\_trigrams\\
\midrule
Source & 2 & ide\\
Source & 2 & dec\\
Source & 2 & ecl\\
Source & 2 & cla\\
Source & 2 & lar\\
\addlinespace
Source & 2 & are\\
Source & 2 & rer\\
Source & 2 & ere\\
Source & 2 & res\\
Source & 2 & esu\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{generation}{%
\subsubsection{Generation}\label{generation}}

The process of generation aims to \emph{augment} a variable or set of
variables. In essence this aims to make implicit attributes explicit to
that they are directly accessible. This often targeted at the automatic
generation of linguistic annotations such as grammatical category
(part-of-speech) or syntactic structure.

In the examples below I've added linguistic annotation to a target
(English) and source (Spanish) example sentence from the Europarle
Parallel Corpus. First, note the variables that are added to our dataset
that correspond to grammatical category. In addition to the
\texttt{type} and \texttt{sentence\_id} we have an assortment of
variables which replace the \texttt{sentence} variable. As part of the
process of annotation the input text to be annotated \texttt{sentence}
is tokenized \texttt{token} and indexed \texttt{token\_id}. Then
\texttt{upos} contains the Universal Part of Speech tags\footnote{\href{https://universaldependencies.org/u/pos/}{Descriptions
  of the UPOS tagset}}, and a detailed list of features is included in
\texttt{feats}. The syntactic annotation is reflected in the
\texttt{token\_id\_source} and \texttt{syntactic\_relation} variables.
These variables correspond to the type of syntactic parsing that has
been done, in this case Dependency Parsing (using the
\href{https://universaldependencies.org/}{Universal Dependencies}
framework). Another common syntactic parsing framework is phrase
constituency parsing (Jurafsky and Martin 2020).

\hypertarget{tbl-generation-europarle-en-example}{}
\begin{table}
\caption{\label{tbl-generation-europarle-en-example}Automatic linguistic annotation for grammatical category and syntactic
structure for an example English sentence from the Europarle Corpus. }\tabularnewline

\centering
\begin{tabular}{lrllllll}
\toprule
type & sentence\_id & token\_id & token & upos & feats & token\_id\_source & syntactic\_relation\\
\midrule
Target & 6 & 1 & Invito & ADP & NA & 3 & case\\
Target & 6 & 2 & a & DET & Definite=Ind|PronType=Art & 3 & det\\
Target & 6 & 3 & todos & NOUN & Number=Plur & 6 & nmod\\
Target & 6 & 4 & a & DET & Definite=Ind|PronType=Art & 6 & det\\
Target & 6 & 5 & que & ADJ & Degree=Pos & 6 & amod\\
\addlinespace
Target & 6 & 6 & nos & NOUN & Number=Plur & 0 & root\\
Target & 6 & 7 & pongamos & X & NA & 13 & goeswith\\
Target & 6 & 8 & de & X & Foreign=Yes & 13 & goeswith\\
Target & 6 & 9 & pie & X & NA & 13 & goeswith\\
Target & 6 & 10 & para & X & NA & 13 & goeswith\\
\addlinespace
Target & 6 & 11 & guardar & X & NA & 13 & goeswith\\
Target & 6 & 12 & un & X & NA & 13 & goeswith\\
Target & 6 & 13 & minuto & NOUN & Number=Sing & 6 & appos\\
Target & 6 & 14 & de & PROPN & Number=Sing & 15 & compound\\
Target & 6 & 15 & silencio & PROPN & Number=Sing & 13 & flat\\
\addlinespace
Target & 6 & 16 & . & PUNCT & NA & 6 & punct\\
\bottomrule
\end{tabular}
\end{table}

Now compare the English example sentence dataset in
Table~\ref{tbl-generation-europarle-en-example} with the parallel
sentence in Spanish. Note that the grammatical features are language
specific. For example, Spanish has gender which is apparent when
scanning the \texttt{feats} variable.

\hypertarget{tbl-generation-europarle-es-example}{}
\begin{table}
\caption{\label{tbl-generation-europarle-es-example}Automatic linguistic annotation for grammatical category and syntactic
structure for an example Spanish sentence from the Europarle Corpus. }\tabularnewline

\centering
\begin{tabular}{lrllllll}
\toprule
type & sentence\_id & token\_id & token & upos & feats & token\_id\_source & syntactic\_relation\\
\midrule
Source & 6 & 1 & Please & PROPN & Gender=Fem|Number=Sing & 4 & nsubj\\
Source & 6 & 2 & rise & PROPN & Number=Sing & 1 & flat\\
Source & 6 & 3 & , & PUNCT & NA & 1 & punct\\
Source & 6 & 4 & then & VERB & Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin & 0 & root\\
Source & 6 & 5 & , & PUNCT & NA & 9 & punct\\
\addlinespace
Source & 6 & 6 & for & ADP & NA & 9 & compound\\
Source & 6 & 7 & this & X & NA & 9 & compound\\
Source & 6 & 8 & minute's & X & Gender=Masc|Number=Sing & 9 & compound\\
Source & 6 & 9 & silence & X & Gender=Masc|Number=Sing & 4 & conj\\
Source & 6 & 10 & . & PUNCT & NA & 4 & punct\\
\bottomrule
\end{tabular}
\end{table}

There is much more to explore with linguistic annotation, and syntactic
parsing in particular, but at this point it will suffice to note that it
is possible to augment a dataset with grammatical information
automatically.

There are strengths and shortcomings with automatic linguistic
annotation that a research should be aware of. First, automatic
linguistic annotation provides quick access to rich and highly reliable
linguistic information for a large number of languages. However,
part-of-speech taggers and syntactic parsers are not magic. They are
resources that are built by training a computational algorithm to
recognize patterns in manually annotated datasets producing a language
model. This model is then used to predict the linguistic annotations for
new language (as we just did in the previous examples). The shortcomings
of automatic linguistic annotation is first, not all languages have
trained language models and second, the data used to train the model
inevitably reflect a particular variety, register, modality, \emph{etc}.
The accuracy of the linguistic annotation is highly dependent on
alignment between the language sampling frame of the trained data and
the language data to be automatically annotated. Many (most) of the
language models available for automatic linguistic annotation are based
on language that is most readily available and for most languages this
has traditionally been newswire text. It is important to be aware of
these characteristics when using linguistic annotation tools.

\hypertarget{merging}{%
\subsubsection{Merging}\label{merging}}

The process of merging aims to \emph{join} a variable or set of
variables with another variable or set of variables from another
dataset. The option to merge two (or more) datasets requires that there
is a shared variable that indexes and aligns the datasets.

To provide an example let's look at the Switchboard Diaglog Act Corpus.
Our existing, disfluency recoded, version includes the following
variables.

\begin{verbatim}
#> Rows: 5
#> Columns: 11
#> $ doc_id           <chr> "4325", "4325", "4325", "4325", "4325"
#> $ speaker_id       <dbl> 1632, 1632, 1519, 1632, 1519
#> $ topic_num        <dbl> 323, 323, 323, 323, 323
#> $ topicality       <chr> "3", "3", "3", "3", "3"
#> $ naturalness      <chr> "2", "2", "2", "2", "2"
#> $ damsl_tag        <chr> "o", "qw", "qy^d", "+", "+"
#> $ speaker          <chr> "A", "A", "B", "A", "B"
#> $ turn_num         <chr> "1", "1", "2", "3", "4"
#> $ utterance_num    <chr> "1", "2", "1", "1", "1"
#> $ utterance_text   <chr> "Okay.  /", "{D So, }", "[ [ I guess, +", "What kind ~
#> $ disfluency_count <int> 0, 0, 0, 0, 1
\end{verbatim}

It turns out that on the
\href{https://catalog.ldc.upenn.edu/docs/LDC97S62/}{corpus website} a
number of meta-data files are available, including files pertaining to
speakers and the topics of the conversations.

The speaker meta-data for this corpus is in a the
\texttt{caller\_tab.csv} file and contains a \texttt{speaker\_id}
variable which corresponds to each speaker in the corpus and other
potentially relevant variables for a language research project including
\texttt{sex}, \texttt{birth\_year}, \texttt{dialect\_area}, and
\texttt{education}.

\hypertarget{tbl-merging-swda-speaker}{}
\begin{table}
\caption{\label{tbl-merging-swda-speaker}Speaker meta-data for the Switchboard Dialog Act Corpus. }\tabularnewline

\centering
\begin{tabular}{rlrlr}
\toprule
speaker\_id & sex & birth\_year & dialect\_area & education\\
\midrule
1632 & FEMALE & 1962 & WESTERN & 2\\
1632 & FEMALE & 1962 & WESTERN & 2\\
1519 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
1632 & FEMALE & 1962 & WESTERN & 2\\
1519 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
\bottomrule
\end{tabular}
\end{table}

Since both datasets contain a shared index, \texttt{speaker\_id} we can
merge these two datasets. The result is found in
Table~\ref{tbl-merging-swda-speaker-added}.

\hypertarget{tbl-merging-swda-speaker-added}{}
\begin{table}
\caption{\label{tbl-merging-swda-speaker-added}Merged conversations and speaker meta-data for the Switchboard Dialog
Act Corpus. }\tabularnewline

\centering
\begin{tabular}{lrlrlrrlllllllr}
\toprule
doc\_id & speaker\_id & sex & birth\_year & dialect\_area & education & topic\_num & topicality & naturalness & damsl\_tag & speaker & turn\_num & utterance\_num & utterance\_text & disfluency\_count\\
\midrule
4325 & 1632 & FEMALE & 1962 & WESTERN & 2 & 323 & 3 & 2 & o & A & 1 & 1 & Okay.  / & 0\\
4325 & 1632 & FEMALE & 1962 & WESTERN & 2 & 323 & 3 & 2 & qw & A & 1 & 2 & \{D So, \} & 0\\
4325 & 1519 & FEMALE & 1971 & SOUTH MIDLAND & 1 & 323 & 3 & 2 & qy\textasciicircum{}d & B & 2 & 1 & {}[ [ I guess, + & 0\\
4325 & 1632 & FEMALE & 1962 & WESTERN & 2 & 323 & 3 & 2 & + & A & 3 & 1 & What kind of experience [ do you, + do you ] have, then with child care? / & 0\\
4325 & 1519 & FEMALE & 1971 & SOUTH MIDLAND & 1 & 323 & 3 & 2 & + & B & 4 & 1 & I think, ] + \{F uh, \} I wonder ] if that worked. / & 1\\
\bottomrule
\end{tabular}
\end{table}

In this example case the dataset that was merged was already in a
structured format (.csv). Many corpus resources contain meta-data in
stand-off files that are structured.

In some cases a researcher would like to merge information that does not
already accompany the corpus resource. This is possible as long as a
dataset can be created that contains a variable that is shared. Without
a shared variable to index the datasets the merge cannot take place.

In sum, the transformation steps described here collectively aim to
produce higher quality datasets that are relevant in content and
structure to submit to analysis. The process may include one or more of
the previous transformations but is rarely linear and is most often
iterative. It is typical to do some normalization then generation, then
recoding, and then return to normalizing, and so forth. This process is
highly idiosyncratic given the characteristics of the derived dataset
and the ultimate goals for the analysis dataset.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

Note in some cases we may convert our tidy tabular dataset to other data
formats that may be required for some particular statistic approaches
but at all times the relationship between the variables should be
maintained in line with our research purpose. We will touch on examples
of other types of data formats (\emph{e.g.} Corpus and Document-Term
Matrix (DTM) objects in R) when we dive into particular statistical
approaches that require them later in the textbook.

\end{tcolorbox}

\hypertarget{documentation}{%
\section{Documentation}\label{documentation}}

As we have seen in this chapter that acquiring data and converting that
data into information involves a number of conscious decisions and
implementation steps. As a favor to ourselves as researchers and to the
research community, it is crucial to document these decisions and steps.
This makes it both possible to retrace our own steps and also provides a
guide for future researchers that want to reproduce and/ or build on
your research. A programmatic approach to quantitative research helps
ensure that the implementation steps are documented and reproducible but
it is also vital that the decisions that are made are documented as
well. This includes the creation/ selection of the corpus data, the
description of the variables chosen from the corpus for the derived
dataset, and the description of the variables created from the derived
dataset for the analysis dataset.

For an existing corpus sample acquired from a repository (\emph{e.g.}
\href{https://catalog.ldc.upenn.edu/LDC97S62}{Switchboard Dialog Act
Corpus}, Language Data Consortium), a research group (\emph{e.g.}
\href{http://cedel2.learnercorpora.com/}{CEDEL2}), or an individual
researcher (\emph{e.g.}
\href{https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/}{SMS Spam
Collection}), there is often documentation provided describing key
attributes of the resource. This documentation should be included with
the acquisition of the corpus and added to the research project. For a
corpus that a researcher compiles themselves, they will need to generate
this documentation.

The curation and transformation steps conducted on the original corpus
data to produce the datasets should also be documented. The steps
themselves can be included in the programming scripts as code comments
(or in prose if using a literate programming strategy (\emph{e.g.} R
Markdown)). The structure of each resulting dataset should include what
is called a \textbf{data dictionary}. This is a table which includes the
variable names, the values they contain, and a short prose description
of each variable (\emph{e.g.} \href{https://osf.io/9jafz/}{ACTIV-ES
Corpus}).

\hypertarget{summary-2}{%
\section*{Summary}\label{summary-2}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we have focused on data and information --the first two
components of DIKI Hierarchy. This process is visualized in
Figure~\ref{fig-understanding-data-vis-sum}.

\begin{figure}[h]

{\centering \includegraphics[width=6.91in,height=\textheight]{figures/understanding-data/understanding-data_visual-summary-paper.png}

}

\caption{\label{fig-understanding-data-vis-sum}Understanding data:
visual summary}

\end{figure}

First a distinction is made between populations and samples, the latter
being a intentional and subjective selection of observations from the
world which attempt to represent the population of interest. The result
of this process is known as a corpus. Whether developing a corpus or
selecting an existing a corpus it is important to vet the sampling frame
for its applicability and viability as a resource for a given research
project.

Once a viable corpus is identified, then that corpus is converted into a
derived dataset which adopts the tidy dataset format where each column
is a variable, each row is an observation, and the intersection of
columns and rows contain values. This derived dataset serves to
establish the base informational relationships from which your research
will stem.

The derived dataset will most likely require transformations including
normalization, recoding, generation, and/ or merging to enhance the
usefulness of the information to analysis. An analysis dataset is the
result of this process.

Finally, documentation should be implemented at each stage of the
analysis project process. Employing a programmatic approach establishes
documentation of the implementation steps but the motivation behind the
decisions taken and the content of the corpus data and datasets
generated also need documentation to ensure transparent and reproducible
research.

\hypertarget{activities-1}{%
\section*{Activities}\label{activities-1}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_3.html}{Reading,
inspecting, and writing data}\\
\textbf{How}: Read Recipe 3 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To use literate programming in R markdown to work with R
coding strategies for reading, inspecting, and writing datasets.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/lab_3}{Reading,
inspecting, and writing data}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 3.\\
\textbf{Why}: To read datasets from packages and from plain-text files,
inspect and report characteristics of datasets, and write datasets to
plain-text files.

\end{tcolorbox}

\hypertarget{questions-2}{%
\section*{Questions}\label{questions-2}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Conceptual questions}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the difference between a population and a sample in linguistic
  research?
\item
  Why is it important to vet a corpus before using it in a research
  project?
\item
  What is a derived dataset in the context of linguistic research?
\item
  How does the tidy table format help with linguistic analysis?
\item
  What kinds of transformations may be performed on a derived dataset to
  enhance its usefulness for analysis?
\item
  What is an analysis dataset and why is it important in linguistic
  research?
\item
  Why is documentation important in the process of conducting linguistic
  analysis?
\item
  How does a programmatic approach enhance documentation in linguistic
  research?
\item
  Why is it important to document the motivation behind decisions taken
  in linguistic analysis projects?
\item
  How does documenting the corpus data and generated datasets contribute
  to transparent and reproducible research in linguistics?
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

To add:

\begin{itemize}
\tightlist
\item
  What is the difference between a variable and a value?
\item
  Why is it important to identify the levels of measurement of variables
  in a dataset?
\item
  What is the difference between the unit of analysis and the unit of
  observation?
\item
  How does the unit of analysis and the unit of observation relate to
  the levels of measurement of variables in a dataset?
\item
  In what ways can a dataset be considered a model of the world?
\item
  To what extent does the sample represent the population? What
  implications does this have for the generalizability of the results of
  a research project?
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Technical exercises}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Creating a Sample Corpus:
\item
  Writing a Corpus Documentation
\item
  Converting a Corpus to a Derived Dataset:
\item
  Writing a Data Dictionary
\item
  Transforming a Derived Dataset:
\item
  Merging Datasets:
\item
  Writing a dataset to disk
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Consider (an example dataset) and its data dictionary, write a script
  to read the dataset, inspect it, and write it to disk.
\item
  Consider a dataset and its data dictionary what appears to be the unit
  of analysis and the unit of observation?
\end{itemize}

\end{tcolorbox}

\hypertarget{sec-approaching-analysis}{%
\chapter{Approaching analysis}\label{sec-approaching-analysis}}

\begin{quote}
Statistical thinking will one day be as necessary for efficient
citizenship as the ability to read and write.

--- H.G. Wells
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  What is the role of statistics in data analysis?
\item
  What is the importance of descriptive assessment in data analysis?
\item
  In what ways are the main approaches to data analysis similar and
  different?
\end{itemize}

\end{tcolorbox}

In this chapter I will build on the notions of data and information from
the previous chapter. The aim of statistics in quantitative analysis is
to uncover patterns in datasets. Thus statistics is aimed at deriving
knowledge from information, the next step in the DIKI Hierarchy
Figure~\ref{fig-diki-hierarchy}. Where the creation of information from
data involves human intervention and conscious decisions, as we have
seen, deriving knowledge from information involves even more conscious
subjective decisions on what information to assess, and what method to
select to interrogate the information, and ultimately how to interpret
the findings. The first step is to conduct a descriptive assessment of
the information, both at the individual variable level and also between
variables, the second is to interrogate the dataset either through
exploratory, predictive, or inferential analysis methods, and the third
is to interpret and report the findings.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Interactive programming}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/swirl}{Data
visualization}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To explore data visually in text and in graphics.

\end{tcolorbox}

\hypertarget{aa-descriptive-assessment}{%
\section{Descriptive assessment}\label{aa-descriptive-assessment}}

A descriptive assessment of the dataset includes a set of diagnostic
measures and tabular and visual summaries which provide researchers a
better understanding of the structure of a dataset, prepare the
researcher to make decisions about which statistical methods and/ or
tests are most appropriate, and to safeguard against false assumptions
(missing data, data distributions, etc.). In this section we will
\emph{EDIT: first cover the importance of understanding the
informational value that variables can represent and then move to use
this understanding to approach summarizing individual variables and
relationships between variables.}

To ground this discussion I will introduce a new dataset. This dataset
is drawn from the
\href{https://slabank.talkbank.org/access/English/BELC.html}{Barcelona
English Language Corpus (BELC)}, which is found in the
\href{http://talkbank.org/}{TalkBank repository}. I've selected the
``Written composition'' task from this corpus which contains writing
samples from second language learners of English at different ages.
Participants were given the task of writing for 15 minutes on the topic
of ``Me: my past, present and future''. Data was collected for many (but
not all) participants up to four times over the course of seven years.
In Table~\ref{tbl-belc-overview} I've included the first 10 observations
from the dataset which reflects structural and transformational steps
I've done so we start with a tidy dataset.

\hypertarget{tbl-belc-overview}{}
\begin{table}
\caption{\label{tbl-belc-overview}First 10 observations of the BELC dataset for demonstration. }\tabularnewline

\centering
\begin{tabular}{l|l|l|r|r|r}
\hline
participant\_id & age\_group & sex & num\_tokens & num\_types & ttr\\
\hline
L02 & 10-year-olds & female & 48 & 12 & 0.250\\
\hline
L05 & 10-year-olds & female & 72 & 15 & 0.208\\
\hline
L10 & 10-year-olds & female & 144 & 26 & 0.181\\
\hline
L11 & 10-year-olds & female & 40 & 8 & 0.200\\
\hline
L12 & 10-year-olds & female & 164 & 23 & 0.140\\
\hline
L16 & 10-year-olds & female & 52 & 12 & 0.231\\
\hline
L22 & 10-year-olds & female & 188 & 30 & 0.160\\
\hline
L27 & 10-year-olds & female & 32 & 8 & 0.250\\
\hline
L28 & 10-year-olds & female & 336 & 34 & 0.101\\
\hline
L29 & 10-year-olds & female & 212 & 34 & 0.160\\
\hline
\end{tabular}
\end{table}

The entire dataset includes 79 observations from 36 participants. Each
observation in the BELC dataset corresponds to an individual learner's
composition. It includes which participant wrote the composition
(\texttt{participant\_id}), the age group they were part of at the time
(\texttt{age\_group}), their sex (\texttt{sex}), and the number of
English words they produced (\texttt{num\_tokens}), the number of unique
English words they produced (\texttt{num\_types}). The final variable
(\texttt{ttr}) is the calculated ratio of number of unique words
(\texttt{num\_types}) to total words (\texttt{num\_tokens}) for each
composition. This is known as the Type-Token Ratio and it is a standard
metric for measuring lexical diversity.

\hypertarget{information-values}{%
\subsection{Information values}\label{information-values}}

Understanding the informational value, or \textbf{level of measurement},
of a variable or set of variables in key to preparing for analysis as it
has implications for what visualization techniques and statistical
measures we can use to interrogate the dataset. There are two main
levels of measurement a variable can take: categorical and continuous.
\textbf{Categorical variables} reflect class or group values.
\textbf{Continuous variables} reflect values that are measured along a
continuum.

The BELC dataset contains three categorical variables
(\texttt{participant\_id}, \texttt{age\_group}, and \texttt{sex}) and
three continuous variables (\texttt{num\_tokens}, \texttt{num\_types},
and \texttt{ttr}). The categorical variables identify class or group
membership; which participant wrote the composition, what age group they
were in, and their biological sex. The continuous variables measure
attributes that can take a range of values without a fixed limit and the
differences between each value are regular. The number of words and
number of unique words for each composition can range from 1 to \(n\)
and the Type-Token Ratio being derived from these two variables is also
continuous for the same reason. Furthermore, the differences between the
each of values of these measures is on a defined interval, so for
example a composition which has a word count (\texttt{num\_tokens}) of
40 is exactly two times as large as a composition with a word count of
20.

The distinction between categorical an continuous levels of measurement,
as mentioned above, are the main two and for some statistical approaches
the only distinction that needs to be made to conduct an analysis.
However, categorical and continuous can each be broken down into
subcategories and for some descriptive and analytic purposes these
distinctions are important. For categorical variables a distinction can
be made between variables in which there is a structured relationship
between the values and those in which there is not. \emph{Nominal
variables} contain values which are labels denoting the membership in a
class in which there is no relationship between the labels.
\emph{Ordinal variables} also contain labels of classes, but in contrast
to nominal variables, there is a relationship between the classes,
namely one in which there is a precedence relationship or order. With
this in mind, our categorical variables be sub-classified. There is no
order between the values of \texttt{participant\_id} and \texttt{sex}
and they are therefore nominal whereas the values of \texttt{age\_group}
are ordered, each value refers to a sequential age group, and therefore
it is ordinal.

Turning to continuous variables, another subdivision can be made which
hinges on the existence of a non-arbitrary zero or not. \emph{Interval
variables} contain values in which the difference between the values is
regular and defined, but the measure has an arbitrary zero value. A
typical example of an interval variable is temperature measurements on
the Fahrenheit scale. A value of 0 on this scale does not mean there is
0 temperature. \emph{Ratio variables} have all the properties of
interval variables but also include a non-arbitrary definition of zero.
All of the continuous variables in the BELC dataset
(\texttt{num\_tokens}, \texttt{num\_types}, and \texttt{ttr}) are ratio
variables as a value of 0 would indicate the lack of this attribute.

An hierarchical overview of the relationship between the two main and
four sub-types of levels of measurement appear in
Figure~\ref{fig-info-values-paper}.

\begin{figure}[h]

{\centering \includegraphics[width=5.04in,height=\textheight]{figures/approaching-analysis/Informational-values-paper.png}

}

\caption{\label{fig-info-values-paper}Levels of measurement graphic
representation.}

\end{figure}

\begin{figure}

{\centering 

\begin{figure}[H]

{\centering \includegraphics[width=4.68in,height=2.8in]{approaching-analysis_files/figure-latex/mermaid-figure-1.png}

}

\end{figure}

}

\caption{\label{fig-intro-values}Levels of measurement graphic
representation.}

\end{figure}

A few notes of practical importance; First, the distinction between
interval and ratio variables is often not applicable in text analysis
and therefore often treated together as continuous variables. Second,
the distinction between ordinal and interval/continuous variables is not
as clear cut as it may seem. Both variables contain values which have an
ordered relationship. By definition the values of an ordinal variable do
not reflect regular intervals between the units of measurement. But in
practice interval/ continuous variables with a defined number of values
(say from a Likert scale used on a survey) may be treated as an ordinal
variable as they may be better understood as reflecting class
membership. Third, all continuous variables can be converted to
categorical variables, but the reverse is not true. We could, for
example, define a criterion for binning the word counts in
\texttt{num\_tokens} for each composition into ordered classes such as
``low'', ``mid'', ``high''. On the other hand, \texttt{sex} (as it has
been measured here) cannot take intermediate values on a unfixed range.
The upshot is that variables can be down-typed but not up-typed. In most
cases it is preferred to treat continuous variables as such, if the
nature of the variable permits it, as the down-typing of continuous data
to categorical data results in a loss of information --which will result
in a loss of information and hence statistical power which may lead to
results that obscure meaningful patterns in the data (R. Harald Baayen
2004).

\hypertarget{summaries}{%
\subsection{Summaries}\label{summaries}}

It is always key to gain insight into shape of the information through
numeric, tabular and/ or visual summaries before jumping in to analytic
statistical approaches. The most appropriate form of summarizing
information will depend on the number and informational value(s) of our
target variables. To get a sense of how this looks, let's continue to
work with the BELC dataset and pose different questions to the data with
an eye towards seeing how various combinations of variables are
descriptively explored.

\hypertarget{single-variables}{%
\subsubsection{Single variables}\label{single-variables}}

The way to statistically summarize a variable into a single measure is
to derive a \textbf{measure of central tendency}. For a continuous
variable the most common measure is the (arithmetic) \emph{mean}, or
average, which is simply the sum of all the values divided by the number
of values. As a measure of central tendency, however, the mean can be
less-than-reliable as it is sensitive to outliers which is to say that
data points in the variable that are extreme relative to the overall
distribution of the other values in the variable affect the value of the
mean depending on how extreme the deviate. One way to assess the effects
of outliers is to calculate a \textbf{measure of dispersion}. The most
common of these is the \emph{standard deviation} which estimates the
average amount of variability between the values in a continuous
variable. Another way to assess, or rather side-step, outliers is to
calculate another measure of central tendency, the \emph{median}. A
median is calculated by sorting all the values in the variable and then
selecting the value which falls in the middle of all the other values. A
median is less sensitive to outliers as extreme values (if there are
few) only indirectly affect the selection of the middle value. Another
measure of dispersion is to calculate quantiles. A \emph{quantile}
slices the data in four percentile ranges providing a five value numeric
summary of the spread of the values in a continuous variable. The spread
between the first and third quantile is known as the Interquartile Range
(IQR) and is also used as a single statistic to summarize variability
between values in a continuous variable.

Below is a list of central tendency and dispersion scores for the
continuous variables in the BELC dataset
Table~\ref{tbl-summaries-continuous-measures-belc}.

\hypertarget{tbl-summaries-continuous-measures-belc}{}
\begin{table}
\caption{\label{tbl-summaries-continuous-measures-belc}Central tendency and dispersion measures for the continuous variables in
the BELC dataset. }\tabularnewline

\centering
\begin{tabular}{lrrrrrrrr}
\toprule
skim\_variable & mean & sd & p0 & p25 & p50 & p75 & p100 & iqr\\
\midrule
num\_tokens & 264.911 & 175.611 & 4.000 & 116.000 & 220.00 & 360.000 & 740.00 & 244.000\\
num\_types & 40.253 & 22.801 & 1.000 & 22.000 & 38.00 & 54.000 & 97.00 & 32.000\\
ttr & 0.167 & 0.032 & 0.101 & 0.144 & 0.16 & 0.182 & 0.25 & 0.039\\
\bottomrule
\end{tabular}
\end{table}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

The descriptive statistics returned above were generated by the
\texttt{skimr} package.

\end{tcolorbox}

In the above summary, we see the mean, standard deviation (sd), and the
quantiles (the five-number summary, p0, p25, p50, p75, and p100). The
middle quantile (p50) is the median and the IQR is listed last.

These are important measures for assessing the central tendency and
dispersion and will be useful for reporting purposes, but to get a
better feel of how a variable is distributed, nothing beats a visual
summary. A boxplot graphically summarizes many of these metrics. In
Figure~\ref{fig-summaries-boxplots-belc} we see the same three
continuous variables, but now in graphical form.

\begin{figure}[h]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-summaries-boxplots-belc-1.pdf}

}

\caption{\label{fig-summaries-boxplots-belc}Boxplots for each of the
continuous variables in the BELC dataset.}

\end{figure}

In a boxplot, the bold line is the median. The surrounding box around
the median is the interquantile range. The extending lines above and
below the IQR mark the largest and lowest value that is within 1.5 times
either the 3rd (top of the box) or 1st (bottom of the box). Any values
that fall outside, above or below, the extending lines are considered
statistical outliers and are marked as dots. \footnote{Note that each of
  these three variables are to be considered separately here
  (vertically). Later we will see the use of boxplots to compare a
  continuous variable across levels of a categorical variable
  (horizontally).}

Boxplots provide a robust and visually intuitive way of assessing
central tendency and variability in a continuous variable but this type
of plot can be complemented by looking at the overall distribution of
the values in terms of their frequencies. A histogram provides a
visualization of the frequency (and density in this case with the blue
overlay) of the values across a continuous variable binned at regular
intervals.

In Figure~\ref{fig-summaries-histograms-belc} I've plotted histograms in
the top row and density plots in the bottom row for the same three
continuous variables from the BELC dataset.

\begin{figure}[h]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-summaries-histograms-belc-1.pdf}

}

\caption{\label{fig-summaries-histograms-belc}Histograms and density
plots for the continuous variables in the BELC dataset.}

\end{figure}

\begin{figure}[h]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-histogram-boxplot-sim-1.pdf}

}

\caption{\label{fig-histogram-boxplot-sim}Histogram and boxplot for a
simulated normal distribution.}

\end{figure}

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, rightrule=.15mm, opacityback=0, arc=.35mm, leftrule=.75mm, toprule=.15mm, bottomrule=.15mm, colback=white]

Histograms and Boxplots

A histogram is a graphical representation of the frequency distribution
of a dataset. It is created by dividing the data into intervals or
``bins'' and counting the number of data points that fall into each bin.
The height of the bars in a histogram represents the frequency or count
of data points in each bin. The shape of the histogram can give us
insights into the distribution's central tendency, spread, and skewness.

A boxplot, on the other hand, is a graphical representation of the
five-number summary of a dataset: the minimum, first quartile (Q1),
median (Q2), third quartile (Q3), and maximum. It provides a visual
summary of the dataset's central tendency, dispersion, and potential
outliers. The box in a boxplot represents the interquartile range (IQR),
which contains the middle 50\% of the data. The whiskers extend from the
box to the minimum and maximum values within 1.5 * IQR, and any data
points beyond the whiskers are considered potential outliers.

Relating histograms and boxplots:

\begin{itemize}
\tightlist
\item
  Both histograms and boxplots provide information about the central
  tendency and spread of the data. The peak(s) in a histogram can give
  us a sense of where the data is centered, similar to the median in a
  boxplot. The width of the histogram can indicate the spread of the
  data, akin to the IQR in a boxplot.
\item
  The shape of a histogram can provide insights into the skewness of a
  dataset. A histogram that is symmetric with a single peak can
  correspond to a roughly symmetric boxplot. A positively skewed
  histogram (with a long tail to the right) can correspond to a boxplot
  where Q3 - Q2 \textgreater{} Q2 - Q1, and a negatively skewed
  histogram (with a long tail to the left) can correspond to a boxplot
  where Q2 - Q1 \textgreater{} Q3 - Q2.
\item
  While histograms focus on the frequency distribution of data points,
  boxplots focus on the data's quartiles and potential outliers. Both
  types of plots can be used in tandem to get a more comprehensive
  understanding of a dataset's distribution.
\end{itemize}

In summary, histograms and boxplots are related in that they both offer
graphical representations of data distributions. They provide different
perspectives on the data's central tendency, spread, and skewness, and
when used together, they can offer a more complete understanding of a
dataset.

\end{tcolorbox}

Histograms provide insight into the distribution of the data. For our
three continuous variables, the distributions happen not to be too
strikingly distinct. They are, however, not the same either. When we
explore continuous variables with histograms we are often trying to
assess whether there is skew or not. There are three general types of
skew, visualized in Figure~\ref{fig-summaries-skew-graphic}.

\begin{figure}[h]

{\centering \includegraphics[width=7.13in,height=\textheight]{figures/approaching-analysis/skew-types-paper.png}

}

\caption{\label{fig-summaries-skew-graphic}Examples of skew types in
density plots.}

\end{figure}

\begin{figure}[h]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-summaries-skew-graphic-2-1.pdf}

}

\caption{\label{fig-summaries-skew-graphic-2}Examples of skew types in
density plots.}

\end{figure}

In histograms/ density plots in which the distribution is either left or
right, the median and the mean are not aligned. The \emph{mode}, which
indicates the most frequent value in the variable is also not aligned
with the other two measures. In a left-skewed distribution the mean will
be to the left of the median which is left of the mode whereas in a
right-skewed distribution the opposite occurs. In a distribution with
absolutely no skew these three measures are the same. In practice these
measures rarely align perfectly but it is very typical for these three
measures to approximate alignment. It is common enough that this
distribution is called the Normal Distribution \footnote{formally known
  as a Gaussian Distribution} as it is very common in real-world data.

Another and potentially more informative way to inspect the normality of
a distribution is to create Quantile-Quantile plots (QQ Plot). In
Figure~\ref{fig-summaries-qqnorm-plot-belc} I've created QQ plots for
our three continuous variables. The line in each plot is the normal
distribution and the more points that fall off of this line, the less
likely that the distribution is normal.

\begin{figure}[h]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-summaries-qqnorm-plot-belc-1.pdf}

}

\caption{\label{fig-summaries-qqnorm-plot-belc}QQ Plots for the
continuous variables in the BELC dataset.}

\end{figure}

A visual inspection can often be enough to detect non-normality, but in
cases which visually approximate the normal distribution (such as these)
we can perform the Shapiro-Wilk test of normality. This is an
inferential test that compares a variable's distribution to the normal
distribution. The likelihood that the distribution differs from the
normal distribution is reflected in a \(p\)-value. A \(p\)-value below
the .05 threshold suggests the distribution is non-normal. In
Table~\ref{tbl-summaries-normality-test-belc} we see that given this
criterion only the distribution of \texttt{num\_types} is normally
distributed.

\hypertarget{tbl-summaries-normality-test-belc}{}
\begin{table}
\caption{\label{tbl-summaries-normality-test-belc}Results from Shapiro-Wilk test of normality for continuous variables in
the BELC dataset. }\tabularnewline

\centering
\begin{tabular}{lrr}
\toprule
variable & statistic & p\_value\\
\midrule
Number of tokens & 0.942 & 0.001\\
Number of types & 0.970 & 0.058\\
Type-Token Ratio & 0.948 & 0.003\\
\bottomrule
\end{tabular}
\end{table}

Downstream in the analytic analysis, the distribution of continuous
variables will need to be taken into account for certain statistical
tests. Tests that assume `normality' are parametric tests, those that do
not are non-parametric. Distributions which approximate the normal
distribution can sometimes be transformed to conform to the normal
distribution either by outlier trimming or through statistical
procedures (e.g.~square root, log, or inverse transformation), if
necessary. At this stage, however, the most important thing is to
recognize whether the distributions approximate or wildly diverge from
the normal distribution.

Before we leave continuous variables, let's consider another approach
for visually summarizing a single continuous variable. The Empirical
Cumulative Distribution Function, or \emph{ECDF}, is a summary of the
cumulative proportion of each of the values of a continuous variable
over the domain of possible values. An ECDF plot can be useful in
determining what proportion of the values fall above or below a certain
percentage of the data.

In Figure~\ref{fig-summarize-ecdf-belc} we see ECDF plots for our three
continuous variables.

\begin{figure}[h]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-summarize-ecdf-belc-1.pdf}

}

\caption{\label{fig-summarize-ecdf-belc}ECDF plots for the continuous
variables in the BELC dataset.}

\end{figure}

Take, for example, the number of tokens (\texttt{num\_tokens}) per
composition. The ECDF plot tells us that 50\% of the values in this
variable are 56 words or less. In the three variables plotted, the
cumulative growth is quite steady. In some cases it is not. When it is
not, an ECDF goes a long way to provide us a glimpse into key bends in
the proportions of values in a variable.

Now let's turn to the descriptive assessment of categorical variables.
For categorical variables, central tendency can be calculated as well
but only a subset of measures given the reduced informational value of
categorical variables. For nominal variables where there is no
relationship between the levels the central tendency is simply the mode.
The levels of ordinal variables, however, are relational and therefore
the median, in addition to the mode, can also be used as a measure of
central tendency. Note that a variable with one mode is unimodal, two
modes, bimmodal, and in variables that have two or more modes
multimodal.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

To get numeric value of the median for an ordinal variable the levels of
the variable will need to be numeric as well. Non-numeric levels can be
recoded to numeric for this purpose if necessary.

\end{tcolorbox}

Below is a list of the central tendency metrics for the categorical
variables in the BELC dataset.

\textbf{Variable type: factor}

\begin{tabular}{l|l|r|l}
\hline
skim\_variable & ordered & n\_unique & top\_counts\\
\hline
participant\_id & FALSE & 36 & L05: 3, L10: 3, L11: 3, L12: 3\\
\hline
age\_group & TRUE & 4 & 10-: 24, 16-: 24, 12-: 16, 17-: 15\\
\hline
sex & FALSE & 2 & fem: 48, mal: 31\\
\hline
\end{tabular}

In practice when a categorical variable has few levels it is common to
simply summarize the counts of each level in a table to get an overview
of the variable. With ordinal variables with more numerous levels, the
five-score summary (quantiles) can be useful to summarize the
distribution. In contrast to continuous variables where a graphical
representation is very helpful to get perspective on the shape of the
distribution of the values, the exploration of single categorical
variables is rarely enhanced by plots.

\hypertarget{multiple-variables}{%
\subsubsection{Multiple variables}\label{multiple-variables}}

In addition to the single variable summaries (univariate), it is very
useful to understand how two (bivariate) or more variables
(multivariate) are related to add to our understanding of the shape of
the relationships in the dataset. Just as with univariate summaries, the
informational values of the variables frame our approach.

To explore the relationship between two continuous variables we can
statistically summarize a relationship with a \textbf{coefficient of
correlation} which is a measure of \textbf{effect size} between
continuous variables. If the continuous variables approximate the normal
distribution \emph{Pearson's r} is used, if not \emph{Kendall's tau} is
the appropriate measure. A correlation coefficient ranges from -1 to 1
where 0 is no correlation and -1 or 1 is perfect correlation (either
negative or positive). Let's assess the correlation coefficient for the
variables \texttt{num\_tokens} and \texttt{ttr}. Since these variables
are not normally distributed, we use Kendall's tau. Using this measure
the correlation coefficient is \(-0.565\) suggesting there is a
correlation, but not a particularly strong one.

Correlation measures are important for reporting but to really
appreciate a relationship it is best to graphically represent the
variables in a \emph{scatterplot}. In
Figure~\ref{fig-summaries-bivariate-scatterplot-belc} we see the
relationship between \texttt{num\_tokens} and \texttt{ttr}.

\begin{figure}[h]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-summaries-bivariate-scatterplot-belc-1.pdf}

}

\caption{\label{fig-summaries-bivariate-scatterplot-belc}Scatterplot\ldots{}}

\end{figure}

In both plots \texttt{ttr} is on the y-axis and \texttt{num\_tokens} on
the x-axis. The points correspond to the intersection between these
variables for each single observation. In the left pane only the points
are represented. Visually (and given the correlation coefficient) we can
see that there is a negative relationship between the number of tokens
and the Type-Token ratio: in other words, the more tokens a composition
has the lower the Type-Token Ratio. In this case this trend is quite
apparent, but in other cases is may not be. To provide an additional
visual cue a trend line is often added to a scatterplot. In the right
pane I've added a linear trend line. This line demarcates the optimal
central tendency across the relationship, assuming a linear
relationship. The steeper the line, or slope, the more likely the
correlation is strong. The band, or ribbon, around this trend line
indicates the \textbf{confidence interval} which means that real central
tendency could fall anywhere within this space. The wider the ribbon,
the larger the variation between the observations. In this case we see
that the ribbon widens when the number of tokens is either low or high.
This means that the trend line could be potentially be drawn either
steeper (more strongly correlated) or flatter (less strongly
correlated).

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

In plots comparing two or more variables, the choice of which variable
to plot on the x- and y-axis is contingent on the research question and/
or the statistical approach. The language varies between statistical
approaches: in inferential methods the x-axis is used to plot what is
known as the dependent variable and the y-axis an independent variable.
In predictive methods the dependent variable is known as the outcome and
the independent variable a predictor. Exploratory methods do not draw
distinctions between variables along these lines so the choice between
which variable to plot along the x- and y-axis is often arbitrary.

\end{tcolorbox}

Let's add another variable to the mix, in this case the categorical
variable \texttt{sex}, taking our bivariate exploration to a
multivariate exploration. Again each point corresponds to an observation
where the values for \texttt{num\_tokens} and \texttt{ttr} intersect.
But now each of these points is given a color that reflects which level
of \texttt{sex} it is associated with.

\begin{figure}[h]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-summaries-multivariate-scatterplot-belc-1.pdf}

}

\caption{\label{fig-summaries-multivariate-scatterplot-belc}Scatterplot
visualizing the relationship between \texttt{num\_tokens} and
\texttt{ttr}.}

\end{figure}

In this multivariate case, the scatterplot without the trend line is
more difficult to interpret. The trend lines for the levels of
\texttt{sex} help visually understand the variation of the relationship
of \texttt{num\_tokens}and \texttt{ttr} much better. But it is important
to note that when there are multiple trend lines there is more than one
slope to evaluate. The correlation coefficient can be calculated for
each level of \texttt{sex} (i.e.~`male' and `female') independently but
the relationship between the each slope can be visually inspected and
provide important information regarding each level's relative
distribution. If the trend lines are parallel (ignoring the ribbons for
the moment), as it appears in this case, this suggests that the
relationship between the continuous variables is stable across the
levels of the categorical variable, with males showing more lexical
diversity than females declining at a similar rate. If the lines were to
cross, or suggest that they would cross at some point, then there would
be a potentially important difference between the levels of the
categorical variable (known as an interaction). Now let's consider the
meaning of the ribbons. Since the ribbons reflect the range in which the
real trend line could fall, and these ribbons overlap, the differences
between the levels of our categorical variable are likely not distinct.
So at a descriptive level, this visual summary would suggest that there
are no differences between the relationship between \texttt{num\_tokens}
and \texttt{ttr} for the distinct levels of \texttt{sex}.

Characterizing the relationship between two continuous variables, as we
have seen is either performed through a correlation coefficient metric
or visually. The approach for summarizing a bivariate relationship which
combines a continuous and categorical variable is distinct. Since a
categorical variable is by definition a class-oriented variable, a
descriptive evaluation can include a tabular representation, with some
type of summary statistic. For example, if we consider the relationship
between \texttt{num\_tokens} and \texttt{age\_group} we can calculate
the mean for \texttt{num\_tokens} for each level of \texttt{age\_group}.
To provide a metric of dispersion we can include either the standard
error of the mean (SEM) and/ or the confidence interval (CI).

\hypertarget{tbl-summarize-bivariate-cont-cat-belc}{}
\begin{table}
\caption{\label{tbl-summarize-bivariate-cont-cat-belc}Summary table for \texttt{tokens} by \texttt{age\_group}. }\tabularnewline

\centering
\begin{tabular}{lrrr}
\toprule
age\_group & mean\_num\_tokens & sem & ci\\
\midrule
10-year-olds & 111 & 14.8 & 24.3\\
12-year-olds & 230 & 28.5 & 46.9\\
16-year-olds & 327 & 24.6 & 40.4\\
17-year-olds & 450 & 51.9 & 85.4\\
\bottomrule
\end{tabular}
\end{table}

The SEM is a metric which summarizes variation based on the number of
values and the CI, as we have seen, summarizes the potential range of in
which the mean may fall given a likelihood criterion (usually the same
as the \(p\)-value, .05).

Because we are assessing a categorical variable in combination with a
continuous variable a table is an available visual summary. But as I
have said before, a graphic summary is hard to beat. In the following
figure (Figure~\ref{fig-summaries-bivariate-barplot-belc}) a barplot is
provided which includes the means of \texttt{num\_tokens} for each level
of \texttt{age\_group}. The overlaid bars represent the confidence
interval for each mean score.

\begin{figure}[h]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-summaries-bivariate-barplot-belc-1.pdf}

}

\caption{\label{fig-summaries-bivariate-barplot-belc}Barplot comparing
the mean \texttt{num\_tokens} by \texttt{age\_group} from the BELC
dataset.}

\end{figure}

When CI ranges overlap, just as with ribbons in scatterplots, the
likelihood that the differences between levels are `real' is diminished.

To gauge the effect size of this relationship we can use
\emph{Spearman's rho} for rank-based coefficients. The score is 0.708
indicating that the relationship between \texttt{age\_group} and
\texttt{num\_tokens} is quite strong. \footnote{To calculate effect
  sizes for the difference between two means, \emph{Cohen's d} is used.}

Now, if we want to explore a multivariate relationship and add
\texttt{sex} to the current descriptive summary, we can create a summary
table, but let's jump straight to a barplot.

\begin{figure}[h]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-summaries-multivariate-barplot-belc-1.pdf}

}

\caption{\label{fig-summaries-multivariate-barplot-belc}Barplot
comparing the mean \texttt{num\_tokens} by \texttt{age\_group} and
\texttt{sex} from the BELC dataset.}

\end{figure}

We see in Figure~\ref{fig-summaries-multivariate-barplot-belc} that on
the whole, the appears to be general trend towards more tokens in a
composition for more advanced learner levels. However, the non-overlap
in CI bars for the `12-year-olds' for the levels of \texttt{sex} (`male'
and `female') suggest that 12-year-old females may produce more tokens
per composition than males --a potential divergence from the overall
trend.

Barplots are a familiar and common visualization for summaries of
continuous variables across levels of categorical variables, but a
boxplot is another useful visualization of this type of relationship.

\begin{figure}[h]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-summaries-bivariate-boxplots-belc-1.pdf}

}

\caption{\label{fig-summaries-bivariate-boxplots-belc}Boxplot of the
relationship between \texttt{age\_group} and \texttt{num\_tokens} from
the BELC dataset.}

\end{figure}

As seen when summarizing single continuous variables, boxplots provide a
rich set of information concerning the distribution of a continuous
variable. In this case we can visually compare the continuous variable
\texttt{num\_tokens} with the categorical variable \texttt{age\_group}.
The plot in the right pane includes `notches'. Notches represent the
confidence interval, in boxplots this interval surrounds the median.
When compared horizontally across levels of a categorical variable the
overlap of notched spaces suggest that the true median may be within the
same range. Additionally, when the confidence interval goes outside the
interquantile range (the box) the notches hinge back to the either the
1st (lower) or the 3rd (higher) IQR range and suggests that the
variability is high.

We can also add a third variable to our exploration. As in the barplot
in Figure~\ref{fig-summaries-multivariate-barplot-belc}, the boxplot in
Figure~\ref{fig-summaries-multivariate-boxplots-belc} suggests that
there is an overall trend towards more tokens per composition as a
learner advances in experience, except at the `12-year-old' level where
there appears to be a difference between `males' and `females'.

\begin{figure}[h]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-summaries-multivariate-boxplots-belc-1.pdf}

}

\caption{\label{fig-summaries-multivariate-boxplots-belc}Boxplot of the
relationship between \texttt{age\_group}, \texttt{num\_tokens} and
\texttt{sex} from the BELC dataset.}

\end{figure}

Up to this point in our exploration of multiple variables we have always
included at least one continuous variable. The central tendency for
continuous variables can be summarized in multiple ways (mean, median,
and mode) and when calculating means and medians, measures of dispersion
are also provide helpful information summarize variability. When working
with categorical variables, however, measures of central tendency and
dispersion are more limited. For ordinal variables central tendency can
be summarized by the median or mode and dispersion can be assessed with
an interquantile range. For nominal variables the mode is the only
measure of central tendency and dispersion is not applicable. For this
reason relationships between categorical variables are typically
summarized using \textbf{contingency tables} which provide
cross-variable counts for each level of the target categorical
variables.

Let's explore the relationship between the categorical variables
\texttt{sex} and \texttt{age\_group}. In
Table~\ref{tbl-summaries-bivariate-categorical-table-belc} we see the
contingency table with summary counts and percentages.

\hypertarget{tbl-summaries-bivariate-categorical-table-belc}{}
\begin{table}
\caption{\label{tbl-summaries-bivariate-categorical-table-belc}Contingency table for \texttt{age\_group} and \texttt{sex}. }\tabularnewline

\centering
\begin{tabular}{llllll}
\toprule
sex/age\_group & 10-year-olds & 12-year-olds & 16-year-olds & 17-year-olds & Total\\
\midrule
female & 58\% (14) & 69\% (11) & 54\% (13) & 67\% (10) & 61\% (48)\\
male & 42\% (10) & 31\%  (5) & 46\% (11) & 33\%  (5) & 39\% (31)\\
Total & 100\% (24) & 100\% (16) & 100\% (24) & 100\% (15) & 100\% (79)\\
\bottomrule
\end{tabular}
\end{table}

As the size of the contingency table increases, visual inspection
becomes more difficult. As we have seen, a graphical summary often
proves more helpful to detect patterns.

\begin{figure}[h]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-summaries-bivariate-categorical-barplot-belc-1.pdf}

}

\caption{\label{fig-summaries-bivariate-categorical-barplot-belc}Barplot\ldots{}}

\end{figure}

In Figure~\ref{fig-summaries-bivariate-categorical-barplot-belc} the
left pane shows the counts. Counts alone can be tricky to evaluate and
adjusting the barplot to account for the proportions of males to females
in each group, as shown in the right pane, provides a clearer picture of
the relationship. From these barplots we can see there were more females
in the study overall and particularly in the 12-year-olds and
17-year-olds groups. To gauge the association strength between
\texttt{sex} and \texttt{age\_group} we can calculate \emph{Cramer's V}
which, in spirit, is like our correlation coefficients for the
relationship between continuous variables. The Cramer's V score for this
relationship is 0 which is low, suggesting that there is not a strong
association between \texttt{sex} and \texttt{age\_group} --in other
words, the relationship is stable.

Let's look at a more complex case in which we have three categorical
variables. Now the dataset, as is, does not have a third categorical
variable for us to explore but we can recast the continuous
\texttt{num\_tokens} variable as a categorical variable if we bin the
scores into groups. I've binned tokens into three score groups with
equal ranges in a new variable called \texttt{rank\_tokens}.

Adding a second categorical independent variable ups the complexity of
our analysis and as a result our visualization strategy will change. Our
numerical summary will include individual two-way cross-tabulations for
each of the levels for the third variable. In this case it is often best
to use the variable with the fewest levels as the third variable, in
this case \texttt{sex}.

\hypertarget{tbl-summaries-multivariate-categorical-table-belc-female}{}
\begin{table}
\caption{\label{tbl-summaries-multivariate-categorical-table-belc-female}Contingency table for \texttt{age\_group}, \texttt{rank\_tokens}, and
\texttt{sex} (female). }\tabularnewline

\centering
\begin{tabular}{llllll}
\toprule
rank\_tokens/age\_group & 10-year-olds & 12-year-olds & 16-year-olds & 17-year-olds & Total\\
\midrule
low & 27\% (13) & 10\%  (5) & 4\%  (2) & 6\%  (3) & 48\% (23)\\
mid & 2\%  (1) & 13\%  (6) & 21\% (10) & 6\%  (3) & 42\% (20)\\
high & 0\%  (0) & 0\%  (0) & 2\%  (1) & 8\%  (4) & 10\%  (5)\\
Total & 29\% (14) & 23\% (11) & 27\% (13) & 21\% (10) & 100\% (48)\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{tbl-summaries-multivariate-categorical-table-belc-male}{}
\begin{table}
\caption{\label{tbl-summaries-multivariate-categorical-table-belc-male}Contingency table for \texttt{age\_group}, \texttt{rank\_tokens}, and
\texttt{sex} (male). }\tabularnewline

\centering
\begin{tabular}{llllll}
\toprule
rank\_tokens/age\_group & 10-year-olds & 12-year-olds & 16-year-olds & 17-year-olds & Total\\
\midrule
low & 32\% (10) & 13\% (4) & 13\%  (4) & 3\% (1) & 61\% (19)\\
mid & 0\%  (0) & 3\% (1) & 23\%  (7) & 6\% (2) & 32\% (10)\\
high & 0\%  (0) & 0\% (0) & 0\%  (0) & 6\% (2) & 6\%  (2)\\
Total & 32\% (10) & 16\% (5) & 35\% (11) & 16\% (5) & 100\% (31)\\
\bottomrule
\end{tabular}
\end{table}

Contingency tables with this many levels are notoriously difficult to
interpret. A plot that is often used for three-way contingency table
summaries is a mosaic plot. In
Figure~\ref{fig-summaries-multivariate-mosaic-belc} I have created a
mosaic plot for the three categorical variables in the previous
contingency tables.

\begin{figure}[h]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-summaries-multivariate-mosaic-belc-1.pdf}

}

\caption{\label{fig-summaries-multivariate-mosaic-belc}Mosaic plot for
three categorical variables \texttt{age\_group}, \texttt{rank\_tokens},
and \texttt{sex} in the BELC dataset.}

\end{figure}

The mosaic plot suggests that the number of tokens per composition
increase as the learner age group increases and that females show more
tokens earlier.

In sum, a dataset is information but when the observations become
numerous or complex they are visually difficult to inspect and
understand at a pattern level. The descriptive methods described in this
section are indispensable for providing the researcher an overview of
the nature of each variable and any (potential) relationships between
variables in a dataset. Importantly, the understanding derived from this
exploration underlies all subsequent investigation and will counted on
to frame your approach to analysis regardless of the research goals and
the methods employed to derive more substantial knowledge.

\hypertarget{aa-types-of-analysis}{%
\section{Types of analysis}\label{aa-types-of-analysis}}

From identifying a target population, to selecting a data sample that
represents that population, and then to structuring the sample into a
dataset, the goals of a research project inform and frame the process.
So it will be unsurprising to know that the process of selecting an
approach to analysis is also intimately linked with a researcher's
objectives. The goal of analysis, generally, is to generate knowledge
from information. The type of knowledge generated and the process by
which it is generated, however, differ and can be broadly grouped into
three analysis types: exploratory, predictive, and inferential. In this
section I will provide an overview of how each of these analysis types
are tied to research goals and how the general goals of teach type
affect: (1) how to \emph{identify} the variables of interest, (2) how to
\emph{interrogate} these variables, and (3) how to \emph{interpret} the
results. I will structure the discussion of these analysis types moving
from the least structured (inductive) to most structured (deductive)
approach to deriving knowledge from information with the aim to provide
enough information to the would-be-researcher to identify these research
approaches in the literature and to make appropriate decisions as to
which approach their research should adopt.

\hypertarget{aa-types-of-analysis-inferential}{%
\subsection{Inferential data
analysis}\label{aa-types-of-analysis-inferential}}

The most commonly recognized of the three data analysis approaches,
inferential data analysis (IDA) is the bread-and-butter of science. IDA
is a deductive, or top-down, approach to investigation in which every
step in research stems from a premise, or hypothesis, about the nature
of a relationship in the world and then aims to test whether this
relationship is statistically supported given the evidence. The aim is
to infer conclusions about a certain relationship in the population
based on a statistical evaluation of a (corpus) sample. So, if a
researcher's aim is to draw conclusions that generalize, then, this is
the analysis approach a researcher will take.

Given the fact that this approach aims at making claims that can be
generalized to the larger population, the IDA approach has the most
rigorous set of methodological restrictions. First and foremost of these
is the fact that a testable hypothesis must be formulated \emph{before}
research begins. The hypothesis guides the collection of data, the
organization of the data into a dataset and the transformation,
selection of the variables to be used to address the hypothesis, and the
interpretation of the results. To conduct an analysis and then draw a
hypothesis which conforms to the results is known as ``Hypothesis After
Result is Known'' (HARKing) (Kerr 1998) and this practice violates the
principles of significance testing. A second key stipulation is that the
reliability of the sample data, the corpus in text analysis, to provide
evidence to test the hypothesis must be representative of the
population. A corpus used in a study which is misaligned with the
hypothesis undermines the ability of the researcher to make valid claims
about the population. In essence, IDA is only as good as the primary
data is is based on.

At this point, let me elaborate on the potentially counterintuitive
nature of hypothesis formulation and testing. The IDA, or
Null-Hypothesis Significance Testing (NHST), paradigm is in fact
approached by proposing two mutually exclusive hypotheses. The first is
the \textbf{Alternative Hypothesis} (\(H_1\)). \(H_1\) is a precise
statement grounded in the previous literature outlining a predicted
relationship (and in some cases the directionality of a relationship).
This is the effect that the research aims to investigate. The second
hypothesis is the \textbf{Null Hypothesis} (\(H_0\)). \(H_0\) is the
flip-side of the hypothesis testing coin and states that there is no
difference or relationship. Together \(H_1\) and \(H_0\) cover all
logical outcomes.

So to provide an example consider a hypothetical study which is aimed at
investigating the claim that men and women differ in terms of the number
of questions they use in spontaneous conversations. The Alternative
Hypothesis would be formulated in this way:

\(H_1\): Men and women differ in the frequency of the use of questions
in spontaneous conversations.

The Null Hypothesis, then, would be a statement describing the remaining
logical outcomes. Formally:

\(H_0\): There is no difference between how men and women use questions
in spontaneous conversations.

Note that stated in this way our hypothesis makes no prediction about
the directionality of the difference between men and women, only that
there is a difference. It is a likely scenario that a hypothesis will
stake a claim on the direction of the difference. A directional
hypothesis would look like this:

\(H_1\): Women use more questions than men in spontaneous conversations.

\(H_0\): There is no difference between how men and women use questions
in spontaneous conversations or men use more questions than women.

A further aspect which may run counter to expectations is that the aim
of hypothesis testing is not to find evidence in support of \(H_1\), but
rather the aim is to assess the likelihood that we can reliably reject
\(H_0\). The default assumption is that \(H_0\) is true until there is
sufficient evidence to reject it and accept \(H_1\), the
\emph{alternative}. The metric used to determine if there is sufficient
evidence is based on the probability that given the nature of the
relationship and the characteristics of the data, the likelihood of
there being no difference or relationship is low. The threshold for
likelihood has traditionally been summarized in the p-value statistic.
In the Social Sciences, a p-value lower that .05 is considered
\emph{statistically significant} which when interpreted correctly means
that there is more than a 95\% chance that the observed relationship
would not be predicted by \(H_0\). Note that we are working in the realm
of probability, not in absolutes, therefore an analysis that produces a
significant result does not prove \(H_1\) is correct or that \(H_0\) is
incorrect, for that matter. A margin of error is always present.

Let's now turn to the identification of variables, the statistical
interrogation of these variables, and the interpretation of the
statistical results. First, since a clearly defined and testable
hypothesis is at the center of the IDA approach, the variables are in
some sense pre-defined. The goal of the researcher is to select data and
curate that data to produce variables that are operationalized
(practically measured) to test the hypothesis. A second consideration
are the roles that the variables will play in the analysis. In standard
IDA one variable will be the \textbf{dependent variable} and one or more
variables will be \textbf{independent variables}. The dependent
variable, sometimes referred to as the outcome or response variable, is
the variable which contains the information which is predicted to depend
on the information in the independent variable(s). It is the variable
whose variation a research study seeks to explain. An independent
variable, sometimes referred to as a predictor or explanatory variable,
is a variable whose variation is predicted to explain the variation in
the dependent variable.

Returning to our hypothetical study on the use of questions between men
and women in spontaneous conversation, the frequency of questions used
by each speaker would be our dependent variable and the biological sex
of the speakers our independent variable. This is so because hypothesis
(\(H_1\)) states the proposition that a speaker's sex will predict the
frequency of questions used.

In our hypothetical study we've identified two variables, one dependent
and one independent. It is important keep in mind that there can be
multiple independent variables in cases where the dependent variable's
variation is predicted to be related to multiple variables. This
relationship would need to be explicitly part of the original
hypothesis, however.

Say we formulate a more complex relationship where the educational level
of our speakers is also related to the number of questions. We can
update our hypothesis to reflect such a scenario.

\(H_1\): Less educated women use more questions than men in spontaneous
conversations.

\(H_0\): There is no difference between how men and women use questions
in spontaneous conversations regardless of educational level, or more
educated women use more questions than less educated women, or men use
more questions than women.

The hypothesis we have described predicts what is known as an
\emph{interaction}; the relationship between our independent variables
predict different variational patterns in the dependent variable. As you
most likely can appreciate the more independent variables we include in
our hypothesis, and by extension our analysis, the more difficult it
becomes to interpret. Due to the increasing difficulty for
interpretation, in practice, IDA studies rarely include more than two or
three independent variables in the same analysis.

Independent variables add to the complexity of a study because they are
part of our research focus, specifically our hypothesis. It is, however,
common to include other variables which are not of central focus, but
are commonly assumed to contribute to the explanation of the variation
of the dependent variable. Let's assume that the background literature
suggests that the age of speakers also plays a role in the number of
questions that men and women use in spontaneous conversation. Let's also
assume that the data we have collected includes information about the
age of speakers. If we would like to factor out the potential influence
of age on the use of questions and focus on the particular independent
variables we've defined in our hypothesis, we can include the age of
speakers as a \textbf{control variable}. A control variable will be
added to the statistical analysis and documented in our report but it
will not be included in the hypothesis nor interpreted in our results.

\begin{figure}[h]

{\centering \includegraphics[width=4.21in,height=\textheight]{figures/approaching-analysis/inferential-variables.png}

}

\caption{\label{fig-aa-inferential-variables}Variable roles in
inferential analysis.}

\end{figure}

At this point let's look at the main characteristics that need to be
taken into account to statistically interrogate the variables we have
chosen to test our hypothesis. The type of statistical test that one
chooses is based on (1) the informational value of the dependent
variable and (2) the number of independent variables included in the
analysis. Together these two characteristics go a long way in
determining the appropriate class of statistical test, but other
considerations about the distribution of particular variables
(i.e.~normality), relationships between variables (i.e.~independence),
and expected directionality of the predicted effect may condition the
appropriate method to be applied.

As you can imagine, there are a host of combinations and statistical
tests that apply in particular scenarios, too many to consider in given
the scope of this coursebook (see Gries (2013) and Paquot and Gries
(2020) for a more exhaustive description). Below I've summarized some
common statistical scenarios and their associated tests which focus on
the juxtaposition of informational values and the number of variables,
leaving aside alternative tests which deal with non-normal
distributions, ordinal variables, non-independent variables, etc.

In Table~\ref{tbl-ida-statistical-monofactorial-listing} we see
\textbf{monofactorial tests}, tests with only one independent variable.

\hypertarget{tbl-ida-statistical-monofactorial-listing}{}
\begin{table}
\caption{\label{tbl-ida-statistical-monofactorial-listing}Common monofactorial tests. }\tabularnewline

\centering
\begin{tabular}{lll}
\toprule
\multicolumn{2}{c}{Variable roles} & \multicolumn{1}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){1-2}
Dependent & Independent & Test\\
\midrule
Categorical & Categorical & Pearson's Chi-squared test\\
Continuous & Categorical & Student's t-Test\\
Continuous & Continuous & Pearson's correlation test\\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tbl-ida-statistical-multifactorial-listing} includes a
listing of \textbf{multifactorial tests}, tests with more than one
independent and/ or control variables.

\hypertarget{tbl-ida-statistical-multifactorial-listing}{}
\begin{table}
\caption{\label{tbl-ida-statistical-multifactorial-listing}Common multifactorial tests. }\tabularnewline

\centering
\begin{tabular}{l>{}l>{}ll}
\toprule
\multicolumn{3}{c}{Variable roles} & \multicolumn{1}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){1-3}
Dependent & Independent & Control & Test\\
\midrule
Categorical & \em{varied} & \em{varied} & Logistic regression\\
Continuous & \em{varied} & \em{varied} & Linear regression\\
\bottomrule
\end{tabular}
\end{table}

One key point to make before we turn to how to interpret the statistical
results is concerns the use of the data in IDA. In contrast to the other
two analysis methods we will cover, the data in IDA is only used once.
That is to say, that the entire dataset is used a single time to
statistically interrogate the relationship(s) of interest. The resulting
confidence metrics (p-values, etc.) are evaluated and the findings are
interpreted. The practice of running multiple tests until a
statistically significant result is found is called ``p-hacking'' (Head
et al. 2015) and like HARKing (described earlier) violates statistical
hypothesis testing practice. For this reason it is vital to identify
your statistical approach from the outset of your research project.

Now let's consider how to approach interpreting the results from a
statistical test. As I have now made reference to multiple times, the
results of statistical procedure in hypothesis testing will result in a
confidence metric. The most standard and widely used of these confidence
metrics is the p-value. The p-value provides a probability that the
results of our statistical test could be explained by the null
hypothesis. When this probability crosses below the threshold of .05,
the result is considered statistically significant, otherwise we have a
`null result' (i.e.~non-significant). However, this sets up a binary
distinction that can be problematic. On the one hand what is one to do
if a test returns a p-value of .051 or something `marginally
significant'? According to standard practice these results would not be
statistically significant. But it is important to note that a p-value is
sensitive to the sample size. A small sample may return a
non-significant result, but a larger sample size with the same
underlying characteristics may very well return a significant result. On
the other hand, if we get a statistically significant result, do we move
on --case closed? As I just pointed out the sample size plays a role in
finding statistically significant results, but that does not mean that
the results are `important' for even small effects in large samples can
return a significant p-value.

It is important to underscore that the purpose of IDA is to draw
conclusions from a dataset which are generalizable to the population.
These conclusions require that there are rigorous measures to ensure
that the results of the analysis do not overgeneralize (suggest there is
a relationship when there is not one) and balance that with the fact
that we don't want to undergeneralize (miss the fact that there is an
relationship in the population, but our analysis was not capable of
detecting it). Overgeneralization is known as \textbf{Type I error} or
false positive and undergeneralization is a \textbf{Type II error} or
false negative.

For these reasons it is important to calculate the size and magnitude of
the result to gauge the uncertainty of our result in standardized,
sample size-independent way. This is performed by analyzing the
\textbf{effect size} and reporting a \textbf{confidence interval (CI)}
for the results. The wider the CI the more uncertainty surrounds our
statistical result, and therefore the more likely that our significant
p-value could be the result of Type I error. A non-significant p-value
and large effect size could be the result of Type II error. In addition
to vetting our p-value, the CI and effect size can help determine if a
significant result is reliable and `important'. Together effect size and
CIs aid in our ability to realistically interpret confidence metrics in
statistical hypothesis testing.

\hypertarget{predictive-data-analysis}{%
\subsection{Predictive data analysis}\label{predictive-data-analysis}}

Predictive data analysis (PDA) is the first of the two types of
statistical approaches we will cover that fall under \textbf{machine
learning}. A branch of artificial intelligence (AI), machine learning
aims to develop computer algorithms that can essentially learn patterns
from data automatically. In the case of PDA, also known as
\textbf{supervised learning}, the learning process is guided
(supervised) by directing an algorithm to associate patterns in a
variable or set of variables to single particular variable. The
particular variable is analogous to some degree to a dependent variable
in IDA, but in the machine learning literature this variable is known as
the \textbf{target} variable. The other variable or (more often than
not) variables are known as \textbf{features}. The goal of PDA is to
develop a statistical generalization that can accurately predict the
values of a target variable using the values of the feature variables.
PDA can be seen as a mix of deductive (top-down) and inductive
(bottom-up) methods in that the target variable is determined by a
research goal but the feature variables and choice of statistical method
(algorithm) are not fixed and can vary depending on their usefulness in
effectively predicting the target variable. PDA is a versatile method
that often employed to derive intelligent action from data, but it can
also be used for hypothesis generation and even hypothesis testing,
under certain conditions. If a researcher's aim is to create model that
can perform a language related task, explore association strength
between a target variable and various types and combinations of
features, or to perform emerging alternative approaches to hypothesis
testing \footnote{see Deshors and Gries (2016) and R. Harald Baayen
  (2011)}, this is the analysis approach a researcher will take.

At this point let's consider some departures from the inferential data
analysis (IDA) approach we covered in the last subsection that are
important to highlight to orient our overview of PDA. First, while the
cornerstone of IDA is the hypothesis, in PDA this is typically not the
case. A research question which identifies a source of potential
uncertainty in an area and outlines a strategy for addressing this
uncertainty is sufficient groundwork to embark on an analysis. A second
divergence, is the fact that the data is used in a very distinct way. In
IDA the entire dataset is statistically interrogated once and only once.
In PDA the dataset is (minimally) partitioned into a \textbf{training
set} and a \textbf{test set}. The training set is used to train a
statistical model and the test set is left to test the accuracy of the
statistical model. The training set typically constitutes a larger
portion of the data (typically around 75\%) and serves as the test bed
for iteratively applying one or more algorithms and/ or feature
combinations to produce the most successful learning model. The test set
is reserved for a final evaluation of the model's performance. Depending
on the application and the amount of available data, a third
\emph{development set} is sometimes created as a pseudo test set to
facilitate the testing of multiple approaches on data outside the
training set before the final evaluation on the test set is performed.
In this scenario the proportions of the partitions vary, but a good rule
of thumb is to reserve 60\% of the data for training, 20\% for
development, and 20\% for testing.

Let's now turn to the identification of variables, the statistical
interrogation of these variables, and the interpretation of the
statistical results. In IDA the variables (features) are pre-determined
by the hypothesis and the informational values and number of these
variables plays a significant role in selecting a statistical procedure
(algorithm). Lacking a hypothesis, a PDA approach's main goal is to make
accurate predictions on the target variable and is free to explore any
number of features and feature combinations to that end. The target
variable is the only variable which necessarily fixed and in this light
pre-determined.

To give an example, let's consider a language task in which the goal is
to take text messages (SMS) and develop a language model that predict if
a message is spam or not. Minimally we would need data which includes
individual text messages and each of these text message will need to be
labeled as being either spam or legitimate messages (`ham' in this
case). In Table~\ref{tbl-pda-sma-preview} we see the first ten of 5574
observations from the SMS Spam Collection (v.1) dataset collected by
Almeida, Gómez Hildago, and Yamakami (2011).

\hypertarget{tbl-pda-sma-preview}{}
\begin{table}
\caption{\label{tbl-pda-sma-preview}First ten observations from the SMS Spam Collection (v.1) }\tabularnewline

\centering
\begin{tabular}{ll}
\toprule
sms\_type & message\\
\midrule
ham & Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\
ham & Ok lar... Joking wif u oni...\\
spam & Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T\&C's apply 08452810075over18's\\
ham & U dun say so early hor... U c already then say...\\
ham & Nah I don't think he goes to usf, he lives around here though\\
\addlinespace
spam & FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\\
ham & Even my brother is not like to speak with me. They treat me like aids patent.\\
ham & As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\\
spam & WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\\
spam & Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\\
\bottomrule
\end{tabular}
\end{table}

As it stands we have two variables; \texttt{sms\_type} is clearly the
target and \texttt{message} contain the full messages. The question is
how best to transform the information in the \texttt{message} variable
such that it will provide an algorithm useful information to predict
each value of \texttt{sms\_type}. Since the informational value of
\texttt{sms\_type} is categorical we will call the values
\textbf{classes}. The process of deciding on how to transform the
information in \texttt{message} into useful features is called
\textbf{feature engineering} and it is a process which is much an art as
a science. On the creative side of things it is often helpful to have a
mixture of relevant domain knowledge and clever hacking skills to
envision what features may work best. The more logistic side of things
requires some knowledge about the strengths and weaknesses of various
learning algorithms when dealing with certain number and informational
value feature combinations.

Leaving the choice of algorithm aside, let's focus on feature
engineering. Since each \texttt{message} value is a unique message, the
chance that using \texttt{message} as it is, is not likely to help us
make reliable predictions about the status of new message (`spam' or
`ham'). A simple first-pass approach to decomposing \texttt{message} to
draw out similarities and distinctions between the classes may be to
break each message into words. Now SMS messages are not your average
type of text --there are many non-standard forms. So our definition of
word may simply be character groupings broken apart by whitespace. To
avoid confusion between our common-sense understanding of word and the
types of character strings, it is often the case that language feature
values are called \textbf{terms}. Other term types may work better,
n-grams, character sequences, stems/lemmas, or even combinations of
these. Certain terms may be removed that are potentially uninformative
either based on their class (stopwords, numerals, punctuation, etc.) or
due to their distribution. The process of systematic isolation of terms
which are more informative than others is called \textbf{dimensionality
reduction} (Kowsari et al. 2019). With experience a research will become
more adept a recognizing advantages and potential issues and alternative
ways of approaching the creation of features but there is almost always
some level of trial and error in the process. Feature engineering is
very much an exploratory process. It is also iterative. You can try a
set of features with an algorithm and produce a language model and test
it on the training set --if is accurate, great. If not, you can
brainstorm some more --you are free to try further engineer the features
trying new features or feature measures (term weights) and/ or change
the learning algorithm.

\begin{figure}[h]

{\centering \includegraphics[width=3.88in,height=\textheight]{figures/approaching-analysis/predictive-variables.png}

}

\caption{\label{fig-aa-predictive-variables}Variable roles in predictive
analysis.}

\end{figure}

Let's now turn to some considerations to take into account when
selecting a statistical algorithm. First, just as in IDA, variable
informational value plays a role in algorithm selection, specifically
the informational value of the target variable. If the target variable
is categorical, then we are looking for a \textbf{classification}
algorithm. If the target variable is continuous, we will employ a
\textbf{regression} algorithm. \footnote{The name regression can be a
  bit confusing given a very common classification algorithm is
  ``Logistic Regression''.} Some common classification algorithms are
listed in Table~\ref{tbl-pda-algorithms}.

\hypertarget{tbl-pda-algorithms}{}
\begin{table}
\caption{\label{tbl-pda-algorithms}Some common supervised learning algorithms. }\tabularnewline

\centering
\begin{tabular}{ll}
\toprule
Classification & Regression\\
\midrule
Logistic Regression & Linear Regression\\
Support Vector Machine & Support Vector Regression\\
Naïve Bayes Classifier & Poisson Regression\\
Neural Network & \\
Decision Tree & \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}

\end{table}

\begin{table}

\end{table}

Another consideration to take into account is the whether the researcher
aims to go beyond simply using an algorithm to make accurate
predictions, but also wants to understand how the algorithm made its
predictions and what contribution features made in the process. There
are algorithms that produce models that allow the researcher to peer
into and understand its inner workings (e.g.~logistic regression, naïve
bayes classifiers, inter alia) and those that do not (e.g.~neural
networks, support vector machines, inter alia). Those that do not are
called \textbf{`black-box' algorithms}. Neither type assures the best
prediction accuracy. Important trade-offs need to be considered,
however, if the best prediction comes from a black-box method, but the
goal of the research is to understand the contribution of the features
to the model's predictions.

Once we have identified our target variable, engineered a promising set
of features, and selected an algorithm to employ that meets our research
goals, it is now time to interrogate the dataset. The first step is to
partition the dataset into a training and test set. The training set is
the dataset we will use to try out different features and/ or algorithms
with the aim of developing a model which can most accurately predict the
target variable values in this training set. This is the second step and
it's done by first training an algorithm to associate the features with
the (actual) target values. Next, the resulting model is then applied to
the same training data, yet with the target variable removed, or hidden,
from the machine learner. The target values predicted by the model for
each observation are compared to the actual target values. The more
predicted and actual values for the target variable coincide, the more
accurate the model. If the model shows high accuracy, then we are ready
to move to evaluate this model on the test set (again removing the
target variable). If the model accuracy is low, it's back to the drawing
board either returning to feature engineering and/ or algorithm
selection in hopes to improve model performance. In this way, the
training data can be used multiple times, a clear divergence from
standard IDA methods in which the data is interrogated and analyzed once
and only once.

\begin{figure}[h]

{\centering \includegraphics[width=4.8in,height=\textheight]{figures/approaching-analysis/predictive-phases.png}

}

\caption{\label{fig-aa-predictive-phases}Phases in predictive analysis.}

\end{figure}

For all applications of PDA the interpretation of the prediction model
includes some metric or metrics of accuracy comparing the extent to
which the models predictions and the actual targets align. In cases in
which the inner workings of the model are of interest, a researcher can
dive into features and their contributions to the prediction model in an
exploratory fashion according to the research goals. The exploration of
features, then, varies, so at this time let's focus on the metrics of
prediction accuracy.

The standard form for evaluating a model's performance differs between
classification models (naive bayes) and regression models (linear
regression). For classification models, a cross-tabulation of the
predicted and actual classes results in a \textbf{contingency table}
which can be used to calculate \textbf{accuracy} which is the sum of all
the correctly predicted observations divided by the total number of
observations in the test set. In addition to accuracy, there are various
other measures which aim to assess a model's performance to gain more
insight into the potential over- or under-generalization of the model
(\emph{Precision} and \emph{Recall}). For regression models, differences
between predicted and actual values can be assessed using a
\textbf{coefficient of correlation} (typically \(R^2\)). Again, more
fine-grained detail about the model's performance can be calculated
(\emph{Root Mean Square Error}).

Another component worthy of consideration when evaluating a model's
performance is how do we determine if the performance is actually good.
One the one hand, accuracy rates into the 90+\% range on the test set is
usually a good sign that the model is performing well. No model will
perform with perfect accuracy, however, and depending on the goal of the
research particular error patterns may be more important, and
problematic, than the overall prediction accuracy. On the other hand,
another eventuality is that the model performs very well on the training
set but that on the test set (new data) the performance drops
significantly. This is a sign that during the training phrase the
machine learning algorithm learned nuances in the data (`noise') that
obscure the signal pattern to be learned. This problem is called
\textbf{overfitting} and to avoid it researchers iteratively run
evaluations of the training data using resampling. The two most common
resampling methods are \textbf{bootstrapping} (resampling with
replacement) and \textbf{cross-validation} (resampling without
replacement). The performance of these multiple models are summarized
and the error between them is assessed. The goal is to minimize the
performance differences between the models while maximizing the overall
performance. These measures go a long way to avoiding overfitting and
therefore maximizing the chance that the training phase will produce a
model which is robust.

\hypertarget{exploratory-data-analysis}{%
\subsection{Exploratory data analysis}\label{exploratory-data-analysis}}

The last of the three analysis types, exploratory data analysis (EDA)
includes a wide range of methods whose objective is to identify
structure in datasets using only the data itself. In this way, EDA is an
inductive, bottom-up approach to data analysis, which does not make any
formal assumptions about the relationship(s) between variables. EDA can
be roughly broken into two subgroups of analysis. \textbf{Unsupervised
learning}, like supervised learning (PDA), is a subtype of machine
learning. However, unlike prediction, unsupervised learning does not
include a target variable to guide associations. The second subgroup of
EDA methods can be seen as a (more robust) extension of the
\textbf{descriptive analysis methods} covered earlier in this chapter.
Either through unsupervised learning or descriptive methods, EDA employs
quantitative methods to summarize, reduce, and sort complex datasets and
statistically and visually interrogate a dataset in order to provide the
researcher novel perspective to be qualitatively assessed. These
qualitative assessments may prove useful to generate hypotheses or to
generate groupings to be used in predictive analyses. So, if a
researcher's aim is to probe a dataset in order to explore potential
relationships in an area where predictions and/ or hypotheses cannot be
clearly made, this is the analysis approach to choose.

In contrast to both IDA and even PDA in which there are assumptions made
about what relationship(s) to explore, EDA makes no such assumptions.
Furthermore, given the exploratory nature of the process, EDA is not an
approach which can in itself be used to make conclusive generalizations
about the populations from which the (corpus) sample in which it is
drawn. For IDA the fidelity of the sample and the process of selection
of the variables is of utmost importance to ensure that the statistical
results are reliably generalizable. Even in the case of PDA, the sample
and variables selected are key to building a robust predictive model.
However, in contrast to IDA, but similar to PDA, EDA methods may reuse
the data selecting different variables and/or methods as research goals
dictate. If a machine learning approach to EDA is adopted, the dataset
can be partitioned into training and test sets, in a similar fashion to
PDA. And as with PDA, the training set is used for refining statistical
measures and the test set is used to evaluate the refined measures.
Although the evaluation results still cannot be used to generalize, the
insight can be taken as stronger evidence that there is a potential
relationship, or set of relationships, worthy of further study.

Another notable point of contrast concerns the interpretation of EDA
results. Although quantitative in nature, exploratory methods involve a
high level of human interpretation. Human interpretation is a part of
each stage of data analysis, and each statistical approach, in general,
but exploratory methods produce results that require associative
thinking and pattern detection which is distinct from the other two
analysis approaches, in particular, IDA.

Again, as we have done for the other two analysis approaches, let's turn
to the process of variable identification, data interrogation, and
interpretation methods. As in the case of PDA, EDA only requires a
research goal. But in PDA, the research goal centered around predicting
a target variable. In EDA, there is no such focus. The research goal may
in fact be less defined and a researcher may consider various
relationships in turn or simultaneously. The curation of the variables,
however, does overlap in spirit to the process of \textbf{feature
engineering} that we touched on for creating variables for predictive
models. But in EDA the measure to gauge whether the engineered variables
are good, is left to the qualitative evaluation of the researcher.

\begin{figure}[h]

{\centering \includegraphics[width=3.61in,height=\textheight]{figures/approaching-analysis/exploratory-variables.png}

}

\caption{\label{fig-aa-exploratory-variables}Variable roles in
exploratory analysis.}

\end{figure}

For illustrative purposes let's consider the State of the Union Corpus
(SOTU) (Benoit 2020). The presidential addresses and a set of meta-data
variables are included in the corpus. I've subsetted this corpus to only
include U.S. presidents since 1946. A tabular preview of the first 10
addresses (truncated for display) can be found in
Table~\ref{tbl-eda-sotu-corpus}.

\hypertarget{tbl-eda-sotu-corpus}{}
\begin{table}
\caption{\label{tbl-eda-sotu-corpus}First ten addresses from the SOTU Corpus. }\tabularnewline

\centering
\begin{tabular}{l|l|l|l|l}
\hline
president & date & delivery & party & addresses\\
\hline
Truman & 1946-01-21 & written & Democratic & To the Congress of the United States: A quarter...\\
\hline
Truman & 1947-01-06 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\hline
Truman & 1948-01-07 & spoken & Democratic & Mr. President, Mr. Speaker, and Members of the ...\\
\hline
Truman & 1949-01-05 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\hline
Truman & 1950-01-04 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\hline
Truman & 1951-01-08 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\hline
Truman & 1952-01-09 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\hline
Truman & 1953-01-07 & written & Democratic & To the Congress of the United States: I have th...\\
\hline
Eisenhower & 1953-02-02 & spoken & Republican & Mr. President, Mr. Speaker, Members of the Eigh...\\
\hline
Eisenhower & 1954-01-07 & spoken & Republican & Mr. President, Mr. Speaker, Members of the Eigh...\\
\hline
\end{tabular}
\end{table}

A dataset such as this one could be leveraged to explore many different
types of research questions. Key to guiding the engineering of features,
however, is to clarify from the outset of the research project what the
entity of study is, or \textbf{unit of analysis}. In IDA and PDA
approaches, the unit of analysis forms an explicit part of the research
hypothesis or goal. In EDA the research question may have multiple
fronts, which may be reflected in differing units of analysis. For
example, based on the SOTU dataset, we could be interested in political
rhetoric, language of particular presidents, party ideology, etc.
Depending on the perspective we are interested in investigating, the
choice of how to approach engineering features to gain insight will
vary.

By the same token, approaches for interrogating the dataset can vary
widely, between and within the same research project, but for
instructive purposes we can draw a distinction between descriptive
methods and unsupervised learning methods.

\hypertarget{tbl-eda-approaches-table}{}
\begin{table}
\caption{\label{tbl-eda-approaches-table}Some common EDA analysis methods }\tabularnewline

\centering
\begin{tabular}{ll}
\toprule
Descriptive & Unsupervised learning\\
\midrule
Term frequency analysis & Cluster analysis\\
Term keyness analysis & Topic Modeling\\
Collocation analysis & Dimensionality reduction\\
\bottomrule
\end{tabular}
\end{table}

EDA leans heavily on visual representations of both descriptive and
unsupervised learning methods. Visualizations enable humans to identify
and extrapolate associative patterns. Visualizations range from standard
barplots and scatterplots to network graphs and dendrograms and more.
Some sample visualizations based on the SOTU Corpus are found in
Figure~\ref{fig-eda-visualizations-grid}.

\begin{figure}[h]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-eda-visualizations-grid-1.pdf}

}

\caption{\label{fig-eda-visualizations-grid}Sample visualizations from
the SOTU Corpus (1946-2020).}

\end{figure}

Just as feature selection and analysis method, the interpretation of the
results in EDA are much more varied than in the other analysis methods.
EDA methods provide information which requires much more human
intervention and associative interpretation. In this way, EDA can be
seen as a quantitatively informed qualitative assessment approach. The
results from one approach can be used as the input to another. Findings
can lead to further exploration and probing of nuances in the data.
Speculative as they are the results from exploratory methods can be
highly informative and lead to new insight and inspire further study in
directions that may not have been expected.

\hypertarget{reporting}{%
\section{Reporting}\label{reporting}}

Much of the necessary reporting for an analysis features in prose as
part of the write-up of a report or article. This will include
descriptive summaries, a blueprint of the method(s) used, and the
results. Descriptive summaries will often include assessments of
individual variables and/ or relationships between variables (central
tendency, dispersion, association strength, etc.). Any procedures
applied to diagnose or to correct the data should also be included in
the final report. This information is key to helping readers assess the
results from the analysis. A blueprint of the methods used will describe
the variable selection process, how the variables were used in the
statistical analysis, and any other information that is relevant for a
reader to understand what was done and why it was done. Reporting
results from an analysis will depend on the type of analysis and the
particular method(s) employed. For inferential analyses this will
include the test statistic(s) (\(X^2\), \(R^2\), etc.) and some measure
of confidence (\(p\)-value, confidence interval, effect size). In
predictive analyses accuracy results and related information will need
to be reported. For exploratory analyses, the reporting of results will
vary and often include visualizations and metrics that require more
human interpretation than the other analysis types.

While a good article write-up will include the most vital information to
understand the procedures taken in an analysis, there are many more
details which do not traditionally appear in prose. If a research
project was conducted programmatically, however, the programming files
(scripts) used to generate the analysis can (and should) be shared.
While the scripts themselves are highly useful for other researchers to
consult and understand in fine-grained detail the steps that were taken,
it is important to also recognize that the research project should be
well documented --through organized project directory and file structure
as well as through code commenting. This description and instructions on
how to run the analysis form a \textbf{research compendium} which ensure
that the research conducted is easily understood and able to be
reproduced and/ or enhanced by other researchers.

\hypertarget{summary-3}{%
\section*{Summary}\label{summary-3}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we have focused on description and analysis --the third
component of DIKI Hierarchy. This process is visually summarized in
Figure~\ref{fig-approaching-analysis-visual-summary-graphic}.

\begin{figure}[h]

{\centering \includegraphics[width=7.17in,height=\textheight]{figures/approaching-analysis/approaching-analysis-visual-summary-paper.png}

}

\caption{\label{fig-approaching-analysis-visual-summary-graphic}Approaching
analysis: visual summary}

\end{figure}

Building on the strategies covered in
\protect\hyperlink{sec-understanding-data}{Chapter 2 ``Understanding
data''} to derive a rich relational dataset, in this chapter we outlined
key points in approaching analysis. The first key step in any analysis
is to perform a descriptive assessment of the individual variables and
relationships between variables. To select the appropriate descriptive
measures we covered the various informational values that a variable can
take. In addition to providing key information for reporting purposes,
descriptive measures are important to explore so the researcher can get
a better feel for the dataset before conducting an analysis.

We covered three data analysis types in this chapter: inferential,
predictive, and exploratory. Each of these embodies very distinct
approaches to deriving knowledge from data. Ultimately the choice of
analysis type is highly dependent on the goals of the research.
Inferential analysis is centered around the goal of testing a
hypothesis, and for this reason it is the most highly structured
approach to analysis. This structure is aimed at providing the
mechanisms to draw conclusions from the results that can be generalized
to the target population. Predictive analysis has a less-ambitious but
at times more relevant goal of discovering the extent to which a given
relationship can be extrapolated from the data to provide a model of
language that can accurately predict an outcome using new data. While
many times predictive analysis is used to perform language tasks, it can
also be a highly effective methodology for applying different
algorithmic approaches and exploring relationships a target variable and
various configurations of variables. The ability to explore the data in
multiple ways, is also a key strength of employing an exploratory
analysis. The least structured and most variable of the analysis types,
exploratory analyses are a powerful approach to deriving knowledge from
data in an area where clear predictions cannot be made.

I rounded out this chapter with a short description of the importance of
reporting the metrics, procedures, and results from analysis. Reporting,
in its traditional form, is documented in prose in an article. This
reporting aims to provide the key information that a reader will need to
understand what was done, how it was done, and why it was done. This
information also provides the necessary information for reader's with a
critical eye to understand the analysis in more detail. Yet even the
most detailed reporting in a write-up still leaves many practical, but
key, points of the analysis obscured. A programming approach provides
the procedural steps taken that when shared provide the exact methods
applied. Together with the write-up a research compendium which provides
the scripts to run the analysis and documentation on how to run the
analysis forms an integral part of creating reproducible research.

\hypertarget{activities-2}{%
\section*{Activities}\label{activities-2}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_4.html}{Descriptive
assessment of datasets}\\
\textbf{How}: Read Recipe 4 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To explore appropriate methods for summarizing variables
in datasets given the number and informational values of the
variable(s).

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/lab_4}{Descriptive
assessment of datasets}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 4.\\
\textbf{Why}: To identify and apply the appropriate descriptive methods
for a vector's informational value and to assess both single variables
and multiple variables with the appropriate statistical, tabular, and/
or graphical summaries.

\end{tcolorbox}

\hypertarget{questions-3}{%
\section*{Questions}\label{questions-3}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Conceptual questions}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  What are the key differences between descriptive and analytic
  statistics?
\item
  What informational values can a variable take?
\item
  What are the potential measures of central tendency and dispersion for
  a variable? Does it depend on the informational value of the variable?
\item
  Consider the following variables: \(X\) = number of children, \(Y\) =
  number of siblings, \(Z\) = number of siblings who are older than the
  participant. Which of these variables are categorical and which are
  continuous? What are the informational values of each variable? What
  are the measures of central tendency and dispersion for each variable?
\item
  What type(s) of tables or plots are appropriate for summarizing a
  variable? What type(s) of tables or plots are appropriate for
  summarizing the relationship between two variables?
\item
  In the following variables and information values, identify if the
  plots are appropriate for summarizing the relationship.
\item
  What are the key differences between inferential, predictive, and
  exploratory analysis?
\item
  How do the goals of the research influence the choice of analysis
  type?
\item
  Given the following research questions, identify which type of
  analysis is most appropriate and why:
\item
  Given the following research questions, identify which type of
  analysis is most appropriate and why:
\item
  Given the following research questions, identify which type of
  analysis is most appropriate and why:
\item
  How are the results of inferential, predictive, and exploratory
  analysis evaluated?
\item
  Research compendia are an important part of reproducible research.
  What are the key components of a research compendium? What are the
  benefits of sharing a research compendium?
\item
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Technical exercises}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  Create a contingency table for the following variables:
\item
  Create a plot for the following variables:
\item
  Report these tables and plots with a short interpretation of what they
  show.
\item
  \ldots{}
\end{itemize}

\end{tcolorbox}

\hypertarget{sec-framing-research}{%
\chapter{Framing research}\label{sec-framing-research}}

\begin{quote}
If we knew what it was we were doing, it would not be called research,
would it?

―-- Albert Einstein
\end{quote}

\begin{quote}
``The reproducibility of studies and the ability to follow up on the
work of others is key for innovation in science and engineering.''

--- Leland Wilkinson
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  What are the strategies for selecting a research area and identifying
  a research problem?
\item
  How does a research problem and research aim frame the development of
  a research statement?
\item
  What is a `research blueprint' and how do the conceptual and practical
  steps involved in developing it aid the researcher as well as the
  scientific community?
\end{itemize}

\end{tcolorbox}

At this point in this part of the textbook, we have covered Data,
Information, and Knowledge from the Data to Insight Hierarchy. The goal
has been to provide an orientation to the main building blocks of doing
text analysis. Insight is the last component of the hierarchy. However,
in practical terms, it is the first step to address in an research
project as goals of a research project influence all subsequent steps.

In this chapter we discuss how to frame research, that is how to
position your research project's findings to contribute insight to
understanding of the world. We will cover how to connect with the
literature, selecting a research area and identifying a research
problem, and how to design research best positioned to return relevant
findings that will connect with this literature, establishing a research
aim and research question. We will round out this chapter with a guide
on developing a research blueprint --a working plan to organize the
conceptual and practical steps to implement the research effectively and
in a way that supports communicating the research findings and the
process by which the findings were obtained.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Interactive programming}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/swirl}{Version control}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To \ldots.

\end{tcolorbox}

\hypertarget{fr-keys}{%
\section{Keys to strong research}\label{fr-keys}}

Together a research area, problem, aim and question and the research
blueprint that forms the conceptual and practical scaffolding of the
project ensure from the outset that the project is solidly grounded in
the main characteristics of good research. These characteristics,
summarized by Cross (2006), are found in
Table~\ref{tbl-fr-cross-research-char-table}.

\hypertarget{tbl-fr-cross-research-char-table}{}
\begin{table}
\caption{\label{tbl-fr-cross-research-char-table}Characteristics of research (Cross, 2006). }\tabularnewline

\centering
\begin{tabular}{ll}
\toprule
Characteristic & Description\\
\midrule
Purposive & Based on identification of an issue or problem worthy and capable of investigation\\
Inquisitive & Seeking to acquire new knowledge\\
Informed & Conducted from an awareness of previous, related research\\
Methodical & Planned and carried out in a disciplined manner\\
Communicable & Generating and reporting results which are feasible and accessible by others\\
\bottomrule
\end{tabular}
\end{table}

With these characteristics in mind, let's get started with the first
component to address --connecting with the literature.

\hypertarget{fr-connect}{%
\section{Connect}\label{fr-connect}}

\hypertarget{research-area}{%
\subsection{Research area}\label{research-area}}

The area of research is the first decision to make in terms of where to
make a contribution to understanding. At this point, the aim is to
identify a general area of interest where a researcher wants to derive
insight. For those with an established research trajectory in language,
the area of research to address through text analysis will likely be an
extension of their prior work. For others, which include new researchers
or researcher's that want to explore new areas of language research or
approach an area through a language-based lens, the choice of area may
be less obvious. In either case, the choice of a research area should be
guided by a desire to contribute something relevant to a theoretical,
social, and/ or practical matter of personal interest. Personal
relevance goes a long way to developing and carrying out
\textbf{purposive} and \textbf{inquisitive} research.

So how do we get started? The first step is to reflect on your own areas
of interest and knowledge, be it academic, professional, or personal.
Language is at the heart of the human experience and therefore found in
some fashion anywhere one seeks to find it. But it is a big world and
more often than not the general question about what area to explore
language use is sometimes the most difficult. To get the ball rolling,
it is helpful to peruse disciplinary encyclopedias or handbooks of
linguistics and language-related an academic fields
(e.g.~\href{https://www.sciencedirect.com/referencework/9780080448541/encyclopedia-of-language-and-linguistics}{Encyclopedia
of Language and Linguistics} (Brown 2005),
\href{https://www.sciencedirect.com/book/9781843345978/a-practical-guide-to-electronic-resources-in-the-humanities}{A
Practical Guide to Electronic Resources in the Humanities} (Dubnjakovic
and Tomlin 2010),
\href{https://www.routledgehandbooks.com/doi/10.4324/9781315749129}{Routledge
encyclopedia of translation technology} (Chan 2014))

A more personal, less academic, approach is to consult online forums,
blogs, etc. that one already frequents or can be accessed via an online
search. For example, \href{https://www.reddit.com/}{Reddit} has a wide
variety of active subreddits
(\href{https://www.reddit.com/r/LanguageTechnology/}{r/LanguageTechnology},
\href{https://www.reddit.com/r/linguistics/}{r/Linguistics},
\href{https://www.reddit.com/r/corpuslinguistics/}{r/corpuslinguistics},
\href{https://www.reddit.com/r/DigitalHumanities/}{r/DigitalHumanities},
etc.). Twitter and Facebook also have interesting posts on linguistics
and language-related fields worth following. Through one of these social
media site you may find particular people that maintain a blog worth
browsing. For example, I follow \href{https://juliasilge.com/}{Julia
Silge}, \href{http://www.rctatman.com/}{Rachel Tatman}, and
\href{https://tedunderwood.com/}{Ted Underwood}, inter alia. Perusing
these resources can help spark ideas and highlight the kinds of
questions that interest you.

Regardless of whether your inquiry stems from academic, professional, or
personal interest, try to connect these findings to academic areas of
research. Academic research is highly structured and well-documented and
making associations with this network will aid in subsequent steps in
developing a research project.

\hypertarget{research-problem}{%
\subsection{Research problem}\label{research-problem}}

Once you've made a rough-cut decision about the area of research, it is
now time to take a deeper dive into the subject area and jump into the
literature. This is where the rich structure of disciplinary research
will provide aid to traverse the vast world of academic knowledge and
identify a research problem. A research problem highlights a particular
topic of debate or uncertainty in existing knowledge which is worthy of
study.

Surveying the relevant literature is key to ensuring that your research
is \textbf{informed}, that is, connected to previous work. Identifying
relevant research to consult can be a bit of a `chicken or the egg'
problem --some knowledge of the area is necessary to find relevant
topics, some knowledge of the topics is necessary to narrow the area of
research. Many times the only way forward is to jump in conducting
searches. These can be world-accessible resources
(e.g.~\href{https://scholar.google.com/}{Google Scholar}) or
limited-access resources that are provided through an academic
institution
(e.g.~\href{https://about.proquest.com/en/products-services/llba-set-c}{Linguistics
and Language Behavior Abstracts}), \href{https://eric.ed.gov/}{ERIC},
\href{https://www.ebsco.com/products/research-databases/apa-psycinfo}{PsycINFO},
etc.). Some organizations and academic institutions provide
\href{https://guides.zsr.wfu.edu/linguistics}{research guides} to help
researcher's access the primary literature.

Another avenue to explore are journals dedicated to areas in which
linguistics and language-related research is published. In the following
tables I've listed a number of highly visible journals in linguistics,
digital humanities, and computational linguistics.

\hypertarget{tbl-pinboard-journals-linguistics}{}
\begin{table}
\caption{\label{tbl-pinboard-journals-linguistics}A list of some linguistics journals. }\tabularnewline

\centering
\begin{tabular}{l|l}
\hline
Resource & Description\\
\hline
<a href="https://www.euppublishing.com/loi/cor">Corpora</a> & An international, peer-reviewed journal of corpus linguistics focusing on the many and varied uses of corpora both in linguistics and beyond.\\
\hline
<a href="https://www.degruyter.com/journal/key/CLLT/html">Corpus Linguistics and Linguistic Theory</a> & Corpus Linguistics and Linguistic Theory (CLLT) is a peer-reviewed journal publishing high-quality original corpus-based research focusing on theoretically relevant issues in all core areas of linguistic research, or other recognized topic areas.\\
\hline
<a href="https://benjamins.com/catalog/ijcl">International Journal of Corpus Linguistics</a> & The International Journal of Corpus Linguistics (IJCL) publishes original research covering methodological, applied and theoretical work in any area of corpus linguistics.\\
\hline
<a href="http://ijls.net/">International Journal of Language Studies</a> & It is a refereed international journal publishing articles and reports dealing with theoretical as well as practical issues focusing on language, communication, society and culture.\\
\hline
<a href="https://www.cambridge.org/core/journals/journal-of-child-language">Journal of Child Language</a> & A key publication in the field, Journal of Child Language publishes articles on all aspects of the scientific study of language behaviour in children, the principles which underlie it, and the theories which may account for it.\\
\hline
<a href="https://www.cambridge.org/core/journals/journal-of-linguistic-geography/all-issues">Journal of Linguistic Geography</a> & The Journal of Linguistic Geography focuses on dialect geography and the spatial distribution of language relative to questions of variation and change.\\
\hline
<a href="http://www.tandfonline.com/toc/njql20/current">Journal of Quantitative Linguistics</a> & Publishes research on the quantitative characteristics of language and text in mathematical form, introducing methods of advanced scientific disciplines.\\
\hline
\end{tabular}
\end{table}

\hypertarget{tbl-pinboard-journals-humanities}{}
\begin{table}
\caption{\label{tbl-pinboard-journals-humanities}A list of some humanities journals. }\tabularnewline

\centering
\begin{tabular}{l|l}
\hline
Resource & Description\\
\hline
<a href="http://www.digitalhumanities.org/dhq/">Digital Humanities Quarterly</a> & Digital Humanities Quarterly (DHQ), an open-access, peer-reviewed, digital journal covering all aspects of digital media in the humanities.\\
\hline
<a href="https://academic.oup.com/dsh">Digital Scholarship in the Humanities</a> & DSH or Digital Scholarship in the Humanities is an international, peer reviewed journal which publishes original contributions on all aspects of digital scholarship in the Humanities including, but not limited to, the field of what is currently called the Digital Humanities.\\
\hline
<a href="https://culturalanalytics.org/">Journal of Cultural Analytics</a> & Cultural Analytics is an open-access journal dedicated to the computational study of culture. Its aim is to promote high quality scholarship that applies computational and quantitative methods to the study of cultural objects (sound, image, text), cultural processes (reading, listening, searching, sorting, hierarchizing) and cultural agents (artists, editors, producers, composers).\\
\hline
\end{tabular}
\end{table}

\hypertarget{tbl-pinboard-journals-cl}{}
\begin{table}
\caption{\label{tbl-pinboard-journals-cl}A list of some computational linguistics journals. }\tabularnewline

\centering
\begin{tabular}{l|l}
\hline
Resource & Description\\
\hline
<a href="https://direct.mit.edu/coli">Computational Linguistics</a> & Computational Linguistics is the longest-running publication devoted exclusively to the computational and mathematical properties of language and the design and analysis of natural language processing systems.\\
\hline
<a href="http://lrec-conf.org/">LREC Conferences</a> & The International Conference on Language Resources and Evaluation is organised by ELRA biennially with the support of institutions and organisations involved in HLT.\\
\hline
<a href="https://transacl.org/index.php/tacl/index">Transactions of the Association for Computational Linguistics</a> & Transactions of the Association for Computational Linguistics (TACL) is an ACL-sponsored journal published by MIT Press that publishes papers in all areas of computational linguistics and natural language processing.\\
\hline
\end{tabular}
\end{table}

To explore research related to text analysis it is helpful to start with
the (sub)discipline name(s) you identified in when selecting your
research area, more specific terms that occur to you or key terms from
the literature, and terms such as `corpus study' or `corpus-based'. The
results from first searches may not turn out to be sources that end up
figuring explicitly in your research, but it is important to skim these
results and the publications themselves to mine information that can be
useful to formulate better and more targeted searches. Relevant
information for honing your searches can be found throughout an academic
publication (article or book). However, pay particular attention to the
abstract, in articles, and the table of contents, in books, and the
cited references. Abstracts and tables of contents often include
discipline-specific jargon that is commonly used in the field. In some
articles there is even a short list of key terms listed below the
abstract which can be extremely useful to seed better and more precise
search results. The references section will contain relevant and
influential research. Scan these references for publications which
appear to narrowing in on topic of interest and treat it like a search
in its own right.

Once your searches begin to show promising results it is time to keep
track and organize these references. Whether you plan to collect
thousands of references over a lifetime of academic research or your aim
is centered around one project, software such as
\href{https://www.zotero.org/}{Zotero}\footnote{\href{https://guides.zsr.wfu.edu/zotero}{Zotero
  Guide}},
\href{https://www.mendeley.com/reference-management/reference-manager}{Mendeley},
or \href{https://bibdesk.sourceforge.io/}{BibDesk} provide powerful,
flexible, and easy-to-use tools to collect, organize, annotate, search,
and export references. Citation management software is indispensable for
modern research --and often free!

As your list of relevant references grows, you will want to start the
investigation process in earnest. Begin skimming (not reading) the
contents of each of these publications, starting with the most relevant
first\footnote{Or what appears to be most relevant. This may change as
  you start to take a closer look.}. Annotate these publications using
highlighting features of the citation management software to identify:
(1) the stated goal(s) of the research, (2) the data source(s) used, (3)
the information drawn from the data source(s), (4) the analysis approach
employed, and (5) the main finding(s) of the research as they pertain to
the stated goal(s). Next, in your own words, summarize these five key
areas in prose adding your summary to the notes feature of the citation
management software. This process will allow you to efficiently gather
and document references with the relevant information to guide the
identification of a research problem, to guide the formation of your
problem statement, and ultimately, to support the literature review that
will figure in your project write-up.

From your preliminary annotated summaries you will undoubtedly start to
recognize overlapping and contrasting aspects in the research
literature. These aspects may be topical, theoretical, methodological,
or appear along other lines. Note these aspects and continue to conduct
more refine searches, annotate new references, and monitor for any
emerging patterns of uncertainty or debate (gaps) which align with your
research interest(s). When a promising pattern takes shape, it is time
to engage with a more detailed reading of those references which appear
most relevant highlighting the potential gap(s) in the literature. At
this point you can focus energy on more nuanced aspects of a particular
gap in the literature with the goal to formulate a problem statement. A
problem statement directly acknowledges a gap in the literature and puts
a finer point on the nature and relevance of this gap for understanding.
This statement reflects your first deliberate attempt to establish a
line of inquiry. It will be a targeted, but still somewhat general,
statement framing the gap in the literature that will guide subsequent
research design decisions.

\hypertarget{findings}{%
\section{Findings}\label{findings}}

\hypertarget{research-aim}{%
\subsection{Research aim}\label{research-aim}}

With a problem statement in hand, it is now time to consider the goal(s)
of the research. A research aim frames the type of inquiry to be
conducted. Will the research aim to explore, evaluate, or explain? In
other words, will the research seek to uncover novel relationships,
assess the potential strength of a particular relationship, or test a
particular relationship? As you can appreciate, the research aim is
directly related to the analysis methods we touched upon in
\protect\hyperlink{sec-approaching-analysis}{Chapter 3}.

To gauge how to frame your research aim, reflect on the literature that
led you to your problem statement and the nature of the problem
statement itself. If the gap at the center of the problem statement is a
lack of knowledge, your research aim may be exploratory. If the gap
concerns a conjecture about a relationship, then your research may take
a predictive approach. When the gap points to the validation of a
relationship, then your research will likely be inferential in nature.
Before selecting your research aim it is also helpful to consult the
research aims of the primary literature that led you to your research
statement. Consider how your research statement relates the previous
literature. Do you aim to test a hypothesis based on previous
exploratory analyses? Are you looking to generate new knowledge in an
(apparently) uncharted area? Etc.

In general, a problem statement which addresses a smaller, nuanced gap
will tend to adopt similar research aims as the previous literature
while a larger, more divergent gap will tend to adopt a distinct
research aim. This is not a hard rule, but more of a heuristic, however,
and it is important to be familiar with both the previous literature,
the nature of different types of analysis, and the goals of the research
to ensure that the research is best-positioned to generate findings that
will contribute to the existing body of understanding in a principled
way.

\hypertarget{research-question}{%
\subsection{Research question}\label{research-question}}

The next step in research design is to craft the research question. A
\index{research question}research question is clearly defined statement
which identifies an aspect of uncertainty and the particular
relationships that this uncertainty concerns. The research question
extends and narrows the line of inquiry established in the research
statement and research aim. The research statement can be seen as the
content and the research aim as the form.

The form of a research question will vary based on the analysis
approach.

For inferential-based research, the research question will actually be a
statement, not a question. This statement makes a testable claim about
the nature of a particular relationship --i.e.~asserts a hypothesis. For
illustration, let's return to one of the hypotheses we previously
sketched out in \protect\hyperlink{sec-approaching-analysis}{Chapter 3},
leaving aside the implicit null hypothesis.

Women use more questions than men in spontaneous conversations.

For predictive- and exploratory-based research, the research question is
in fact a question. A reframing of the example hypothesis for a
predictive-based research question might looks something like this.

Can the number of questions used in spontaneous conversations predict if
a speaker is male or female?

And a similar exploratory-based research question would take this form.

Do men and women differ in terms of the number of questions they use in
spontaneous conversations?

The central research interest behind these hypothetical research
questions is, admittedly, quite basic. But from these simplified
examples, we are able to appreciate the similarities and differences
between the forms of research statements that correspond to distinct
research aims.

In terms of content, the research question will make reference to two
key components. First, is the \textbf{unit of analysis}. The unit of
analysis is the entity which the research aims to investigate. For our
three example research aims, the unit of analysis is the same, namely
men and women. Note, however, that the current unit of analysis is
somewhat vague in the example research questions. A more precise unit of
analysis would include more information about the population from which
the men and women are drawn (i.e.~English speakers, American English
speakers, American English speakers of the Southeast, etc.).

The second key component is the \textbf{unit of observation}. The unit
of observation is the primary element on which the insight into the unit
of analysis is derived and in this way constitutes the essential
organizational unit of the dataset to be analyzed. In our examples, the
unit of observation, again, is unchanged and is spontaneous
conversations. Note that while the unit of observation is key to
identify as it forms the organizational backbone of the research, it is
very common for the research to derive variables from this unit to
provide evidence to investigate the research question. In the previous
examples, we identified the number of conversations as part of the
research question. But in other cases a researcher may seek to
understand other aspects of questions in spontaneous conversations (i.e
type of question, features of questions, etc.). The unit of observation,
however, would remain the same.

\hypertarget{blueprint}{%
\section{Blueprint}\label{blueprint}}

Efforts to craft a research question are a very important aspect of
developing purposive, inquisitive, and informed research (returning to
Cross's characteristics of research). Moving beyond the research
question in the project means developing and laying out the research
design in a way such that the research is \textbf{Methodical} and
\textbf{Communicable}. In this textbook, the method to achieve these
goals is through the development of a research blueprint. The blueprint
includes two components: (1) the process of identifying the data,
information, and analysis methods to be used and (2) the creation of a
plan to structure and document the project.

As Ignatow and Mihalcea (2017) point out:

\begin{quote}
Research design is essentially concerned with the basic architecture of
research projects, with designing projects as systems that allow theory,
data, and research methods to interface in such a way as to maximize a
project's ability to achieve its goals {[}\ldots{]}. Research design
involves a sequence of decisions that have to be taken in a project's
early stages, when one oversight or poor decision can lead to results
that are ultimately trivial or untrustworthy. Thus, it is critically
important to think carefully and systematically about research design
before committing time and resources to acquiring texts or mastering
software packages or programming languages for your text mining project.
\end{quote}

\hypertarget{identify}{%
\subsection{Identify}\label{identify}}

Importance of identifying and documenting the key aspects required to
conduct the research cannot be understated. On the one hand this process
links concept to implementation. In doing so, a researcher is
better-positioned to conduct research with a clear view of what will be
entailed. On the other hand, a promising research question, on paper,
may present challenges that may require modification or reevaluation of
the viability of the project. It is not uncommon to encounter roadblocks
or even dead-ends for moving a well-founded research question forward
when considering the available data, a researcher's (current) technical
and/ or research skills, and the given time frame for the project. In
practice, the process of identifying the data, information, and methods
of analysis are considered in tandem with the investigative work to
develop a research aim and research question. In this subsection I will
cover the main characteristics to consider when developing a research
blueprint.

The first, and most important, part of establishing a research blueprint
is to \textbf{identify a viable data source}. Regardless of how you find
and access the data, it is essential to vet the corpus sample in light
of the research question. In the case that research is inferential in
nature, the sampling frame of the corpus is of primary importance as the
goal is to generalize the findings to a target population. A corpus
resource should align, to the extent feasible, with this target
population. For predictive and exploratory research, the goal to
generalize a claim is not central and for this reason the there is some
freedom in terms of how representative a corpus sample is of a target
population. Ideally a researcher will find and be able to model a
language population of target interest. Since the goal, however, is not
to test a hypothesis, but rather to explore particular or evaluate
potential relationships, either in an exploratoy or predictive fashion,
the research can often continue with the stipulation that the results
are interpreted in the light of the characteristics of the available
corpus sample.

The second step is to \textbf{identify the key variables} need to
conduct the research are and then ensure that this information can be
derived from the corpus data. The research question will reference the
unit of analysis and the unit of observation, but it is important at
this point to then pinpoint what the key variables will be. If the unit
of observation is spontaneous conversations. The question as to what
aspects of these conversations will be used in the analysis. In the
research questions presented in this chapter, we will want to envision
what needs to be done to derive a variable which measures the number of
questions in each of the conversations. In other research, their may be
features that need to be extracted, recoded, and/ or generated to
address the research question. Other variables of importance may be
non-linguistic in nature. In cases where there the meta-data is
incomplete for the goals of the research, it is sometimes possible to
merge meta-data from other sources.

The third step is to \textbf{identify a method of analysis}. The
selection of the analysis approach that was part of the research aim and
then the research question goes a long way to narrowing the methods that
a researcher must consider. But there are a number of factors which will
make some methods more appropriate than others. In inferential research,
the number and information values of the variables to be analyzed will
be of key importance (Gries 2013). The informational value of the
dependent variable will again narrow the search for the appropriate
method. The number of independent variables also plays an important
role. For example, a study with a categorical dependent variable with a
single categorical independent variable will lead the researcher to the
Chi-squared test. A study with a continuous dependent variable with
multiple independent variables will lead to linear regression. Another
aspect of note for inference studies is the consideration of the
distribution of continuous variables --a normal distribution will use a
parametric test where a non-normal distribution will use a
non-parametric test. These details need not be nailed down at this
point, but it is helpful to have them on your radar to ensure that when
the time comes to analyze the data, the appropriate steps are taken to
test for normality and then apply the correct test.

For predictive-based research, the informational value of the target
variable is key to deciding whether the prediction will be a
classification task or a numeric prediction task. This has downstream
effects when it comes time to evaluate and interpret the results.
Although the feature engineering process in predictive analyses means
that the features do not need to be specified from the outset and can be
tweaked and changed as needed during an analysis, it is a good idea to
start with a basic sense of what features most likely will be helpful in
developing a robust predictive model. Furthermore, while the number and
informational values of the features (predictor variables) are not as
important to selecting a prediction method (algorithm) as they are in
inferential analysis methods, it is important to recognize that
algorithms have strengths and shortcomings when working large numbers
and/ or types of features (Lantz 2013).

Exploratory research is the least restricted of the three types of
analysis approaches. Although it may be the case that a research will
not be able to specify from the outset of a project what the exact
analysis methods will be, an attempt to consider what types of analysis
methods will be most promising to provide results to address the
research question goes a long way to steering a project in the right
direction and grounding the research. As with the other analysis
approaches, it is important to be aware of what the analysis methods
available and what type of information they produce in light of the
research question.

In sum, the identification of the data, information, and analysis
methods that will be used in the proposed research are key to ensuring
the research is viable. Be sure to document this process in prose and
describe the strengths and potential shortcomings of (1) the corpus data
selected, (2) the information to be extracted for analysis, and (3) the
analysis method(s) that are appropriate for the research aim and what
the evaluation method will be. Furthermore, not every eventuality can be
foreseen. It is helpful to include a description of aspects of this
process which may pose challenges and to include potential contingency
plans as part of this prose description.

\hypertarget{fr-plan}{%
\subsection{Plan}\label{fr-plan}}

The next step in creating a research blueprint is to consider how to
physically implement your project. This includes how to organize files
and directories in a fashion that both provides the researcher a logical
and predictable structure to work with but also ensures that the
research is \textbf{Communicable}. On the one hand, communicable
research includes a strong write-up of the research, but, on the other
hand, it is also important that the research is reproducible.
Reproducibility strategies are a benefit to the researcher (in the
moment and in the future) as it leads to better work habits and to
better teamwork and it makes changes to the project easier.
Reproducibility is also of benefit to the scientific community as shared
reproducible research enhances replicability and encourages cumulative
knowledge development (Gandrud 2015).

\hypertarget{fr-principles-reproducible-projects}{%
\subsubsection{Principles of reproducible
projects}\label{fr-principles-reproducible-projects}}

There are a set of guiding principles to accomplish these goals
(Gentleman and Temple Lang 2007; Marwick, Boettiger, and Mullen 2018).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All files should be plain text which means they contain no formatting
  information other than whitespace.
\item
  There should be a clear separation between the data, method, and
  output of research. This should be apparent from the directory
  structure.
\item
  A separation between original data and derived data should be made.
  Original data should be treated as `read-only'. Any changes to the
  original data should be justified, generated by the code, and
  documented (see point 6).
\item
  Each analysis file (script) should represent a particular,
  well-defined step in the research process.
\item
  Each analysis script should be modular --that is, each file should
  correspond to a specific goal in the analysis procedure with input and
  output only corresponding to this step.
\item
  All analysis scripts should be tied together by a `master' script that
  is used to coordinate the execution of all the analysis steps.
\item
  Everything should be documented. This includes data preprocessing,
  analysis steps, script code comments, data description in data
  dictionaries, information about the computing environment and packages
  used to conduct the analysis, and detailed instructions on how to
  reproduce the research.
\end{enumerate}

These seven principles can be physically implemented in countless ways.
In recent years, there has been a growing number of efforts to create R
packages and templates to quickly generate the scaffolding and tools to
facilitate reproducible research. Some notable R packages include
\href{https://jdblischak.github.io/workflowr/}{workflowr} and
\href{http://projecttemplate.net/}{ProjectTemplate} but there are many
other resources for R included on the
\href{https://cran.r-project.org/web/views/ReproducibleResearch.html}{CRAN
Task View for Reproducible Research}. There are many advantages to
working with pre-existing frameworks for the savvy R programmer.

\hypertarget{fr-project-template}{%
\subsubsection{Project template}\label{fr-project-template}}

In this textbook, however, I have developed a project template
(\href{https://github.com/lin380/project_template}{available on GitHub})
which I believe simplifies and makes the process more transparent for
beginning and intermediate R programmers, the directory structure is
provided below.

\begin{verbatim}
#> ../project_template/
#> +-- README.md
#> +-- _pipeline.R
#> +-- analysis
#> |   +-- 1_acquire_data.Rmd
#> |   +-- 2_curate_dataset.Rmd
#> |   +-- 3_transform_dataset.Rmd
#> |   +-- 4_analyze_dataset.Rmd
#> |   +-- 5_generate_article.Rmd
#> |   +-- _session-info.Rmd
#> |   +-- _site.yml
#> |   +-- index.Rmd
#> |   \-- references.bib
#> +-- data
#> |   +-- derived
#> |   \-- original
#> \-- output
#>     +-- figures
#>     \-- results
\end{verbatim}

Let me now describe how this template structure aligns with the seven
principles of quality reproducible research.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All files are plain text (e.g.~\texttt{.R}, \texttt{.Rmd},
  \texttt{.csv}, \texttt{.txt}, etc.).
\item
  There are three main directories \texttt{analysis/}, \texttt{data/},
  and \texttt{ouput/}.
\item
  The \texttt{data/} directory contains sub-directories for
  \texttt{original} (`read-only') data and \texttt{derived} data.
\item
  The \texttt{analysis/} directory contains five scripts which are
  numbered to correspond with their sequential role in the research
  process.
\item
  Each of these analysis scripts are designed to be modular; input and
  output must be explicit and no intermediate objects are carried over
  to other analysis scripts. Dataset output should be written to and
  read from the \texttt{data/derived/} directory. Figures and
  statistical results should be written to and read from
  \texttt{output/figures/} and \texttt{output/results} respectively.
\item
  All of the analysis scripts, and therefore the entire project, are
  tied to the \texttt{\_pipeline.R} script. To reproduce the entire
  project only this script need be run.
\item
  Documentation takes place at many levels. The \texttt{README.md} file
  is the first file that a researcher will consult. It contains a brief
  description of the project goals and how to reproduce the analysis.
  Analysis scripts use the Rmarkdown format (\texttt{.Rmd}). This format
  allows researchers to interleave prose description and executable code
  in the same script. This ensures that the rationale for the steps
  taken are described in prose, the code is made available to consult,
  and that code comments can be added to every line. The
  \texttt{\_sesssion-info.Rmd} script is merged with each analysis
  script to provide information about the computing environment and
  packages used to conduct each step analysis. As this is a template, no
  data or datasets appear. However, once data is acquired and that data
  is curated and transformed, documentation for these resources should
  be documented for each resource in a data dictionary along side the
  data(set) itself.
\end{enumerate}

The aspects of the project template described in points 1-7 together
form the backbone for reproducible research. This template, however,
includes additional functionality to enhance efficient and communicable
research. The \texttt{\_pipeline.R} script executes the analysis scripts
in the \texttt{analysis} directory, but as a side effect also produces
\href{https://lin380.github.io/project_template_demo/}{a working
website} and a journal-ready article for publishing your analysis,
results, and findings to the web in
\href{https://lin380.github.io/project_template_demo/5_generate_article.html}{HTML}
and
\href{https://lin380.github.io/project_template_demo/article.pdf}{PDF}
format. The \texttt{index.Rmd} file is the splash page for the website
and is a good place to house your pre-analysis investigative work
including your research area, problem, aim, and question and to document
your research blueprint including the identification of viable data
resource(s), the key variables for the analysis, the analysis method,
and the method of assessment. All Rmarkdown files provide functionality
for citing and organizing references. The \texttt{references.bib} file
is where references are stored and can be used to include citations that
support your research throughout your project.

\hypertarget{scaffold}{%
\subsection{Scaffold}\label{scaffold}}

This template will allow you to organize your research design and align
it with implementation steps to conduct quality reproducible research.
To set the structure for you to conduct your analysis, you will need to
download or fork and clone this template from the GitHub repository and
then make some adjustments to personalize this template for your
research.

To create a local copy of this project template either:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Download and decompress the
  \href{https://github.com/lin380/project_template/archive/refs/heads/main.zip}{.zip
  file}
\item
  If you have \href{https://github.com/git-guides/install-git}{git}
  installed on your machine and a
  \href{https://github.com/signup?ref_cta=Sign+up\&ref_loc=header+logged+out\&ref_page=\%2F\&source=header-home}{GitHub
  account},
  \href{https://docs.github.com/en/get-started/quickstart/fork-a-repo\#forking-a-repository}{fork
  the repository} to your own GitHub account. Then open a terminal in
  the desired location and
  \href{https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository-from-github/cloning-a-repository\#cloning-a-repository}{clone
  the repository}. If you are using RStudio, you can setup a new RStudio
  Project with the clone using the `New Project\ldots{}' dialog,
  choosing `Version Control', and following the steps.
\end{enumerate}

Before you begin configuring and adding your project-specific details to
this template. Reproduce this project `as-is' to confirm that it builds
on your local machine.

In RStudio or in R session in a Terminal application, open the console
in the root directory of the project. Then run:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"\_pipeline.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

It will take some time to complete, when it does the prompt
(\texttt{\textgreater{}}) in the console will return. Then navigate to
and open \texttt{docs/index.html} in a browser.

Once you have confirmed that the project template builds, then you can
begin to configure the template to reflect your project. There a few
files to consider first. These files are places where the title of your
project should appear.

\begin{itemize}
\tightlist
\item
  \texttt{README.md}
\item
  \texttt{\_pipeline.R}
\item
  \texttt{analysis/index.Rmd}
\end{itemize}

After updating these files, build the project again and make sure that
the new changes appear as you would like them. You are now ready to
start your research project!

\hypertarget{summary-4}{%
\section*{Summary}\label{summary-4}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

The aim of this chapter is to provide the key conceptual and practical
points to guide the development of a viable research project. Good
research is purposive, inquisitive, informed, methodological, and
communicable. It is not, however, always a linear process. Exploring
your area(s) of interest and connecting with existing work will help
couch and refine your research. But practical considerations, such as
the existence of viable data, technical skills, and/ or time constrains,
sometimes pose challenges and require a researcher to rethink and/ or
redirect the research in sometimes small and other times more
significant ways. The process of formulating a research question and
developing a viable research plan is key to supporting viable,
successful, and insightful research. To ensure that the effort to derive
insight from data is of most value to the researcher and the research
community, the research should strive to be methodological and
communicable adopting best practices for reproducible research.

This chapter concludes the Orientation section of this textbook. At this
point the fundamental characteristics of research are in place to move a
project towards implementation. The next section, Preparation, aims to
cover the acquisition, curation, and transformation of data in
preparation for analysis. These are the first steps in putting a
research blueprint into action and by no coincidence the first
components in the Data to Insight Hierarchy. Following the Preparation
section our attention will turn to the implementation of the three
analysis approaches we have covered: inference, prediction, and
exploration. Throughout these next sections we will maintain our aim to
develop methodological and communicable research by connecting our
implementation process to reproducible programming strategies.

\begin{figure}[h]

{\centering \includegraphics[width=6.98in,height=\textheight]{figures/framing-research/framing-research-visual-summary.png}

}

\caption{\label{fig-framing-research-visual-summary-graphic}Framing
research: visual summary}

\end{figure}

\hypertarget{activities-3}{%
\section*{Activities}\label{activities-3}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_5.html}{Project
management with Git, GitHub, and RStudio Cloud}\\
\textbf{How}: Read Recipe 5 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To learn how to use Git, GitHub, and RStudio to manage,
store, and publish reproducible research projects.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/lab_5}{Project management
with Git, GitHub, and RStudio Cloud}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 5.\\
\textbf{Why}: To set up a GitHub account, fork and copy a GitHub
repository to RStudio Cloud, and use R, Git, and GitHub to manage,
store, and publish changes to a reproducible research project.

\end{tcolorbox}

\hypertarget{questions-4}{%
\section*{Questions}\label{questions-4}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Conceptual questions}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  What is the difference between a research question and a research
  hypothesis?
\item
  What is the difference between a research design and a research plan?
\item
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Technical exercises}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Matching research questions with data sources
\item
  Matching research questions with research designs
\item
  Preregistering a research project (?)
\item
  Propose a quantitative research topic (or question if possible).
  Support your topic with supporting literature. (?)
\end{enumerate}

\end{tcolorbox}

\part{Preparation}

At this point we will turn our attention to implementing the specifics
outlined in our research blueprint. This section will group the
components which concern the acquisition, curation, and transformation
of data into a dataset which is prepared to be submitted to analysis. In
each of these three chapters I will outline some of the main
characteristics to consider in each of these research steps and provide
authentic examples of working with R to implement these steps. In
\protect\hyperlink{acquire-data}{Chapter 5} this includes downloads,
working with APIs, and webscraping. In
\protect\hyperlink{curate-data}{Chapter 6} we turn to organize data into
rectangular, or `tidy', format. Depending on the data or dataset
acquired for the research project, the steps necessary to shape our data
into a base dataset will vary, as we will see. In
\protect\hyperlink{transform-data}{Chapter 7} we will work to manipulate
curated datasets to create datasets which are aligned with the research
aim and research question. This often includes normalizing values,
recoding variables, and generating new variables as well as and sourcing
and merging information from other datasets with the dataset to be
submitted for analysis.

\hypertarget{sec-acquire-data}{%
\chapter{Acquire data}\label{sec-acquire-data}}

\begin{quote}
The scariest moment is always just before you start.

―-- Stephen King
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  What are the most common strategies for acquiring corpus data?
\item
  What programmatic steps can we take to ensure the acquisition process
  is reproducible?
\item
  What is the importance of documenting data?
\end{itemize}

\end{tcolorbox}

There are three main ways to acquire corpus data using R that I will
introduce you to: \textbf{downloads}, \textbf{APIs}, and \textbf{web
scraping}. In this chapter we will start by manual and programmatically
downloading a corpus as it is the most straightforward process for the
novice R programmer and typically incurs the least number of steps.
Along the way I will introduce some key R coding concepts including
control statements and custom functions. Next I will cover using R
packages to interface with APIs, both open-access and
authentication-based. APIs will require us to delve into more detail
about R objects and custom functions. Finally acquiring data from the
web via webscraping is the most idiosyncratic and involves both
knowledge of the web, more sophisticated R skills, and often some clever
hacking skills. I will start with a crash course on the structure of web
documents (HTML) and then scale up to a real-world example. To round out
the chapter we will cover the process of ensuring that our data is
documented in such a way as to provide sufficient information to
understand its key sampling characteristics and the source from which it
was drawn.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Swirl}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/swirl}{Loops and
vectorization}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To \ldots.

\end{tcolorbox}

\hypertarget{downloads}{%
\section{Downloads}\label{downloads}}

\hypertarget{manual}{%
\subsection{Manual}\label{manual}}

The first acquisition method I will cover here is inherently
non-reproducible from the standpoint that the programming implementation
cannot acquire the data based solely on running the project code itself.
In other words, it requires manual intervention. Manual downloads are
typical for data resources which are not openly accessible on the public
facing web. These can be resources that require institutional or private
licensing (\href{https://www.ldc.upenn.edu/}{Language Data Consortium},
\href{http://ice-corpora.net/ice/}{International Corpus of English},
\href{https://www.corpusdata.org/}{BYU Corpora}, etc.), require
authorization/ registration (\href{https://archive.mpi.nl/tla/}{The
Language Archive}, \href{https://www.webcorpora.org/}{COW Corpora},
etc.), and/ or are only accessible via resource search interfaces
(\href{https://cesa.arizona.edu/}{Corpus of Spanish in Southern
Arizona}, \href{http://cedel2.learnercorpora.com/}{Corpus Escrito del
Español como L2 (CEDEL2)}, etc.).

Let's work with the CEDEL2 corpus (Lozano 2009) which provides a search
interface and open access to the data through the search interface. The
homepage can be seen in Figure~\ref{fig-ad-show-page-cedel2-1}.

\begin{figure}[h]

{\centering \includegraphics[width=9.28in,height=\textheight]{figures/acquire-data/ad-cedel2-site.png}

}

\caption{\label{fig-ad-show-page-cedel2-1}CEDEL2 Corpus homepage}

\end{figure}

Following the search/ download link you can find a search interface that
allows the user to select the sub-corpus of interest. I've selected the
subcorpus ``Learners of L2 Spanish'' and specified the L1 as English.

\begin{figure}[h]

{\centering \includegraphics[width=9.28in,height=\textheight]{figures/acquire-data/ad-cedel2-search-download.png}

}

\caption{\label{fig-ad-show-page-cedel2-2}Search and download interface
for the CEDEL2 Corpus}

\end{figure}

The `Download' link now appears for this search criteria. Following this
link will provide the user a form to fill out. This particular resource
allows for access to different formats to download (Texts only, Texts
with metadata, CSV (Excel), CSV (Others)). I will select the `CSV
(Others)' option so that the data is structured for easier processing
downstream when we work to curate the data in our next processing step.
Then I will choose to save the CSV in the \texttt{data/original/}
directory of my project and create a sub-directory called
\texttt{cedel2/}.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{└──}\NormalTok{ cedel2}
       \ExtensionTok{└──}\NormalTok{ texts.csv}
\end{Highlighting}
\end{Shaded}

Other resources will inevitably include unique processes to obtaining
the data, but in the end the data should be archived in the research
structure in the \texttt{data/original/} directory and be treated as
`read-only'.

\hypertarget{programmatic}{%
\subsection{Programmatic}\label{programmatic}}

There are many resources that provide corpus data is directly accessible
for which programmatic approaches can be applied. Let's take a look at
how this works starting with the a sample from the Switchboard Corpus, a
corpus of 2,400 telephone conversations by 543 speakers. First we
navigate to the site with a browser and download the file that we are
looking for. In this case I found the Switchboard Corpus on the
\href{http://www.nltk.org/nltk_data/}{NLTK data repository site}. More
often than not this file will be some type of compressed archive file
with an extension such as \texttt{.zip} or \texttt{.tz}, which is the
case here. Archive files make downloading large single files or multiple
files easy by grouping files and directories into one file. In R we can
used the \texttt{download.file()} function from the base R
library\footnote{Remember base R packages are installed by default with
  R and are loaded and accessible by default in each R session.}. There
are a number of \textbf{arguments} that a function may require or
provide optionally. The \texttt{download.file()} function minimally
requires two: \texttt{url} and \texttt{destfile}. That is the file to
download and the location where it is to be saved to disk.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Download .zip file and write to disk}
\FunctionTok{download.file}\NormalTok{(}\AttributeTok{url =} \StringTok{"https://raw.githubusercontent.com/nltk/nltk\_data/gh{-}pages/packages/corpora/switchboard.zip"}\NormalTok{, }\AttributeTok{destfile =} \StringTok{"../data/original/switchboard.zip"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As we can see looking at the directory structure for \texttt{data/} the
\texttt{switchboard.zip} file has been downloaded.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{└──}\NormalTok{ switchboard.zip}
\end{Highlighting}
\end{Shaded}

Once an archive file is downloaded, however, the file needs to be
`decompressed' to reveal the file structure. To decompress this file we
use the \texttt{unzip()} function with the arguments \texttt{zipfile}
pointing to the \texttt{.zip} file and \texttt{exdir} specifying the
directory where we want the files to be extracted to.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Decompress .zip file and extract to our target directory}
\FunctionTok{unzip}\NormalTok{(}\AttributeTok{zipfile =} \StringTok{"../data/original/switchboard.zip"}\NormalTok{, }\AttributeTok{exdir =} \StringTok{"../data/original/"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The directory structure of \texttt{data/} now should look like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{├──}\NormalTok{ switchboard}
    \ExtensionTok{│}\NormalTok{   ├── README}
    \ExtensionTok{│}\NormalTok{   ├── discourse}
    \ExtensionTok{│}\NormalTok{   ├── disfluency}
    \ExtensionTok{│}\NormalTok{   ├── tagged}
    \ExtensionTok{│}\NormalTok{   ├── timed{-}transcript}
    \ExtensionTok{│}\NormalTok{   └── transcript}
    \ExtensionTok{└──}\NormalTok{ switchboard.zip}
\end{Highlighting}
\end{Shaded}

At this point we have acquired the data programmatically and with this
code as part of our workflow anyone could run this code and reproduce
the same results. The code as it is, however, is not ideally efficient.
Firstly the \texttt{switchboard.zip} file is not strictly needed after
we decompress it and it occupies disk space if we keep it. And second,
each time we run this code the file will be downloaded from the remote
serve leading to unnecessary data transfer and server traffic. Let's
tackle each of these issues in turn.

To avoid writing the \texttt{switchboard.zip} file to disk (long-term)
we can use the \texttt{tempfile()} function to open a temporary holding
space for the file. This space can then be used to store the file, unzip
it, and then the temporary file will be destroyed. We assign the
temporary space to an R object we will name \texttt{temp} with the
\texttt{tempfile()} function. This object can now be used as the value
of the argument \texttt{destfile} in the \texttt{download.file()}
function. Let's also assign the web address to another object
\texttt{url} which we will use as the value of the \texttt{url}
argument.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a temporary file space for our .zip file}
\NormalTok{temp }\OtherTok{\textless{}{-}} \FunctionTok{tempfile}\NormalTok{()}
\CommentTok{\# Assign our web address to \textasciigrave{}url\textasciigrave{}}
\NormalTok{url }\OtherTok{\textless{}{-}} \StringTok{"https://raw.githubusercontent.com/nltk/nltk\_data/gh{-}pages/packages/corpora/switchboard.zip"}
\CommentTok{\# Download .zip file and write to disk}
\FunctionTok{download.file}\NormalTok{(url, temp)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

In the previous code I've used the values stored in the objects
\texttt{url} and \texttt{temp} in the \texttt{download.file()} function
without specifying the argument names --only providing the names of the
objects. R will assume that values of a function map to the ordering of
the arguments. If your values do not map to ordering of the arguments
you are required to specify the argument name and the value. To view the
ordering of objects hit \texttt{TAB} after entering the function name or
consult the function documentation by prefixing the function name with
\texttt{?} and hitting \texttt{ENTER}.

\end{tcolorbox}

At this point our downloaded file is stored temporarily on disk and can
be accessed and decompressed to our target directory using \texttt{temp}
as the value for the argument \texttt{zipfile} from the \texttt{unzip()}
function. I've assigned our target directory path to
\texttt{target\_dir} and used it as the value for the argument
\texttt{exdir} to prepare us for the next tweak on our approach.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Assign our target directory to \textasciigrave{}target\_dir\textasciigrave{}}
\NormalTok{target\_dir }\OtherTok{\textless{}{-}} \StringTok{"../data/original/"}
\CommentTok{\# Decompress .zip file and extract to our target directory}
\FunctionTok{unzip}\NormalTok{(}\AttributeTok{zipfile =}\NormalTok{ temp, }\AttributeTok{exdir =}\NormalTok{ target\_dir)}
\end{Highlighting}
\end{Shaded}

Our directory structure now looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{└──}\NormalTok{ switchboard}
        \ExtensionTok{├──}\NormalTok{ README}
        \ExtensionTok{├──}\NormalTok{ discourse}
        \ExtensionTok{├──}\NormalTok{ disfluency}
        \ExtensionTok{├──}\NormalTok{ tagged}
        \ExtensionTok{├──}\NormalTok{ timed{-}transcript}
        \ExtensionTok{└──}\NormalTok{ transcript}
\end{Highlighting}
\end{Shaded}

The second issue I raised concerns the fact that running this code as
part of our project will repeat the download each time. Since we would
like to be good citizens and avoid unnecessary traffic on the web it
would be nice if our code checked to see if we already have the data on
disk and if it exists, then skip the download, if not then download it.

To achieve this we need to introduce two new functions \texttt{if()} and
\texttt{dir.exists()}. \texttt{dir.exists()} takes a path to a directory
as an argument and returns the logical value, \texttt{TRUE}, if that
directory exists, and \texttt{FALSE} if it does not. \texttt{if()}
evaluates logical statements and processes subsequent code based on the
logical value it is passed as an argument. Let's look at a toy example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num }\OtherTok{\textless{}{-}} \DecValTok{1}
\ControlFlowTok{if}\NormalTok{(num }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) \{ }
  \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is 1"}\NormalTok{) }
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
  \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is not 1"}\NormalTok{)}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 1 is 1
\end{verbatim}

I assigned \texttt{num} to the value \texttt{1} and created a logical
evaluation \texttt{num\ ==} whose result is passed as the argument to
\texttt{if()}. If the statement returns \texttt{TRUE} then the code
withing the first set of curly braces \texttt{\{...\}} is run. If
\texttt{num\ ==\ 1} is false, like in the code below, the code withing
the braces following the \texttt{else} will be run.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num }\OtherTok{\textless{}{-}} \DecValTok{2}
\ControlFlowTok{if}\NormalTok{(num }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) \{ }
  \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is 1"}\NormalTok{) }
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
  \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is not 1"}\NormalTok{)}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 2 is not 1
\end{verbatim}

The function \texttt{if()} is one of various functions that are called
\textbf{control statements}. Theses functions provide a lot of power to
make dynamic choices as code is run.

Before we get back to our key objective to avoid downloading resources
that we already have on disk, let me introduce another strategy to
making code more powerful and ultimately more efficient and as well as
more legible --the \textbf{custom function}. Custom functions are
functions that the user writes to create a set of procedures that can be
run in similar contexts. I've created a custom function named
\texttt{eval\_num()} below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eval\_num }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(num) \{}
  \ControlFlowTok{if}\NormalTok{(num }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) \{ }
  \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is 1"}\NormalTok{) }
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
  \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is not 1"}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Let's take a closer look at what's going on here. The function
\texttt{function()} creates a function in which the user decides what
arguments are necessary for the code to perform its task. In this case
the only necessary argument is the object to store a numeric value to be
evaluated. I've called it \texttt{num} because it reflects the name of
the object in our toy example, but there is nothing special about this
name. It's only important that the object names be consistently used.
I've included our previous code (except for the hard-coded assignment of
\texttt{num}) inside the curly braces and assigned the entire code chunk
to \texttt{eval\_num}.

We can now use the function \texttt{eval\_num()} to perform the task of
evaluating whether a value of \texttt{num} is or is not equal to
\texttt{1}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{eval\_num}\NormalTok{(}\AttributeTok{num =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 1 is 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{eval\_num}\NormalTok{(}\AttributeTok{num =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 2 is not 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{eval\_num}\NormalTok{(}\AttributeTok{num =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 3 is not 1
\end{verbatim}

I've put these coding strategies together with our previous code in a
custom function I named \texttt{get\_zip\_data()}. There is a lot going
on here. Take a look first and see if you can follow the logic involved
given what you now know.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_zip\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(url, target\_dir) \{}
  \CommentTok{\# Function: to download and decompress a .zip file to a target directory}
  
  \CommentTok{\# Check to see if the data already exists}
  \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{dir.exists}\NormalTok{(target\_dir)) \{ }\CommentTok{\# if data does not exist, download/ decompress}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Creating target data directory }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
    \FunctionTok{dir.create}\NormalTok{(}\AttributeTok{path =}\NormalTok{ target\_dir, }\AttributeTok{recursive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# create target data directory}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Downloading data... }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
\NormalTok{    temp }\OtherTok{\textless{}{-}} \FunctionTok{tempfile}\NormalTok{() }\CommentTok{\# create a temporary space for the file to be written to}
    \FunctionTok{download.file}\NormalTok{(}\AttributeTok{url =}\NormalTok{ url, }\AttributeTok{destfile =}\NormalTok{ temp) }\CommentTok{\# download the data to the temp file}
    \FunctionTok{unzip}\NormalTok{(}\AttributeTok{zipfile =}\NormalTok{ temp, }\AttributeTok{exdir =}\NormalTok{ target\_dir, }\AttributeTok{junkpaths =} \ConstantTok{TRUE}\NormalTok{) }\CommentTok{\# decompress the temp file in the target directory}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Data downloaded! }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{ }\CommentTok{\# if data exists, don\textquotesingle{}t download it again}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Data already exists }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

OK. You should have recognized the general steps in this function: the
argument \texttt{url} and \texttt{target\_dir} specify where to get the
data and where to write the decompressed files, the \texttt{if()}
statement evaluates whether the data already exists, if not
(\texttt{!dir.exists(target\_dir)}) then the data is downloaded and
decompressed, if it does exist (\texttt{else}) then it is not
downloaded.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

The prefixed \texttt{!} in the logical expression
\texttt{dir.exists(target\_dir)} returns the opposite logical value.
This is needed in this case so when the target directory exists, the
expression will return \texttt{FALSE}, not \texttt{TRUE}, and therefore
not proceed in downloading the resource.

\end{tcolorbox}

There are a couple key tweaks I've added that provide some additional
functionality. For one I've included the function \texttt{dir.create()}
to create the target directory where the data will be written. I've also
added an additional argument to the \texttt{unzip()} function,
\texttt{junkpaths\ =\ TRUE}. Together these additions allow the user to
create an arbitrary directory path where the files, and only the files,
will be extracted to on our disk. This will discard the containing
directory of the \texttt{.zip} file which can be helpful when we want to
add multiple \texttt{.zip} files to the same target directory.

A practical scenario where this applies is when we want to download data
from a corpus that is contained in multiple \texttt{.zip} files but
still maintain these files in a single primary data directory. Take for
example the
\href{http://www.linguistics.ucsb.edu/research/santa-barbara-corpus}{Santa
Barbara Corpus}. This corpus resource includes a series of interviews in
which there is one \texttt{.zip} file, \texttt{SBCorpus.zip} which
contains the
\href{http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip}{transcribed
interviews} and another \texttt{.zip} file, \texttt{metadata.zip} which
organizes the
\href{http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip}{meta-data}
associated with each speaker. Applying our initial strategy to download
and decompress the data will lead to the following directory structure:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{├──}\NormalTok{ SBCorpus}
    \ExtensionTok{│  }\NormalTok{ ├── TRN}
    \ExtensionTok{│  }\NormalTok{ └── \_\_MACOSX}
    \ExtensionTok{│  }\NormalTok{     └── TRN}
    \ExtensionTok{└──}\NormalTok{ metadata}
        \ExtensionTok{└──}\NormalTok{ \_\_MACOSX}
\end{Highlighting}
\end{Shaded}

By applying our new custom function \texttt{get\_zip\_data()} to the
transcriptions and then the meta-data we can better organize the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Download corpus transcriptions}
\FunctionTok{get\_zip\_data}\NormalTok{(}\AttributeTok{url =} \StringTok{"http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip"}\NormalTok{, }\AttributeTok{target\_dir =} \StringTok{"../data/original/sbc/transcriptions/"}\NormalTok{)}

\CommentTok{\# Download corpus meta{-}data}
\FunctionTok{get\_zip\_data}\NormalTok{(}\AttributeTok{url =} \StringTok{"http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip"}\NormalTok{, }\AttributeTok{target\_dir =} \StringTok{"../data/original/sbc/meta{-}data/"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{└──}\NormalTok{ sbc}
        \ExtensionTok{├──}\NormalTok{ meta{-}data}
        \ExtensionTok{└──}\NormalTok{ transcriptions}
\end{Highlighting}
\end{Shaded}

If we add data from other sources we can keep them logical separate and
allow our data collection to scale without creating unnecessary
complexity. Let's add the Switchboard Corpus sample using our
\texttt{get\_zip\_data()} function to see this in action.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Download corpus}
\FunctionTok{get\_zip\_data}\NormalTok{(}\AttributeTok{url =} \StringTok{"https://raw.githubusercontent.com/nltk/nltk\_data/gh{-}pages/packages/corpora/switchboard.zip"}\NormalTok{, }\AttributeTok{target\_dir =} \StringTok{"../data/original/scs/"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{├──}\NormalTok{ sbc}
    \ExtensionTok{│}\NormalTok{   ├── meta{-}data}
    \ExtensionTok{│}\NormalTok{   └── transcriptions}
    \ExtensionTok{└──}\NormalTok{ scs}
        \ExtensionTok{├──}\NormalTok{ README}
        \ExtensionTok{├──}\NormalTok{ discourse}
        \ExtensionTok{├──}\NormalTok{ disfluency}
        \ExtensionTok{├──}\NormalTok{ tagged}
        \ExtensionTok{├──}\NormalTok{ timed{-}transcript}
        \ExtensionTok{└──}\NormalTok{ transcript}
\end{Highlighting}
\end{Shaded}

At this point we have what we need to continue to the next step in our
data analysis project. But before we go, we should do some housekeeping
to document and organize this process to make our work reproducible. We
will take advantage of the \texttt{project-template} directory
structure, seen below.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{├──}\NormalTok{ README.md}
\ExtensionTok{├──}\NormalTok{ \_pipeline.R}
\ExtensionTok{├──}\NormalTok{ analysis}
\ExtensionTok{│}\NormalTok{   ├── 1\_acquire\_data.Rmd}
\ExtensionTok{│}\NormalTok{   ├── 2\_curate\_dataset.Rmd}
\ExtensionTok{│}\NormalTok{   ├── 3\_transform\_dataset.Rmd}
\ExtensionTok{│}\NormalTok{   ├── 4\_analyze\_dataset.Rmd}
\ExtensionTok{│}\NormalTok{   ├── 5\_generate\_article.Rmd}
\ExtensionTok{│}\NormalTok{   ├── \_session{-}info.Rmd}
\ExtensionTok{│}\NormalTok{   ├── \_site.yml}
\ExtensionTok{│}\NormalTok{   ├── index.Rmd}
\ExtensionTok{│}\NormalTok{   └── references.bib}
\ExtensionTok{├──}\NormalTok{ data}
\ExtensionTok{│}\NormalTok{   ├── derived}
\ExtensionTok{│}\NormalTok{   └── original}
\ExtensionTok{│}\NormalTok{       ├── sbc}
\ExtensionTok{│}\NormalTok{       └── scs}
\ExtensionTok{├──}\NormalTok{ functions}
\ExtensionTok{└──}\NormalTok{ output}
    \ExtensionTok{├──}\NormalTok{ figures}
    \ExtensionTok{└──}\NormalTok{ results}
\end{Highlighting}
\end{Shaded}

First it is good practice to separate custom functions from our
processing scripts. We can create a file in our \texttt{functions/}
directory named \texttt{acquire\_functions.R} and add our custom
function \texttt{get\_zip\_data()} there.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

Note that that the \texttt{acquire\_functions.R} file is an R script,
not an Rmarkdown document. Therefore code chunks that are used in
\texttt{.Rmd} files are not used, only the R code itself.

\end{tcolorbox}

We then use the \texttt{source()} function to read that function into
our current script to make it available to use as needed. It is good
practice to source your functions in the SETUP section of your script.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load custom functions for this project}
\FunctionTok{source}\NormalTok{(}\AttributeTok{file =} \StringTok{"../functions/acquire\_functions.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this section, to sum up, we've covered how to access, download, and
organize data contained in .zip files; the most common format for
language data found on repositories and individual sites. This included
an introduction to a few key R programming concepts and strategies
including using functions, writing custom functions, and controlling
program flow with control statements. Our approach was to gather data
while also keeping in mind the reproducibility of the code. To this end
I introduced programming strategies for avoiding unnecessary web traffic
(downloads), scalable directory creation, and data documentation.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

The custom function \texttt{get\_zip\_data()} works with \texttt{.zip}
files. There are many other compressed file formats (e.g.~\texttt{.gz},
\texttt{.tar}, \texttt{.tgz}), however. In the R package \texttt{tadr}
that accompanies this coursebook, a modified version of the
\texttt{get\_zip\_data()} function, \texttt{get\_compressed\_data()},
extends the same logic to deal with a wider range of compressed file
formats, including \texttt{.zip} files.

Explore this function's documentation
(\texttt{?tadr::get\_compressed\_data()}) and/ or view the code
(\texttt{tadr::get\_compressed\_data}) to better understand this
function.

\end{tcolorbox}

\hypertarget{apis}{%
\section{APIs}\label{apis}}

A convenient alternative method for acquiring data in R is through
package interfaces to web services. These interfaces are built using R
code to make connections with resources on the web through
\textbf{Application Programming Interfaces} (APIs). Websites such as
Project Gutenberg, Twitter, Facebook, and many others provide APIs to
allow access to their data under certain conditions, some more limiting
for data collection than others. Programmers (like you!) in the R
community take up the task of wrapping calls to an API with R code to
make accessing that data from R possible. For example,
\href{https://CRAN.R-project.org/package=gutenbergr}{gutenbergr}
provides access to Project Gutenberg,
\href{https://CRAN.R-project.org/package=rtweet}{rtweet} to Twitter, and
\href{https://CRAN.R-project.org/package=Rfacebook}{Rfacebook} to
Facebook.\footnote{See Section @ref(sources) for a list of some other
  API packages.}

\hypertarget{open-access}{%
\subsection{Open access}\label{open-access}}

Using R package interfaces, however, often requires some more knowledge
about R objects and functions. Let's take a look at how to access data
from Project Gutenberg through the \texttt{gutenbergr} package. Along
the way we will touch upon various functions and concepts that are key
to working with the R data types vectors and data frames including
filtering and writing tabular data to disk in plain-text format.

To get started let's install and/ or load the \texttt{gutenbergr}
package. If a package is not part of the R base library, we cannot
assume that the user will have the package in their library. The
standard approach for installing and then loading a package is by using
the \texttt{install.packages()} function and then calling
\texttt{library()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"gutenbergr"}\NormalTok{) }\CommentTok{\# install \textasciigrave{}gutenbergr\textasciigrave{} package}
\FunctionTok{library}\NormalTok{(gutenbergr) }\CommentTok{\# load the \textasciigrave{}gutenbergr\textasciigrave{} package}
\end{Highlighting}
\end{Shaded}

This approach works just fine, but luck has it that there is an R
package for installing and loading packages! The
\href{https://CRAN.R-project.org/package=pacman}{pacman} package
includes a set of functions for managing packages. A very useful one is
\texttt{p\_load()} which will look for a package on a system, load it if
it is found, and install and then load it if it is not found. This helps
potentially avoid using unnecessary bandwidth to install packages that
may already exist on a user's system. But, to use \texttt{pacman} we
need to include the code to install and load it with the functions
\texttt{install.packages()} and \texttt{library()}. I've included some
code that will mimic the behavior of \texttt{p\_load()} for installing
\texttt{pacman} itself, but as you can see it is not elegant, luckily
it's only used once as we add it to the SETUP section of our master
file, \texttt{\_pipeline.R}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load \textasciigrave{}pacman\textasciigrave{}. If not installed, install then load.}
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(}\StringTok{"pacman"}\NormalTok{, }\AttributeTok{character.only =} \ConstantTok{TRUE}\NormalTok{)) \{}
  \FunctionTok{install.packages}\NormalTok{(}\StringTok{"pacman"}\NormalTok{)}
  \FunctionTok{library}\NormalTok{(}\StringTok{"pacman"}\NormalTok{, }\AttributeTok{character.only =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now that we have \texttt{pacman} installed and loaded into our R
session, let's use the \texttt{p\_load()} function to make sure to
install/ load the two packages we will need for the upcoming tasks. If
you are following along with the \texttt{project\_template}, add this
code within the SETUP section of the \texttt{1\_acquire\_data.Rmd} file.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Script{-}specific options or packages}
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(tidyverse, gutenbergr)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

Note that the arguments \texttt{tidyverse} and \texttt{gutenbergr} are
comma-separated but not quoted when using \texttt{p\_load()}. When using
\texttt{install.packages()} to install, package names need to be quoted
(character strings). \texttt{library()} can take quotes or no quotes,
but only one package at a time.

\end{tcolorbox}

Project Gutenberg provides access to thousands of texts in the public
domain. The \texttt{gutenbergr} package contains a set of tables, or
\textbf{data frames} in R speak, that index the meta-data for these
texts broken down by text (\texttt{gutenberg\_metadata}), author
(\texttt{gutenberg\_authors}), and subject
(\texttt{gutenberg\_subjects}). I'll use the \texttt{glimpse()} function
loaded in the
\href{https://CRAN.R-project.org/package=tidyverse}{tidyverse} package
\footnote{\texttt{tidyverse} is not a typical package. It is a set of
  packages: \texttt{ggplot2}, \texttt{dplyr}, \texttt{tidyr},
  \texttt{readr}, \texttt{purrr}, and \texttt{tibble}. These packages
  are all installed/ loaded with \texttt{tidyverse} and form the
  backbone for the type of work you will typically do in most analyses.}
to summarize the structure of these data frames.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(gutenberg\_metadata) }\CommentTok{\# summarize text meta{-}data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 69,199
#> Columns: 8
#> $ gutenberg_id        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,~
#> $ title               <chr> "The Declaration of Independence of the United Sta~
#> $ author              <chr> "Jefferson, Thomas", "United States", "Kennedy, Jo~
#> $ gutenberg_author_id <int> 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7, 8, ~
#> $ language            <chr> "en", "en", "en", "en", "en", "en", "en", "en", "e~
#> $ gutenberg_bookshelf <chr> "Politics/American Revolutionary War/United States~
#> $ rights              <chr> "Public domain in the USA.", "Public domain in the~
#> $ has_text            <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(gutenberg\_authors) }\CommentTok{\# summarize authors meta{-}data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 21,323
#> Columns: 7
#> $ gutenberg_author_id <int> 1, 3, 4, 5, 7, 8, 9, 10, 12, 14, 16, 17, 18, 20, 2~
#> $ author              <chr> "United States", "Lincoln, Abraham", "Henry, Patri~
#> $ alias               <chr> "U.S.A.", NA, NA, NA, "Dodgson, Charles Lutwidge",~
#> $ birthdate           <int> NA, 1809, 1736, 1849, 1832, NA, 1819, 1860, 1805, ~
#> $ deathdate           <int> NA, 1865, 1799, 1931, 1898, NA, 1891, 1937, 1844, ~
#> $ wikipedia           <chr> "https://en.wikipedia.org/wiki/United_States", "ht~
#> $ aliases             <chr> NA, "United States President (1861-1865)/Lincoln, ~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(gutenberg\_subjects) }\CommentTok{\# summarize subjects meta{-}data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 230,993
#> Columns: 3
#> $ gutenberg_id <int> 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, ~
#> $ subject_type <chr> "lcsh", "lcsh", "lcc", "lcc", "lcsh", "lcsh", "lcc", "lcc~
#> $ subject      <chr> "United States -- History -- Revolution, 1775-1783 -- Sou~
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

The \texttt{gutenberg\_metadata}, \texttt{gutenberg\_authors}, and
\texttt{gutenberg\_subjects} are periodically updated. To check to see
when each data frame was last updated run:

\texttt{attr(gutenberg\_metadata,\ "date\_updated")}

\end{tcolorbox}

To download the text itself we use the \texttt{gutenberg\_download()}
function which takes one required argument, \texttt{gutenberg\_id}. The
\texttt{gutenberg\_download()} function is what is known as
`vectorized', that is, it can take a single value or multiple values for
the argument \texttt{gutenberg\_id}. Vectorization refers to the process
of applying a function to each of the elements stored in a
\textbf{vector} --a primary object type in R. A vector is a grouping of
values of one of various types including character (\texttt{chr}),
integer (\texttt{int}), double (\texttt{dbl}), and logical
(\texttt{lgl}) and a data frame is a grouping of vectors. The
\texttt{gutenberg\_download()} function takes an integer vector which
can be manually added or selected from the \texttt{gutenberg\_metadata}
or \texttt{gutenberg\_subjects} data frames using the \texttt{\$}
operator (e.g.~\texttt{gutenberg\_metadata\$gutenberg\_id}).

Let's first add them manually here as a toy example by generating a
vector of integers from 1 to 5 assigned to the variable name
\texttt{ids}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ids }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5} \CommentTok{\# integer vector of values 1 to 5}
\NormalTok{ids}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 1 2 3 4 5
\end{verbatim}

To download the works from Project Gutenberg corresponding to the
\texttt{gutenberg\_id}s 1 to 5, we pass the \texttt{ids} object to the
\texttt{gutenberg\_download()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{works\_sample }\OtherTok{\textless{}{-}} \FunctionTok{gutenberg\_download}\NormalTok{(}\AttributeTok{gutenberg\_id =}\NormalTok{ ids) }\CommentTok{\# download works with \textasciigrave{}gutenberg\_id\textasciigrave{} 1{-}5}
\FunctionTok{glimpse}\NormalTok{(works\_sample) }\CommentTok{\# summarize \textasciigrave{}works\textasciigrave{} dataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 2,959
#> Columns: 2
#> $ gutenberg_id <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~
#> $ text         <chr> "December, 1971  [Etext #1]", "", "", "The Project Gutenb~
\end{verbatim}

Two attributes are returned: \texttt{gutenberg\_id} and \texttt{text}.
The \texttt{text} column contains values for each line of text
(delimited by a carriage return) for each of the 5 works we downloaded.
There are many more attributes available from the Project Gutenberg API
that can be accessed by passing a character vector of the attribute
names to the argument \texttt{meta\_fields}. The column names of the
\texttt{gutenberg\_metadata} data frame contains the available
attributes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(gutenberg\_metadata) }\CommentTok{\# print the column names of the \textasciigrave{}gutenberg\_metadata\textasciigrave{} data frame}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "gutenberg_id"        "title"               "author"             
#> [4] "gutenberg_author_id" "language"            "gutenberg_bookshelf"
#> [7] "rights"              "has_text"
\end{verbatim}

Let's augment our previous download with the title and author of each of
the works. To create a character vector we use the \texttt{c()}
function, then, quote and delimit the individual elements of the vector
with a comma.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# download works with \textasciigrave{}gutenberg\_id\textasciigrave{} 1{-}5 including \textasciigrave{}title\textasciigrave{} and \textasciigrave{}author\textasciigrave{} as attributes}
\NormalTok{works\_sample }\OtherTok{\textless{}{-}} 
  \FunctionTok{gutenberg\_download}\NormalTok{(}\AttributeTok{gutenberg\_id =}\NormalTok{ ids, }\AttributeTok{meta\_fields =} \FunctionTok{c}\NormalTok{(}\StringTok{"title"}\NormalTok{, }\StringTok{"author"}\NormalTok{)) }\CommentTok{\#}

\FunctionTok{glimpse}\NormalTok{(works\_sample) }\CommentTok{\# summarize dataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 2,959
#> Columns: 4
#> $ gutenberg_id <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~
#> $ text         <chr> "December, 1971  [Etext #1]", "", "", "The Project Gutenb~
#> $ title        <chr> "The Declaration of Independence of the United States of ~
#> $ author       <chr> "Jefferson, Thomas", "Jefferson, Thomas", "Jefferson, Tho~
\end{verbatim}

Now, in a more practical scenario we would like to select the values of
\texttt{gutenberg\_id} by some principled query such as works from a
specific author, language, or subject. To do this we first query either
the \texttt{gutenberg\_metadata} data frame or the
\texttt{gutenberg\_subjects} data frame. Let's say we want to download a
random sample of 10 works from English Literature (Library of Congress
Classification, ``PR''). Using the \texttt{dplyr::filter()} function
(\texttt{dplyr} is part of the \texttt{tidyverse} package set) we first
extract all the Gutenberg ids from \texttt{gutenberg\_subjects} where
\texttt{subject\_type\ ==\ "lcc"} and \texttt{subject\ ==\ "PR"}
assigning the result to \texttt{ids}.\footnote{See
  \href{https://www.loc.gov/catdir/cpso/lcco/}{Library of Congress
  Classification} documentation for a complete list of subject codes.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# filter for only English literature}
\NormalTok{ids }\OtherTok{\textless{}{-}} 
  \FunctionTok{filter}\NormalTok{(gutenberg\_subjects, subject\_type }\SpecialCharTok{==} \StringTok{"lcc"}\NormalTok{, subject }\SpecialCharTok{==} \StringTok{"PR"}\NormalTok{)}
\FunctionTok{glimpse}\NormalTok{(ids)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 9,900
#> Columns: 3
#> $ gutenberg_id <int> 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43, 46, 58, 60, 8~
#> $ subject_type <chr> "lcc", "lcc", "lcc", "lcc", "lcc", "lcc", "lcc", "lcc", "~
#> $ subject      <chr> "PR", "PR", "PR", "PR", "PR", "PR", "PR", "PR", "PR", "PR~
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

The operators \texttt{=} and \texttt{==} are not equivalents.
\texttt{==} is used for logical evaluation and \texttt{=} is an
alternate notation for variable assignment (\texttt{\textless{}-}).

\end{tcolorbox}

The \texttt{gutenberg\_subjects} data frame does not contain information
as to whether a \texttt{gutenberg\_id} is associated with a plain-text
version. To limit our query to only those English Literature works with
text, we filter the \texttt{gutenberg\_metadata} data frame by the ids
we have selected in \texttt{ids} and the attribute \texttt{has\_text} in
the \texttt{gutenberg\_metadata} data frame.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter for only those works that have text}
\NormalTok{ids\_has\_text }\OtherTok{\textless{}{-}} 
  \FunctionTok{filter}\NormalTok{(gutenberg\_metadata, }
\NormalTok{         gutenberg\_id }\SpecialCharTok{\%in\%}\NormalTok{ ids}\SpecialCharTok{$}\NormalTok{gutenberg\_id, }
\NormalTok{         has\_text }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{glimpse}\NormalTok{(ids\_has\_text)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 9,548
#> Columns: 8
#> $ gutenberg_id        <int> 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43, 46, 58~
#> $ title               <chr> "Alice's Adventures in Wonderland", "Through the L~
#> $ author              <chr> "Carroll, Lewis", "Carroll, Lewis", "Carroll, Lewi~
#> $ gutenberg_author_id <int> 7, 7, 7, 10, 17, 17, 23, 30, 30, 35, 35, 37, 17, 4~
#> $ language            <chr> "en", "en", "en", "en", "en", "en", "en", "en", "e~
#> $ gutenberg_bookshelf <chr> "Children's Literature", "Best Books Ever Listings~
#> $ rights              <chr> "Public domain in the USA.", "Public domain in the~
#> $ has_text            <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR~
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

A couple R programming notes on the code phrase
\texttt{gutenberg\_id\ \%in\%\ ids\$gutenberg\_id}. First, the
\texttt{\$} symbol in \texttt{ids\$gutenberg\_id} is the programmatic
way to target a particular column in an R data frame. In this example we
select the \texttt{ids} data frame and the column
\texttt{gutenberg\_id}, which is a integer vector. The
\texttt{gutenberg\_id} variable that precedes the \texttt{\%in\%}
operator does not need an explicit reference to a data frame because the
primary argument of the \texttt{filter()} function is this data frame
(\texttt{gutenberg\_metadata}). Second, the \texttt{\%in\%} operator
logically evaluates whether the vector elements in
\texttt{gutenberg\_metadata\$gutenberg\_ids} are also found in the
vector \texttt{ids\$gutenberg\_id} returning \texttt{TRUE} and
\texttt{FALSE} accordingly. This effectively filters those ids which are
not in both vectors.

\end{tcolorbox}

As we can see the number of works with text is fewer than the number of
works listed, 9900 versus 9548. Now we can safely do our random
selection of 10 works, with the function \texttt{slice\_sample()} and be
confident that the ids we select will contain text when we take the next
step by downloading the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# make the sampling reproducible}
\NormalTok{ids\_sample }\OtherTok{\textless{}{-}} \FunctionTok{slice\_sample}\NormalTok{(ids\_has\_text, }\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\CommentTok{\# sample 10 works}
\FunctionTok{glimpse}\NormalTok{(ids\_sample) }\CommentTok{\# summarize the dataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 10
#> Columns: 8
#> $ gutenberg_id        <int> 10542, 10734, 60253, 13776, 7532, 67002, 16604, 28~
#> $ title               <chr> "The Boats of the \"Glen Carrig\"\r\nBeing an acco~
#> $ author              <chr> "Hodgson, William Hope", NA, "Orczy, Emmuska Orczy~
#> $ gutenberg_author_id <int> 3260, NA, 45, NA, NA, 30, 3579, 6137, 797, 1865
#> $ language            <chr> "en", "en", "en", "en", "en", "hu", "en", "en", "e~
#> $ gutenberg_bookshelf <chr> "Horror", NA, NA, NA, NA, NA, NA, "Humor", NA, NA
#> $ rights              <chr> "Public domain in the USA.", "Public domain in the~
#> $ has_text            <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{works\_pr }\OtherTok{\textless{}{-}} \FunctionTok{gutenberg\_download}\NormalTok{(}\AttributeTok{gutenberg\_id =}\NormalTok{ ids\_sample}\SpecialCharTok{$}\NormalTok{gutenberg\_id, }
                               \AttributeTok{meta\_fields =} \FunctionTok{c}\NormalTok{(}\StringTok{"author"}\NormalTok{, }\StringTok{"title"}\NormalTok{))}
\FunctionTok{glimpse}\NormalTok{(works\_pr) }\CommentTok{\# summarize the dataset}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{works\_pr }\OtherTok{\textless{}{-}} \FunctionTok{gutenberg\_download}\NormalTok{(}\AttributeTok{gutenberg\_id =}\NormalTok{ ids\_sample}\SpecialCharTok{$}\NormalTok{gutenberg\_id, }
                               \AttributeTok{meta\_fields =} \FunctionTok{c}\NormalTok{(}\StringTok{"author"}\NormalTok{, }\StringTok{"title"}\NormalTok{))}
\FunctionTok{write\_rds}\NormalTok{(works\_pr, }\AttributeTok{file =} \StringTok{"data/acquire{-}data/gutenberg\_works\_pr.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 86,086
#> Columns: 4
#> $ gutenberg_id <int> 7532, 7532, 7532, 7532, 7532, 7532, 7532, 7532, 7532, 753~
#> $ text         <chr> "A BOOK OF OLD BALLADS", "", "Selected and with an Introd~
#> $ author       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~
#> $ title        <chr> "A Book of Old Ballads — Volume 2", "A Book of Old Ballad~
\end{verbatim}

At this point we have data and could move on to processing this dataset
in preparation for analysis. However, we are aiming for a reproducible
workflow and this code does not conform to our principle of modularity:
each subsequent step in our analysis will depend on running this code
first. Furthermore, running this code as it is creates issues with
bandwidth, as in our previous examples from direct downloads. To address
modularity we will write the dataset to disk in \textbf{plain-text
format}. In this way each subsequent step in our analysis can access the
dataset locally. To address bandwidth concerns, we will devise a method
for checking to see if the dataset is already downloaded and skip the
download, if possible, to avoid accessing the Project Gutenberg server
unnecessarily.

To write our data frame to disk we will export it into a standard
plain-text format for two-dimensional datasets: a CSV file
(comma-separated value). The CSV structure for this dataset will look
like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{works\_pr }\SpecialCharTok{|\textgreater{}} \FunctionTok{head}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \FunctionTok{format\_csv}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \FunctionTok{cat}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> gutenberg_id,text,author,title
#> 7532,A BOOK OF OLD BALLADS,NA,A Book of Old Ballads — Volume 2
#> 7532,,NA,A Book of Old Ballads — Volume 2
#> 7532,Selected and with an Introduction,NA,A Book of Old Ballads — Volume 2
#> 7532,,NA,A Book of Old Ballads — Volume 2
#> 7532,by,NA,A Book of Old Ballads — Volume 2
#> 7532,,NA,A Book of Old Ballads — Volume 2
\end{verbatim}

The first line contains the names of the columns and subsequent lines
the observations. Data points that contain commas themselves
(e.g.~``Shaw, Bernard'') are quoted to avoid misinterpreting these
commas a deliminators in our data. To write this dataset to disk we will
use the \texttt{reader::write\_csv()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(works\_pr, }\AttributeTok{file =} \StringTok{"../data/original/gutenberg\_works\_pr.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To avoid downloading dataset that already resides on disk, let's
implement a similar strategy to the one used for direct downloads
(\texttt{get\_zip\_data()}). I've incorporated the code for sampling and
downloading data for a particular subject from Project Gutenberg with a
control statement to check if the dataset file already exists into a
function I named \texttt{get\_gutenberg\_subject()}. Take a look at this
function below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_gutenberg\_subject }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(subject, target\_file, }\AttributeTok{sample\_size =} \DecValTok{10}\NormalTok{) \{}
  \CommentTok{\# Function: to download texts from Project Gutenberg with }
  \CommentTok{\# a specific LCC subject and write the data to disk.}
  
\NormalTok{  pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(tidyverse, gutenbergr) }\CommentTok{\# install/load necessary packages}
  
  \CommentTok{\# Check to see if the data already exists}
  \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(target\_file)) \{ }\CommentTok{\# if data does not exist, download and write}
\NormalTok{    target\_dir }\OtherTok{\textless{}{-}} \FunctionTok{dirname}\NormalTok{(target\_file) }\CommentTok{\# generate target directory for the .csv file}
    \FunctionTok{dir.create}\NormalTok{(}\AttributeTok{path =}\NormalTok{ target\_dir, }\AttributeTok{recursive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# create target data directory}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Downloading data... }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
    \CommentTok{\# Select all records with a particular LCC subject}
\NormalTok{    ids }\OtherTok{\textless{}{-}} 
      \FunctionTok{filter}\NormalTok{(gutenberg\_subjects, }
\NormalTok{             subject\_type }\SpecialCharTok{==} \StringTok{"lcc"}\NormalTok{, subject }\SpecialCharTok{==}\NormalTok{ subject) }\CommentTok{\# select subject}
    \CommentTok{\# Select only those records with plain text available}
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# make the sampling reproducible}
\NormalTok{    ids\_sample }\OtherTok{\textless{}{-}} 
      \FunctionTok{filter}\NormalTok{(gutenberg\_metadata, }
\NormalTok{             gutenberg\_id }\SpecialCharTok{\%in\%}\NormalTok{ ids}\SpecialCharTok{$}\NormalTok{gutenberg\_id, }\CommentTok{\# select ids in both data frames }
\NormalTok{             has\_text }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# select those ids that have text}
      \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =}\NormalTok{ sample\_size) }\CommentTok{\# sample N works }
    \CommentTok{\# Download sample with associated \textasciigrave{}author\textasciigrave{} and \textasciigrave{}title\textasciigrave{} metadata}
\NormalTok{    works\_sample }\OtherTok{\textless{}{-}} 
      \FunctionTok{gutenberg\_download}\NormalTok{(}\AttributeTok{gutenberg\_id =}\NormalTok{ ids\_sample}\SpecialCharTok{$}\NormalTok{gutenberg\_id, }
                         \AttributeTok{meta\_fields =} \FunctionTok{c}\NormalTok{(}\StringTok{"author"}\NormalTok{, }\StringTok{"title"}\NormalTok{))}
    \CommentTok{\# Write the dataset to disk in .csv format}
    \FunctionTok{write\_csv}\NormalTok{(works\_sample, }\AttributeTok{file =}\NormalTok{ target\_file)}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Data downloaded! }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{ }\CommentTok{\# if data exists, don\textquotesingle{}t download it again}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Data already exists }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Adding this function to our function script
\texttt{functions/acquire\_functions.R}, we can now source this function
in our \texttt{analysis/1\_acquire\_data.Rmd} script to download
multiple subjects and store them in on disk in their own file.

Let's download American Literature now (LCC code ``PQ'').

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Download Project Gutenberg text for subject \textquotesingle{}PQ\textquotesingle{} (American Literature)}
\CommentTok{\# and then write this dataset to disk in .rds format}

\CommentTok{\# Select all records with a particular LCC subject}
\NormalTok{ids }\OtherTok{\textless{}{-}} 
  \FunctionTok{filter}\NormalTok{(gutenberg\_subjects, }
\NormalTok{         subject\_type }\SpecialCharTok{==} \StringTok{"lcc"}\NormalTok{, subject }\SpecialCharTok{==} \StringTok{"PQ"}\NormalTok{) }\CommentTok{\# select subject}
\CommentTok{\# Select only those records with plain text available}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# make the sampling reproducible}
\NormalTok{ids\_sample }\OtherTok{\textless{}{-}} 
  \FunctionTok{filter}\NormalTok{(gutenberg\_metadata, }
\NormalTok{         gutenberg\_id }\SpecialCharTok{\%in\%}\NormalTok{ ids}\SpecialCharTok{$}\NormalTok{gutenberg\_id, }\CommentTok{\# select ids in both data frames }
\NormalTok{         has\_text }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# select those ids that have text}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =}\NormalTok{ sample\_size) }\CommentTok{\# sample N works }
\CommentTok{\# Download sample with associated \textasciigrave{}author\textasciigrave{} and \textasciigrave{}title\textasciigrave{} metadata}
\NormalTok{works\_pq }\OtherTok{\textless{}{-}} 
  \FunctionTok{gutenberg\_download}\NormalTok{(}\AttributeTok{gutenberg\_id =}\NormalTok{ ids\_sample}\SpecialCharTok{$}\NormalTok{gutenberg\_id, }
                     \AttributeTok{meta\_fields =} \FunctionTok{c}\NormalTok{(}\StringTok{"author"}\NormalTok{, }\StringTok{"title"}\NormalTok{))}

\FunctionTok{write\_rds}\NormalTok{(}\AttributeTok{x =}\NormalTok{ works\_pq, }\AttributeTok{file =} \StringTok{"data/acquire{-}data/gutenberg\_works\_pq.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Download Project Gutenberg text for subject \textquotesingle{}PQ\textquotesingle{} (American Literature)}
\CommentTok{\# and then write this dataset to disk in .csv format}
\FunctionTok{get\_gutenberg\_subject}\NormalTok{(}\AttributeTok{subject =} \StringTok{"PQ"}\NormalTok{, }\AttributeTok{target\_file =} \StringTok{"../data/original/gutenberg/works\_pq.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Applying this function to both the English and American Literature
datasets, our data directory structure now looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{├──}\NormalTok{ gutenberg}
    \ExtensionTok{│}\NormalTok{   ├── works\_pq.csv}
    \ExtensionTok{│}\NormalTok{   └── works\_pr.csv}
    \ExtensionTok{├──}\NormalTok{ sbc}
    \ExtensionTok{│}\NormalTok{   ├── meta{-}data}
    \ExtensionTok{│}\NormalTok{   └── transcriptions}
    \ExtensionTok{└──}\NormalTok{ scs}
        \ExtensionTok{├──}\NormalTok{ README}
        \ExtensionTok{├──}\NormalTok{ discourse}
        \ExtensionTok{├──}\NormalTok{ disfluency}
        \ExtensionTok{├──}\NormalTok{ documentation}
        \ExtensionTok{├──}\NormalTok{ tagged}
        \ExtensionTok{├──}\NormalTok{ timed{-}transcript}
        \ExtensionTok{└──}\NormalTok{ transcript}
\end{Highlighting}
\end{Shaded}

\hypertarget{authentication}{%
\subsection{Authentication}\label{authentication}}

Some APIs and the R interfaces that provide access to them require
authentication. This may either be through an interactive process that
is mediated between R and the web service and/ or by visiting the
developer website of the particular API. In either case, there is an
extra step that is necessary to make the connect to the API to access
the data.

Let's take a look at the popular micro-blogging platform Twitter. The
rtweet package (Kearney, Revilla Sancho, and Wickham 2023) provides
access to tweets in various ways. To get started install and/or load the
rtweet package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(rtweet) }\CommentTok{\# install/load rtweet package}
\end{Highlighting}
\end{Shaded}

Now before a researcher can access data from Twitter with rtweet,
\href{https://docs.ropensci.org/rtweet/articles/auth.html}{an
authentication token must be setup and made accessible}. After following
the steps for setting up an authentication token and saving it, that
token can be accessed with the \texttt{auth\_as()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{auth\_as}\NormalTok{(twitter\_auth) }\CommentTok{\# load the saved \textasciigrave{}twitter\_auth\textasciigrave{} token}
\end{Highlighting}
\end{Shaded}

Now that we the R session is authenticated, we can explore a popular
method for querying the Twitter API which searchs tweets
(\texttt{search\_tweets}) posted in the recent past (6-9 days).

Let's look at a typical query using the \texttt{search\_tweets()}
function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rt\_latinx }\OtherTok{\textless{}{-}} 
  \FunctionTok{search\_tweets}\NormalTok{(}\AttributeTok{q =} \StringTok{"latinx"}\NormalTok{, }\CommentTok{\# query term}
                \AttributeTok{n =} \DecValTok{100}\NormalTok{, }\CommentTok{\# number of tweets desired}
                \AttributeTok{type =} \StringTok{"mixed"}\NormalTok{, }\CommentTok{\# a mix of \textasciigrave{}recent\textasciigrave{} and \textasciigrave{}popular\textasciigrave{} tweets}
                \AttributeTok{include\_rts =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# do not include RTs}
\end{Highlighting}
\end{Shaded}

Looking at the arguments in this function, we see I've specified the
query term to be `latinx'. This is a single word query but if the query
included multiple words, the spaces between would be interpreted as the
logical \texttt{AND} (only match tweets with all the individual terms).
If one would like to include multi-word expressions, the expressions
should be enclosed by single quotes
(i.e.~\texttt{q\ =\ "\textquotesingle{}spanish\ speakers\textquotesingle{}\ AND\ latinx"}).
Another approach would be to include the logical \texttt{OR} (match
tweets with either of the terms). Multi-word expressions can be included
as in the previous case. Of note, hashtags are acceptable terms, so
\texttt{q\ =\ "\#latinx"} would match tweets with this hashtag.

The number of results has been set at `100', but this is the default, so
I could have left it out. But you can increase the number of desired
tweets. There are rate limits which cap the number of tweets you can
access in a given 15-minute time period.

Another argument of importance is the \texttt{type} argument. This
argument has three possible attributes \texttt{popular},
\texttt{recent}, and \texttt{mixed}. When the \texttt{popular} attribute
he Twitter API will tend to return fewer tweets than specified by
\texttt{n}. With \texttt{recent} or \texttt{mixed} you will most likely
get the \texttt{n} you specified (note that \texttt{mixed} is a mix of
\texttt{popular} and \texttt{recent}).

A final argument to note is the \texttt{include\_rts} whose attribute is
logical. If \texttt{FALSE} no retweets will be included in the results.
This is often what a language researcher will want.

Now, once the \texttt{search\_tweets} query has been run, there a a
large number of variables that are included in the resulting data frame.
Here's an overview of the names of the variables and the vector types
for each variable.

\hypertarget{tbl-ad-rtweet-variables-table}{}
\begin{table}
\caption{\label{tbl-ad-rtweet-variables-table}Variables and variable types returned from Twitter API via rtweet's
\texttt{search\_tweets()} function. }\tabularnewline

\centering
\begin{tabular}{ll}
\toprule
created\_at & character\\
id & double\\
id\_str & character\\
full\_text & character\\
truncated & logical\\
\addlinespace
display\_text\_range & double\\
entities & list\\
metadata & list\\
source & character\\
in\_reply\_to\_status\_id & double\\
\addlinespace
in\_reply\_to\_status\_id\_str & character\\
in\_reply\_to\_user\_id & double\\
in\_reply\_to\_user\_id\_str & character\\
in\_reply\_to\_screen\_name & character\\
geo & logical\\
\addlinespace
coordinates & list\\
place & list\\
contributors & logical\\
is\_quote\_status & logical\\
retweet\_count & integer\\
\addlinespace
favorite\_count & integer\\
favorited & logical\\
retweeted & logical\\
lang & character\\
possibly\_sensitive & logical\\
\addlinespace
quoted\_status\_id & double\\
quoted\_status\_id\_str & character\\
quoted\_status & list\\
text & character\\
favorited\_by & logical\\
\addlinespace
display\_text\_width & logical\\
retweeted\_status & logical\\
quoted\_status\_permalink & logical\\
query & logical\\
possibly\_sensitive\_appealable & logical\\
\bottomrule
\end{tabular}
\end{table}

The
\href{https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets}{Twitter
API documentation for the standard Search Tweets call}, which is what
\texttt{search\_tweets()} interfaces with has quite a few variables (35
to be exact). For many purposes it is not necessary to keep all the
variables. Furthermore, since we will want to write a plain-text file to
disk as part of our project, we will need to either convert or eliminate
any of the variables that are marked as type \texttt{list}. The most
common variable to convert is the \texttt{coordinates} variable, as it
will contain the geolocation codes for those Twitter users' tweets
captured in the query that have geolocation enabled on their device. It
is of note, however, that using \texttt{search\_tweets()} without
specifying that only tweets with geocodes should be captured
(\texttt{geocode\ =}) will tend to return very few, if any, tweets with
geolocation information as the majority of Twitter users do not have
geolocation enabled.

Let's assume that we want to keep all the variables that are not of type
\texttt{list}. One option is to use \texttt{select()} and name each
variable we want to keep. On the other hand we can use a combination of
\texttt{select()} and negated \texttt{!where()} to select all the
variables that are not lists (\texttt{is\_list}). Let's do the later
approach.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rt\_latinx\_subset }\OtherTok{\textless{}{-}} 
\NormalTok{  rt\_latinx }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{where}\NormalTok{(is\_list))  }\CommentTok{\# select all variables that are NOT lists}

\NormalTok{rt\_latinx\_subset }\SpecialCharTok{|\textgreater{}} \CommentTok{\# subsetted dataset}
  \FunctionTok{glimpse}\NormalTok{() }\CommentTok{\# overview}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 100
#> Columns: 30
#> $ created_at                    <chr> "Sun Sep 26 17:38:06 +0000 2021", "Sun S~
#> $ id                            <dbl> 1.44e+18, 1.44e+18, 1.44e+18, 1.44e+18, ~
#> $ id_str                        <chr> "1442181701967302659", "1442196629801488~
#> $ full_text                     <chr> "If we call it Latinx Mass they can't ca~
#> $ truncated                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE~
#> $ display_text_range            <dbl> 57, 177, 166, 23, 261, 153, 202, 211, 57~
#> $ source                        <chr> "<a href=\"https://mobile.twitter.com\" ~
#> $ in_reply_to_status_id         <dbl> NA, NA, NA, 1.44e+18, NA, NA, NA, NA, 1.~
#> $ in_reply_to_status_id_str     <chr> NA, NA, NA, "1437436224042635269", NA, N~
#> $ in_reply_to_user_id           <dbl> NA, NA, NA, 4.26e+08, NA, NA, NA, NA, 2.~
#> $ in_reply_to_user_id_str       <chr> NA, NA, NA, "426159377", NA, NA, NA, NA,~
#> $ in_reply_to_screen_name       <chr> NA, NA, NA, "MorganStanley", NA, NA, NA,~
#> $ geo                           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ contributors                  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ is_quote_status               <lgl> FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,~
#> $ retweet_count                 <int> 351, 124, 62, 0, 0, 0, 0, 0, 0, 0, 0, 1,~
#> $ favorite_count                <int> 3902, 898, 280, 0, 0, 0, 0, 0, 0, 7, 0, ~
#> $ favorited                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE~
#> $ retweeted                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE~
#> $ lang                          <chr> "en", "en", "es", "en", "en", "en", "en"~
#> $ possibly_sensitive            <lgl> NA, FALSE, FALSE, FALSE, FALSE, FALSE, F~
#> $ quoted_status_id              <dbl> NA, NA, NA, NA, 1.44e+18, NA, NA, NA, NA~
#> $ quoted_status_id_str          <chr> NA, NA, NA, NA, "1442475408058830856", N~
#> $ text                          <chr> "If we call it Latinx Mass they can't ca~
#> $ favorited_by                  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ display_text_width            <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ retweeted_status              <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ quoted_status_permalink       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ query                         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ possibly_sensitive_appealable <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
\end{verbatim}

Now we have the 30 variables which can be written to disk as a
plain-text file. Let's go ahead a do this, but wrap it in a function
that does all the work we've just laid out in one function. In addition
we will check to see if the same query has been run, and skip running
the query if the dataset is on disk.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{write\_search\_tweets }\OtherTok{\textless{}{-}} 
  \ControlFlowTok{function}\NormalTok{(query, path, }\AttributeTok{n =} \DecValTok{100}\NormalTok{, }\AttributeTok{type =} \StringTok{"mixed"}\NormalTok{, }\AttributeTok{include\_rts =} \ConstantTok{FALSE}\NormalTok{) \{}
    \CommentTok{\# Function}
    \CommentTok{\# Conduct a Twitter search query and write the results to a csv file}
    
    \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(path)) \{ }\CommentTok{\# check to see if the file already exists}
      \FunctionTok{cat}\NormalTok{(}\StringTok{"File does not exist }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# message}
      
      \FunctionTok{library}\NormalTok{(rtweet) }\CommentTok{\# to use Twitter API}
      \FunctionTok{library}\NormalTok{(tidyverse) }\CommentTok{\# to manipulate data}
      
      \FunctionTok{auth\_get}\NormalTok{() }\CommentTok{\# get authentication token}
      
\NormalTok{      results }\OtherTok{\textless{}{-}} \CommentTok{\# query results}
        \FunctionTok{search\_tweets}\NormalTok{(}\AttributeTok{q =}\NormalTok{ query, }\CommentTok{\# query term}
                      \AttributeTok{n =}\NormalTok{ n, }\CommentTok{\# number of tweets desired (default 100)}
                      \AttributeTok{type =}\NormalTok{ type, }\CommentTok{\# type of query}
                      \AttributeTok{include\_rts =}\NormalTok{ include\_rts) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# to include RTs}
        \FunctionTok{select}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{where}\NormalTok{(is\_list))  }\CommentTok{\# remove list variables}
      
      \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{dir.exists}\NormalTok{(}\FunctionTok{dirname}\NormalTok{(path))) \{ }\CommentTok{\# isolate directory and check if exists}
        \FunctionTok{cat}\NormalTok{(}\StringTok{"Creating directory }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# message}
        
        \FunctionTok{dir.create}\NormalTok{(}\AttributeTok{path =} \FunctionTok{dirname}\NormalTok{(path), }\CommentTok{\# isolate and create directory (remove file name)}
                   \AttributeTok{recursive =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# create embedded directories if necessary}
                   \AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# do not report warnings}
\NormalTok{      \}}
      
      \FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{x =}\NormalTok{ results, }\AttributeTok{file =}\NormalTok{ path) }\CommentTok{\# write results to csv file }
      \FunctionTok{cat}\NormalTok{(}\StringTok{"Twitter search results written to disk }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# message}
      
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
      \FunctionTok{cat}\NormalTok{(}\StringTok{"File already exists! }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# message}
\NormalTok{    \}}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

Let's run this function with the same query as above.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_search\_tweets}\NormalTok{(}\AttributeTok{query =} \StringTok{"latinx"}\NormalTok{, }\AttributeTok{path =} \StringTok{"../data/original/twitter/rt\_latinx.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And the appropriate directory structure and file have been written to
disk.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/original/twitter/}
\ExtensionTok{└──}\NormalTok{ rt\_latinx.csv}
\end{Highlighting}
\end{Shaded}

In sum, this subsection provided an overview to acquiring data from web
service APIs through R packages. We took at closer look at the
\texttt{gutenbergr} package which provides programmatic access to works
available on Projec t Gutenberg and the \texttt{rtweet} package which
provides authenticated access to Twitter. Working with package
interfaces requires more knowledge of R including loading/ installing
packages, working with vectors and data frames, and exporting data from
an R session. We touched on these programming concepts and also outlined
a method to create a reproducible workflow.

\hypertarget{web-scraping}{%
\section{Web scraping}\label{web-scraping}}

There are many resources available through manual and direct downloads
from repositories and individual sites and R package interfaces to web
resources with APIs, but these resources are relatively limited to the
amount of public-facing textual data recorded on the web. In the case
that you want to acquire data from webpages, R can be used to access the
web programmatically through a process known as web scraping. The
complexity of web scrapes can vary but in general it requires more
advanced knowledge of R as well as the structure of the language of the
web: HTML (Hypertext Markup Language).

\hypertarget{a-toy-example}{%
\subsection{A toy example}\label{a-toy-example}}

HTML is a cousin of XML (eXtensible Markup Language) and as such
organizes web documents in a hierarchical format that is read by your
browser as you navigate the web. Take for example the toy webpage I
created as a demonstration in Figure~\ref{fig-ad-example-webpage}.

\begin{figure}[h]

{\centering \includegraphics[width=2.44in,height=\textheight]{figures/acquire-data/example-webpage.png}

}

\caption{\label{fig-ad-example-webpage}Example web page.}

\end{figure}

The file accessed by my browser to render this webpage is
\texttt{test.html} and in plain-text format looks like this:

\begin{verbatim}

<html>
  <head>
    <title>My website</title>
  </head>
  <body>
    <div class="intro">
      <p>Welcome!</p>
      <p>This is my first website. </p>
    </div>
    <table>
      <tr>
        <td>Contact me:</td>
        <td>
          <a href="mailto:francojc@wfu.edu">francojc@wfu.edu</a>
        </td>
      </tr>
    </table>
    <div class="conc">
      <p>Good-bye!</p>
    </div>
  </body>
</html>
\end{verbatim}

Each element in this file is delineated by an opening and closing tag,
\texttt{\textless{}head\textgreater{}\textless{}/head\textgreater{}}.
Tags are nested within other tags to create the structural hierarchy.
Tags can take class and id labels to distinguish them from other tags
and often contain other attributes that dictate how the tag is to behave
when rendered visually by a browser. For example, there are two
\texttt{\textless{}div\textgreater{}} tags in our toy example: one has
the label \texttt{class\ =\ "intro"} and the other
\texttt{class\ =\ "conc"}. \texttt{\textless{}div\textgreater{}} tags
are often used to separate sections of a webpage that may require
special visual formatting. The \texttt{\textless{}a\textgreater{}} tag,
on the other hand, creates a web link. As part of this tag's function,
it requires the attribute \texttt{href=} and a web protocol --in this
case it is a link to an email address \texttt{mailto:francojc@wfu.edu}.
More often than not, however, the \texttt{href=} contains a URL (Uniform
Resource Locator). A working example might look like this:
\texttt{\textless{}a\ href="https://francojc.github.io/"\textgreater{}My\ homepage\textless{}/a\textgreater{}}.

The aim of a web scrape is to download the HTML file, parse the document
structure, and extract the elements containing the relevant information
we wish to capture. Let's attempt to extract some information from our
toy example. To do this we will need the
\href{https://CRAN.R-project.org/package=rvest}{rvest}(Wickham 2022)
package. First, install/load the package, then, read and parse the HTML
from the character vector named \texttt{web\_file} assigning the result
to \texttt{html}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(rvest) }\CommentTok{\# install/ load \textasciigrave{}rvest\textasciigrave{}}

\NormalTok{html }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(web\_file) }\CommentTok{\# read raw html and parse to xml}
\NormalTok{html}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> {html_document}
#> <html>
#> [1] <head>\n<meta http-equiv="Content-Type" content="text/html; charset=UTF-8 ...
#> [2] <body>\n    <div class="intro">\n      <p>Welcome!</p>\n      <p>This is  ...
\end{verbatim}

\texttt{read\_html()} parses the raw HTML into an object of class
\texttt{xml\_document}. The summary output above shows that tags the
HTML structure have been parsed into `elements'. The tag elements can be
accessed by using the \texttt{html\_elements()} function by specifying
the tag to isolate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"div"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> {xml_nodeset (2)}
#> [1] <div class="intro">\n      <p>Welcome!</p>\n      <p>This is my first web ...
#> [2] <div class="conc">\n      <p>Good-bye!</p>\n    </div>
\end{verbatim}

Notice that \texttt{html\_elements("div")} has returned both
\texttt{div} tags. To isolate one of tags by its class, we add the class
name to the tag separating it with a \texttt{.}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"div.intro"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> {xml_nodeset (1)}
#> [1] <div class="intro">\n      <p>Welcome!</p>\n      <p>This is my first web ...
\end{verbatim}

Great. Now say we want to drill down and isolate the subordinate
\texttt{\textless{}p\textgreater{}} nodes. We can add \texttt{p} to our
node filter.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"div.intro p"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> {xml_nodeset (2)}
#> [1] <p>Welcome!</p>
#> [2] <p>This is my first website. </p>
\end{verbatim}

To extract the text contained within a node we use the
\texttt{html\_text()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"div.intro p"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_text}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Welcome!"                   "This is my first website. "
\end{verbatim}

The result is a character vector with two elements corresponding to the
text contained in each \texttt{\textless{}p\textgreater{}} tag. If you
were paying close attention you might have noticed that the second
element in our vector includes extra whitespace after the period. To
trim leading and trailing whitespace from text we can add the
\texttt{trim\ =\ TRUE} argument to \texttt{html\_text()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"div.intro p"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_text}\NormalTok{(}\AttributeTok{trim =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Welcome!"                  "This is my first website."
\end{verbatim}

From here we would then work to organize the text into a format we want
to store it in and write the results to disk. Let's leave writing data
to disk for later in the chapter. For now keep our focus on working with
\texttt{rvest} to acquire data from html documents working with a more
practical example.

\hypertarget{a-practical-example}{%
\subsection{A practical example}\label{a-practical-example}}

With some basic understanding of HTML and how to use the \texttt{rvest}
package, let's turn to a realistic example. Say we want to acquire
lyrics from the online music website and database
\href{https://www.last.fm/}{last.fm}. The first step in any web scrape
is to investigate the site and page(s) we want to scrape to ascertain if
there any licensing restrictions. Many, but not all websites, will
include a plain text file
\href{https://www.cloudflare.com/learning/bots/what-is-robots.txt/}{\texttt{robots.txt}}
at the root of the main URL. This file is declares which webpages a
`robot' (including web scraping scripts) can and cannot access. We can
use the \texttt{robotstxt} package to find out which URLs are accessible
\footnote{It is important to check the paths of sub-domains as some
  website allow access in some areas and not in others}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(robotstxt) }\CommentTok{\# load/ install \textasciigrave{}robotstxt\textasciigrave{}}

\FunctionTok{paths\_allowed}\NormalTok{(}\AttributeTok{paths =} \StringTok{"https://www.last.fm/"}\NormalTok{) }\CommentTok{\# check permissions}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] TRUE
\end{verbatim}

The next step includes identifying the URL we want to target and
exploring the structure of the HTML document. Take the following webpage
I have identified, seen in
Figure~\ref{fig-ad-example-lyrics-page-lastfm}.

\begin{figure}[h]

{\centering \includegraphics[width=9.28in,height=\textheight]{figures/acquire-data/ad-lastfm-webpage-lyrics.png}

}

\caption{\label{fig-ad-example-lyrics-page-lastfm}Lyrics page from
last.fm}

\end{figure}

As in our toy example, first we want to feed the HTML web address to the
\texttt{read\_html()} function to parse the tags into elements. We will
then assign the result to \texttt{html}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read and parse html as an xml object}
\NormalTok{lyrics\_url }\OtherTok{\textless{}{-}} \StringTok{"https://www.last.fm/music/Radiohead/\_/Karma+Police/+lyrics"}
\NormalTok{html }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(lyrics\_url) }\CommentTok{\# read raw html and parse to xml}
\NormalTok{html}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> {html_document}
#> <html lang="en" class="
#>         no-js
#>         playbar-masthead-release-shim
#>         youtube-provider-not-ready
#>     ">
#> [1] <head>\n<meta http-equiv="Content-Type" content="text/html; charset=UTF-8 ...
#> [2] <body>\n<div id="initial-tealium-data" data-require="tracking/tealium-uta ...
\end{verbatim}

At this point we have captured and parsed the raw HTML assigning it to
the object named \texttt{html}. The next step is to identify the html
elements that contain the information we want to extract from the page.
To do this it is helpful to use a browser to inspect specific elements
of the webpage. Your browser will be equipped with a command that you
can enable by hovering your mouse over the element of the page you want
to target and using a right click to select ``Inspect'' (Chrome) or
``Inspect Element'' (Safari, Brave). This will split your browser window
vertical or horizontally showing you the raw HTML underlying the
webpage.

\begin{figure}[h]

{\centering \includegraphics[width=9.28in,height=\textheight]{figures/acquire-data/ad-lastfm-artist-inspect.png}

}

\caption{\label{fig-ad-inspect-element-artist-lastfm}Using the ``Inspect
Element'' command to explore raw html.}

\end{figure}

From Figure~\ref{fig-ad-inspect-element-artist-lastfm} we see that the
element we want to target is contained within an
\texttt{\textless{}a\textgreater{}\textless{}/a\textgreater{}} tag. Now
this tag is common and we don't want to extract every \texttt{a} so we
use the class \texttt{header-new-crumb} to specify we only want the
artist name. Using the convention described in our toy example, we can
isolate the artist of the lyrics page.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_element}\NormalTok{(}\StringTok{"a.header{-}new{-}crumb"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> {html_node}
#> <a class="header-new-crumb" itemprop="url" href="/music/Radiohead">
#> [1] <span itemprop="name">Radiohead</span>
\end{verbatim}

We can then extract the text with \texttt{html\_text()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{artist }\OtherTok{\textless{}{-}} 
\NormalTok{  html }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_element}\NormalTok{(}\StringTok{"a.header{-}new{-}crumb"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_text}\NormalTok{()}
\NormalTok{artist}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Radiohead"
\end{verbatim}

Let's extract the song title in the same way.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{song }\OtherTok{\textless{}{-}} 
\NormalTok{  html }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_element}\NormalTok{(}\StringTok{"h1.header{-}new{-}title"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_text}\NormalTok{()}
\NormalTok{song}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Karma Police"
\end{verbatim}

Now if we inspect the HTML of the lyrics page, we will notice that the
lyrics are contained in
\texttt{\textless{}p\textgreater{}\textless{}/p\textgreater{}} tags with
the class \texttt{lyrics-paragraph}.

\begin{figure}[h]

{\centering \includegraphics[width=9.28in,height=\textheight]{figures/acquire-data/ad-lastfm-lyrics-inspect.png}

}

\caption{\label{fig-ad-inspect-element-lyrics-lastfm}Using the ``Inspect
Element'' command to explore raw html.}

\end{figure}

Since there are multiple elements that we want to extract, we will need
to use the \texttt{html\_elements()} function instead of the
\texttt{html\_element()} which only targets one element.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lyrics }\OtherTok{\textless{}{-}} 
\NormalTok{  html }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"p.lyrics{-}paragraph"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_text}\NormalTok{()}
\NormalTok{lyrics}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Karma policeArrest this manHe talks in mathsHe buzzes like a fridgeHe's like a detuned radio"      
#> [2] "Karma policeArrest this girlHer Hitler hairdoIs making me feel illAnd we have crashed her party"   
#> [3] "This is what you'll getThis is what you'll getThis is what you'll getWhen you mess with us"        
#> [4] "Karma policeI've given all I canIt's not enoughI've given all I canBut we're still on the payroll" 
#> [5] "This is what you'll getThis is what you'll getThis is what you'll getWhen you mess with us"        
#> [6] "For a minute thereI lost myself, I lost myselfPhew, for a minute thereI lost myself, I lost myself"
#> [7] "For a minute thereI lost myself, I lost myselfPhew, for a minute thereI lost myself, I lost myself"
\end{verbatim}

At this point, we have isolated and extracted the artist, song, and
lyrics from the webpage. Each of these elements are stored in character
vectors in our R session. To complete our task we need to write this
data to disk as plain text. With an eye towards a tidy dataset, an ideal
format to store the data is in a CSV file where each column corresponds
to one of the elements from our scrape and each row an observation. A
CSV file is a tabular format and so before we can write the data to disk
let's coerce the data that we have into tabular format. We will use the
\texttt{tibble()} function here to streamline our data frame creation.
\footnote{\texttt{tibble} objects are \texttt{data.frame} objects with
  some added extra bells and whistles that we won't get into here.}
Feeding each of the vectors \texttt{artist}, \texttt{song}, and
\texttt{lyrics} as arguments to \texttt{tibble()} creates the tabular
format we are looking for.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tibble}\NormalTok{(artist, song, lyrics) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 7
#> Columns: 3
#> $ artist <chr> "Radiohead", "Radiohead", "Radiohead", "Radiohead", "Radiohead"~
#> $ song   <chr> "Karma Police", "Karma Police", "Karma Police", "Karma Police",~
#> $ lyrics <chr> "Karma policeArrest this manHe talks in mathsHe buzzes like a f~
\end{verbatim}

Notice that there are seven rows in this data frame, one corresponding
to each paragraph in \texttt{lyrics}. R has a bias towards working with
vectors of the same length. As such each of the other vectors
(\texttt{artist}, and \texttt{song}) are replicated, or recycled, until
they are the same length as the longest vector \texttt{lyrics}, which a
length of seven.

For good documentation let's add our object \texttt{lyrics\_url} to the
data frame, which contains the actual web link to this page, and assign
the result to \texttt{song\_lyrics}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{song\_lyrics }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(artist, song, lyrics, lyrics\_url)}
\end{Highlighting}
\end{Shaded}

The final step is to write this data to disk. To do this we will use the
\texttt{write\_csv()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{x =}\NormalTok{ song\_lyrics, }\AttributeTok{path =} \StringTok{"../data/original/lyrics.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{scaling-up}{%
\subsection{Scaling up}\label{scaling-up}}

At this point you may be think, `Great, I can download data from a
single page, but what about downloading multiple pages?' Good question.
That's really where the strength of a programming approach takes hold.
Extracting information from multiple pages is not fundamentally
different than working with a single page. However, it does require more
sophisticated understanding of the web and R coding strategies, in
particular \textbf{iteration}.

Before we get to iteration, let's first create a couple functions to
make it possible to efficiently reuse the code we have developed so far:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the \texttt{get\_lyrics} function wraps the code for scraping a single
  lyrics webpage from last.fm.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_lyrics }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(lyrics\_url) \{}
  \CommentTok{\# Function: Scrape last.fm lyrics page for: artist, song, }
  \CommentTok{\# and lyrics from a provided content link. }
  \CommentTok{\# Return as a tibble/data.frame}
  
  \FunctionTok{cat}\NormalTok{(}\StringTok{"Scraping song lyrics from:"}\NormalTok{, lyrics\_url, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
  
\NormalTok{  pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(tidyverse, rvest) }\CommentTok{\# install/ load package(s)}
  
\NormalTok{  url }\OtherTok{\textless{}{-}} \FunctionTok{url}\NormalTok{(lyrics\_url, }\StringTok{"rb"}\NormalTok{) }\CommentTok{\# open url connection }
\NormalTok{  html }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(url) }\CommentTok{\# read and parse html as an xml object}
  \FunctionTok{close}\NormalTok{(url) }\CommentTok{\# close url connection}
  
\NormalTok{  artist }\OtherTok{\textless{}{-}} 
\NormalTok{    html }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{html\_element}\NormalTok{(}\StringTok{"a.header{-}new{-}crumb"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{html\_text}\NormalTok{()}
  
\NormalTok{  song }\OtherTok{\textless{}{-}} 
\NormalTok{    html }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{html\_element}\NormalTok{(}\StringTok{"h1.header{-}new{-}title"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{html\_text}\NormalTok{()}
  
\NormalTok{  lyrics }\OtherTok{\textless{}{-}} 
\NormalTok{    html }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"p.lyrics{-}paragraph"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{html\_text}\NormalTok{()}
  
  \FunctionTok{cat}\NormalTok{(}\StringTok{"...one moment "}\NormalTok{)}
  
  \FunctionTok{Sys.sleep}\NormalTok{(}\DecValTok{1}\NormalTok{) }\CommentTok{\# sleep for 1 second to reduce server load}
  
\NormalTok{  song\_lyrics }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(artist, song, lyrics, lyrics\_url)}
  
  \FunctionTok{cat}\NormalTok{(}\StringTok{"... done! }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
  
  \FunctionTok{return}\NormalTok{(song\_lyrics)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

The \texttt{get\_lyrics} function includes all of the code developed
previously, but also includes: (1) output messages (\texttt{cat()}), (2)
a processing pause (\texttt{Sys.sleep()}), and (3) code to manage
opening and closing web connections (\texttt{url()} and
\texttt{close()}).

Points (1) and (2) will be useful when we iterate over this function to
provide status messages and to reduce server load when processing
multiple webpages from a web domain. (3) will be necessary to manage
webpages that are non-existent. As we will see, we will generate url
link to multiple song lyrics some of which will not be valid. To avoid
errors that will stop the processing these steps have been incorporated
here.

\end{tcolorbox}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  the \texttt{write\_content} writes the webscraped data to our local
  machine, including functionality to create the necessary directory
  structure of the target file path we choose.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{write\_content }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(content, target\_file) \{}
  \CommentTok{\# Function: Write the tibble content to disk. Create the directory if}
  \CommentTok{\# it does not already exist.}
  
\NormalTok{  pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(tidyverse) }\CommentTok{\# install/ load packages}
  
\NormalTok{  target\_dir }\OtherTok{\textless{}{-}} \FunctionTok{dirname}\NormalTok{(target\_file) }\CommentTok{\# identify target file directory structure}
  \FunctionTok{dir.create}\NormalTok{(}\AttributeTok{path =}\NormalTok{ target\_dir, }\AttributeTok{recursive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# create directory}
  \FunctionTok{write\_csv}\NormalTok{(content, target\_file) }\CommentTok{\# write csv file to target location}
  
  \FunctionTok{cat}\NormalTok{(}\StringTok{"Content written to disk!}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

With just these two functions, we can take a lyrics URL from last.fm and
scrape and write the data to disk like this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lyrics\_url }\OtherTok{\textless{}{-}} \StringTok{"https://www.last.fm/music/Pixies/\_/Where+Is+My+Mind\%3F/+lyrics"}

\NormalTok{lyrics\_url }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{get\_lyrics}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{write\_content}\NormalTok{(}\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/lyrics.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/original/lastfm/}
\ExtensionTok{└──}\NormalTok{ lyrics.csv}
\end{Highlighting}
\end{Shaded}

Now we could manually search and copy URLs and run this function
pipeline. This would be fine if we had just a few particular URLs that
we wanted to scrape. But if we want to, say, scrape a set of lyrics
grouped by genre. We would probably want a more programmatic approach.
The good news is we can leverage our understanding of webscraping to
scrape last.fm to harvest the information needed to create and store
links to songs by genre. We can then pass these links to a pipeline,
similar to the previous one, to scrape lyrics for many songs and store
the results in files grouped by genre.

Last.fm provides a genres page where some of the top genres are listed
and can be further explored.

\begin{figure}[h]

{\centering \includegraphics[width=9.28in,height=\textheight]{figures/acquire-data/ad-lastfm-genres.png}

}

\caption{\label{fig-ad-genre-page-lastfm}Genre page on last.fm}

\end{figure}

Diving into a a particular genre, `rock' for example, you will get a
listing of the top tracks in that genre.

\begin{figure}[h]

{\centering \includegraphics[width=9.28in,height=\textheight]{figures/acquire-data/ad-lastfm-genre-tracks-list.png}

}

\caption{\label{fig-ad-genre-tracks-list-lastfm}Tracks by genre list
page on last.fm}

\end{figure}

If we inspect the HTML elements for the track names in
Figure~\ref{fig-ad-genre-tracks-list-lastfm}, we can see that a relative
URL is found for the track. In this case, I have `Smells Like Teen
Spirit' by Nirvana highlighted in the inspector. If we follow this link
to the track page and then to the lyrics for the track, you will notice
that the relative URL on the track listings page has all the unique
information. Only the web domain \texttt{https://www.last.fm} and the
post-pended \texttt{/+lyrics} is missing.

So with this we can put together a function which gets the track listing
for a last.fm genre, scrapes the relative URLs for each of the tracks,
and creates a full absolute URL to the lyrics page.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_genre\_lyrics\_urls }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(last\_fm\_genre) \{}
  \CommentTok{\# Function: Scrapes a given last.fm genre title for top tracks in}
  \CommentTok{\# that genre and then creates links to the lyrics pages for these tracks}
  
  \FunctionTok{cat}\NormalTok{(}\StringTok{"Scraping top songs from:"}\NormalTok{, last\_fm\_genre, }\StringTok{"genre: }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
  
\NormalTok{  pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(tidyverse, rvest) }\CommentTok{\# install/ load packages}
  
  \CommentTok{\# create web url for the genre listing page}
\NormalTok{  genre\_listing\_url }\OtherTok{\textless{}{-}} 
    \FunctionTok{paste0}\NormalTok{(}\StringTok{"https://www.last.fm/tag/"}\NormalTok{, last\_fm\_genre, }\StringTok{"/tracks"}\NormalTok{) }
  
\NormalTok{  genre\_lyrics\_urls }\OtherTok{\textless{}{-}} 
    \FunctionTok{read\_html}\NormalTok{(genre\_listing\_url) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# read raw html and parse to xml}
    \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"td.chartlist{-}name a"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# isolate the track elements}
    \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"href"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# extract the href attribute}
    \FunctionTok{paste0}\NormalTok{(}\StringTok{"https://www.last.fm"}\NormalTok{, ., }\StringTok{"/+lyrics"}\NormalTok{) }\CommentTok{\# join the domain, relative artist path, and the post{-}pended /+lyrics to create an absolute URL}
  
  \FunctionTok{return}\NormalTok{(genre\_lyrics\_urls)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

With this function, all we need is to identify the verbatim way last.fm
lists the genres. For Rock, it is \texttt{rock} but for Hip Hop, it is
\texttt{hip+hop}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_genre\_lyrics\_urls}\NormalTok{(}\StringTok{"hip+hop"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# get urls for top hip hop tracks}
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\CommentTok{\# only display 10 tracks}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Scraping top songs from: hip+hop genre:
\end{verbatim}

\begin{verbatim}
#>  [1] "https://www.last.fm/music/Juzhin/_/Charlie+Conscience+(feat.+MMAIO)/+lyrics"
#>  [2] "https://www.last.fm/music/Juzhin/_/Railways/+lyrics"                        
#>  [3] "https://www.last.fm/music/Juzhin/_/Coming+Down/+lyrics"                     
#>  [4] "https://www.last.fm/music/Juzhin/_/Tupona/+lyrics"                          
#>  [5] "https://www.last.fm/music/Juzhin/_/Sakhalin/+lyrics"                        
#>  [6] "https://www.last.fm/music/Juzhin/_/3+Simple+Minutes/+lyrics"                
#>  [7] "https://www.last.fm/music/Juzhin/_/Lost+Sense/+lyrics"                      
#>  [8] "https://www.last.fm/music/Juzhin/_/Wonderful/+lyrics"                       
#>  [9] "https://www.last.fm/music/Gina+Moryson/_/Vanilla+Smoothy+(Live)/+lyrics"    
#> [10] "https://www.last.fm/music/Juzhin/_/Flunk-Down+(Juzhin+Remix)/+lyrics"
\end{verbatim}

So now we have a method to scrape URLs by genre and list them in a
vector. Our approach, then, could be to pass these lyrics URLs to our
existing pipeline which downloads the lyrics (\texttt{get\_lyrics()})
and then writes them to disk (\texttt{write\_content()}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note: will not run}
\FunctionTok{get\_genre\_lyrics\_urls}\NormalTok{(}\StringTok{"hip+hop"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# get lyrics urls for specific genre}
  \FunctionTok{get\_lyrics}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# scrape lyrics url}
  \FunctionTok{write\_content}\NormalTok{(}\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/hip\_hop.csv"}\NormalTok{) }\CommentTok{\# write to disk}
\end{Highlighting}
\end{Shaded}

This approach, however, has a couple of problems. (1) our
\texttt{get\_lyrics()} function only takes one URL at a time, but the
result of \texttt{get\_genre\_lyrics\_urls()} will produce many URLs. We
will be able to solve this with iteration using the \href{}{purrr}
package, specifically the \texttt{map()} function which will iteratively
map each URL output from \texttt{get\_genre\_lyrics\_urls()} to
\texttt{get\_lyrics()} in turn. (2) the output from our iterative
application of \texttt{get\_lyrics()} will produce a tibble for each
URL, which then sets up a problem with writing the tibbles to disk with
the \texttt{write\_content()} function. To avoid this we will want to
combine the tibbles into one single tibble and then send it to be
written to disk. The \texttt{bind\_rows()} function will do just this.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note: will run, but with occasional errors}
\FunctionTok{get\_genre\_lyrics\_urls}\NormalTok{(}\StringTok{"hip+hop"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# get lyrics urls for specific genre}
  \FunctionTok{map}\NormalTok{(get\_lyrics) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# scrape lyrics url}
  \FunctionTok{bind\_rows}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# combine tibbles into one}
  \FunctionTok{write\_content}\NormalTok{(}\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/hip\_hop.csv"}\NormalTok{) }\CommentTok{\# write to disk}
\end{Highlighting}
\end{Shaded}

This preceding pipeline conceptually will work. However, on my testing,
it turns out that some of the URLs that are generated in the
\texttt{get\_genre\_lyrics\_urls()} do not exist on the site. That is,
the song is listed but no lyrics have been added to the song site. This
will mean that when the URL is sent to the \texttt{get\_lyrics()}
function, there will be an error when attempting to download and parse
the page with \texttt{read\_html()} which will halt the entire process.
To avoid this error, we can wrap the \texttt{get\_lyrics()} function in
a function designed to attempt to download and parse the URL
(\texttt{tryCatch()}), but if there is an error, it will skip it and
move on to the next URL without stopping the processing. This approach
is reflected in the \texttt{get\_lyrics\_catch()} function below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Wrap the \textasciigrave{}get\_lyrics()\textasciigrave{} function with \textasciigrave{}tryCatch()\textasciigrave{} to }
\CommentTok{\# skip URLs that have no lyrics}

\NormalTok{get\_lyrics\_catch }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(lyrics\_url) \{}
  \FunctionTok{tryCatch}\NormalTok{(}\FunctionTok{get\_lyrics}\NormalTok{(lyrics\_url), }
           \AttributeTok{error =} \ControlFlowTok{function}\NormalTok{(e) }\FunctionTok{return}\NormalTok{(}\ConstantTok{NULL}\NormalTok{)) }\CommentTok{\# no, URL, return(NULL)/ skip}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Updating the pipeline with the \texttt{get\_lyrics\_catch()} function
would look like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note: will run, but we can do better}
\FunctionTok{get\_genre\_lyrics\_urls}\NormalTok{(}\StringTok{"hip+hop"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# get lyrics urls for specific genre}
  \FunctionTok{map}\NormalTok{(get\_lyrics\_catch) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# scrape lyrics url}
  \FunctionTok{bind\_rows}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# combine tibbles into one}
  \FunctionTok{write\_content}\NormalTok{(}\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/hip\_hop.csv"}\NormalTok{) }\CommentTok{\# write to disk}
\end{Highlighting}
\end{Shaded}

This will work, but as we have discussed before one of this goals we
have we acquiring data for a reproducible research project is to make
sure that we are developing efficient code that will not burden site's
server we are scraping from. In this case, we would like to check to see
if the data is already downloaded. If not, then the script should run.
If so, then the script does not run. Of course this is a perfect use of
a conditional statement. To make this a single function we can call,
I've wrapped the functions we created for getting lyric URLs from
last.fm, scraping the URLs, and writing the results to disk in the
\texttt{download\_lastfm\_lyrics()} function below. I also added a line
to add a \texttt{last\_fm\_genre} column to the combined tibble to store
the name of the genre we scraped
(i.e.~\texttt{mutate(genre\ =\ last\_fm\_genre)}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{download\_lastfm\_lyrics }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(last\_fm\_genre, target\_file) \{}
  \CommentTok{\# Function: get last.fm lyric urls by genre and write them to disk}
  
  \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(target\_file)) \{}
    
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Downloading data.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
    
    \FunctionTok{get\_genre\_lyrics\_urls}\NormalTok{(last\_fm\_genre) }\SpecialCharTok{|\textgreater{}} 
      \FunctionTok{map}\NormalTok{(get\_lyrics\_catch) }\SpecialCharTok{|\textgreater{}} 
      \FunctionTok{bind\_rows}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
      \FunctionTok{mutate}\NormalTok{(}\AttributeTok{genre =}\NormalTok{ last\_fm\_genre) }\SpecialCharTok{|\textgreater{}} 
      \FunctionTok{write\_content}\NormalTok{(target\_file)}
    
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Data already downloaded!}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we can call this function on any genre on the last.fm site and
download the top 50 song lyrics for that genre (provided they all have
lyrics pages).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Scrape lyrics for \textquotesingle{}pop\textquotesingle{}}
\FunctionTok{download\_lastfm\_lyrics}\NormalTok{(}\AttributeTok{last\_fm\_genre =} \StringTok{"pop"}\NormalTok{, }\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/pop.csv"}\NormalTok{)}

\CommentTok{\# Scrape lyrics for \textquotesingle{}rock\textquotesingle{}}
\FunctionTok{download\_lastfm\_lyrics}\NormalTok{(}\AttributeTok{last\_fm\_genre =} \StringTok{"rock"}\NormalTok{, }\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/rock.csv"}\NormalTok{)}

\CommentTok{\# Scrape lyrics for \textquotesingle{}hip hop\textquotesingle{}}
\FunctionTok{download\_lastfm\_lyrics}\NormalTok{(}\AttributeTok{last\_fm\_genre =} \StringTok{"hip+hop"}\NormalTok{, }\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/hip\_hop.csv"}\NormalTok{)}

\CommentTok{\# Scrape lyrics for \textquotesingle{}metal\textquotesingle{}}
\FunctionTok{download\_lastfm\_lyrics}\NormalTok{(}\AttributeTok{last\_fm\_genre =} \StringTok{"metal"}\NormalTok{, }\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/metal.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we can see that our web scrape data is organized in a similar
fashion to the other data we acquired in this chapter.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{├──}\NormalTok{ cedel2/}
    \ExtensionTok{│}\NormalTok{   └── texts.csv}
    \ExtensionTok{├──}\NormalTok{ gutenberg/}
    \ExtensionTok{│}\NormalTok{   ├── works\_pq.csv}
    \ExtensionTok{│}\NormalTok{   └── works\_pr.csv}
    \ExtensionTok{├──}\NormalTok{ lastfm/}
    \ExtensionTok{│}\NormalTok{   ├── country.csv}
    \ExtensionTok{│}\NormalTok{   ├── hip\_hop.csv}
    \ExtensionTok{│}\NormalTok{   ├── lyrics.csv}
    \ExtensionTok{│}\NormalTok{   ├── metal.csv}
    \ExtensionTok{│}\NormalTok{   ├── pop.csv}
    \ExtensionTok{│}\NormalTok{   └── rock.csv}
    \ExtensionTok{├──}\NormalTok{ sbc/}
    \ExtensionTok{│}\NormalTok{   ├── meta{-}data/}
    \ExtensionTok{│}\NormalTok{   └── transcriptions/}
    \ExtensionTok{├──}\NormalTok{ scs/}
    \ExtensionTok{│}\NormalTok{   ├── README}
    \ExtensionTok{│}\NormalTok{   ├── discourse}
    \ExtensionTok{│}\NormalTok{   ├── disfluency}
    \ExtensionTok{│}\NormalTok{   ├── documentation/}
    \ExtensionTok{│}\NormalTok{   ├── tagged}
    \ExtensionTok{│}\NormalTok{   ├── timed{-}transcript}
    \ExtensionTok{│}\NormalTok{   └── transcript}
    \ExtensionTok{└──}\NormalTok{ twitter/}
        \ExtensionTok{└──}\NormalTok{ rt\_latinx.csv}
\end{Highlighting}
\end{Shaded}

Again, it is important to add these custom functions to our
\texttt{acquire\_functions.R} script in the \texttt{functions/}
directory so we can access them in our scripts more efficiently and make
our analysis steps more succinct and legible.

In this section we covered scraping language data from the web. The
rvest package provides a host of functions for downloading and parsing
HTML. We first looked at a toy example to get a basic understanding of
how HTML works and then moved to applying this knowledge to a practical
example. To maintain a reproducible workflow, the code developed in this
example was grouped into task-oriented functions which were in turn
joined and wrapped into a function that provided convenient access to
our workflow and avoided unnecessary downloads (in the case the data
already exists on disk).

Here we have built on previously introduced R coding concepts and
demonstrated various others. Web scraping often requires more knowledge
of and familiarity with R as well as other web technologies. Rest
assured, however, practice will increase confidence in your abilities. I
encourage you to practice on your own with other websites. You will
encounter problems. Consult the R documentation in RStudio or online and
lean on the R community on the web at sites such as
\href{https://stackoverflow.com/}{Stack Overflow} \emph{inter alia}.

\hypertarget{documentation-1}{%
\section{Documentation}\label{documentation-1}}

As part of the data acquisition process it is important include
documentation that describes the data resource(s) that will serve as the
base for a research project. For all resources the data should include
as much information possible that outlines the sampling frame of the
data (Ädel 2020). For a corpus sample acquired from a repository will
often include documentation which will outline the sampling frame --this
most likely will be the very information which leads a researcher to
select this resource for the project at hand. It is important to include
this documentation (HTML or PDF file) or reference to the documentation
(article citation or link\footnote{Note that web links can change and it
  is often best to safeguard the documentation by downloading the HTML
  documentation page instead of linking}) within the reproducible
project's directory structure.

In other cases where the data acquisition process is formulated and
conducted by the researcher for the specific aims of the research
(i.e.~API and web scraping approaches), the researcher should make an
effort to document those aspects which are key for the study, but that
may also be of interest to other researchers for similar research
questions. This will may include language characteristics such as
modality, register, genre, etc., speaker/ writer characteristics such as
demographics, time period(s), context of the linguistic communication,
etc. and process characteristics such as the source of the data, the
process of acquisition, date of acquisition, etc. However, it is
important to recognize that each language sample and the resource from
which it is drawn is unique. As a general rule of thumb, a researcher
should document the resource as if this were a resource \emph{they} were
to encounter for the first time. To archive this information, it is
standard practice to include a \texttt{README} file in the relevant
directory where the data is stored.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{├──}\NormalTok{ cedel2/}
    \ExtensionTok{│  }\NormalTok{ ├── documentation/}
    \ExtensionTok{│  }\NormalTok{ └── texts.csv}
    \ExtensionTok{├──}\NormalTok{ gutenberg/}
    \ExtensionTok{│  }\NormalTok{ ├── README.md}
    \ExtensionTok{│  }\NormalTok{ ├── works\_pq.csv}
    \ExtensionTok{│  }\NormalTok{ └── works\_pr.csv}
    \ExtensionTok{├──}\NormalTok{ lastfm/}
    \ExtensionTok{│  }\NormalTok{ ├── README.md}
    \ExtensionTok{│  }\NormalTok{ ├── country.csv}
    \ExtensionTok{│  }\NormalTok{ ├── hip\_hop.csv}
    \ExtensionTok{│  }\NormalTok{ ├── lyrics.csv}
    \ExtensionTok{│  }\NormalTok{ ├── metal.csv}
    \ExtensionTok{│  }\NormalTok{ ├── pop.csv}
    \ExtensionTok{│  }\NormalTok{ └── rock.csv}
    \ExtensionTok{├──}\NormalTok{ sbc/}
    \ExtensionTok{│  }\NormalTok{ ├── meta{-}data/}
    \ExtensionTok{│  }\NormalTok{ └── transcriptions/}
    \ExtensionTok{├──}\NormalTok{ scs/}
    \ExtensionTok{│  }\NormalTok{ ├── README}
    \ExtensionTok{│  }\NormalTok{ ├── discourse}
    \ExtensionTok{│  }\NormalTok{ ├── disfluency}
    \ExtensionTok{│  }\NormalTok{ ├── documentation/}
    \ExtensionTok{│  }\NormalTok{ ├── tagged}
    \ExtensionTok{│  }\NormalTok{ ├── timed{-}transcript}
    \ExtensionTok{│  }\NormalTok{ └── transcript}
    \ExtensionTok{└──}\NormalTok{ twitter/}
        \ExtensionTok{├──}\NormalTok{ README.md}
        \ExtensionTok{└──}\NormalTok{ rt\_latinx.csv}
\end{Highlighting}
\end{Shaded}

For both existing corpora and data samples acquired by the researcher it
is also important to signal if there are conditions and/ or licensing
restrictions that one should heed when using and potentially sharing the
data. In some cases existing corpus data come with restrictions on data
sharing. These can be quite restrictive and ultimately require that the
corpus data not be included in publically available reproducible project
or data can only be shared in a derived format. If this the case, it is
important to document the steps to legally acquire the data so that a
researcher can acquire their own license and take full advantage of your
reproducible project.

In the case of data from APIs or web scraping, there too may be
stipulations on sharing data. A growing number of data sources apply one
of \href{https://creativecommons.org/about/cclicenses/}{the available
Creative Common Licenses}. Check the source of your data for more
information and if you are a member of a research institution you will
likely have a
\href{https://zsr.wfu.edu/digital-scholarship/copyright/}{specialist} on
\href{https://www.copyright.gov/fair-use/more-info.html}{Copyright and
Fair Use}.

\hypertarget{summary-5}{%
\section*{Summary}\label{summary-5}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we have covered a lot of ground. On the surface we have
discussed three methods for acquiring corpus data for use in text
analysis. In the process we have delved into various aspects of the R
programming language. Some key concepts include writing custom functions
and working with those function in an iterative manner. We have also
considered topics that are more general in nature and concern
interacting with data found on the internet.

Each of these methods should be approached in a way that is transparent
to the researcher and to would-be collaborators and the general research
community. For this reason the documentation of the steps taken to
acquire data are key both in the code and in human-facing documentation.

At this point you have both a bird's eye view of the data available on
the web and strategies on how to access a great majority of it. It is
now time to turn to the next step in our data analysis project: data
curation. In the next posts I will cover how to wrangle your raw data
into a tidy dataset. This will include working with and incorporating
meta-data as well as augmenting a dataset with linguistic annotations.

\hypertarget{activities-4}{%
\section*{Activities}\label{activities-4}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_6.html}{Control
statements, custom functions, and iteration}\\
\textbf{How}: Read Recipe 6 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To increase your ability to produce effective, concise,
and reproducible code. The three main areas we will cover are working
with control statements, writing custom functions, and leveraging
iteration. These programming strategies are often useful for acquiring
data but, as we will see, they are powerful concepts that can be used
throughout a reproducible research project.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/lab_6}{Control
statements, custom functions, and iteration}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 6.\\
\textbf{Why}: To gain experience working with coding strategies such as
control statements, custom functions, and iteration, practice working
with direct downloads and API interfaces to acquire data, and implement
organizational strategies for organizing data in reproducible fashion.

\end{tcolorbox}

\hypertarget{questions-5}{%
\section*{Questions}\label{questions-5}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Conceptual questions}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Technical exercises}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{enumerate}

\end{tcolorbox}

\hypertarget{sec-curate-datasets}{%
\chapter{Curate data(sets)}\label{sec-curate-datasets}}

\begin{quote}
The hardest bit of information to extract is the first piece.

--- Robert Ferrigno
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  what are some of the formats that data can take?
\item
  what R programming strategies are used to read these formats into
  tabular, tidy dataset structures?
\item
  what is the importance of maintaining modularity between data and data
  processing in a reproducible research project?
\end{itemize}

\end{tcolorbox}

In this chapter we will now look at the next step in a text analysis
project: data curation. That is, the process of converting the original
data we acquire to a tidy dataset. As Acquired data can come in a wide
variety of formats that depend largely on the richness of the metadata
that is included, but also can reflect individual preferences. In this
chapter we will consider three general types of formats: (1)
unstructured data, (2) structured data, and (3) semi-structured data.
Regardless of the file type and the structure of the data, it will be
necessary to consider how to curate a dataset that such that the
structure reflects the basic the unit of analysis that we wish to
investigate (see
\protect\hyperlink{sec-framing-research.htmlux5cux23research-question}{Chapter
4, section 4.2}. The resulting dataset will be the base from which we
will work to further transform the dataset such that it aligns with the
analysis method(s) that we will implement. And as in previous
implementation steps, we will discuss the important role of
documentation.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Swirl}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/swirl}{Regular
Expressions}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To learn the basics of how to define search patterns to
match strings, or characters, using Regular Expressions.

\end{tcolorbox}

\hypertarget{unstructured}{%
\section{Unstructured}\label{unstructured}}

The bulk of text that is available in the wild is of the unstructured
variety. Unstructured data is data that has not been organized to make
the information contained within explicit. Explicit information that is
included with data is called metadata. Metadata can be linguistic or
non-linguistic in nature. So for unstructured data there is little to no
metadata directly associated with the data. This information needs to be
added or derived for the purposes of the research, either through manual
inspection or (semi-)automatic processes. For now, however, our job is
just to get the unstructured data into a structured format with a
minimal set of metadata that we can derive from the resource.

As an example of an unstructured source of text data, let's take a look
at the \href{https://www.statmt.org/europarl/}{Europarle Parallel
Corpus}, as introduced in
\protect\hyperlink{sec-understanding-data}{Chapter 2 ``Understanding
data''}. This data contains parallel texts (source and translated
documents) from the European Parliamentary proceedings for some 21
European languages. Here we will focus in on the translation from
Spanish to English (Spanish-English).

\hypertarget{orientation}{%
\subsection{Orientation}\label{orientation}}

With the data downloaded into the \texttt{data/original/europarle/}
directory we see that there are two files. One corresponding to the
source language (Spanish) and one for the target language (English).

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/original/europarle/}
\ExtensionTok{├──}\NormalTok{ europarl{-}v7.es{-}en.en}
\ExtensionTok{└──}\NormalTok{ europarl{-}v7.es{-}en.es}
\end{Highlighting}
\end{Shaded}

Looking at the first 10 lines of the first file, we can see that this is
running text.

\begin{verbatim}
> Resumption of the session
> I declare resumed the session of the European Parliament adjourned on
Friday 17 December 1999, and I would like once again to wish you a
happy new year in the hope that you enjoyed a pleasant festive period.
> Although, as you will have seen, the dreaded 'millennium bug' failed
to materialise, still the people in a number of countries suffered a
series of natural disasters that truly were dreadful.
> You have requested a debate on this subject in the course of the next
few days, during this part-session.
> In the meantime, I should like to observe a minute' s silence, as a
number of Members have requested, on behalf of all the victims
concerned, particularly those of the terrible storms, in the various
countries of the European Union.
> Please rise, then, for this minute' s silence.
> (The House rose and observed a minute' s silence)
> Madam President, on a point of order.
> You will be aware from the press and television that there have been
a number of bomb explosions and killings in Sri Lanka.
> One of the people assassinated very recently in Sri Lanka was Mr
Kumar Ponnambalam, who had visited the European Parliament just a few
months ago.
\end{verbatim}

The only meta information that we can surmise from these files is the
fact that we know one is the source language and one is the target
language and that each sentence is aligned (parallel) with the lines in
the other file.

So with what we have we'd like to create a data frame that has the seen
in Table~\ref{tbl-cd-unstructured-europarle-structure-example}.

\hypertarget{tbl-cd-unstructured-europarle-structure-example}{}
\begin{table}
\caption{\label{tbl-cd-unstructured-europarle-structure-example}Idealized structure for the Europarle Corpus dataset. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 1 & ...sentence from source language\\
Target & 1 & ...sentence from target language\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{tidy-the-data}{%
\subsection{Tidy the data}\label{tidy-the-data}}

To create this dataset structure lets's read the files with the
\texttt{readtext()} function from readtext package and assign them to a
meaningful variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read the Europarle files}
\NormalTok{europarle\_en }\OtherTok{\textless{}{-}}  \CommentTok{\# English target text}
\NormalTok{  readtext}\SpecialCharTok{::}\FunctionTok{readtext}\NormalTok{(}\StringTok{"../data/original/europarle/europarl{-}v7.es{-}en.en"}\NormalTok{, }\CommentTok{\# path to the data}
                     \AttributeTok{verbosity =} \DecValTok{0}\NormalTok{) }\CommentTok{\# don\textquotesingle{}t show warnings}

\NormalTok{europarle\_es }\OtherTok{\textless{}{-}} \CommentTok{\# Spanish source text}
\NormalTok{  readtext}\SpecialCharTok{::}\FunctionTok{readtext}\NormalTok{(}\StringTok{"../data/original/europarle/europarl{-}v7.es{-}en.es"}\NormalTok{, }\CommentTok{\# path to the data}
                     \AttributeTok{verbosity =} \DecValTok{0}\NormalTok{) }\CommentTok{\# don\textquotesingle{}t show warnings}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

The \texttt{readtext()} function can read many different types of file
formats, from structured to unstructured. However, it depends in large
part on the extension of the file to recognize what algorithm to use
when reading a file. In this particular case the Europarle files do not
have a typical extension (they have \texttt{.en} and \texttt{.es}). The
\texttt{readtext()} function will treat them as plain text
(\texttt{.txt}), but it will throw a warning message. To suppress the
warning message you can add the \texttt{verbosity\ =\ 0} argument.

\end{tcolorbox}

Now there are a couple things to note about thbe \texttt{europarle\_en}
and \texttt{europarle\_es} objects. If we inspect their structure, we
will find that the dimensions of the data frame that is created is one
row by two columns.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(europarle\_en) }\CommentTok{\# inspect the structure of the object}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Classes 'readtext' and 'data.frame': 1 obs. of 2 variables:
#> $ doc_id: chr "europarl-v7.es-en.en"
#> $ text : chr "Resumption of the session\nI declare resumed the
session of the European Parliament adjourned on Friday 17 Dece"|
__truncated__
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

Note that the \texttt{str()} function from base R is similar to
\texttt{glimpse()}. However, \texttt{glimpse()} will attempt to show you
as much data as possible. In this case since our column \texttt{text} is
a very long character vector it will take a long time to render. I've
chosen the \texttt{str()} function as it will automatically truncate the
data.

\end{tcolorbox}

The columns are \texttt{doc\_id} and \texttt{text}. \texttt{doc\_id} is
created by readtext to index each file that is read in. The
\texttt{text} column is where the text appears. The fact that we only
have one row means that all the text in the entire file is contained in
one cell! We will want to break this cell up into rows for each
sentence, but for now let's work with getting the columns to line up
with our idealized dataset structure.

First let's change the type of data frame that we are working with to a
tibble. This will make sure we don't accidentally print hundreds of
lines to our R Markdown output and/ or the R Console. Then we will
rename the \texttt{doc\_id} column to \texttt{type} and change the value
of that column to ``Target'' (for English) and ``Source'' (for Spanish).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle\_target }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle\_en }\SpecialCharTok{|\textgreater{}} \CommentTok{\# readtext data frame}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# convert to tibble}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{type =}\NormalTok{ doc\_id) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# rename doc\_id to type}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"Target"}\NormalTok{) }\CommentTok{\# change type value to \textquotesingle{}Target\textquotesingle{}}

\NormalTok{europarle\_source }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle\_es }\SpecialCharTok{|\textgreater{}} \CommentTok{\# readtext data frame}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# convert to tibble}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{type =}\NormalTok{ doc\_id) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# rename doc\_id to type}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"Source"}\NormalTok{) }\CommentTok{\# change type value to \textquotesingle{}Source\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

We have two objects now, one corresponding to the `Source' and the other
the `Target' parallel texts. Let's now join these two datasets, one on
top of the other --that is, by rows. We wil use the
\texttt{bind\_rows()} function for this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle }\OtherTok{\textless{}{-}} 
  \FunctionTok{bind\_rows}\NormalTok{(europarle\_target, europarle\_source)}

\FunctionTok{str}\NormalTok{(europarle) }\CommentTok{\# inspect the structure of the object}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> tibble [2 x 2] (S3: tbl_df/tbl/data.frame)
#>  $ type: chr [1:2] "Target" "Source"
#>  $ text: chr [1:2] "Resumption of the session\nI declare resumed the session of the European Parliament adjourned on Friday 17 Dece"| __truncated__ "Reanudación del período de sesiones\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrump"| __truncated__
\end{verbatim}

The \texttt{europarle} dataset now has 2 columns, as before, and 2 rows
--each corresponding to the distinct language types (Source/ Target).

Remember our goal is to create a dataset structure with three columns
\texttt{type}, \texttt{sentence\_id}, and \texttt{sentence}. At the
moment we have \texttt{type} and \texttt{text} --where \texttt{text} has
all of the sentences in for each type in a cell. So we are going to want
to break up the \texttt{text} column into sentences, group the sentences
that are created by \texttt{type}, and then number these sentences so
that they are aligned between the distinct types.

To break up the text into sentences we are going to turn to the tidytext
package. This package has a extremely useful function
\texttt{unnest\_tokens()} which provides an effective way to break text
into various units (see \texttt{?tidytext::unnest\_tokens} for a full
list of token types). Since I know from looking at the raw text that
each sentence is on its own line, the best strategy to break the text
into sentence units is to find a way to break each line into a new row
in our dataset. To do this we need to use the \texttt{token\ =\ "regex"}
(for Regular Expression) and use the
\texttt{pattern\ =\ "\textbackslash{}\textbackslash{}n"} which tells R
to look for carriage returns to use as the breaking criterion.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle\_sentences }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle }\SpecialCharTok{|\textgreater{}} 
\NormalTok{  tidytext}\SpecialCharTok{::}\FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{output =}\NormalTok{ sentence, }\CommentTok{\# new column}
                          \AttributeTok{input =}\NormalTok{ text, }\CommentTok{\# column to find text}
                          \AttributeTok{token =} \StringTok{"regex"}\NormalTok{, }\CommentTok{\# use a regular expression to break up the text}
                          \AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\CommentTok{\# break text by carriage returns (returns after lines)}
                          \AttributeTok{to\_lower =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# do not lowercase the text}

\FunctionTok{glimpse}\NormalTok{(europarle\_sentences) }\CommentTok{\# preview the structure}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 3,926,375
#> Columns: 2
#> $ type     <chr> "Target", "Target", "Target", "Target", "Target", "Target", "~
#> $ sentence <chr> "Resumption of the session", "I declare resumed the session o~
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

Regular Expressions are a powerful pattern matching syntax. They are
used extensively in text manipulation and we will see them again and
again. A good website to practice Regular Expressions is
\href{https://regex101.com/}{RegEx101}. You can also install the
regexplain package in R to get access to a useful
\href{https://rstudio.github.io/rstudioaddins/}{RStudio Addin}.

\end{tcolorbox}

Our new \texttt{europarle\_sentences} object is a data frame with almost
4 million rows! The final step to get to our envisioned dataset
structure is to add the \texttt{sentence\_id} column which will be
calculated by grouping the data by \texttt{type} and then assigning a
row number to each of the sentences in each group.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle\_sentences\_id }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle\_sentences }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{group\_by}\NormalTok{(type) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# group by type}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{sentence\_id =} \FunctionTok{row\_number}\NormalTok{()) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# add a row number for each sentence for each level of type}
  \FunctionTok{select}\NormalTok{(type, sentence\_id, sentence) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# select the relevant columns to keep}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# ungroup by type}
  \FunctionTok{arrange}\NormalTok{(sentence\_id, type) }\CommentTok{\# arrange the dataset}

\NormalTok{europarle\_sentences\_id }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-cd-unstructured-europarle-add-sentence-id}{}
\begin{table}
\caption{\label{tbl-cd-unstructured-europarle-add-sentence-id}First ten sentences in the Europarle Corpus curated dataset. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 1 & Reanudación del período de sesiones\\
Target & 1 & Resumption of the session\\
Source & 2 & Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\\
Target & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Source & 3 & Como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\
\addlinespace
Target & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Source & 4 & Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\
Target & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
Source & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\\
Target & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{write-dataset}{%
\subsection{Write dataset}\label{write-dataset}}

At this point we have the curated dataset
(\texttt{europarle\_sentences\_id}) in a tidy format. This dataset,
however, is only in the current R session. We will want to write this
dataset to disk so that in the next step of the text analysis workflow
(transform data) we will be able to start work on this dataset and make
changes as needed to fit our analysis needs.

We will leverage the project directory structure which has distinct
directories for \texttt{original/} and \texttt{derived/} data(sets).

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
\end{Highlighting}
\end{Shaded}

Since this dataset is derived by our work, we will added it to the
\texttt{derived/} directory. I'll create a \texttt{europarle/} directory
just to keep things organized.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Write the curated dataset to disk}
\NormalTok{fs}\SpecialCharTok{::}\FunctionTok{dir\_create}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/derived/europarle/"}\NormalTok{) }\CommentTok{\# create the europarle directory}
\FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{x =}\NormalTok{ europarle\_sentences\_id, }\CommentTok{\# object to write}
          \AttributeTok{file =} \StringTok{"../data/derived/europarle/europarle\_curated.csv"}\NormalTok{) }\CommentTok{\# target file location/ name}
\end{Highlighting}
\end{Shaded}

This is how the directory structure under the \texttt{derived/}
directory looks now.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{│}\NormalTok{   └── europarle}
\ExtensionTok{│}\NormalTok{       └── europarle\_curated.csv}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{└──europarle}
        \ExtensionTok{├──}\NormalTok{ europarl{-}v7.es{-}en.en}
        \ExtensionTok{└──}\NormalTok{ europarl{-}v7.es{-}en.es}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-6}{%
\subsection{Summary}\label{summary-6}}

In this section we worked with unstructured data and looked at how to
read the data into an R session and manipulate the data to form a tidy
dataset with a few columns that we could derive based on the information
we have about the corpus.

In our discussion we worked step by step to curate the Europarle Corpus,
adding in intermediate steps for illustration purposes. However, in a
more realistic case the code would most likely make more extensive use
of piping (\texttt{\textbar{}\textgreater{}}) to reduce the number of
intermediate objects and make the code more legible. Below I've included
a sample of what that code might look like.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read data and set up \textasciigrave{}type\textasciigrave{} column}
\NormalTok{europarle\_en }\OtherTok{\textless{}{-}}  
\NormalTok{  readtext}\SpecialCharTok{::}\FunctionTok{readtext}\NormalTok{(}\StringTok{"../data/original/europarle/europarl{-}v7.es{-}en.en"}\NormalTok{, }\CommentTok{\# path to the data}
                     \AttributeTok{verbosity =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# don\textquotesingle{}t show warnings}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# covert to tibble}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{type =}\NormalTok{ doc\_id) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# rename doc\_id to type}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"Target"}\NormalTok{) }\CommentTok{\# change type value to \textquotesingle{}Target\textquotesingle{}}

\NormalTok{europarle\_es }\OtherTok{\textless{}{-}} 
\NormalTok{  readtext}\SpecialCharTok{::}\FunctionTok{readtext}\NormalTok{(}\StringTok{"../data/original/europarle/europarl{-}v7.es{-}en.en"}\NormalTok{, }\CommentTok{\# path to the data}
                     \AttributeTok{verbosity =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# don\textquotesingle{}t show warnings}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# covert to tibble}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{type =}\NormalTok{ doc\_id) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# rename doc\_id to type}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"Source"}\NormalTok{) }\CommentTok{\# change type value to \textquotesingle{}Source\textquotesingle{}}

\CommentTok{\# Join the datasets by rows}
\NormalTok{europarle }\OtherTok{\textless{}{-}} 
  \FunctionTok{bind\_rows}\NormalTok{(europarle\_en, europarle\_es)}

\CommentTok{\# Segment the \textasciigrave{}text\textasciigrave{} column into \textasciigrave{}sentence\textasciigrave{} units}
\NormalTok{europarle }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
\NormalTok{  tidytext}\SpecialCharTok{::}\FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{output =}\NormalTok{ sentence, }\CommentTok{\# new column}
                          \AttributeTok{input =}\NormalTok{ text, }\CommentTok{\# column to find text}
                          \AttributeTok{token =} \StringTok{"regex"}\NormalTok{, }\CommentTok{\# use a regular expression to break up the text}
                          \AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\CommentTok{\# break text by carriage returns (returns after lines)}
                          \AttributeTok{to\_lower =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# do not lowercase the text}

\CommentTok{\# Add \textasciigrave{}sentence\_id\textasciigrave{} to each \textasciigrave{}type\textasciigrave{}}
\NormalTok{europarle }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{group\_by}\NormalTok{(type) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# group by type}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{sentence\_id =} \FunctionTok{row\_number}\NormalTok{()) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# add a row number for each sentence for each level of type}
  \FunctionTok{select}\NormalTok{(type, sentence\_id, sentence) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# select the relevant columns to keep}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# ungroup by type}
  \FunctionTok{arrange}\NormalTok{(sentence\_id, type) }\CommentTok{\# arrange the dataset}

\CommentTok{\# Write the curated dataset to disk}
\NormalTok{fs}\SpecialCharTok{::}\FunctionTok{dir\_create}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/derived/europarle/"}\NormalTok{) }\CommentTok{\# create the europarle directory}
\FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{x =}\NormalTok{ europarle\_sentences\_id, }\CommentTok{\# object to write}
          \AttributeTok{file =} \StringTok{"../data/derived/europarle/europarle\_curated.csv"}\NormalTok{) }\CommentTok{\# target file location/ name}
\end{Highlighting}
\end{Shaded}

\hypertarget{structured}{%
\section{Structured}\label{structured}}

On the opposite side of the spectrum from unstructured data, structured
data includes more metadata information --often much more. The
association of metadata with the language to be analyzed means that the
data has already be curated to some degree, therefore it is more apt to
discuss structured data as a dataset. There are two questions, however,
that need to be taken into account. One, logistical question, is what
file format the dataset is in and how to we read it into R. And the
second, more research-based, is whether the data is curated in a fashion
that makes sense for the current research. Let's look at each of these
questions briefly and then get to a practical example.

There are file formats which are purposely designed for storing
structured datasets. Some very common file types are .csv, .xml, .json,
etc. The data within these files is explicitly organized. For example,
in a .csv file, the dataset structure is represented by delimiting the
columns and rows by commas.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{column\_1,column\_2,column\_3}
\NormalTok{row 1 value 1,row 1 value 2,row 1 value 3}
\NormalTok{row 2 value 1,row 2 value 2,row 2 value 3}
\end{Highlighting}
\end{Shaded}

When read into R, the .csv file format is converted to a data frame with
the appropriate structure.

\hypertarget{tbl-cd-structured-example-table-csv}{}
\begin{table}
\caption{\label{tbl-cd-structured-example-table-csv}Example .csv file in R }\tabularnewline

\centering
\begin{tabular}{lll}
\toprule
column\_1 & column\_2 & column\_3\\
\midrule
row 1 value 1 & row 1 value 2 & row 1 value 3\\
row 2 value 1 & row 2 value 2 & row 2 value 3\\
\bottomrule
\end{tabular}
\end{table}

With an understanding of how the information is encoding into a file, we
can now turn to considerations about how the original dataset is
structure and how that structure is to be used for a given research
project. The curation process that is reflected in a structured dataset
may or may not initially align with the goals of our research either in
terms of the type(s) of information or the unit of analysis of the
structured dataset. The aim, then, is to take advantage of the
information and curate it such that it does align.

As an example case of curating structured datasets, we will look at the
song lyric datasets acquired from Last.fm in the
\protect\hyperlink{sec-acquire-data}{previous chapter}.

\hypertarget{orientation-1}{%
\subsection{Orientation}\label{orientation-1}}

The individual datasets from the Last.fm webscrape are found inside the
\texttt{data/original/lastfm/} directory, and includes the
\texttt{README.md} documentation file.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{└──}\NormalTok{ lastfm/}
        \ExtensionTok{├──}\NormalTok{ README.md}
        \ExtensionTok{├──}\NormalTok{ country.csv}
        \ExtensionTok{├──}\NormalTok{ hip\_hop.csv}
        \ExtensionTok{├──}\NormalTok{ lyrics.csv}
        \ExtensionTok{├──}\NormalTok{ metal.csv}
        \ExtensionTok{├──}\NormalTok{ pop.csv}
        \ExtensionTok{└──}\NormalTok{ rock.csv}
\end{Highlighting}
\end{Shaded}

Let's take a look at the structure of one of genres from these set of
lyrics to familiarize ourselves with the structure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lf\_country }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/original/lastfm/country.csv"}\NormalTok{) }\CommentTok{\# read the csv file}
\NormalTok{lf\_country }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# first 10 observations }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{) }\CommentTok{\# print pretty table}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\textbf{?(caption)}}

\end{table}

\hypertarget{tbl-cd-structured-read-single-run}{}
\begin{table}
\caption{\label{tbl-cd-structured-read-single-run}Example file from the Last.fm dataset of song lyrics. }\tabularnewline

\centering
\begin{tabular}{lllll}
\toprule
artist & song & lyrics & lyrics\_url & genre\\
\midrule
Johnny Cash & Hurt & I hurt myself todayTo see if I still feelI focus on the painThe only thing that's realThe needle tears a holeThe old familiar stingTry to kill it all awayBut I remember everything & https://www.last.fm/music/Johnny+Cash/\_/Hurt/+lyrics & country\\
Johnny Cash & Hurt & What have I becomeMy sweetest friendEveryone I know goes awayIn the endAnd you could have it allMy empire of dirtI will let you downI will make you hurt & https://www.last.fm/music/Johnny+Cash/\_/Hurt/+lyrics & country\\
Johnny Cash & Hurt & I wear this crown of thornsUpon my liar's chairFull of broken thoughtsI cannot repairBeneath the stains of timeThe feelings disappearYou are someone elseI am still right here & https://www.last.fm/music/Johnny+Cash/\_/Hurt/+lyrics & country\\
Johnny Cash & Hurt & What have I becomeMy sweetest friendEveryone I know goes awayIn the endAnd you could have it allMy empire of dirtI will let you downI will make you hurt & https://www.last.fm/music/Johnny+Cash/\_/Hurt/+lyrics & country\\
Johnny Cash & Hurt & If I could start againA million miles awayI would keep myselfI would find a way & https://www.last.fm/music/Johnny+Cash/\_/Hurt/+lyrics & country\\
\addlinespace
Johnny Cash & Ring of Fire & Love is a burning thingAnd it makes a fiery ringBound by wild desireI fell into a ring of fire & https://www.last.fm/music/Johnny+Cash/\_/Ring+of+Fire/+lyrics & country\\
Johnny Cash & Ring of Fire & I fell into a burning ring of fireI went down, down, downAnd the flames went higherAnd it burns, burns, burnsThe ring of fire, the ring of fire & https://www.last.fm/music/Johnny+Cash/\_/Ring+of+Fire/+lyrics & country\\
Johnny Cash & Ring of Fire & I fell into a burning ring of fireI went down, down, downAnd the flames went higherAnd it burns, burns, burnsThe ring of fire, the ring of fire & https://www.last.fm/music/Johnny+Cash/\_/Ring+of+Fire/+lyrics & country\\
Johnny Cash & Ring of Fire & The taste of love is sweetWhen hearts like ours meetI fell for you like a child & https://www.last.fm/music/Johnny+Cash/\_/Ring+of+Fire/+lyrics & country\\
Johnny Cash & Ring of Fire & Oh, but the fire went wild & https://www.last.fm/music/Johnny+Cash/\_/Ring+of+Fire/+lyrics & country\\
\bottomrule
\end{tabular}
\end{table}

We can see a couple important characteristics from this preview of the
dataset. First, we see the columns include \texttt{artist},
\texttt{song}, \texttt{lyrics}, \texttt{lyrics\_url}, and
\texttt{genre}. Second, we see that for each son the lyrics are
segmented across multiple rows.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

You may notice that in addition to the lyrics being separated by line,
there appears to be an artifact from the original webscrape of this data
which has individual lyric lines run in to the next. An example is the
the lyrics ``\ldots{} hurt myself toda\textbf{yT}osee if I still
fee\textbf{lI} focus\ldots{}''. We will address this issue when it comes
time to normalize the dataset in the transform process.

\end{tcolorbox}

Given the fact that each of these files will include a \texttt{genre}
label, that means that we will be able to read in each of these files in
one operation and the distinction between genres will be recoverable.
The next thing to think about is how we want to curate the dataset for
our purposes. That is, what should the base structure of our curated
dataset look like?

Let's make the assumption that we want to have the columns
\texttt{artist}, \texttt{song}, \texttt{lyrics}, and \texttt{genre}. The
\texttt{lyrics\_url} could be useful for documentation purposes, but for
our text analysis it does not appear to be very relevant --so we will
drop it. The second aspect concerns the observations. As it stands, the
dataset the observations reflect the formatting of the website from
which the lyrics were drawn. A potentially better organization would to
have each observation correspond to all the lyrics for a single song. In
this case we will want to collapse the current \texttt{lyrics} column's
values into lyrics for the entire song --maintaining the other measure
for each of the other columns.

With this structure in mind, we are shooting for an idealized structure
such as the one below.

\hypertarget{tbl-cd-structured-idealized-dataset}{}
\begin{table}
\caption{\label{tbl-cd-structured-idealized-dataset}Idealized structure for the Last.fm dataset. }\tabularnewline

\centering
\begin{tabular}{llll}
\toprule
artist & song & lyrics & genre\\
\midrule
Johnny Cash & Hurt & ...lyrics for the song... & country\\
Johnny Cash & Ring of Fire & ...lyrics for the song... & country\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{tidy-the-datasets}{%
\subsection{Tidy the datasets}\label{tidy-the-datasets}}

So our objectives are set, let's first read in all the files. To do this
we will again use the \texttt{readtext()} function. But instead of
reading one file at a time we will read all the files of interest (those
with the .csv extension) in one go. The \texttt{readtext()} function
allows for the use of `wildcard' notation (\texttt{*}) in the file(s)
path to enable pattern matching.

So the files in the \texttt{data/original/lastfm/} directory look like
this.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{../data/original/lastfm/README.md}
\ExtensionTok{../data/original/lastfm/country.csv}
\ExtensionTok{../data/original/lastfm/hip\_hop.csv}
\ExtensionTok{../data/original/lastfm/lyrics.csv}
\ExtensionTok{../data/original/lastfm/metal.csv}
\ExtensionTok{../data/original/lastfm/pop.csv}
\ExtensionTok{../data/original/lastfm/rock.csv}
\end{Highlighting}
\end{Shaded}

We want all the files, except the \texttt{REAME.md} file. To do this we
want our path to look like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{../data/original/lastfm/*.csv}
\end{Highlighting}
\end{Shaded}

The wildcard \texttt{*} replaces the genre names and this effectively
only matches files ending in \texttt{.csv}.

Great, that will capture the files we are looking for but when working
with \texttt{readtext()} we will need to set the \texttt{text\_field}
argument to the column that corresponds to the text in our dataset. That
is the \texttt{lyrics} column. Let's go ahead and do this and convert
the result to a tibble.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\OtherTok{\textless{}{-}} 
  \FunctionTok{readtext}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/original/lastfm/*.csv"}\NormalTok{, }\CommentTok{\# files to match using *.csv}
           \AttributeTok{text\_field =} \StringTok{"lyrics"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# text column from the datasets}
  \FunctionTok{as\_tibble}\NormalTok{() }\CommentTok{\# convert to a tibble}

\FunctionTok{glimpse}\NormalTok{(lastfm) }\CommentTok{\# preview}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 2,172
#> Columns: 6
#> $ doc_id     <chr> "country.csv.1", "country.csv.2", "country.csv.3", "country~
#> $ text       <chr> "I hurt myself todayTo see if I still feelI focus on the pa~
#> $ artist     <chr> "Johnny Cash", "Johnny Cash", "Johnny Cash", "Johnny Cash",~
#> $ song       <chr> "Hurt", "Hurt", "Hurt", "Hurt", "Hurt", "Ring of Fire", "Ri~
#> $ lyrics_url <chr> "https://www.last.fm/music/Johnny+Cash/_/Hurt/+lyrics", "ht~
#> $ genre      <chr> "country", "country", "country", "country", "country", "cou~
\end{verbatim}

Looking at the preview of the data frame we now have in \texttt{lastfm}
there are a couple things to note. First, we see that a column
\texttt{doc\_id} has been added. This column is used by
\texttt{readtext()} to index the file from which the data was read. In
our case since we already have sufficient information to index our
dataset, we can drop this column. Next we see that the \texttt{lyrics}
column has been renamed to \texttt{text}. This is because we set this as
the \texttt{text\_field} when we read in the files. We can easily rename
this column, but we'll leave that for later.

Let's go ahead and drop the columns that we have decided will not figure
in our curated dataset. We can use the \texttt{select()} function to
either select those columns we want to keep or by using the \texttt{-}
operator, identify the columns we want to drop. The decision of
`selecting' or `deselecting' is usually one of personal choice and code
succinctness. In this case, we are dropping two columns and keeping
four, so let's deselect. I will assign the result to the same name as
our current dataset, effectively overwriting that dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\OtherTok{\textless{}{-}} \CommentTok{\# new dataset}
  \FunctionTok{select}\NormalTok{(lastfm, }\CommentTok{\# original dataset}
         \SpecialCharTok{{-}}\NormalTok{doc\_id, }\SpecialCharTok{{-}}\NormalTok{lyrics\_url) }\CommentTok{\# drop these columns}

\FunctionTok{glimpse}\NormalTok{(lastfm) }\CommentTok{\# preview}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 2,172
#> Columns: 4
#> $ text   <chr> "I hurt myself todayTo see if I still feelI focus on the painTh~
#> $ artist <chr> "Johnny Cash", "Johnny Cash", "Johnny Cash", "Johnny Cash", "Jo~
#> $ song   <chr> "Hurt", "Hurt", "Hurt", "Hurt", "Hurt", "Ring of Fire", "Ring o~
#> $ genre  <chr> "country", "country", "country", "country", "country", "country~
\end{verbatim}

Now let's work to collapse the lyrics in the \texttt{text} column by
each distinct \texttt{artist}, \texttt{song}, and \texttt{genre}
combination. We will use the \texttt{group\_by()} function to create
\texttt{artist} \texttt{song} \texttt{genre} groupings and then use
\texttt{summarize()} to create a new column in which the \texttt{text}
field is collapsed into all the song lyrics for this grouping. Inside
the \texttt{summarize()} function we use \texttt{str\_flatten()} with
the argument \texttt{collapse\ =\ "\ "} to collapse each observation in
\texttt{text} leaving a single whitespace between the observations
(otherwise each line would then be joined contiguously to the next).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{group\_by}\NormalTok{(artist, song, genre) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# grouping}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{lyrics =} \FunctionTok{str\_flatten}\NormalTok{(text, }\AttributeTok{collapse =} \StringTok{" "}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# collapse text into the new column \textasciigrave{}lyrics\textasciigrave{} (dropping \textasciigrave{}text\textasciigrave{})}
  \FunctionTok{ungroup}\NormalTok{() }\CommentTok{\# unset the groupings}

\FunctionTok{glimpse}\NormalTok{(lastfm) }\CommentTok{\# preview}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 199
#> Columns: 4
#> $ artist <chr> "3 Doors Down", "3 Doors Down", "50 Cent", "a-ha", "ABBA", "Aer~
#> $ song   <chr> "Here Without You", "Kryptonite", "In Da Club", "Take On Me", "~
#> $ genre  <chr> "rock", "rock", "hip-hop", "pop", "pop", "rock", "country", "co~
#> $ lyrics <chr> "A hundred days have made me olderSince the last time that I sa~
\end{verbatim}

Let's take a look at the first 5 observations from this collapsed
dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# first 5 observations}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{) }\CommentTok{\# print pretty table}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-cd-structure-lastfm-collapse-table}{}
\begin{table}
\caption{\label{tbl-cd-structure-lastfm-collapse-table}Sample lyrics from Last.fm dataset collapsed by artist, song, and genre. }\tabularnewline

\centering
\begin{tabular}{llll}
\toprule
artist & song & genre & lyrics\\
\midrule
3 Doors Down & Here Without You & rock & A hundred days have made me olderSince the last time that I saw your pretty faceA thousand lies have made me colderAnd I don't think I can look at this the same But all the miles that separateDisappear now when I'm dreaming of your face I'm here without you, babyBut you're still on my lonely mindI think about you, babyAnd I dream about you all the time I'm here without you, babyBut you're still with me in my dreamsAnd tonight it's only you and me, yeah The miles just keep rollin'As the people leave their way to say helloI've heard this life is overratedBut I hope that it gets better as we go, oh yeah, yeah I'm here without you, babyBut you're still on my lonely mindI think about you, babyAnd I dream about you all the time I'm here without you, babyBut you're still with me in my dreamsAnd tonight, girl, it's only you and me Everything I know, and anywhere I go (Oh whoa)It gets hard but it won't take away my love (Oh whoa)And when the last one fallsWhen it's all said and doneIt gets hard but it won't take away my love, whoa, oh, oh I'm here without you, babyBut you're still on my lonely mindI think about you, babyAnd I dream about you all the time I'm here without you, babyBut you're still with me in my dreamsAnd tonight, girl, it's only you and me, yeahOh girl, oh oh\\
3 Doors Down & Kryptonite & rock & Well I took a walk around the world to ease my troubled mindI left my body lying somewhere in the sands of timeWell I watched the world float to the dark side of the moonI feel there's nothing I can do,yeah I watched the world float to the dark side of the moonAfter all I knew it had to be something to do with youI really don't mind what happens now and thenAs long as you'll be my friend at the end If I go crazy then will you still call me Superman?If I'm alive and well, will you be there a-holding my hand?I'll keep you by my side with my superhuman mightKryptonite You called me strong, you called me weakBut still your secrets I will keepYou took for granted all the times I never let you downYou stumbled in and bumped your headIf not for me then you'd be deadI picked you up and put you back on solid ground If I go crazy then will you still call me Superman?If I'm alive and well, will you be there a-holding my hand?I'll keep you by my side with my superhuman mightKryptonite If I go crazy then will you still call me Superman?If I'm alive and well, will you be there holding my hand?I'll keep you by my side with my superhuman mightKryptoniteYeah! If I go crazy then will you still call me Superman?If I'm alive and well, will you be there a-holding my hand?I'll keep you by my side with my superhuman mightKryptonite Oh, whoa, whoaOh, whoa, whoaOh, whoa, whoa\\
50 Cent & In Da Club & hip-hop & Go, go, go, go, go, goGo, shortyIt's your birthdayWe gon' party like it's your birthdayWe gon' sip Bacardi like it's your birthdayAnd you know we don't give a fuck it's not your birthday You can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin' drugsI'm into havin' sex, I ain't into makin' loveSo come give me a hug, if you into getting rubbed You can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin' drugsI'm into havin' sex, I ain't into makin' loveSo come give me a hug, if you into getting rubbed When I pull out up front, you see the Benz on dubsWhen I roll 20 deep, it's 20 knives in the clubNiggas heard I fuck with Dre, now they wanna show me loveWhen you sell like Eminem, and the hoes they wanna fuckBut, homie, ain't nothing change hoes down, G's upI see Xzibit in the Cut, that nigga roll that weed upIf you watch how I move, you'll mistake me for a playa or pimpBeen hit wit' a few shells, but I don't walk wit' a limp (I'm ight)In the hood, in L.A, they saying ""50 you hot""They like me, I want them to love me like they love 'PacBut holla, in New York them niggas'll tell ya I'm locoAnd the plan is to put the rap game in a choke holdI'm full of focused man, my money on my mindI got a mill out the deal and I'm still on the grindNow shorty said she feeling my style, she feeling my flowHer girlfriend wanna get bi and they ready to go You can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin' drugsI'm into havin' sex, I ain't into makin' loveSo come give me a hug, if you into getting rubbed You can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin' drugsI'm into havin' sex I, ain't into makin' loveSo come give me a hug, if you into getting rubbed My flow, my show brought me the doeThat bought me all my fancy thingsMy crib, my cars, my clothes, my jewelsLook, nigga, I done came up and I ain't changeAnd you should love it, way more then you hate itNigga, you mad? I thought that you'd be happy I made itI'm that cat by the bar toasting to the good lifeYou that faggot ass nigga trying to pull me back right?When my jaws get to bumpin' in the club it's onI wink my eye at you, bitch, if she smiles she goneIf the roof on fire, let the motherfucker burnIf you talking 'bout money, homie, I ain't concernedI'm a tell you what Banks told me 'cuz go 'head switch the style upIf the niggas hate then let 'em hate and watch the money pile upOr we go upside they head wit' a bottle of bubThey know where we fuckin' be You can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin' drugsI'm into havin' sex, I ain't into makin' love So come give me a hug, if you into getting rubbedYou can find me in the club, bottle full of bubLook, mami, I got the X, if you into takin' drugsI'm into havin' sex, I ain't into makin' loveSo come give me a hug, if you into getting rubbed Don't try to act like you ain't know where we been either, niggaWe in the club all the time, nigga, it's about to pop off, niggaG-Unit\\
a-ha & Take On Me & pop & Talking awayI don't know whatWhat to sayI'll say it anywayToday is another day to find youShying awayOh, I'll be coming for your love, okay? Take On Me (Take On Me)Take me on (Take On Me)I'll be goneIn a day or two So needless to sayI'm odds and endsBut I'll beStumbling awaySlowly learning that life is okaySay after meOh, it's no better to be safe than sorry Take On Me (Take On Me)Take me on (Take On Me)I'll be goneIn a day or two Oh, the things that you sayIs it life or just to playMy worries away?You're all the things I've got to rememberYou're shying awayI'll be coming for you anyway Take On Me (Take On Me)Take me on (Take On Me)I'll be goneIn a dayTake On Me (Take On Me)Take me on (Take On Me)I'll be gone (Take On Me)In a day (Take me on, Take On Me)Take On Me (Take On Me)Take me on (Take On Me)\\
ABBA & Dancing Queen & pop & You can dance, you can jiveHaving the time of your life, oohSee that girl, watch that sceneDig in the Dancing Queen Friday night and the lights are lowLooking out for a place to goWhere they play the right music, getting in the swingYou come to look for a king Anybody could be that guyNight is young and the music's highWith a bit of rock music, everything is fineYou're in the mood for a dance And when you get the chance You are the Dancing QueenYoung and sweet, only seventeenDancing QueenFeel the beat from the tambourine, oh yeah You can dance, you can jiveHaving the time of your life, oohSee that girl, watch that sceneDig in the Dancing Queen You're a teaser, you turn 'em onLeave them burning and then you're goneLooking out for another, anyone will doYou're in the mood for a dance And when you get the chance You are the Dancing QueenYoung and sweet, only seventeenDancing QueenFeel the beat from the tambourine, oh yeah You can dance, you can jiveHaving the time of your life, ohSee that girl, watch that sceneDig in the Dancing Queen Dig in the Dancing Queen\\
\bottomrule
\end{tabular}
\end{table}

At this point, the only thing left to do to get this dataset to align
with our idealized dataset structure is to organize the column ordering
(using \texttt{select()}). I will also arrange the dataset
alphabetically by \texttt{genre} and \texttt{artist} (using
\texttt{arrange()}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm }\SpecialCharTok{|\textgreater{}} \CommentTok{\# original dataset}
  \FunctionTok{select}\NormalTok{(artist, song, lyrics, genre) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# order columns (and rename \textasciigrave{}text\textasciigrave{} to \textasciigrave{}lyrics\textasciigrave{})}
  \FunctionTok{arrange}\NormalTok{(genre, artist) }\CommentTok{\# arrange rows by \textasciigrave{}genre\textasciigrave{} and \textasciigrave{}artist\textasciigrave{}}

\NormalTok{lastfm }\SpecialCharTok{|\textgreater{}} \CommentTok{\# curated dataset}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# first 5 observations}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{) }\CommentTok{\# print pretty table}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-cd-structure-lastfm-order-arrange}{}
\begin{table}
\caption{\label{tbl-cd-structure-lastfm-order-arrange}Sample lyrics from curated Last.fm dataset. }\tabularnewline

\centering
\begin{tabular}{llll}
\toprule
artist & song & lyrics & genre\\
\midrule
Alan Jackson & Little Bitty & Have a little love on a little honeymoonYou got a little dish and you got a little spoonA little bitty house and a little bitty yardA little bitty dog and a little bitty car Well, it's alright to be little bittyA little hometown or a big old cityMight as well share, might as well smileLife goes on for a little bitty while A little bitty baby in a little bitty gownIt'll grow up in a little bitty townA big yellow bus and little bitty booksIt all started with a little bitty look Well, it's alright to be little bittyA little hometown or a big old cityMight as well share, might as well smileLife goes on for a little bitty while Heey You know you got a job and a little bitty checkA six pack of beer and a television setLittle bitty world goes around and aroundLittle bit of silence and a little bit of sound A good ol' boy and a pretty little girlStart all over in a little bitty worldLittle bitty plan and a little bitty dreamIt's all part of a little bitty schemeIt's alright to be little bitty A little hometown or a big old cityMight as well share, might as well smileLife goes on for a little bitty whileIt's alright to be little bittyA little hometown or a big old cityMight as well share, might as well smileLife goes on for a little bitty while Whoo & country\\
Alan Jackson & Remember When & Remember when I was young and so were youAnd time stood still and love was all we knewYou were the first so was IWe made love and then you criedRemember when Remember when we vowed the vowsAnd walk the walkGave our hearts, made the start and it was hardWe lived and learned, life threw curvesThere was joy, there was hurtRemember when Remember when old ones died and new were bornAnd life was changed, dissassembled, rearrangedWe came together, fell apartAnd broke each other's heartsRemember when Remember when the sound of little feetWas the music we danced to week to weekBrought back the love, we found trustVowed we'd never give it upRemember when Remember when thirty seemed so oldNow,lookin' back, it's just a steppin' stoneTo where we are, where we've beenSaid we'd do it all againRemember when Remember when we said when we turned grayWhen the children grow up and move awayWe won't be sad, we'll be gladFor all the life we've hadAnd we'll remember when & country\\
Brad Paisley & Mud on the Tires & I've got some big newsThe bank finally came throughAnd I'm holdin' the keys to a brand new ChevroletHave you been outside, it sure is a nice nightHow about a little test driveDown by the lake? There's a place I know about where the dirt road runs outAnd we can try out the four-wheel driveCome on now what do you sayGirl, I can hardly wait to get a little mud on the tires 'Cause it's a good nightTo be out there soakin' up the moonlightStake out a little piece of shore lineI've got the perfect place in mindIt's in the middle of nowhere only one way to get thereYou got to get a little mud on the tires Moonlight on a duck blindCatfish on a trout lineSun sets about nine this time of yearWe can throw a blanket downCrickets singin' in the backgroundAnd more stars that you can count on a night this clear I tell you what we need to doIs grab a sleepin' bag or twoAnd build us a little campfireAnd then with a little luck we might just get stuckLet's get a little mud on the tires 'Cause it's a good nightTo be out there soakin' up the moonlightStake out a little piece of shore lineI've got the perfect place in mind It's in the middle of nowhere only one way to get thereYou got to get a little mud on the tiresAnd then with a little luck we might just get stuckLet's get a little mud on the tires & country\\
Carrie Underwood & Before He Cheats & Right now, he's probably slow dancin'With a bleached-blond tramp and she's probably gettin' friskyRight now, he's probably buyin' her some fruity little drink'Cause she can't shoot whiskeyRight now, he's probably up behind her with a pool-stickShowin' her how to shoot a comboAnd he don't know I dug my key into the side of his pretty little souped-up four-wheel driveCarved my name into his leather seatsI took a Louisville slugger to both headlightsI slashed a hole in all four tiresMaybe next time he'll think before he cheats Right now, she's probably up singing someWhite-trash version of Shania karaokeRight now, she's probably sayin' ""I'm drunk""And he's a-thinkin' that he's gonna get luckyRight now, he's probably dabbin' onThree dollars worth of that bathroom PoloOh, and he don't know That I dug my key into the side of his pretty little souped-up four-wheel driveCarved my name into his leather seatsI took a Louisville slugger to both headlightsI slashed a hole in all four tiresMaybe next time he'll think before he cheats I might have saved a little trouble for the next girlA-'cause the next time that he cheatsOh, you know it won't be on meNo, not on me 'Cause I dug my key into the side of his pretty little souped-up four-wheel driveCarved my name into his leather seatsI took a Louisville slugger to both headlightsI slashed a hole in all four tiresMaybe next time he'll think before he cheats Oh, maybe next time he'll think before he cheatsOh, before he cheatsOh-oh & country\\
Dierks Bentley & What Was I Thinkin' & Becky was a beauty from south AlabamaHer daddy had a heart like a nine pound hammerThink he even did a little time in the slammerWhat was I thinkin'? She snuck out one night and met me by the front gateHer daddy came out wavin' that 12-gaugeWe tore out the drive, he peppered my tailgateWhat was I thinkin'? Oh, I knew there'd be hell to payBut that crossed my mind a little too late 'Cause I was thinkin' 'bout a little white tank topSittin' right there in the middle by meI was thinkin' 'bout a long kissMan, just gotta get goin' where the night might lead I know what I was feelin'But what was I thinkin'?What was I thinkin'? By the county line, the cops were nippin' on our heelsPulled off the road and kicked it in four-wheelShut off the lights and tore through a cornfieldWhat was I thinkin'? Out the other side, she was hollerin', ""Faster""Took a dirt road, had the radio blastin'Hit the honky-tonk for a little close dancin'What was I thinkin'? Oh, I knew there'd be hell to payBut that crossed my mind a little too late 'Cause I was thinkin' 'bout a little white tank topSittin' right there in the middle by meI was thinkin' 'bout a long kissMan, just gotta get goin' where the night might lead I know what I was feelin'But what was I thinkin'? When a mountain of a man with a ""Born to kill"" tattooTried to cut in, I knocked out his front toothWe ran outside, hood slidin' like Bo DukeWhat was I thinkin'? I finally got her home at a half past two, laterDaddy's in a lawn chair sittin' on the drivewayPut it in park as he started my wayWhat was I thinkin'? Oh, what was I thinkin'?Oh, what was I thinkin'? Then she gave a ""Come and get me"" grinAnd like a bullet, we were gone again 'Cause I was thinkin' 'bout a little white tank topSittin' right there in the middle by meI was thinkin' 'bout a long kissMan, just gotta get goin' where the night might leadI know what I was feelin'Yeah, I know what was I feelin'But what was I thinkin'? What was I thinkin'?I know what I was feelin'What was I thinkin'?Guess I was thinkin' 'bout that tank topThose cutoffs & country\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{write-dataset-1}{%
\subsection{Write dataset}\label{write-dataset-1}}

We now have a curated dataset that we can write to disk. Again, as with
the Europarle Corpus dataset we curated before, we will write this
dataset to the \texttt{data/derived/} directory --effectively ensuring
that it is clear that this dataset was created by our project work.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fs}\SpecialCharTok{::}\FunctionTok{dir\_create}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/derived/lastfm/"}\NormalTok{) }\CommentTok{\# create lastfm subdirectory}
\FunctionTok{write\_csv}\NormalTok{(lastfm, }
          \AttributeTok{file =} \StringTok{"../data/derived/lastfm/lastfm\_curated.csv"}\NormalTok{) }\CommentTok{\# write lastfm to disk and label as the curated dataset}
\end{Highlighting}
\end{Shaded}

And here's an overview of our new directory structure.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{│}\NormalTok{   └── lastfm/}
\ExtensionTok{│}\NormalTok{       └── lastfm\_curated.csv}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{└──}\NormalTok{ lastfm/}
        \ExtensionTok{├──}\NormalTok{ README.md}
        \ExtensionTok{├──}\NormalTok{ country.csv}
        \ExtensionTok{├──}\NormalTok{ hip\_hop.csv}
        \ExtensionTok{├──}\NormalTok{ lyrics.csv}
        \ExtensionTok{├──}\NormalTok{ metal.csv}
        \ExtensionTok{├──}\NormalTok{ pop.csv}
        \ExtensionTok{└──}\NormalTok{ rock.csv}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-7}{%
\subsection{Summary}\label{summary-7}}

Again, to summarize, here is the code that will accomplish the steps we
covered in this section on curating structured datasets.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read Last.fm lyrics and subset relevant columns}
\NormalTok{lastfm }\OtherTok{\textless{}{-}} 
  \FunctionTok{readtext}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/original/lastfm/*.csv"}\NormalTok{, }\CommentTok{\# files to match using *.csv}
           \AttributeTok{text\_field =} \StringTok{"lyrics"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# text column from the datasets}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# convert to a tibble}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{doc\_id, }\SpecialCharTok{{-}}\NormalTok{lyrics\_url) }\CommentTok{\# drop these columns}

\CommentTok{\# Collapse text by artist, song, and genre grouping}
\NormalTok{lastfm }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{group\_by}\NormalTok{(artist, song, genre) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# grouping}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{lyrics =} \FunctionTok{str\_flatten}\NormalTok{(text, }\AttributeTok{collapse =} \StringTok{" "}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# collapse text into the new column \textasciigrave{}lyrics\textasciigrave{} (dropping \textasciigrave{}text\textasciigrave{})}
  \FunctionTok{ungroup}\NormalTok{() }\CommentTok{\# unset the groupings}

\CommentTok{\# Order columns and arrange rows}
\NormalTok{lastfm }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm }\SpecialCharTok{|\textgreater{}} \CommentTok{\# original dataset}
  \FunctionTok{select}\NormalTok{(artist, song, lyrics, genre) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# order columns (and rename \textasciigrave{}text\textasciigrave{} to \textasciigrave{}lyrics\textasciigrave{})}
  \FunctionTok{arrange}\NormalTok{(genre, artist) }\CommentTok{\# arrange rows by \textasciigrave{}genre\textasciigrave{} and \textasciigrave{}artist\textasciigrave{}}

\CommentTok{\# Write curated dataset to disk}
\NormalTok{fs}\SpecialCharTok{::}\FunctionTok{dir\_create}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/derived/lastfm/"}\NormalTok{) }\CommentTok{\# create lastfm subdirectory}
\FunctionTok{write\_csv}\NormalTok{(lastfm, }
          \AttributeTok{file =} \StringTok{"../data/derived/lastfm/lastfm\_curated.csv"}\NormalTok{) }\CommentTok{\# write lastfm to disk and label as the curated dataset}
\end{Highlighting}
\end{Shaded}

\hypertarget{semi-structured}{%
\section{Semi-structured}\label{semi-structured}}

At this point we have discussed curating unstructured data and
structured datasets. Between these two extremes falls semi-structured
data. And as the name suggests, it is a hybrid between unstructured and
structured data. This means that there will be important structured
metadata included with unstructured elements. The file formats and
approaches to encoding the structured aspects of the data vary widely
from resource to resource and therefore often requires more detailed
attention to the structure of the data and often includes more
sophisticated programming strategies to curate the data to produce a
tidy dataset.

As an example we will work with the The Switchboard Dialog Act Corpus
(SDAC) which extends the
\href{https://catalog.ldc.upenn.edu/LDC97S62}{Switchboard Corpus} with
speech act annotation. \textbf{(ADD CITATION)}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

The SDAC dialogues (\texttt{swb1\_dialogact\_annot.tar.gz}) are
available as a \href{https://catalog.ldc.upenn.edu/docs/LDC97S62/}{free
download from the LDC}. To download, decompress, and organize this
resource, follow the strategies discussed in
\protect\hyperlink{acquire-data}{``Acquire data''} for Direct Downloads.
The tadr package provides the \texttt{tadr::get\_compressed\_data()}
function to accomplish this step.

\end{tcolorbox}

\hypertarget{orientation-2}{%
\subsection{Orientation}\label{orientation-2}}

The main directory structure of the SDAC data looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{└──}\NormalTok{ sdac/}
        \ExtensionTok{├──}\NormalTok{ README}
        \ExtensionTok{├──}\NormalTok{ doc/}
        \ExtensionTok{├──}\NormalTok{ sw00utt/}
        \ExtensionTok{├──}\NormalTok{ sw01utt/}
        \ExtensionTok{├──}\NormalTok{ sw02utt/}
        \ExtensionTok{├──}\NormalTok{ sw03utt/}
        \ExtensionTok{├──}\NormalTok{ sw04utt/}
        \ExtensionTok{├──}\NormalTok{ sw05utt/}
        \ExtensionTok{├──}\NormalTok{ sw06utt/}
        \ExtensionTok{├──}\NormalTok{ sw07utt/}
        \ExtensionTok{├──}\NormalTok{ sw08utt/}
        \ExtensionTok{├──}\NormalTok{ sw09utt/}
        \ExtensionTok{├──}\NormalTok{ sw10utt/}
        \ExtensionTok{├──}\NormalTok{ sw11utt/}
        \ExtensionTok{├──}\NormalTok{ sw12utt/}
        \ExtensionTok{└──}\NormalTok{ sw13utt/}
\end{Highlighting}
\end{Shaded}

The \texttt{README} file contains basic information about the resource,
the \texttt{doc/} directory contains more detailed information about the
dialog annotations, and each of the following directories prefixed with
\texttt{sw...} contain individual conversation files. Here's a peek at
internal structure of the first couple directories.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{├──}\NormalTok{ README}
\ExtensionTok{├──}\NormalTok{ doc}
\ExtensionTok{│}\NormalTok{   └── manual.august1.html}
\ExtensionTok{├──}\NormalTok{ sw00utt}
\ExtensionTok{│}\NormalTok{   ├── sw\_0001\_4325.utt}
\ExtensionTok{│}\NormalTok{   ├── sw\_0002\_4330.utt}
\ExtensionTok{│}\NormalTok{   ├── sw\_0003\_4103.utt}
\ExtensionTok{│}\NormalTok{   ├── sw\_0004\_4327.utt}
\ExtensionTok{│}\NormalTok{   ├── sw\_0005\_4646.utt}
\end{Highlighting}
\end{Shaded}

Let's take a look at the first conversation file
(\texttt{sw\_0001\_4325.utt}) to see how it is structured.

\begin{verbatim}
>
*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*
>
*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*
> *x* *x*
> *x* Copyright (C) 1995 University of Pennsylvania *x*
> *x* *x*
> *x* The data in this file are part of a preliminary version of the
*x*
> *x* Penn Treebank Corpus and should not be redistributed.  Any *x*
> *x* research using this corpus or based on it should acknowledge *x*
> *x* that fact, as well as the preliminary nature of the corpus.  *x*
> *x* *x*
>
*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*
>
*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*
>
>
> FILENAME: 4325_1632_1519
> TOPIC#: 323
> DATE: 920323
> TRANSCRIBER: glp
> UTT_CODER: tc
> DIFFICULTY: 1
> TOPICALITY: 3
> NATURALNESS: 2
> ECHO_FROM_B: 1
> ECHO_FROM_A: 4
> STATIC_ON_A: 1
> STATIC_ON_B: 1
> BACKGROUND_A: 1
> BACKGROUND_B: 2
> REMARKS: None.
>
>
=========================================================================
>
>
> o A.1 utt1: Okay.  /
> qw A.1 utt2: {D So, }
>
> qy^d B.2 utt1: [ [ I guess, +
>
> + A.3 utt1: What kind of experience [ do you, + do you ] have, then
with child care? /
\end{verbatim}

There are few things to take note of here. First we see that the
conversation files have a meta-data header offset from the conversation
text by a line of \texttt{=} characters. Second the header contains
meta-information of various types. Third, the text is interleaved with
an annotation scheme.

Some of the information may be readily understandable, such as the
various pieces of meta-data in the header, but to get a better
understanding of what information is encoded here let's take a look at
the \texttt{README} file. In this file we get a birds eye view of what
is going on. In short, the data includes 1155 telephone conversations
between two people annotated with 42 `DAMSL' dialog act labels. The
\texttt{README} file refers us to the \texttt{doc/manual.august1.html}
file for more information on this scheme.

At this point we open the the \texttt{doc/manual.august1.html} file in a
browser and do some investigation. We find out that `DAMSL' stands for
`Discourse Annotation and Markup System of Labeling' and that the first
characters of each line of the conversation text correspond to one or a
combination of labels for each utterance. So for our first utterances we
have:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{o = "Other"}
\NormalTok{qw = "Wh{-}Question"}
\NormalTok{qy\^{}d = "Declarative Yes{-}No{-}Question"}
\NormalTok{+ = "Segment (multi{-}utterance)"}
\end{Highlighting}
\end{Shaded}

Each utterance is also labeled for speaker (`A' or `B'), speaker turn
(`1', `2', `3', etc.), and each utterance within that turn (`utt1',
`utt2', etc.). There is other annotation provided withing each
utterance, but this should be enough to get us started on the
conversations.

Now let's turn to the meta-data in the header. We see here that there is
information about the creation of the file: `FILENAME', `TOPIC', `DATE',
etc. The \texttt{doc/manual.august1.html} file doesn't have much to say
about this information so I returned to the
\href{https://catalog.ldc.upenn.edu/docs/LDC97S62/}{LDC Documentation}
and found more information in the
\href{https://catalog.ldc.upenn.edu/docs/LDC97S62/}{Online
Documentation} section. After some poking around in this documentation I
discovered that that meta-data for each speaker in the corpus is found
in the \texttt{caller\_tab.csv} file. This tabular file does not contain
column names, but the \texttt{caller\_doc.txt} does. After inspecting
these files manually and comparing them with the information in the
conversation file I noticed that the `FILENAME' information contained
three pieces of useful information delimited by underscores \texttt{\_}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*}


\NormalTok{FILENAME:   4325\_1632\_1519}
\NormalTok{TOPIC\#:     323}
\NormalTok{DATE:       920323}
\NormalTok{TRANSCRIBER:    glp}
\end{Highlighting}
\end{Shaded}

The first information is the document id (\texttt{4325}), the second and
third correspond to the speaker number: the first being speaker A
(\texttt{1632}) and the second speaker B (\texttt{1519}).

In sum, we have 1155 conversation files. Each file has two parts, a
header and text section, separated by a line of \texttt{=} characters.
The header section contains a `FILENAME' line which has the document id,
and ids for speaker A and speaker B. The text section is annotated with
DAMSL tags beginning each line, followed by speaker, turn number,
utterance number, and the utterance text. With this knowledge in hand,
let's set out to create a tidy dataset with the following column
structure:

\hypertarget{tbl-cd-semi-sdac-idealized-dataset}{}
\begin{table}
\caption{\label{tbl-cd-semi-sdac-idealized-dataset}Idealized structure for the SDAC dataset. }\tabularnewline

\centering
\begin{tabular}{lllllll}
\toprule
doc\_id & damsl\_tag & speaker & turn\_num & utterance\_num & utterance\_text & speaker\_id\\
\midrule
4325 & o & A & 1 & 1 & Okay.  / & 1632\\
4325 & qw & A & 1 & 2 & \{D So, \} & 1632\\
4325 & qy\textasciicircum{}d & B & 2 & 1 & {}[ [ I guess, + & 1519\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{tidy-the-data-1}{%
\subsection{Tidy the data}\label{tidy-the-data-1}}

Let's begin by reading one of the conversation files into R as a
character vector using the \texttt{read\_lines()} function from the
readr package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_lines}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/original/sdac/sw00utt/sw\_0001\_4325.utt"}\NormalTok{) }\CommentTok{\# read a single file as character vector}
\end{Highlighting}
\end{Shaded}

To isolate the vector element that contains the document and speaker
ids, we use \texttt{str\_detect()} from the stringr package. This
function takes two arguments, a string and a pattern, and returns a
logical value, \texttt{TRUE} if the pattern is matched or \texttt{FALSE}
if not. We can use the output of this function, then, to subset the
\texttt{doc} character vector and only return the vector element (line)
that contains \texttt{digits\_digits\_digits} with a regular expression.
The expression combines the digit matching operator
\texttt{\textbackslash{}\textbackslash{}d} with the \texttt{+} operator
to match 1 or more contiguous digits. We then separate three groups of
\texttt{\textbackslash{}\textbackslash{}d+} with underscores
\texttt{\_}. The result is
\texttt{\textbackslash{}\textbackslash{}d+\_\textbackslash{}\textbackslash{}d+\_\textbackslash{}\textbackslash{}d+}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc[}\FunctionTok{str\_detect}\NormalTok{(doc, }\AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)] }\CommentTok{\# isolate pattern}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "FILENAME:\t4325_1632_1519"
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

The stringr package has a handy function \texttt{str\_view()} and
\texttt{str\_view\_all()} which allow for interactive pattern matching.
There is also an RStudio Addin with the regexplain package which also
can be very helpful for developing regular expression syntax.

\end{tcolorbox}

The next step is to extract the three digit sequences that correspond to
the \texttt{doc\_id}, \texttt{speaker\_a\_id}, and
\texttt{speaker\_b\_id}. First we extract the pattern that we have
identified with \texttt{str\_extract()} and then we can break up the
single character vector into multiple parts based on the underscore
\texttt{\_}. The \texttt{str\_split()} function takes a string and then
a pattern to use to split a character vector. It will return a list of
character vectors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc[}\FunctionTok{str\_detect}\NormalTok{(doc, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)] }\SpecialCharTok{|\textgreater{}} \CommentTok{\# isolate pattern}
  \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# extract the pattern}
  \FunctionTok{str\_split}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"\_"}\NormalTok{) }\CommentTok{\# split the character vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [[1]]
#> [1] "4325" "1632" "1519"
\end{verbatim}

A \textbf{list} is a special object type in R. It is an unordered
collection of objects whose lengths can differ (contrast this with a
data frame which is a collection of objects whose lengths are the same
--hence the tabular format). In this case we have a list of length 1,
whose sole element is a character vector of length 3 --one element per
segment returned from our split. This is a desired result in most cases
as if we were to pass multiple character vectors to our
\texttt{str\_split()} function we don't want the results to be conflated
as a single character vector blurring the distinction between the
individual character vectors. If we \emph{would} like to conflate, or
\emph{flatten} a list, we can use the \texttt{unlist()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc[}\FunctionTok{str\_detect}\NormalTok{(doc, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)] }\SpecialCharTok{|\textgreater{}} \CommentTok{\# isolate pattern}
  \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# extract the pattern}
  \FunctionTok{str\_split}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"\_"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# split the character vector}
  \FunctionTok{unlist}\NormalTok{() }\CommentTok{\# flatten the list to a character vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "4325" "1632" "1519"
\end{verbatim}

Let's flatten the list in this case, as we have a single character
vector, and assign this result to \texttt{doc\_speaker\_info}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc\_speaker\_info }\OtherTok{\textless{}{-}} 
\NormalTok{  doc[}\FunctionTok{str\_detect}\NormalTok{(doc, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)] }\SpecialCharTok{|\textgreater{}} \CommentTok{\# isolate pattern}
  \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# extract the pattern}
  \FunctionTok{str\_split}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"\_"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# split the character vector}
  \FunctionTok{unlist}\NormalTok{() }\CommentTok{\# flatten the list to a character vector}
\end{Highlighting}
\end{Shaded}

\texttt{doc\_speaker\_info} is now a character vector of length three.
Let's subset each of the elements and assign them to meaningful variable
names so we can conveniently use them later on in the tidying process.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc\_id }\OtherTok{\textless{}{-}}\NormalTok{ doc\_speaker\_info[}\DecValTok{1}\NormalTok{] }\CommentTok{\# extract by index}
\NormalTok{speaker\_a\_id }\OtherTok{\textless{}{-}}\NormalTok{ doc\_speaker\_info[}\DecValTok{2}\NormalTok{] }\CommentTok{\# extract by index}
\NormalTok{speaker\_b\_id }\OtherTok{\textless{}{-}}\NormalTok{ doc\_speaker\_info[}\DecValTok{3}\NormalTok{] }\CommentTok{\# extract by index}
\end{Highlighting}
\end{Shaded}

The next step is to isolate the text section extracting it from rest of
the document. As noted previously, a sequence of \texttt{=} separates
the header section from the text section. What we need to do is to index
the point in our character vector \texttt{doc} where that line occurs
and then subset the \texttt{doc} from that point until the end of the
character vector. Let's first find the point where the \texttt{=}
sequence occurs. We will again use the \texttt{str\_detect()} function
to find the pattern we are looking for (a contiguous sequence of
\texttt{=}), but then we will pass the logical result to the
\texttt{which()} function which will return the element index number of
this match.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{str\_detect}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"=+"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# match 1 or more \textasciigrave{}=\textasciigrave{}}
  \FunctionTok{which}\NormalTok{() }\CommentTok{\# find vector index}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 31
\end{verbatim}

So for this file \texttt{31} is the index in \texttt{doc} where the
\texttt{=} sequence occurs. Now it is important to keep in mind that we
are working with a single file from the \texttt{sdac/} data. We need to
be cautious to not create a pattern that may be matched multiple times
in another document in the corpus. As the \texttt{=+} pattern will match
\texttt{=}, or \texttt{==}, or \texttt{===}, etc. it is not implausible
to believe that there might be a \texttt{=} character on some other line
in one of the other files. Let's update our regular expression to avoid
this potential scenario by only matching sequences of three or more
\texttt{=}. In this case we will make use of the curly bracket operators
\texttt{\{\}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{doc }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{str\_detect}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"=\{3,\}"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# match 3 or more \textasciigrave{}=\textasciigrave{}}
  \FunctionTok{which}\NormalTok{() }\CommentTok{\# find vector index}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 31
\end{verbatim}

We will get the same result for this file, but will safeguard ourselves
a bit as it is unlikely we will find multiple matches for \texttt{===},
\texttt{====}, etc.

\texttt{31} is the index for the \texttt{=} sequence, but we want the
next line to be where we start reading the text section. To do this we
increment the index by 1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text\_start\_index }\OtherTok{\textless{}{-}} 
\NormalTok{  doc }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{str\_detect}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"=\{3,\}"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# match 3 or more \textasciigrave{}=\textasciigrave{} }
  \FunctionTok{which}\NormalTok{() }\CommentTok{\# find vector index}
\NormalTok{text\_start\_index }\OtherTok{\textless{}{-}}\NormalTok{ text\_start\_index }\SpecialCharTok{+} \DecValTok{1} \CommentTok{\# increment index by 1}
\end{Highlighting}
\end{Shaded}

The index for the end of the text is simply the length of the
\texttt{doc} vector. We can use the \texttt{length()} function to get
this index.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text\_end\_index }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(doc)}
\end{Highlighting}
\end{Shaded}

We now have the bookends, so to speak, for our text section. To extract
the text we subset the \texttt{doc} vector by these indices.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text }\OtherTok{\textless{}{-}}\NormalTok{ doc[text\_start\_index}\SpecialCharTok{:}\NormalTok{text\_end\_index] }\CommentTok{\# extract text}
\FunctionTok{head}\NormalTok{(text) }\CommentTok{\# preview first lines of \textasciigrave{}text\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] ""                                      
#> [2] ""                                      
#> [3] "o          A.1 utt1: Okay.  /"         
#> [4] "qw          A.1 utt2: {D So, }"        
#> [5] ""                                      
#> [6] "qy^d          B.2 utt1: [ [ I guess, +"
\end{verbatim}

The text has some extra whitespace on some lines and there are blank
lines as well. We should do some cleaning up before moving forward to
organize the data. To get rid of the whitespace we use the
\texttt{str\_trim()} function which by default will remove leading and
trailing whitespace from each line.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text }\OtherTok{\textless{}{-}} \FunctionTok{str\_trim}\NormalTok{(text) }\CommentTok{\# remove leading and trailing whitespace}
\FunctionTok{head}\NormalTok{(text) }\CommentTok{\# preview first lines of \textasciigrave{}text\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] ""                                      
#> [2] ""                                      
#> [3] "o          A.1 utt1: Okay.  /"         
#> [4] "qw          A.1 utt2: {D So, }"        
#> [5] ""                                      
#> [6] "qy^d          B.2 utt1: [ [ I guess, +"
\end{verbatim}

To remove blank lines we will use the a logical expression to subset the
\texttt{text} vector. \texttt{text\ !=\ ""} means return TRUE where
lines are not blank, and FALSE where they are.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text }\OtherTok{\textless{}{-}}\NormalTok{ text[text }\SpecialCharTok{!=} \StringTok{""}\NormalTok{] }\CommentTok{\# remove blank lines}
\FunctionTok{head}\NormalTok{(text) }\CommentTok{\# preview first lines of \textasciigrave{}text\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "o          A.1 utt1: Okay.  /"                                                                  
#> [2] "qw          A.1 utt2: {D So, }"                                                                 
#> [3] "qy^d          B.2 utt1: [ [ I guess, +"                                                         
#> [4] "+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /"
#> [5] "+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /"                        
#> [6] "qy          A.5 utt1: Does it say something? /"
\end{verbatim}

Our first step towards a tidy dataset is to now combine the
\texttt{doc\_id} and each element of \texttt{text} in a data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(doc\_id, text) }\CommentTok{\# tidy format \textasciigrave{}doc\_id\textasciigrave{} and \textasciigrave{}text\textasciigrave{}}
\FunctionTok{slice\_head}\NormalTok{(data, }\AttributeTok{n =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# preview first lines of \textasciigrave{}text\textasciigrave{}}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-cd-semi-sdac-text-8}{}
\begin{table}
\caption{\label{tbl-cd-semi-sdac-text-8}First 5 observations of prelim data curation of the SDAC data. }\tabularnewline

\centering
\begin{tabular}{ll}
\toprule
doc\_id & text\\
\midrule
4325 & o          A.1 utt1: Okay.  /\\
4325 & qw          A.1 utt2: \{D So, \}\\
4325 & qy\textasciicircum{}d          B.2 utt1: [ [ I guess, +\\
4325 & +          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /\\
4325 & +          B.4 utt1: I think, ] + \{F uh, \} I wonder ] if that worked. /\\
\bottomrule
\end{tabular}
\end{table}

With our data now in a data frame, its time to parse the \texttt{text}
column and extract the damsl tags, speaker, speaker turn, utterance
number, and the utterance text itself into separate columns. To do this
we will make extensive use of regular expressions. Our aim is to find a
consistent pattern that distinguishes each piece of information from
other other text in a given row of \texttt{data\$text} and extract it.

The best way to learn regular expressions is to use them. To this end
I've included a link to the interactive regular expression practice
website \href{https://regex101.com}{regex101}.

Open this site and copy the text below into the `TEST STRING' field.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{o          A.1 utt1: Okay.  /}
\NormalTok{qw          A.1 utt2: \{D So, \}}
\NormalTok{qy\^{}d          B.2 utt1: [ [ I guess, +}
\NormalTok{+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /}
\NormalTok{+          B.4 utt1: I think, ] + \{F uh, \} I wonder ] if that worked. /}
\NormalTok{qy          A.5 utt1: Does it say something? /}
\NormalTok{sd          B.6 utt1: I think it usually does.  /}
\NormalTok{ad          B.6 utt2: You might try, \{F uh, \}  /}
\NormalTok{h          B.6 utt3: I don\textquotesingle{}t know,  /}
\NormalTok{ad          B.6 utt4: hold it down a little longer,  /}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h]

{\centering \includegraphics[width=6.39in,height=\textheight]{figures/curate-datasets/cd-regex-101.png}

}

\caption{\label{fig-cd-regex-101-image}RegEx101}

\end{figure}

Now manually type the following regular expressions into the `REGULAR
EXPRESSION' field one-by-one (each is on a separate line). Notice what
is matched as you type and when you've finished typing. You can find out
exactly what the component parts of each expression are doing by
toggling the top right icon in the window or hovering your mouse over
the relevant parts of the expression.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\^{}.+?\textbackslash{}s}
\NormalTok{[AB]\textbackslash{}.\textbackslash{}d+}
\NormalTok{utt\textbackslash{}d+}
\NormalTok{:.+$}
\end{Highlighting}
\end{Shaded}

As you can now see, we have regular expressions that will match the
damsl tags, speaker and speaker turn, utterance number, and the
utterance text. To apply these expressions to our data and extract this
information into separate columns we will make use of the
\texttt{mutate()} and \texttt{str\_extract()} functions.
\texttt{mutate()} will take our data frame and create new columns with
values we match and extract from each row in the data frame with
\texttt{str\_extract()}. Notice that \texttt{str\_extract()} is
different than \texttt{str\_extract\_all()}. When we work with
\texttt{mutate()} each row will be evaluated in turn, therefore we only
need to make one match per row in \texttt{data\$text}.

I've chained each of these steps in the code below, dropping the
original \texttt{text} column with \texttt{select(-text)}, and
overwriting \texttt{data} with the results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract column information from \textasciigrave{}text\textasciigrave{}}
\NormalTok{data }\OtherTok{\textless{}{-}} 
\NormalTok{  data }\SpecialCharTok{|\textgreater{}} \CommentTok{\# current dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{damsl\_tag =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{"\^{}.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# extract damsl tags}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{speaker\_turn =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{"[AB]}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# extract speaker\_turn pairs}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_num =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{"utt}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# extract utterance number}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_text =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{":.+$"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# extract utterance text}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{text) }\CommentTok{\# drop the \textasciigrave{}text\textasciigrave{} column}

\FunctionTok{glimpse}\NormalTok{(data) }\CommentTok{\# preview the data set}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 159
#> Columns: 5
#> $ doc_id         <chr> "4325", "4325", "4325", "4325", "4325", "4325", "4325",~
#> $ damsl_tag      <chr> "o ", "qw ", "qy^d ", "+ ", "+ ", "qy ", "sd ", "ad ", ~
#> $ speaker_turn   <chr> "A.1", "A.1", "B.2", "A.3", "B.4", "A.5", "B.6", "B.6",~
#> $ utterance_num  <chr> "utt1", "utt2", "utt1", "utt1", "utt1", "utt1", "utt1",~
#> $ utterance_text <chr> ": Okay.  /", ": {D So, }", ": [ [ I guess, +", ": What~
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

One twist you will notice is that regular expressions in R require
double backslashes
(\texttt{\textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}})
where other programming environments use a single backslash
(\texttt{\textbackslash{}\textbackslash{}}).

\end{tcolorbox}

There are a couple things left to do to the columns we extracted from
the text before we move on to finishing up our tidy dataset. First, we
need to separate the \texttt{speaker\_turn} column into \texttt{speaker}
and \texttt{turn\_num} columns and second we need to remove unwanted
characters from the \texttt{damsl\_tag}, \texttt{utterance\_num}, and
\texttt{utterance\_text} columns.

To separate the values of a column into two columns we use the
\texttt{separate()} function. It takes a column to separate and
character vector of the names of the new columns to create. By default
the values of the input column will be separated by non-alphanumeric
characters. In our case this means the \texttt{.} will be our separator.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}}
\NormalTok{  data }\SpecialCharTok{|\textgreater{}} \CommentTok{\# current dataset}
  \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ speaker\_turn, }\CommentTok{\# source column}
           \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"speaker"}\NormalTok{, }\StringTok{"turn\_num"}\NormalTok{)) }\CommentTok{\# separate speaker\_turn into distinct columns: speaker and turn\_num}

\FunctionTok{glimpse}\NormalTok{(data) }\CommentTok{\# preview the data set}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 159
#> Columns: 6
#> $ doc_id         <chr> "4325", "4325", "4325", "4325", "4325", "4325", "4325",~
#> $ damsl_tag      <chr> "o ", "qw ", "qy^d ", "+ ", "+ ", "qy ", "sd ", "ad ", ~
#> $ speaker        <chr> "A", "A", "B", "A", "B", "A", "B", "B", "B", "B", "B", ~
#> $ turn_num       <chr> "1", "1", "2", "3", "4", "5", "6", "6", "6", "6", "6", ~
#> $ utterance_num  <chr> "utt1", "utt2", "utt1", "utt1", "utt1", "utt1", "utt1",~
#> $ utterance_text <chr> ": Okay.  /", ": {D So, }", ": [ [ I guess, +", ": What~
\end{verbatim}

To remove unwanted leading or trailing whitespace we apply the
\texttt{str\_trim()} function. For removing other characters we matching
the character(s) and replace them with an empty string (\texttt{""})
with the \texttt{str\_replace()} function. Again, I've chained these
functions together and overwritten \texttt{data} with the results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Clean up column information}
\NormalTok{data }\OtherTok{\textless{}{-}} 
\NormalTok{  data }\SpecialCharTok{|\textgreater{}} \CommentTok{\# current dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{damsl\_tag =} \FunctionTok{str\_trim}\NormalTok{(damsl\_tag)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# remove leading/ trailing whitespace}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_num =} \FunctionTok{str\_replace}\NormalTok{(}\AttributeTok{string =}\NormalTok{ utterance\_num, }\AttributeTok{pattern =} \StringTok{"utt"}\NormalTok{, }\AttributeTok{replacement =} \StringTok{""}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# remove \textquotesingle{}utt\textquotesingle{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_text =} \FunctionTok{str\_replace}\NormalTok{(}\AttributeTok{string =}\NormalTok{ utterance\_text, }\AttributeTok{pattern =} \StringTok{":}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{, }\AttributeTok{replacement =} \StringTok{""}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# remove \textquotesingle{}: \textquotesingle{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_text =} \FunctionTok{str\_trim}\NormalTok{(utterance\_text)) }\CommentTok{\# trim leading/ trailing whitespace}

\FunctionTok{glimpse}\NormalTok{(data) }\CommentTok{\# preview the data set}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 159
#> Columns: 6
#> $ doc_id         <chr> "4325", "4325", "4325", "4325", "4325", "4325", "4325",~
#> $ damsl_tag      <chr> "o", "qw", "qy^d", "+", "+", "qy", "sd", "ad", "h", "ad~
#> $ speaker        <chr> "A", "A", "B", "A", "B", "A", "B", "B", "B", "B", "B", ~
#> $ turn_num       <chr> "1", "1", "2", "3", "4", "5", "6", "6", "6", "6", "6", ~
#> $ utterance_num  <chr> "1", "2", "1", "1", "1", "1", "1", "2", "3", "4", "5", ~
#> $ utterance_text <chr> "Okay.  /", "{D So, }", "[ [ I guess, +", "What kind of~
\end{verbatim}

To round out our tidy dataset for this single conversation file we will
connect the \texttt{speaker\_a\_id} and \texttt{speaker\_b\_id} with
speaker A and B in our current dataset adding a new column
\texttt{speaker\_id}. The \texttt{case\_when()} function does exactly
this: allows us to map rows of \texttt{speaker} with the value ``A'' to
\texttt{speaker\_a\_id} and rows with value ``B'' to
\texttt{speaker\_b\_id}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Link speaker with speaker\_id}
\NormalTok{data }\OtherTok{\textless{}{-}} 
\NormalTok{  data }\SpecialCharTok{|\textgreater{}} \CommentTok{\# current dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{speaker\_id =} \FunctionTok{case\_when}\NormalTok{( }\CommentTok{\# create speaker\_id}
\NormalTok{    speaker }\SpecialCharTok{==} \StringTok{"A"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ speaker\_a\_id, }\CommentTok{\# speaker\_a\_id value when A}
\NormalTok{    speaker }\SpecialCharTok{==} \StringTok{"B"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ speaker\_b\_id }\CommentTok{\# speaker\_b\_id value when B}
\NormalTok{  ))}

\FunctionTok{glimpse}\NormalTok{(data) }\CommentTok{\# preview the data set}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 159
#> Columns: 7
#> $ doc_id         <chr> "4325", "4325", "4325", "4325", "4325", "4325", "4325",~
#> $ damsl_tag      <chr> "o", "qw", "qy^d", "+", "+", "qy", "sd", "ad", "h", "ad~
#> $ speaker        <chr> "A", "A", "B", "A", "B", "A", "B", "B", "B", "B", "B", ~
#> $ turn_num       <chr> "1", "1", "2", "3", "4", "5", "6", "6", "6", "6", "6", ~
#> $ utterance_num  <chr> "1", "2", "1", "1", "1", "1", "1", "2", "3", "4", "5", ~
#> $ utterance_text <chr> "Okay.  /", "{D So, }", "[ [ I guess, +", "What kind of~
#> $ speaker_id     <chr> "1632", "1632", "1519", "1632", "1519", "1632", "1519",~
\end{verbatim}

We now have the tidy dataset we set out to create. But this dataset only
includes one conversation file! We want to apply this code to all 1155
conversation files in the \texttt{sdac/} corpus. The approach will be to
create a custom function which groups the code we've done for this
single file and then iterative send each file from the corpus through
this function and combine the results into one data frame.

Here's the custom function with some extra code to print a progress
message for each file when it runs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extract\_sdac\_metadata }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(file) \{}
  \CommentTok{\# Function: to read a Switchboard Corpus Dialogue file and extract meta{-}data}
  \FunctionTok{cat}\NormalTok{(}\StringTok{"Reading"}\NormalTok{, }\FunctionTok{basename}\NormalTok{(file), }\StringTok{"..."}\NormalTok{)}
  
  \CommentTok{\# Read \textasciigrave{}file\textasciigrave{} by lines}
\NormalTok{  doc }\OtherTok{\textless{}{-}} \FunctionTok{read\_lines}\NormalTok{(file) }
  
  \CommentTok{\# Extract \textasciigrave{}doc\_id\textasciigrave{}, \textasciigrave{}speaker\_a\_id\textasciigrave{}, and \textasciigrave{}speaker\_b\_id\textasciigrave{}}
\NormalTok{  doc\_speaker\_info }\OtherTok{\textless{}{-}} 
\NormalTok{    doc[}\FunctionTok{str\_detect}\NormalTok{(doc, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)] }\SpecialCharTok{|\textgreater{}} \CommentTok{\# isolate pattern}
    \FunctionTok{str\_extract}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+\_}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# extract the pattern}
    \FunctionTok{str\_split}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"\_"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# split the character vector}
    \FunctionTok{unlist}\NormalTok{() }\CommentTok{\# flatten the list to a character vector}
\NormalTok{  doc\_id }\OtherTok{\textless{}{-}}\NormalTok{ doc\_speaker\_info[}\DecValTok{1}\NormalTok{] }\CommentTok{\# extract \textasciigrave{}doc\_id\textasciigrave{}}
\NormalTok{  speaker\_a\_id }\OtherTok{\textless{}{-}}\NormalTok{ doc\_speaker\_info[}\DecValTok{2}\NormalTok{] }\CommentTok{\# extract \textasciigrave{}speaker\_a\_id\textasciigrave{}}
\NormalTok{  speaker\_b\_id }\OtherTok{\textless{}{-}}\NormalTok{ doc\_speaker\_info[}\DecValTok{3}\NormalTok{] }\CommentTok{\# extract \textasciigrave{}speaker\_b\_id\textasciigrave{}}
  
  \CommentTok{\# Extract \textasciigrave{}text\textasciigrave{}}
\NormalTok{  text\_start\_index }\OtherTok{\textless{}{-}} \CommentTok{\# find where header info stops}
\NormalTok{    doc }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{str\_detect}\NormalTok{(}\AttributeTok{pattern =} \StringTok{"=\{3,\}"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# match 3 or more \textasciigrave{}=\textasciigrave{}}
    \FunctionTok{which}\NormalTok{() }\CommentTok{\# find vector index}
  
\NormalTok{  text\_start\_index }\OtherTok{\textless{}{-}}\NormalTok{ text\_start\_index }\SpecialCharTok{+} \DecValTok{1} \CommentTok{\# increment index by 1}
\NormalTok{  text\_end\_index }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(doc) }\CommentTok{\# get the end of the text section}
  
\NormalTok{  text }\OtherTok{\textless{}{-}}\NormalTok{ doc[text\_start\_index}\SpecialCharTok{:}\NormalTok{text\_end\_index] }\CommentTok{\# extract text}
\NormalTok{  text }\OtherTok{\textless{}{-}} \FunctionTok{str\_trim}\NormalTok{(text) }\CommentTok{\# remove leading and trailing whitespace}
\NormalTok{  text }\OtherTok{\textless{}{-}}\NormalTok{ text[text }\SpecialCharTok{!=} \StringTok{""}\NormalTok{] }\CommentTok{\# remove blank lines}
  
\NormalTok{  data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(doc\_id, text) }\CommentTok{\# tidy format \textasciigrave{}doc\_id\textasciigrave{} and \textasciigrave{}text\textasciigrave{}}
  
  \CommentTok{\# Extract column information from \textasciigrave{}text\textasciigrave{}}
\NormalTok{  data }\OtherTok{\textless{}{-}} 
\NormalTok{    data }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{damsl\_tag =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{"\^{}.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# extract damsl tags}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{speaker\_turn =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{"[AB]}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# extract speaker\_turn pairs}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_num =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{"utt}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# extract utterance number}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_text =} \FunctionTok{str\_extract}\NormalTok{(}\AttributeTok{string =}\NormalTok{ text, }\AttributeTok{pattern =} \StringTok{":.+$"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# extract utterance text}
    \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{text)}
  
  \CommentTok{\# Separate speaker\_turn into distinct columns}
\NormalTok{  data }\OtherTok{\textless{}{-}}
\NormalTok{    data }\SpecialCharTok{|\textgreater{}} \CommentTok{\# current dataset}
    \FunctionTok{separate}\NormalTok{(}\AttributeTok{col =}\NormalTok{ speaker\_turn, }\CommentTok{\# source column}
             \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"speaker"}\NormalTok{, }\StringTok{"turn\_num"}\NormalTok{)) }\CommentTok{\# separate speaker\_turn into distinct columns: speaker and turn\_num}
  
  \CommentTok{\# Clean up column information}
\NormalTok{  data }\OtherTok{\textless{}{-}} 
\NormalTok{    data }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{damsl\_tag =} \FunctionTok{str\_trim}\NormalTok{(damsl\_tag)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# remove leading/ trailing whitespace}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_num =} \FunctionTok{str\_replace}\NormalTok{(}\AttributeTok{string =}\NormalTok{ utterance\_num, }\AttributeTok{pattern =} \StringTok{"utt"}\NormalTok{, }\AttributeTok{replacement =} \StringTok{""}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# remove \textquotesingle{}utt\textquotesingle{}}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_text =} \FunctionTok{str\_replace}\NormalTok{(}\AttributeTok{string =}\NormalTok{ utterance\_text, }\AttributeTok{pattern =} \StringTok{":}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{, }\AttributeTok{replacement =} \StringTok{""}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# remove \textquotesingle{}: \textquotesingle{}}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utterance\_text =} \FunctionTok{str\_trim}\NormalTok{(utterance\_text)) }\CommentTok{\# trim leading/ trailing whitespace}
  
  \CommentTok{\# Link speaker with speaker\_id}
\NormalTok{  data }\OtherTok{\textless{}{-}} 
\NormalTok{    data }\SpecialCharTok{|\textgreater{}} \CommentTok{\# current dataset}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{speaker\_id =} \FunctionTok{case\_when}\NormalTok{( }\CommentTok{\# create speaker\_id}
\NormalTok{      speaker }\SpecialCharTok{==} \StringTok{"A"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ speaker\_a\_id, }\CommentTok{\# speaker\_a\_id value when A}
\NormalTok{      speaker }\SpecialCharTok{==} \StringTok{"B"} \SpecialCharTok{\textasciitilde{}}\NormalTok{ speaker\_b\_id }\CommentTok{\# speaker\_b\_id value when B}
\NormalTok{    ))}
  \FunctionTok{cat}\NormalTok{(}\StringTok{" done.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(data) }\CommentTok{\# return the data frame object}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

As a sanity check we will run the \texttt{extract\_sdac\_metadata()}
function on a the conversation file we were just working on to make sure
it works as expected.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{extract\_sdac\_metadata}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/original/sdac/sw00utt/sw\_0001\_4325.utt"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Reading sw_0001_4325.utt ... done.
#> Rows: 159
#> Columns: 7
#> $ doc_id         <chr> "4325", "4325", "4325", "4325", "4325", "4325", "4325",~
#> $ damsl_tag      <chr> "o", "qw", "qy^d", "+", "+", "qy", "sd", "ad", "h", "ad~
#> $ speaker        <chr> "A", "A", "B", "A", "B", "A", "B", "B", "B", "B", "B", ~
#> $ turn_num       <chr> "1", "1", "2", "3", "4", "5", "6", "6", "6", "6", "6", ~
#> $ utterance_num  <chr> "1", "2", "1", "1", "1", "1", "1", "2", "3", "4", "5", ~
#> $ utterance_text <chr> "Okay.  /", "{D So, }", "[ [ I guess, +", "What kind of~
#> $ speaker_id     <chr> "1632", "1632", "1519", "1632", "1519", "1632", "1519",~
\end{verbatim}

Looks good!

So now it's time to create a vector with the paths to all of the
conversation files. \texttt{fs::dir\_ls()} interfaces with our OS file
system and will return the paths to the files in the specified
directory. We also add a pattern to match conversation files
(\texttt{regexp\ =\ \textbackslash{}\textbackslash{}.utt\$}) so we don't
accidentally include other files in the corpus. \texttt{recurse} set to
\texttt{TRUE} means we will get the full path to each file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_files }\OtherTok{\textless{}{-}} 
\NormalTok{  fs}\SpecialCharTok{::}\FunctionTok{dir\_ls}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/original/sdac/"}\NormalTok{, }\CommentTok{\# source directory}
             \AttributeTok{recurse =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# traverse all sub{-}directories}
             \AttributeTok{type =} \StringTok{"file"}\NormalTok{, }\CommentTok{\# only return files}
             \AttributeTok{regexp =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.utt$"}\NormalTok{) }\CommentTok{\# only return files ending in .utt}
\FunctionTok{head}\NormalTok{(sdac\_files) }\CommentTok{\# preview file paths}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{../data/original/sdac/sw00utt/sw\_0001\_4325.utt}
\NormalTok{../data/original/sdac/sw00utt/sw\_0002\_4330.utt}
\NormalTok{../data/original/sdac/sw00utt/sw\_0003\_4103.utt}
\NormalTok{../data/original/sdac/sw00utt/sw\_0004\_4327.utt}
\NormalTok{../data/original/sdac/sw00utt/sw\_0005\_4646.utt}
\NormalTok{../data/original/sdac/sw00utt/sw\_0006\_4108.utt}
\end{Highlighting}
\end{Shaded}

o pass each conversation file in the vector of paths to our conversation
files iteratively to the \texttt{extract\_sdac\_metadata()} function we
use \texttt{map()}. This will apply the function to each conversation
file and return a data frame for each. \texttt{bind\_rows()} will then
join the resulting data frames by rows to give us a single tidy dataset
for all 1155 conversations. Note there is a lot of processing going on
here we have to be patient.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read files and return a tidy dataset}
\NormalTok{sdac }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_files }\SpecialCharTok{|\textgreater{}} \CommentTok{\# pass file names}
  \FunctionTok{map}\NormalTok{(extract\_sdac\_metadata) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# read and tidy iteratively }
  \FunctionTok{bind\_rows}\NormalTok{() }\CommentTok{\# bind the results into a single data frame}
\end{Highlighting}
\end{Shaded}

We now see that we have 223606 observations (individual utterances in
this dataset).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(sdac) }\CommentTok{\# preview complete curated dataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 223,606
#> Columns: 7
#> $ doc_id         <chr> "4325", "4325", "4325", "4325", "4325", "4325", "4325",~
#> $ damsl_tag      <chr> "o", "qw", "qy^d", "+", "+", "qy", "sd", "ad", "h", "ad~
#> $ speaker        <chr> "A", "A", "B", "A", "B", "A", "B", "B", "B", "B", "B", ~
#> $ turn_num       <chr> "1", "1", "2", "3", "4", "5", "6", "6", "6", "6", "6", ~
#> $ utterance_num  <chr> "1", "2", "1", "1", "1", "1", "1", "2", "3", "4", "5", ~
#> $ utterance_text <chr> "Okay.  /", "{D So, }", "[ [ I guess, +", "What kind of~
#> $ speaker_id     <chr> "1632", "1632", "1519", "1632", "1519", "1632", "1519",~
\end{verbatim}

\hypertarget{write-datasets}{%
\subsection{Write datasets}\label{write-datasets}}

Again as in the previous cases, we will write this dataset to disk to
prepare for the next step in our text analysis project.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fs}\SpecialCharTok{::}\FunctionTok{dir\_create}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/derived/sdac/"}\NormalTok{) }\CommentTok{\# create sdac subdirectory}
\FunctionTok{write\_csv}\NormalTok{(sdac, }
          \AttributeTok{file =} \StringTok{"../data/derived/sdac/sdac\_curated.csv"}\NormalTok{) }\CommentTok{\# write sdac to disk and label as the curated dataset}
\end{Highlighting}
\end{Shaded}

The directory structure now looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{│}\NormalTok{   └── sdac/}
\ExtensionTok{│}\NormalTok{       └── sdac\_curated.csv}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{└──}\NormalTok{ sdac/}
        \ExtensionTok{├──}\NormalTok{ README}
        \ExtensionTok{├──}\NormalTok{ doc/}
        \ExtensionTok{├──}\NormalTok{ sw00utt/}
        \ExtensionTok{├──}\NormalTok{ sw01utt/}
        \ExtensionTok{├──}\NormalTok{ sw02utt/}
        \ExtensionTok{├──}\NormalTok{ sw03utt/}
        \ExtensionTok{├──}\NormalTok{ sw04utt/}
        \ExtensionTok{├──}\NormalTok{ sw05utt/}
        \ExtensionTok{├──}\NormalTok{ sw06utt/}
        \ExtensionTok{├──}\NormalTok{ sw07utt/}
        \ExtensionTok{├──}\NormalTok{ sw08utt/}
        \ExtensionTok{├──}\NormalTok{ sw09utt/}
        \ExtensionTok{├──}\NormalTok{ sw10utt/}
        \ExtensionTok{├──}\NormalTok{ sw11utt/}
        \ExtensionTok{├──}\NormalTok{ sw12utt/}
        \ExtensionTok{└──}\NormalTok{ sw13utt/}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-8}{%
\subsection{Summary}\label{summary-8}}

In this section we looked at semi-structured data. This type of data
often requires the most work to organize into a tidy dataset. We
continued to work with many of the R programming strategies introduced
to this point in the coursebook. We also made more extensive use of
regular expressions to pick out information from a semi-structured
document format.

To round out this section I've provided a code summary of the steps
involved to conduct the curation of the Switchboard Dialogue Act Corpus
files. Note that I've added the \texttt{extract\_sdac\_metadata()}
custom function to a file called \texttt{curate\_functions.R} and
sourced this file. This will make the code more succinct and legible
here, as well in your own research projects.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Source the \textasciigrave{}extract\_sdac\_metadata()\textasciigrave{} function}
\FunctionTok{source}\NormalTok{(}\StringTok{"../functions/curate\_functions.R"}\NormalTok{) }

\CommentTok{\# Get list of the corpus files (.utt)}
\NormalTok{sdac\_files }\OtherTok{\textless{}{-}} 
\NormalTok{  fs}\SpecialCharTok{::}\FunctionTok{dir\_ls}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/original/sdac/"}\NormalTok{, }\CommentTok{\# source directory}
             \AttributeTok{recurse =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# traverse all sub{-}directories}
             \AttributeTok{type =} \StringTok{"file"}\NormalTok{, }\CommentTok{\# only return files}
             \AttributeTok{regexp =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.utt$"}\NormalTok{) }\CommentTok{\# only return files ending in .utt}

\CommentTok{\# Read files and return a tidy dataset}
\NormalTok{sdac }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_files }\SpecialCharTok{|\textgreater{}} \CommentTok{\# pass file names}
  \FunctionTok{map}\NormalTok{(extract\_sdac\_metadata) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# read and tidy iteratively }
  \FunctionTok{bind\_rows}\NormalTok{() }\CommentTok{\# bind the results into a single data frame}
  
\CommentTok{\# Write curated dataset to disk}
\NormalTok{fs}\SpecialCharTok{::}\FunctionTok{dir\_create}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/derived/sdac/"}\NormalTok{) }\CommentTok{\# create sdac subdirectory}
\FunctionTok{write\_csv}\NormalTok{(sdac, }
          \AttributeTok{file =} \StringTok{"../data/derived/sdac/sdac\_curated.csv"}\NormalTok{) }\CommentTok{\# write sdac to disk and label as the curated dataset}
\end{Highlighting}
\end{Shaded}

\hypertarget{documentation-2}{%
\section{Documentation}\label{documentation-2}}

At this stage we again want to ensure that the data that we have derived
is well-documented. Where in the data acquisition process the
documentation was focused on the sampling frame, curated datasets
require documentation that describes the structure of the now
rectangular dataset and its attributes. This documentation is known as a
\textbf{data dictionary}. At the curation stage this documentation often
contains the following information ({``How to Make a Data Dictionary''}
2021):

\begin{itemize}
\tightlist
\item
  names of the variables (as they appear in the dataset)
\item
  human-readable names for the variables
\item
  short prose descriptions of the variables, including units of
  measurement (where applicable)
\end{itemize}

A data dictionary will take the format of a table and can be stored in a
tabular-oriented file format (such as .csv). It is often easier to work
with a spreadsheet to create this documentation. I suggest creating a
.csv file with the basic structure of the documentation. You can do this
however you choose, but I suggest using something along these lines as
seen in the following custom function, \texttt{data\_dic\_starter()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_dic\_starter }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, file\_path) \{}
  \CommentTok{\# Function:}
  \CommentTok{\# Creates a .csv file with the basic information}
  \CommentTok{\# to document a curated dataset}
  
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{variable\_name =} \FunctionTok{names}\NormalTok{(data), }\CommentTok{\# column with existing variable names }
       \AttributeTok{name =} \StringTok{""}\NormalTok{, }\CommentTok{\# column for human{-}readable names}
       \AttributeTok{description =} \StringTok{""}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# column for prose description}
  \FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{file =}\NormalTok{ file\_path) }\CommentTok{\# write to disk}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Running this function in the R Console on the curated dataset (in this
case the \texttt{sdac} dataset), will provide this structure.

\hypertarget{tbl-cd-documentation-dic-starter-structure}{}
\begin{table}
\caption{\label{tbl-cd-documentation-dic-starter-structure}Data dictionary starter structure for the SDAC curated dataset. }\tabularnewline

\centering
\begin{tabular}{lll}
\toprule
variable\_name & name & description\\
\midrule
doc\_id &  & \\
damsl\_tag &  & \\
speaker &  & \\
turn\_num &  & \\
utterance\_num &  & \\
\addlinespace
utterance\_text &  & \\
speaker\_id &  & \\
\bottomrule
\end{tabular}
\end{table}

The resulting .csv file can then be opened with spreadsheet software
(such as MS Excel, Google Sheets, etc.) and edited.\footnote{Note on
  RStudio Cloud you will need to download the .csv file and, after
  editing, upload the complete data dictionary file. Make sure to save
  the edited file as a .csv file.}

\begin{figure}[h]

{\centering \includegraphics[width=5.31in,height=\textheight]{figures/curate-datasets/cd-data-dictionary.png}

}

\end{figure}

Save this file as a .csv file and replace the original starter file.
Note that it is important to use a plain-text file format for the
official documentation file and avoid proprietary formats to ensure open
accessibility and future compatibility.\footnote{Although based on
  spreadsheets, Broman and Woo (2018) outlines many of the best for good
  data organization regardless of the technology.}

Our \texttt{data/derived/} directory now looks like this.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{└──}\NormalTok{ derived/}
    \ExtensionTok{└──}\NormalTok{ sdac/}
        \ExtensionTok{├──}\NormalTok{ sdac\_curated.csv}
        \ExtensionTok{└──}\NormalTok{ data\_dictionary\_sdac.csv}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-9}{%
\section*{Summary}\label{summary-9}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we looked at the process of structuring data into a
dataset. This included a discussion on three main types of data
--unstructured, structured, and semi-structured. The level of structure
of the original data(set) will vary from resource to resource and by the
same token so will the file format used to support the level of
meta-information included. The results from our data curation resulted
in a curated dataset that is saved separate from the original data to
maintain modularity between what the data(set) looked like before we
intervene and afterwards. In addition to the code we use to derived the
curated dataset's structure, we also include a data dictionary which
documents the names of the variables and provides sufficient description
of these variables so that it is clear what our dataset contains.

It is important to recognized that this curated dataset will form the
base for the next step in our text analysis project and the last step in
data preparation for analysis: dataset transformation. This last step in
preparing data for analysis is to convert this curated dataset into a
dataset that is directly aligned with the research aims (i.e.~analysis
method(s)) of the project. Since there can be multiple analysis
approaches applied the original data in a research project, this curated
dataset serves as the point of departure for each of the subsequent
datasets derived from the transformational steps.

\hypertarget{activities-5}{%
\section*{Activities}\label{activities-5}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_7.html}{Regular
Expressions and reshaping datasets}\\
\textbf{How}: Read Recipe 7 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To how regular expressions are helpful in developing
strategies for matching, extracting, and/ or replacing patterns in
character sequences and how to change the dimensions of a dataset to
either expand or collapse columns or rows.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/lab_7}{Regular
Expressions and reshaping datasets}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 7.\\
\textbf{Why}: To gain experience working with coding strategies
reshaping data using tidyverse functions and regular expressions, to
practice reading/ writing data from/ to disk, and to implement
organizational strategies for organizing and documenting a dataset in
reproducible fashion.

\end{tcolorbox}

\hypertarget{questions-6}{%
\section*{Questions}\label{questions-6}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Conceptual questions}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Technical exercises}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{enumerate}

\end{tcolorbox}

\hypertarget{sec-transform-datasets}{%
\chapter{Transform datasets}\label{sec-transform-datasets}}

\begin{quote}
Nothing is lost. Everything is transformed.

--- Michael Ende, The Neverending Story
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  What is the role of data transformation in a text analysis project?
\item
  What are the general processes for preparing datasets for analysis?
\item
  How do each of these general processes transform datasets?
\end{itemize}

\end{tcolorbox}

In this chapter we turn our attention to the process of moving a curated
dataset one step closer to analysis. Where in the process of curating
data into a dataset the goal was to derived a tidy dataset that
contained the main relational characteristics of the data for our text
analysis project, the transformation step refines and potential expands
these characteristics such that they are more in line with our analysis
aims. In this chapter I have grouped various transformation steps into
four categories: normalization, recoding, generation, and merging. It is
of note that the these categories have been ordered and are covered
separately for descriptive reasons. In practice the ordering of which
transformation to apply before another is highly idiosyncratic and
requires that the researcher evaluate the characteristics of the dataset
and the desired results.

Furthermore, since in any given project there may be more than one
analysis that may be performed on the data, there may be distinct
transformation steps which correspond to each analysis approach.
Therefore it is possible that there are more than one transformed
dataset created from the curated dataset. This is one of the reasons
that we create a curated dataset instead of derived a transformed
dataset from the original data. The curated dataset serves as a point of
departure from which multiple transformational methods can derive
distinct formats for distinct analyses.

Let's now turn to demonstrations of some common transformational steps
using datasets with which we are now familiar.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Swirl}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/swirl}{Data
manipulation}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To learn more about data frame objects in R and how to
manipulate them using the \texttt{dplyr} package.

\end{tcolorbox}

\hypertarget{normalize}{%
\section{Normalize}\label{normalize}}

The process of normalizing datasets in essence is to santize the values
of variable or set of variables such that there are no artifacts that
will contaminate subsequent processing. It may be the case that
non-linguistic metadata may require normalization but more often than
not linguistic information is the most common target for normalization
as text often includes artifacts from the acquisition process which will
not be desired in the analysis.

\textbf{Europarle Corpus}

Consider the curated Europarle Corpus dataset. I will read in the
dataset. Since the dataset is quite large, I have also subsetted the
dataset keeping only the first 1,000 observations for each of value of
\texttt{type} for demonstration purposes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/derived/europarle/europarle\_curated.csv"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# read curated dataset}
  \FunctionTok{filter}\NormalTok{(sentence\_id }\SpecialCharTok{\textless{}} \DecValTok{1001}\NormalTok{) }\CommentTok{\# keep first 1000 observations for each type}

\FunctionTok{glimpse}\NormalTok{(europarle)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 2,000
#> Columns: 3
#> $ type        <chr> "Source", "Target", "Source", "Target", "Source", "Target"~
#> $ sentence_id <dbl> 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, ~
#> $ sentence    <chr> "Reanudación del período de sesiones", "Resumption of the ~
\end{verbatim}

Simply looking at the first 14 lines of this dataset, we can see that if
our goal is to work with the transcribed (`Source') and translated
(`Target') language, there are lines which do not appear to be of
interest.

\hypertarget{tbl-td-europarle-preview-1}{}
\begin{table}
\caption{\label{tbl-td-europarle-preview-1}Europarle Corpus curated dataset preview. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 1 & Reanudación del período de sesiones\\
Target & 1 & Resumption of the session\\
Source & 2 & Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\\
Target & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Source & 3 & Como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\
\addlinespace
Target & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Source & 4 & Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\
Target & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
Source & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\\
Target & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
\addlinespace
Source & 6 & Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
Target & 6 & Please rise, then, for this minute' s silence.\\
Source & 7 & (El Parlamento, de pie, guarda un minuto de silencio)\\
Target & 7 & (The House rose and observed a minute' s silence)\\
\bottomrule
\end{tabular}
\end{table}

\texttt{sentence\_id} 1 appears to be title and \texttt{sentence\_id} 7
reflects description of the parliamentary session. Both of these are
artifacts that we would like to remove from the dataset.

To remove these lines we can turn to the programming strategies we've
previously worked with. Namely we will use \texttt{filter()} to filter
observations in combination with \texttt{str\_detect()} to detect
matches for some pattern that is indicative of these lines that we want
to remove and not of the other lines that we want to keep.

Before we remove any lines, let's try craft a search pattern to identify
these lines, and exclude the lines we will want to keep. Condition one
is lines which start with an opening parenthesis \texttt{(}. Condition
two is lines that do not end in standard sentence punctuation
(\texttt{.}, \texttt{!}, or \texttt{?}). I've added both conditions to
one \texttt{filter()} using the logical \emph{OR} operator
(\texttt{\textbar{}}) to ensure that either condition is matched in the
output.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Identify non{-}speech lines}
\NormalTok{europarle }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(sentence, }\StringTok{"\^{}}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{("}\NormalTok{) }\SpecialCharTok{|} \FunctionTok{str\_detect}\NormalTok{(sentence, }\StringTok{"[\^{}.!?]$"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# filter lines that detect a match for either condition 1 or 2}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# random sample of 10 observations}
\NormalTok{  knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-td-europarle-search-non-speech}{}
\begin{table}
\caption{\label{tbl-td-europarle-search-non-speech}Non-speech lines in the Europarle dataset. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 110 & (El Parlamento rechaza la propuesta por 164 votos a favor, 166 votos en contra y 7 abstenciones)\\
Target & 93 & (Parliament rejected the request) President.\\
Source & 85 & (Aplausos del grupo PSE)\\
Source & 674 & A5-0087/1999 del Sr. Jonckheer, en nombre de la Comisión de Asuntos Económicos y Monetarios, sobre el séptimo informe sobre ayudas estatales a la industria y a otros sectores en la Unión Europea (COM(1999) 148- C5-0107/1999 - 1999/2110(COS));\\
Source & 134 & Consejeros de seguridad para el transporte de mercancías peligrosas\\
\addlinespace
Source & 221 & Transporte de mercancías peligrosas por carretera\\
Target & 109 & (Parliament rejected the request, with 164 votes for, 166 votes against and 7 abstentions)\\
Target & 638 & (The sitting was closed at 8.25 p.m.)\\
Target & 675 & A5-0087/1999 by Mr Jonckheer, on behalf of the Committee on Economic and Monetary Affairs, on the seventh survey on state aid in the European Union in the manufacturing and certain other sectors. [COM(1999) 148 - C5-0107/1999 - 1999/2110(COS)] (Report 1995-1997);\\
Target & 293 & Structural Funds - Cohesion Fund coordination\\
\bottomrule
\end{tabular}
\end{table}

Since this search appears to match lines that we do not want to
preserve, let's move now to eliminate these lines from the dataset. To
do this we will use the same regular expression patterns, but now each
condition will have it's own \texttt{filter()} call and the
\texttt{str\_detect()} will be negated with a prefixed \texttt{!}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{str\_detect}\NormalTok{(sentence, }\AttributeTok{pattern =} \StringTok{"\^{}}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{("}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# remove lines starting with (}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{str\_detect}\NormalTok{(sentence, }\AttributeTok{pattern =} \StringTok{"[\^{}.!?]$"}\NormalTok{)) }\CommentTok{\# remove lines not ending in ., !, or ?}
\end{Highlighting}
\end{Shaded}

Let's look at the first 14 lines again, now that we have eliminated
these artifacts.

\hypertarget{tbl-td-europarle-preview-2}{}
\begin{table}
\caption{\label{tbl-td-europarle-preview-2}Europarle Corpus non-speech lines removed. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 2 & Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\\
Target & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Source & 3 & Como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\
Target & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Source & 4 & Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\
\addlinespace
Target & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
Source & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\\
Target & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Source & 6 & Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
Target & 6 & Please rise, then, for this minute' s silence.\\
\addlinespace
Source & 8 & Señora Presidenta, una cuestión de procedimiento.\\
Target & 8 & Madam President, on a point of order.\\
Source & 9 & Sabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en Sri Lanka.\\
Target & 9 & You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\\
\bottomrule
\end{tabular}
\end{table}

One further issue that we may want to resolve concerns the fact that
there are whitespaces between possessive forms (i.e.~``minute' s
silence''). In this case we can employ \texttt{str\_replace\_all()}
inside the \texttt{mutate()} function to overwrite the \texttt{sentence}
values that match an apostrophe \texttt{\textquotesingle{}} with
whitespace (\texttt{\textbackslash{}\textbackslash{}s}) before
\texttt{s}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{sentence =} \FunctionTok{str\_replace\_all}\NormalTok{(}\AttributeTok{string =}\NormalTok{ sentence, }
                                    \AttributeTok{pattern =} \StringTok{"\textquotesingle{}}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{ss"}\NormalTok{, }
                                    \AttributeTok{replacement =} \StringTok{"\textquotesingle{}s"}\NormalTok{)) }\CommentTok{\# replace \textquotesingle{} s with \textasciigrave{}s}
\end{Highlighting}
\end{Shaded}

Now we have normalized text in the \texttt{sentence} column in the
Europarle dataset.

\textbf{Last FM Lyrics}

Let's look at another dataset we have worked with during this
coursebook: the Lastfm lyrics. Reading in the \texttt{lastfm\_curated}
dataset from the \texttt{data/derived/} directory we can see the
structure for the curated structure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/derived/lastfm/lastfm\_curated.csv"}\NormalTok{) }\CommentTok{\# read in lastfm\_curated dataset}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-td-lastfm-read-preview}{}
\begin{table}
\caption{\label{tbl-td-lastfm-read-preview}Last fm lyrics dataset preview with one artist/ song per genre and the
\texttt{lyrics} text truncated at 200 characters for display purposes. }\tabularnewline

\centering
\begin{tabular}{llll}
\toprule
artist & song & lyrics & genre\\
\midrule
Alan Jackson & Little Bitty & Have a little love on a little honeymoonYou got a little dish and you got a little spoonA little bitty house and a little bitty yardA little bitty dog and a little bitty car Well, it's alright to b... & country\\
50 Cent & In Da Club & Go, go, go, go, go, goGo, shortyIt's your birthdayWe gon' party like it's your birthdayWe gon' sip Bacardi like it's your birthdayAnd you know we don't give a fuck it's not your birthday You can fi... & hip-hop\\
Black Sabbath & Paranoid & Finished with my woman'Cause she couldn't help me with my mindPeople think I'm insaneBecause I am frowning all the time All day long, I think of thingsBut nothing seems to satisfyThink I'll lose my... & metal\\
a-ha & Take On Me & Talking awayI don't know whatWhat to sayI'll say it anywayToday is another day to find youShying awayOh, I'll be coming for your love, okay? Take On Me (Take On Me)Take me on (Take On Me)I'll be go... & pop\\
3 Doors Down & Here Without You & A hundred days have made me olderSince the last time that I saw your pretty faceA thousand lies have made me colderAnd I don't think I can look at this the same But all the miles that separateDisap... & rock\\
\bottomrule
\end{tabular}
\end{table}

There are a few things that we might want to clean out of the
\texttt{lyrics} column's values. First, there are lines from the
original webscrape where the end of one stanza runs into the next
without whitespace between them (i.e.~``honeymoonYou''). These reflect
contiguous end-new line segments where stanzas were joined in the
curation process. Second, we see that there are what appear to be
backing vocals which appear between parentheses (i.e.~``(Take On Me)'').

In both cases we will use \texttt{mutate()}. With contiguous end-new
line segments we will use \texttt{str\_replace\_all()} inside and for
backing vocals in parentheses we will use \texttt{str\_remove\_all()}.

The pattern to match for end-new lines from the stanzas will use some
regular expression magic. The base pattern includes finding a pair of
lowercase-uppercase letters (i.e.~``nY'', in ``honeymoo\textbf{nY}ou'').
For this we can use the pattern \texttt{{[}a-z{]}{[}A-Z{]}}. To replace
this pattern using the lowercase letter then a space and then the
uppercase letter we take advantage of the grouping syntax in regular
expressions \texttt{(...)}. So we add parentheses around the two groups
to capture like this \texttt{({[}a-z{]})({[}A-Z{]})}. In the replacement
argument of the \texttt{str\_replace\_all()} function we then specify to
use the captured groups in the order they appear
\texttt{\textbackslash{}\textbackslash{}1} for the lowercase letter
match and \texttt{\textbackslash{}\textbackslash{}2} for the uppercase
letter match.

Now, I've looked more extensively at the \texttt{lyrics} column and
found that there are other combinations that are joined between stanzas.
Namely that \texttt{\textquotesingle{}}, \texttt{!}, \texttt{,},
\texttt{)}, \texttt{?}, and \texttt{I} also may precede the uppercase
letter. To make sure we capture these possibilities as well I've updated
the regular expression to
\texttt{({[}a-z\textquotesingle{}!,.)?I{]})({[}A-Z{]})}.

Now to remove the backing vocals, the regex pattern is
\texttt{\textbackslash{}\textbackslash{}(.+?\textbackslash{}\textbackslash{})}
--match the parentheses and everything within the parentheses. The added
\texttt{?} after the \texttt{+} operator is what is known as a `lazy'
operator. This specifies that the \texttt{.+} will match the minimal
string that is enclosed by the trailing \texttt{)}. If we did not
include this then we would get matches that span from the first
parenthesis \texttt{(} all the way to the last, which would match real
lyrics, not just the backing vocals.

Putting this to work let's clean the \texttt{lyrics} column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lyrics =} 
           \FunctionTok{str\_replace\_all}\NormalTok{(}\AttributeTok{string =}\NormalTok{ lyrics, }
                           \AttributeTok{pattern =} \StringTok{"([a{-}z\textquotesingle{}!,.)?I])([A{-}Z])"}\NormalTok{, }\CommentTok{\# find contiguous end/ new line segments}
                           \AttributeTok{replacement =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{1 }\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{2"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# replace with whitespace between}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lyrics =} \FunctionTok{str\_remove\_all}\NormalTok{(lyrics, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{(.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{)) }\CommentTok{\# remove backing vocals (Take On Me)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-td-lastfm-clean-end-lines-preview}{}
\begin{table}
\caption{\label{tbl-td-lastfm-clean-end-lines-preview}Last fm lyrics with cleaned lyrics\ldots{} }\tabularnewline

\centering
\begin{tabular}{llll}
\toprule
artist & song & lyrics & genre\\
\midrule
Alan Jackson & Little Bitty & Have a little love on a little honeymoon You got a little dish and you got a little spoon A little bitty house and a little bitty yard A little bitty dog and a little bitty car Well, it's alright t... & country\\
50 Cent & In Da Club & Go, go, go, go, go, go Go, shorty It's your birthday We gon' party like it's your birthday We gon' sip Bacardi like it's your birthday And you know we don't give a fuck it's not your birthday You c... & hip-hop\\
Black Sabbath & Paranoid & Finished with my woman' Cause she couldn't help me with my mind People think I'm insane Because I am frowning all the time All day long, I think of things But nothing seems to satisfy Think I'll lo... & metal\\
a-ha & Take On Me & Talking away I don't know what What to say I'll say it anyway Today is another day to find you Shying away Oh, I'll be coming for your love, okay? Take On Me  Take me on  I'll be gone In a day or t... & pop\\
3 Doors Down & Here Without You & A hundred days have made me older Since the last time that I saw your pretty face A thousand lies have made me colder And I don't think I can look at this the same But all the miles that separate D... & rock\\
\bottomrule
\end{tabular}
\end{table}

Now given the fact that songs are poems, there are many lines that are
not complete sentences so there is no practical way to try to segment
these into grammatical sentence units. So in this case, this seems like
a good stopping point for normalizing the lastfm dataset.

\hypertarget{td-recode}{%
\section{Recode}\label{td-recode}}

Normalizing text can be seen as an extension of dataset curation to some
extent in that the structure of the dataset is maintained. In both the
Europarle and Lastfm cases we saw this to be true. In the case of
recoding, and other transformational steps, the aim will be to modify
the dataset structure either by rows, columns, or both. Recoding
processes can be characterized by the creation of structural changes
which are derived from values in variables effectively recasting values
as new variables to enable more direct access in our analyses.

\textbf{Switchboard Dialogue Act Corpus}

The Switchboard Dialogue Act Corpus dataset that was curated in the
previous chapter contains a number of variables describing conversations
between speakers of American English.

Let's read in this dataset and take a closer look.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/derived/sdac/sdac\_curated.csv"}\NormalTok{) }\CommentTok{\# read curated dataset}
\end{Highlighting}
\end{Shaded}

Among a number of metadata variables, curated dataset includes the
\texttt{utterance\_text} column which contains dialogue from the
conversations interleaved with a
\href{https://staff.fnwi.uva.nl/r.fernandezrovira/teaching/DM-materials/DFL-book.pdf}{disfluency
annotation scheme}.

\hypertarget{tbl-td-sdac-preview-curated-dataset}{}
\begin{table}
\caption{\label{tbl-td-sdac-preview-curated-dataset}20 randomly sampled lines of the SDAC curated dataset. }\tabularnewline

\centering
\begin{tabular}{rllrrlr}
\toprule
doc\_id & damsl\_tag & speaker & turn\_num & utterance\_num & utterance\_text & speaker\_id\\
\midrule
3697 & sd & B & 74 & 2 & {}[ I'm, + I'm ] planning on it.  / & 1424\\
3813 & qh & A & 29 & 3 & all I can think of is if you don't keep [ a real, + a real ] tight budget, how do you control expenses,  / & 1461\\
3214 & sv & B & 48 & 2 & \{C and \} you never read it again -- & 1352\\
2967 & b & B & 46 & 1 & Yeah.  / & 1072\\
4733 & sv & A & 73 & 1 & \{D You know, \} I just feel they do better that way. / & 1437\\
\addlinespace
3052 & sd(\textasciicircum{}q) & A & 44 & 2 & \{C and \} the wife said "these are not  his slacks"  / & 1285\\
2537 & sv & B & 62 & 6 & {}[ [ that, +  the, ] +  the ] absolute refusal to accept the possibility [ of, +  of ] mistakes on the testing, & 1142\\
3121 & \% & B & 116 & 2 & \{C and, \} -/ & 1318\\
2324 & sd & B & 22 & 5 & \{C and \} it'll convert every <noise> measurement on there -- & 1138\\
3750 & sd & B & 35 & 2 & \{C and, \} \{F uh, \}  in the afternoon, \{D you know, \} it is still four o'clock.  / & 1051\\
\addlinespace
4723 & b & B & 54 & 1 & Right. / & 1611\\
2602 & aa & B & 2 & 1 & Okay,  / & 1122\\
2441 & + & A & 141 & 1 & -- \{D you know, \}  / & 1151\\
2372 & b & B & 106 & 1 & Huh-uh. / & 1135\\
3080 & bh & B & 100 & 1 & \{F Oh, \} is that right. / & 1095\\
\addlinespace
3777 & sv & A & 101 & 3 & \{D well, \} a couple of the, \{F uh, \} ones Disney's doing aren't too bad  / & 1477\\
2064 & b & A & 69 & 1 & Yeah. / & 1148\\
2515 & b & B & 48 & 1 & Okay. / & 1035\\
4032 & b & A & 55 & 1 & Uh-huh. / & 1514\\
2691 & + & A & 99 & 1 & \# and \# had their house on the market down there,  / & 1233\\
\bottomrule
\end{tabular}
\end{table}

Let's drop a few variables from our dataset to rein in our focus. I will
keep the \texttt{doc\_id}, \texttt{speaker\_id}, and
\texttt{utterance\_text}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_simplified }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(doc\_id, speaker\_id, utterance\_text) }\CommentTok{\# columns to retain}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-td-sdac-simple-preview}{}
\begin{table}
\caption{\label{tbl-td-sdac-simple-preview}First 10 lines of the simplified SDAC curated dataset. }\tabularnewline

\centering
\begin{tabular}{rrl}
\toprule
doc\_id & speaker\_id & utterance\_text\\
\midrule
4325 & 1632 & Okay.  /\\
4325 & 1632 & \{D So, \}\\
4325 & 1519 & {}[ [ I guess, +\\
4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? /\\
4325 & 1519 & I think, ] + \{F uh, \} I wonder ] if that worked. /\\
\addlinespace
4325 & 1632 & Does it say something? /\\
4325 & 1519 & I think it usually does.  /\\
4325 & 1519 & You might try, \{F uh, \}  /\\
4325 & 1519 & I don't know,  /\\
4325 & 1519 & hold it down a little longer,  /\\
\bottomrule
\end{tabular}
\end{table}

In this disfluency annotation system, there are various conventions used
for non-sentence elements. If say, for example, a researcher were to be
interested in understanding the use of filled pauses (`uh' or `uh'), the
aim would be to identify those lines where the \texttt{\{F\ ...\}}
annotation is used around the utterances `uh' and `um'.

To do this we turn to the \texttt{str\_count()} function. This function
will count the number of matches found for a pattern. We can use a
regular expression to identify the pattern of interest which is all the
instances of \texttt{\{F} followed by either \texttt{uh} or \texttt{um}.
Since the disfluencies may start an utterance, and therefore be
capitalized we need to formulate a regular expression which allows for
either \texttt{U} or \texttt{u} for each disfluency type. The result
from each disfluency match will be added to a new column. To create a
new column we will wrap each \texttt{str\_count()} with
\texttt{mutate()} and give the new column a meaningful name. In this
case I've opted for \texttt{uh} and \texttt{um}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_simplified }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{uh =} \FunctionTok{str\_count}\NormalTok{(utterance\_text, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\{F [Uu]h"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# match \{F Uh or \{F uh\}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{um =} \FunctionTok{str\_count}\NormalTok{(utterance\_text, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\{F [Uu]m"}\NormalTok{)) }\CommentTok{\# match \{F Um or \{F um\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-td-sdac-count-disfluencies-show}{}
\begin{table}
\caption{\label{tbl-td-sdac-count-disfluencies-show}First 20 lines of SDAC dataset with counts for the disfluencies `uh' and
`um'. }\tabularnewline

\centering
\begin{tabular}{rrlrr}
\toprule
doc\_id & speaker\_id & utterance\_text & uh & um\\
\midrule
4325 & 1632 & Okay.  / & 0 & 0\\
4325 & 1632 & \{D So, \} & 0 & 0\\
4325 & 1519 & {}[ [ I guess, + & 0 & 0\\
4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? / & 0 & 0\\
4325 & 1519 & I think, ] + \{F uh, \} I wonder ] if that worked. / & 1 & 0\\
\addlinespace
4325 & 1632 & Does it say something? / & 0 & 0\\
4325 & 1519 & I think it usually does.  / & 0 & 0\\
4325 & 1519 & You might try, \{F uh, \}  / & 1 & 0\\
4325 & 1519 & I don't know,  / & 0 & 0\\
4325 & 1519 & hold it down a little longer,  / & 0 & 0\\
\addlinespace
4325 & 1519 & \{C and \} see if it, \{F uh, \} -/ & 1 & 0\\
4325 & 1632 & Okay <beep>.  / & 0 & 0\\
4325 & 1632 & <<long pause>> \{D Well, \} & 0 & 0\\
4325 & 1519 & Okay  / & 0 & 0\\
4325 & 1519 & {}[ I, + & 0 & 0\\
\addlinespace
4325 & 1632 & Does it usually make a recording or s-, / & 0 & 0\\
4325 & 1519 & \{D Well, \} I ] don't remember.  / & 0 & 0\\
4325 & 1519 & It seemed like it did,  / & 0 & 0\\
4325 & 1519 & \{C but \} <laughter> it might not.  / & 0 & 0\\
4325 & 1519 & {}[ I guess + -- & 0 & 0\\
\bottomrule
\end{tabular}
\end{table}

Now we have two new columns, \texttt{uh} and \texttt{um} which indicate
how many times the relevant pattern was matched for a given utterance.
By choosing to focus on disfluencies, however, we have made a decision
to change the unit of observation from the utterance to the use of
filled pauses (\texttt{uh} and \texttt{um}). This means that as the
dataset stands, it is not in tidy format --where each observation
corresponds to the observational unit. When datasets are misaligned in
this particular way, there are in what is known as `wide' format. What
we want to do, then, is to restructure our dataset such that each row
corresponds to the unit of observation --in this case each filled pause
type.

To convert our current (wide) dataset to one where each filler type is
listed and the counts are measured for each utterance we turn to the
\texttt{pivot\_longer()} function. This function creates two new
columns, one in which the column names are listed and one for the values
for each of the column names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"uh"}\NormalTok{, }\StringTok{"um"}\NormalTok{), }\CommentTok{\# columns to convert}
               \AttributeTok{names\_to =} \StringTok{"filler"}\NormalTok{, }\CommentTok{\# column for the column names (i.e. filler types)}
               \AttributeTok{values\_to =} \StringTok{"count"}\NormalTok{) }\CommentTok{\# column for the column values (i.e. counts)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-td-sdac-count-disfluencies-longer-show}{}
\begin{table}
\caption{\label{tbl-td-sdac-count-disfluencies-longer-show}First 20 lines of SDAC dataset with tidy format for \texttt{fillers} as
the unit of observation. }\tabularnewline

\centering
\begin{tabular}{rrllr}
\toprule
doc\_id & speaker\_id & utterance\_text & filler & count\\
\midrule
4325 & 1632 & Okay.  / & uh & 0\\
4325 & 1632 & Okay.  / & um & 0\\
4325 & 1632 & \{D So, \} & uh & 0\\
4325 & 1632 & \{D So, \} & um & 0\\
4325 & 1519 & {}[ [ I guess, + & uh & 0\\
\addlinespace
4325 & 1519 & {}[ [ I guess, + & um & 0\\
4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? / & uh & 0\\
4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? / & um & 0\\
4325 & 1519 & I think, ] + \{F uh, \} I wonder ] if that worked. / & uh & 1\\
4325 & 1519 & I think, ] + \{F uh, \} I wonder ] if that worked. / & um & 0\\
\addlinespace
4325 & 1632 & Does it say something? / & uh & 0\\
4325 & 1632 & Does it say something? / & um & 0\\
4325 & 1519 & I think it usually does.  / & uh & 0\\
4325 & 1519 & I think it usually does.  / & um & 0\\
4325 & 1519 & You might try, \{F uh, \}  / & uh & 1\\
\addlinespace
4325 & 1519 & You might try, \{F uh, \}  / & um & 0\\
4325 & 1519 & I don't know,  / & uh & 0\\
4325 & 1519 & I don't know,  / & um & 0\\
4325 & 1519 & hold it down a little longer,  / & uh & 0\\
4325 & 1519 & hold it down a little longer,  / & um & 0\\
\bottomrule
\end{tabular}
\end{table}

\textbf{Last fm}

In the previous example, we used a matching approach to extract
information embedded in one column of the dataset and recoded the
dataset to maintain the fidelity between the particular unit of
observation and the other metadata.

Another common approach for recoding datasets in text analysis projects
involves recoding linguistic units as smaller units; a process known as
tokenization.

Let's return to the \texttt{lastfm} object we normalized earlier in the
chapter to see the various ways one can choose to tokenize linguistic
information.

\hypertarget{tbl-td-lastfm-clean-end-lines-preview-2}{}
\begin{table}
\caption{\label{tbl-td-lastfm-clean-end-lines-preview-2}Last fm dataset with normalized lyrics. }\tabularnewline

\centering
\begin{tabular}{llll}
\toprule
artist & song & lyrics & genre\\
\midrule
Alan Jackson & Little Bitty & Have a little love on a little honeymoon You got a little dish and you got a little spoon A little bitty house and a little bitty yard A little bitty dog and a little bitty car Well, it's alright t... & country\\
50 Cent & In Da Club & Go, go, go, go, go, go Go, shorty It's your birthday We gon' party like it's your birthday We gon' sip Bacardi like it's your birthday And you know we don't give a fuck it's not your birthday You c... & hip-hop\\
Black Sabbath & Paranoid & Finished with my woman' Cause she couldn't help me with my mind People think I'm insane Because I am frowning all the time All day long, I think of things But nothing seems to satisfy Think I'll lo... & metal\\
a-ha & Take On Me & Talking away I don't know what What to say I'll say it anyway Today is another day to find you Shying away Oh, I'll be coming for your love, okay? Take On Me  Take me on  I'll be gone In a day or t... & pop\\
3 Doors Down & Here Without You & A hundred days have made me older Since the last time that I saw your pretty face A thousand lies have made me colder And I don't think I can look at this the same But all the miles that separate D... & rock\\
\bottomrule
\end{tabular}
\end{table}

In the current \texttt{lastfm} dataset, the unit of observation is the
lyrics for the entire artist, song, and genre combination. If, however,
we would like to change the unit to say words, we would like each word
used to appear on its own row, while still maintaining the other
relevant attributes associated with each word.

The tidytext package includes a very useful function
\texttt{unnest\_tokens()} which allows us to tokenize some textual input
into smaller linguistic units. The `unnest' part of the the function
name refers to the process of extracting the unit of interest while
maintaining the other relevant attributes. Let's see this in action.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{output =}\NormalTok{ word, }\CommentTok{\# column for tokenized output}
                \AttributeTok{input =}\NormalTok{ lyrics, }\CommentTok{\# input column}
                \AttributeTok{token =} \StringTok{"words"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# tokenize unit type}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# preview first 10 lines}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-td-lastfm-tokenize-words}{}
\begin{table}
\caption{\label{tbl-td-lastfm-tokenize-words}First 10 observations for lastfm dataset tokenized by words. }\tabularnewline

\centering
\begin{tabular}{llll}
\toprule
artist & song & genre & word\\
\midrule
Alan Jackson & Little Bitty & country & have\\
Alan Jackson & Little Bitty & country & a\\
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & love\\
Alan Jackson & Little Bitty & country & on\\
\addlinespace
Alan Jackson & Little Bitty & country & a\\
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & honeymoon\\
Alan Jackson & Little Bitty & country & you\\
Alan Jackson & Little Bitty & country & got\\
\bottomrule
\end{tabular}
\end{table}

We can see from the output, each word appears on a separate line in the
order of appearance in the input text (\texttt{lyrics}). Furthermore,
the output is in tidy format as each of the words is still associated
with the relevant attribute values (\texttt{artist}, \texttt{song}, and
\texttt{genre}). By default the tokenized text output is lowercased and
the original text input column is dropped. These can be overridden,
however, if desired.

In addition to `words', the \texttt{unnest\_tokens()} function provides
easy access to a number of common tokenized units including
`characters', `sentences', and `paragraphs'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{output =}\NormalTok{ character, }\CommentTok{\# column for tokenized output}
                \AttributeTok{input =}\NormalTok{ lyrics, }\CommentTok{\# input column}
                \AttributeTok{token =} \StringTok{"characters"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# tokenize unit type}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# preview first 10 lines}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-td-lastfm-tokenize-characters}{}
\begin{table}
\caption{\label{tbl-td-lastfm-tokenize-characters}First 10 observations for lastfm dataset tokenized by characters. }\tabularnewline

\centering
\begin{tabular}{llll}
\toprule
artist & song & genre & character\\
\midrule
Alan Jackson & Little Bitty & country & h\\
Alan Jackson & Little Bitty & country & a\\
Alan Jackson & Little Bitty & country & v\\
Alan Jackson & Little Bitty & country & e\\
Alan Jackson & Little Bitty & country & a\\
\addlinespace
Alan Jackson & Little Bitty & country & l\\
Alan Jackson & Little Bitty & country & i\\
Alan Jackson & Little Bitty & country & t\\
Alan Jackson & Little Bitty & country & t\\
Alan Jackson & Little Bitty & country & l\\
\bottomrule
\end{tabular}
\end{table}

The other two built-in options `sentences' and `paragraphs' depend on
punctuation and/ or line breaks to function, so in this particular
dataset, these options will not work given the particular
characteristics of the \texttt{lyrics} variable.

There are even other options which allow for the creation of sequences
of linguistic units. Say we want to tokenize our lyrics into two-word
sequences, we can specify the \texttt{token} as `ngrams' and then add
the argument \texttt{n\ =\ 2} to reflect we want two-word sequences.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{output =}\NormalTok{ bigram, }\CommentTok{\# column for tokenized output}
                \AttributeTok{input =}\NormalTok{ lyrics, }\CommentTok{\# input column}
                \AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{, }\CommentTok{\# tokenize unit type}
                \AttributeTok{n =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# size of word sequences }
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# preview first 10 lines}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-td-lastfm-tokenize-bigrams}{}
\begin{table}
\caption{\label{tbl-td-lastfm-tokenize-bigrams}First 10 observations for lastfm dataset tokenized by bigrams }\tabularnewline

\centering
\begin{tabular}{llll}
\toprule
artist & song & genre & bigram\\
\midrule
Alan Jackson & Little Bitty & country & have a\\
Alan Jackson & Little Bitty & country & a little\\
Alan Jackson & Little Bitty & country & little love\\
Alan Jackson & Little Bitty & country & love on\\
Alan Jackson & Little Bitty & country & on a\\
\addlinespace
Alan Jackson & Little Bitty & country & a little\\
Alan Jackson & Little Bitty & country & little honeymoon\\
Alan Jackson & Little Bitty & country & honeymoon you\\
Alan Jackson & Little Bitty & country & you got\\
Alan Jackson & Little Bitty & country & got a\\
\bottomrule
\end{tabular}
\end{table}

The `n' in `ngram' refers to the number of word-sequence units we want
to tokenize. Two-word sequences are known as `bigrams', three-word
sequences `trigrams', and so on.

\hypertarget{generate}{%
\section{Generate}\label{generate}}

In the process of recoding a dataset the transformation of the dataset
works with information that is already explicit. The process of
generation, however, aims to make implicit information explicit. The
most common type of operation involved in the generation process is the
addition of linguistic annotation. This process can be accomplished
manually by a researcher or research team or automatically through the
use of pre-trained linguistic resources and/ or software. Ideally the
annotation of linguistic information can be conducted automatically.

There are important considerations, however, that need to be taken into
account when choosing whether linguistic annotation can be conducted
automatically. First and foremost has to do with the type of annotation
desired. Information such as part of speech (grammatical category) and
morpho-syntactic information are the the most common types of linguistic
annotation that can be conducted automatically. Second the degree to
which the resource that will be used to annotate the linguistic
information is aligned with the language variety and/or register is also
a key consideration. As noted, automatic linguistic annotation methods
are contingent on pre-trained resources. The language and language
variety used to develop these resources may not be available for the
language under investigation, or if it does, the language variety and/
or register may not align. The degree to which a resource does not align
with the linguistic information targeted for annotation is directly
related to the quality of the final annotations. To be clear, no
annotation method, whether manual or automatic is guaranteed to be
perfectly accurate.

Let's take a look at annotation some of the language from the Europarle
dataset we normalized.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(type }\SpecialCharTok{==} \StringTok{"Target"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-td-europarle-en-preview}{}
\begin{table}
\caption{\label{tbl-td-europarle-en-preview}First 10 lines in English from the normalized SDAC dataset. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Target & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Target & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Target & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
Target & 5 & In the meantime, I should like to observe a minute's silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Target & 6 & Please rise, then, for this minute's silence.\\
\addlinespace
Target & 8 & Madam President, on a point of order.\\
Target & 9 & You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\\
Target & 10 & One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\\
Target & 11 & Would it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament's regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\\
Target & 12 & Yes, Mr Evans, I feel an initiative of the type you have just suggested would be entirely appropriate.\\
\bottomrule
\end{tabular}
\end{table}

We will use the cleanNLP package to do our linguistic annotation. The
annotation process depends on the pre-trained language models. There is
\href{https://github.com/bnosac/udpipe\#pre-trained-models}{a list of
the models available to access}. The \texttt{load\_model\_udpipe()}
custom function below downloads the specified language model and
initialized the \texttt{udpipe} engine (\texttt{cnlp\_init\_udpipe()})
for conducting annotations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{load\_model\_udpipe }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(model\_lang) \{}
  \CommentTok{\# Function}
  \CommentTok{\# Download and load the specified udpipe language model}
  
  \FunctionTok{cnlp\_init\_udpipe}\NormalTok{(model\_lang) }\CommentTok{\# to download the model, if not downloaded}
\NormalTok{base\_path }\OtherTok{\textless{}{-}} \FunctionTok{system.file}\NormalTok{(}\StringTok{"extdata"}\NormalTok{, }\AttributeTok{package =} \StringTok{"cleanNLP"}\NormalTok{) }\CommentTok{\# get the base path}
\NormalTok{  model\_name }\OtherTok{\textless{}{-}} \CommentTok{\# extract the model\_name}
\NormalTok{    base\_path }\SpecialCharTok{|\textgreater{}} \CommentTok{\# extract the base path}
    \FunctionTok{dir}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# get the directory}
\NormalTok{    stringr}\SpecialCharTok{::}\FunctionTok{str\_subset}\NormalTok{(}\AttributeTok{pattern =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"\^{}"}\NormalTok{, model\_lang)) }\CommentTok{\# extract the name of the model}
  
\NormalTok{  model\_path }\OtherTok{\textless{}{-}}\NormalTok{ udpipe}\SpecialCharTok{::}\FunctionTok{udpipe\_load\_model}\NormalTok{(}\AttributeTok{file =} \FunctionTok{file.path}\NormalTok{(base\_path, model\_name, }\AttributeTok{fsep =} \StringTok{"/"}\NormalTok{)) }\CommentTok{\# create the path to the downloaded model stored on disk}
    \FunctionTok{return}\NormalTok{(model\_path)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

In a test case, let's load the `english' model to annotate a sentence
line from the Europarle dataset to illustrate the basic workflow.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eng\_model }\OtherTok{\textless{}{-}} \FunctionTok{load\_model\_udpipe}\NormalTok{(}\StringTok{"english"}\NormalTok{) }\CommentTok{\# load and initialize the language model, \textquotesingle{}english\textquotesingle{} in this case.}

\NormalTok{eng\_annotation }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset }
  \FunctionTok{filter}\NormalTok{(type }\SpecialCharTok{==} \StringTok{"Target"} \SpecialCharTok{\&}\NormalTok{ sentence\_id }\SpecialCharTok{==} \DecValTok{6}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# select English and sentence\_id 6}
  \FunctionTok{cnlp\_annotate}\NormalTok{(}\AttributeTok{text\_name =} \StringTok{"sentence"}\NormalTok{, }\CommentTok{\# input text (sentence)}
                \AttributeTok{doc\_name =} \StringTok{"sentence\_id"}\NormalTok{) }\CommentTok{\# specify the grouping column (sentence\_id)}

\FunctionTok{glimpse}\NormalTok{(eng\_annotation) }\CommentTok{\# preview structure}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> List of 2
#>  $ token   : tibble [11 x 11] (S3: tbl_df/tbl/data.frame)
#>   ..$ doc_id       : num [1:11] 6 6 6 6 6 6 6 6 6 6 ...
#>   ..$ sid          : int [1:11] 1 1 1 1 1 1 1 1 1 1 ...
#>   ..$ tid          : chr [1:11] "1" "2" "3" "4" ...
#>   ..$ token        : chr [1:11] "Please" "rise" "," "then" ...
#>   ..$ token_with_ws: chr [1:11] "Please " "rise" ", " "then" ...
#>   ..$ lemma        : chr [1:11] "please" "rise" "," "then" ...
#>   ..$ upos         : chr [1:11] "INTJ" "VERB" "PUNCT" "ADV" ...
#>   ..$ xpos         : chr [1:11] "UH" "VB" "," "RB" ...
#>   ..$ feats        : chr [1:11] NA "Mood=Imp|VerbForm=Fin" NA "PronType=Dem" ...
#>   ..$ tid_source   : chr [1:11] "2" "0" "2" "10" ...
#>   ..$ relation     : chr [1:11] "discourse" "root" "punct" "advmod" ...
#>  $ document: tibble [1 x 2] (S3: tbl_df/tbl/data.frame)
#>   ..$ type  : chr "Target"
#>   ..$ doc_id: num 6
#>  - attr(*, "class")= chr [1:2] "cnlp_annotation" "list"
\end{verbatim}

We see that the structure returned by the \texttt{cnlp\_annotate()}
function is a list. This list contains two data frames (tibbles). One
for the tokens (and there annotation information) and the document (the
metadata information). We can inspect the annotation characteristics for
this one sentence by targetting the \texttt{\$tokens} data frame. Let's
take a look at the linguistic annotation information returned.

\hypertarget{tbl-td-generation-test-annotation-english}{}
\begin{table}
\caption{\label{tbl-td-generation-test-annotation-english}Annotation information for a single English sentence from the Europarle
dataset. }\tabularnewline

\centering
\begin{tabular}{rrlllllllll}
\toprule
doc\_id & sid & tid & token & token\_with\_ws & lemma & upos & xpos & feats & tid\_source & relation\\
\midrule
6 & 1 & 1 & Please & Please & please & INTJ & UH & NA & 2 & discourse\\
6 & 1 & 2 & rise & rise & rise & VERB & VB & Mood=Imp|VerbForm=Fin & 0 & root\\
6 & 1 & 3 & , & , & , & PUNCT & , & NA & 2 & punct\\
6 & 1 & 4 & then & then & then & ADV & RB & PronType=Dem & 10 & advmod\\
6 & 1 & 5 & , & , & , & PUNCT & , & NA & 10 & punct\\
\addlinespace
6 & 1 & 6 & for & for & for & ADP & IN & NA & 10 & case\\
6 & 1 & 7 & this & this & this & DET & DT & Number=Sing|PronType=Dem & 8 & det\\
6 & 1 & 8 & minute & minute & minute & NOUN & NN & Number=Sing & 10 & nmod:poss\\
6 & 1 & 9 & 's & 's & 's & PART & POS & NA & 8 & case\\
6 & 1 & 10 & silence & silence & silence & NOUN & NN & Number=Sing & 2 & conj\\
\addlinespace
6 & 1 & 11 & . & . & . & PUNCT & . & NA & 2 & punct\\
\bottomrule
\end{tabular}
\end{table}

There is quite a bit of information which is returned from
\texttt{cnlp\_annotate()}. First note that the input sentence has been
tokenized by word. Each token includes the token, lemma, part of speech
(\texttt{upos} and \texttt{xpos}), morphological features
(\texttt{feats}), and syntactic relationships (\texttt{tid\_source} and
\texttt{relation}). It is also key to note that the \texttt{doc\_id},
\texttt{sid} and \texttt{tid} maintain the relational attributes from
the original dataset --and therefore maintains our annotated dataset in
tidy format.

Let's now annotate the same sentence from the Europarle corpus for the
Source (`Spanish') and note the similarities and differences.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spa\_model }\OtherTok{\textless{}{-}} \FunctionTok{load\_model\_udpipe}\NormalTok{(}\StringTok{"spanish"}\NormalTok{) }\CommentTok{\# load and initialize the language model, \textquotesingle{}spanish\textquotesingle{} in this case.}

\NormalTok{spa\_annotation }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset }
  \FunctionTok{filter}\NormalTok{(type }\SpecialCharTok{==} \StringTok{"Source"} \SpecialCharTok{\&}\NormalTok{ sentence\_id }\SpecialCharTok{==} \DecValTok{6}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# select Spanish and sentence\_id 6}
  \FunctionTok{cnlp\_annotate}\NormalTok{(}\AttributeTok{text\_name =} \StringTok{"sentence"}\NormalTok{, }\CommentTok{\# input text (sentence)}
                \AttributeTok{doc\_name =} \StringTok{"sentence\_id"}\NormalTok{) }\CommentTok{\# specify the grouping column (sentence\_id)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-td-generation-test-annotation-spanish}{}
\begin{table}
\caption{\label{tbl-td-generation-test-annotation-spanish}Annotation information for a single Spanish sentence from the Europarle
dataset. }\tabularnewline

\centering
\begin{tabular}{rrlllllllll}
\toprule
doc\_id & sid & tid & token & token\_with\_ws & lemma & upos & xpos & feats & tid\_source & relation\\
\midrule
6 & 1 & 1 & Invito & Invito & Invito & VERB & NA & Gender=Masc|Number=Sing|VerbForm=Fin & 0 & root\\
6 & 1 & 2 & a & a & a & ADP & NA & NA & 3 & case\\
6 & 1 & 3 & todos & todos & todo & PRON & NA & Gender=Masc|Number=Plur|PronType=Tot & 1 & obj\\
6 & 1 & 4 & a & a & a & ADP & NA & NA & 7 & mark\\
6 & 1 & 5 & que & que & que & SCONJ & NA & NA & 4 & fixed\\
\addlinespace
6 & 1 & 6 & nos & nos & yo & PRON & NA & Case=Acc,Dat|Number=Plur|Person=1|PrepCase=Npr|PronType=Prs|Reflex=Yes & 7 & iobj\\
6 & 1 & 7 & pongamos & pongamos & pongar & VERB & NA & Mood=Ind|Number=Plur|Person=1|Tense=Pres|VerbForm=Fin & 1 & advcl\\
6 & 1 & 8 & de & de & de & ADP & NA & NA & 9 & case\\
6 & 1 & 9 & pie & pie & pie & NOUN & NA & Gender=Masc|Number=Sing & 7 & obl\\
6 & 1 & 10 & para & para & para & ADP & NA & NA & 11 & mark\\
\addlinespace
6 & 1 & 11 & guardar & guardar & guardar & VERB & NA & VerbForm=Inf & 1 & advcl\\
6 & 1 & 12 & un & un & uno & DET & NA & Definite=Ind|Gender=Masc|Number=Sing|PronType=Art & 13 & det\\
6 & 1 & 13 & minuto & minuto & minuto & NOUN & NA & Gender=Masc|Number=Sing & 11 & obj\\
6 & 1 & 14 & de & de & de & ADP & NA & NA & 15 & case\\
6 & 1 & 15 & silencio & silencio & silencio & NOUN & NA & Gender=Masc|Number=Sing & 13 & nmod\\
\addlinespace
6 & 1 & 16 & . & . & . & PUNCT & NA & NA & 1 & punct\\
\bottomrule
\end{tabular}
\end{table}

For the Spanish version of this sentence, we see the same variables.
However, the \texttt{feats} variable has morphological information which
is specific to Spanish --notably gender and mood.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

The rsyntax package (Welbers and van Atteveldt 2022) can be used to
recode and extract patterns from the output from automatic linguistic
annotations using cleanNLP.
\href{https://github.com/vanatteveldt/rsyntax}{See the documentation for
more information}.

\end{tcolorbox}

\hypertarget{merge}{%
\section{Merge}\label{merge}}

One final class of transformations that can be applied to curated
datasets to enhance their informativeness for a research project is the
process of merging two or more datasets. To merge datasets it is
required that the datasets share one or more attributes. With a common
attribute two datasets can be joined to coordinate the attributes of one
dataset with the other effectively adding attributes and one dataset
with extended information. Another approach is to join datasets with the
goal of filtering one of the datasets given the matching attribute.

Let's see this in practice. Take the \texttt{lastfm} dataset. Let's
tokenize the dataset into words, using \texttt{unnest\_tokens()} such
that our unit of observation is words.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_words }\OtherTok{\textless{}{-}} 
\NormalTok{  lastfm }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{output =} \StringTok{"word"}\NormalTok{, }\CommentTok{\# output column}
                \AttributeTok{input =} \StringTok{"lyrics"}\NormalTok{, }\CommentTok{\# input column}
                \AttributeTok{token =} \StringTok{"words"}\NormalTok{) }\CommentTok{\# tokenized unit (words)}

\NormalTok{lastfm\_words }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# first 10 observations}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-td-lastfm-tokens}{}
\begin{table}
\caption{\label{tbl-td-lastfm-tokens}First 10 observations for \texttt{lastfm\_words} dataset. }\tabularnewline

\centering
\begin{tabular}{llll}
\toprule
artist & song & genre & word\\
\midrule
Alan Jackson & Little Bitty & country & have\\
Alan Jackson & Little Bitty & country & a\\
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & love\\
Alan Jackson & Little Bitty & country & on\\
\addlinespace
Alan Jackson & Little Bitty & country & a\\
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & honeymoon\\
Alan Jackson & Little Bitty & country & you\\
Alan Jackson & Little Bitty & country & got\\
\bottomrule
\end{tabular}
\end{table}

Consider the \texttt{get\_sentiments()} function which returns words
which have been classified as `positive'- or `negative'-biased, if the
lexicon is set to `bing' (Hu and Liu 2004).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sentiments\_bing }\OtherTok{\textless{}{-}} 
\NormalTok{  tidytext}\SpecialCharTok{::}\FunctionTok{get\_sentiments}\NormalTok{(}\AttributeTok{lexicon =} \StringTok{"bing"}\NormalTok{) }\CommentTok{\# get \textquotesingle{}bing\textquotesingle{} lexicon from get\_sentiments}

\NormalTok{sentiments\_bing }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\CommentTok{\# preview first 10 observations}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 10 x 2
#>    word        sentiment
#>    <chr>       <chr>    
#>  1 2-faces     negative 
#>  2 abnormal    negative 
#>  3 abolish     negative 
#>  4 abominable  negative 
#>  5 abominably  negative 
#>  6 abominate   negative 
#>  7 abomination negative 
#>  8 abort       negative 
#>  9 aborted     negative 
#> 10 aborts      negative
\end{verbatim}

Since the \texttt{sentiments\_bing} dataset and the
\texttt{lastfm\_words} dataset both share a column \texttt{word} (which
has the same type of values) we can join these two datasets. The
\texttt{sentiments\_bing} dataset has 6786 unique words. Let's check how
many distinct words our \texttt{lastfm\_words} dataset has.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lastfm\_words }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{distinct}\NormalTok{(word) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# find unique words}
  \FunctionTok{nrow}\NormalTok{() }\CommentTok{\# count distinct rows/ words}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 4614
\end{verbatim}

One thing to note is that the \texttt{sentiments\_bing} dataset does not
include function words, that is words that are associated with
closed-class categories (pronouns, determiners, prepositions, etc.) as
these words do not have semantic content along the lines of positive and
negative. So many of the words that appear in the \texttt{lastfm\_words}
will not be matched. Other thing to note is that the
\texttt{sentiments\_bing} lexicon will undoubtly have words that do not
appear in the \texttt{lastfm\_words} and vice versa.

If we want to keep all the words in the \texttt{lastfm\_words} and add
the sentiment information for those words that do match in both
datasets, we can use the \texttt{left\_join()} function.
\texttt{lastfm\_words} will be the dataset on the `left' and therefore
all rows in this dataset will be retained.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{left\_join}\NormalTok{(lastfm\_words, sentiments\_bing) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# first 10 observations}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-td-lastfm-words-bing-left-joing}{}
\begin{table}
\caption{\label{tbl-td-lastfm-words-bing-left-joing}First 10 observations for the \texttt{lastfm\_words} sentiments\_bing`
left join. }\tabularnewline

\centering
\begin{tabular}{lllll}
\toprule
artist & song & genre & word & sentiment\\
\midrule
Alan Jackson & Little Bitty & country & have & NA\\
Alan Jackson & Little Bitty & country & a & NA\\
Alan Jackson & Little Bitty & country & little & NA\\
Alan Jackson & Little Bitty & country & love & positive\\
Alan Jackson & Little Bitty & country & on & NA\\
\addlinespace
Alan Jackson & Little Bitty & country & a & NA\\
Alan Jackson & Little Bitty & country & little & NA\\
Alan Jackson & Little Bitty & country & honeymoon & NA\\
Alan Jackson & Little Bitty & country & you & NA\\
Alan Jackson & Little Bitty & country & got & NA\\
\bottomrule
\end{tabular}
\end{table}

So we see that quite a few of the words from \texttt{lastfm\_words} are
not matched. To focus in on those words in \texttt{lastfm\_words} that
do match, we'll run the same join operation and filter for rows where
\texttt{sentiment} is not empty (i.e.~that there is a match in the
\texttt{sentiments\_bing} lexicon).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{left\_join}\NormalTok{(lastfm\_words, sentiments\_bing) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(sentiment }\SpecialCharTok{!=} \StringTok{""}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# return matched sentiments}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# first 10 observations}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}
\caption{First 10 observations for the \texttt{lastfm\_words} sentiments\_bing`
left join.}\tabularnewline

\centering
\begin{tabular}{lllll}
\toprule
artist & song & genre & word & sentiment\\
\midrule
Alan Jackson & Little Bitty & country & love & positive\\
Alan Jackson & Little Bitty & country & well & positive\\
Alan Jackson & Little Bitty & country & well & positive\\
Alan Jackson & Little Bitty & country & well & positive\\
Alan Jackson & Little Bitty & country & smile & positive\\
\addlinespace
Alan Jackson & Little Bitty & country & well & positive\\
Alan Jackson & Little Bitty & country & well & positive\\
Alan Jackson & Little Bitty & country & well & positive\\
Alan Jackson & Little Bitty & country & smile & positive\\
Alan Jackson & Little Bitty & country & good & positive\\
\bottomrule
\end{tabular}
\end{table}

Let's turn to another type of join: an anti-join. The purpose of an
anti-join is to eliminate matches. This makes sense for a quick and
dirty approach to removing function words (i.e.~those grammatical words
with little semantic content). In this case we use the
\texttt{get\_stopwords()} function to get the dataset. We'll specify
English as the language and we'll use the default lexicon (`Snowball').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{english\_stopwords }\OtherTok{\textless{}{-}} 
  \FunctionTok{get\_stopwords}\NormalTok{(}\AttributeTok{language =} \StringTok{"en"}\NormalTok{) }\CommentTok{\# get English stopwords from the Snowball lexicon}

\NormalTok{english\_stopwords }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\CommentTok{\# preview first 10 observations}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 10 x 2
#>    word      lexicon 
#>    <chr>     <chr>   
#>  1 i         snowball
#>  2 me        snowball
#>  3 my        snowball
#>  4 myself    snowball
#>  5 we        snowball
#>  6 our       snowball
#>  7 ours      snowball
#>  8 ourselves snowball
#>  9 you       snowball
#> 10 your      snowball
\end{verbatim}

Now if we want to eliminate stopwords from our \texttt{lastfm\_words}
dataset we use \texttt{anti\_join()}. All the observations in the
\texttt{lastfm\_words} where there is not a match in
\texttt{english\_stopwords} will be returned.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anti\_join}\NormalTok{(lastfm\_words, english\_stopwords) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-td-lastfm-words-stopwords-anti-join}{}
\begin{table}
\caption{\label{tbl-td-lastfm-words-stopwords-anti-join}First 10 observations in \texttt{lastfm\_words} after filtering for
English stopwords. }\tabularnewline

\centering
\begin{tabular}{llll}
\toprule
artist & song & genre & word\\
\midrule
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & love\\
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & honeymoon\\
Alan Jackson & Little Bitty & country & got\\
\addlinespace
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & dish\\
Alan Jackson & Little Bitty & country & got\\
Alan Jackson & Little Bitty & country & little\\
Alan Jackson & Little Bitty & country & spoon\\
\bottomrule
\end{tabular}
\end{table}

We can also merge datasets that we generate in our analysis or that we
import from other sources. This can be useful when there are cases in
which a corpus has associated metadata that is contained in files
separate from the corpus itself. This is the case for the Switchboard
Dialogue Act Corpus.

Our existing, disfluency recoded, version includes the following
variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\CommentTok{\# preview first 10 observations}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 10 x 5
#>    doc_id speaker_id utterance_text                                 filler count
#>     <dbl>      <dbl> <chr>                                          <chr>  <int>
#>  1   4325       1632 Okay.  /                                       uh         0
#>  2   4325       1632 Okay.  /                                       um         0
#>  3   4325       1632 {D So, }                                       uh         0
#>  4   4325       1632 {D So, }                                       um         0
#>  5   4325       1519 [ [ I guess, +                                 uh         0
#>  6   4325       1519 [ [ I guess, +                                 um         0
#>  7   4325       1632 What kind of experience [ do you, + do you ] ~ uh         0
#>  8   4325       1632 What kind of experience [ do you, + do you ] ~ um         0
#>  9   4325       1519 I think, ] + {F uh, } I wonder ] if that work~ uh         1
#> 10   4325       1519 I think, ] + {F uh, } I wonder ] if that work~ um         0
\end{verbatim}

The \href{https://catalog.ldc.upenn.edu/docs/LDC97S62/}{online
documentation page} provides a key file \texttt{caller\_tab.csv} which
contains speaker metadata information. Included in this \texttt{.csv}
file is a column \texttt{caller\_no} which contains the
\texttt{speaker\_id} we currently have in the
\texttt{sdac\_disfluencies} dataset. Let's read this file into our R
session renaming \texttt{caller\_no} to \texttt{speaker\_id} to prepare
to join these datasets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_speaker\_meta }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"https://catalog.ldc.upenn.edu/docs/LDC97S62/caller\_tab.csv"}\NormalTok{, }
           \AttributeTok{col\_names =} \FunctionTok{c}\NormalTok{(}\StringTok{"speaker\_id"}\NormalTok{, }\CommentTok{\# changed from \textasciigrave{}caller\_no\textasciigrave{}}
                         \StringTok{"pin"}\NormalTok{,}
                         \StringTok{"target"}\NormalTok{,}
                         \StringTok{"sex"}\NormalTok{,}
                         \StringTok{"birth\_year"}\NormalTok{,}
                         \StringTok{"dialect\_area"}\NormalTok{,}
                         \StringTok{"education"}\NormalTok{,}
                         \StringTok{"ti"}\NormalTok{,}
                         \StringTok{"payment\_type"}\NormalTok{,}
                         \StringTok{"amt\_pd"}\NormalTok{,}
                         \StringTok{"con"}\NormalTok{,}
                         \StringTok{"remarks"}\NormalTok{,}
                         \StringTok{"calls\_deleted"}\NormalTok{,}
                         \StringTok{"speaker\_partition"}\NormalTok{))}

\FunctionTok{glimpse}\NormalTok{(sdac\_speaker\_meta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 543
#> Columns: 14
#> $ speaker_id        <dbl> 1000, 1001, 1002, 1003, 1004, 1005, 1007, 1008, 1010~
#> $ pin               <dbl> 32, 102, 104, 5656, 123, 166, 274, 322, 445, 461, 57~
#> $ target            <chr> "N", "N", "N", "N", "N", "Y", "N", "N", "N", "N", "Y~
#> $ sex               <chr> "FEMALE", "MALE", "FEMALE", "MALE", "FEMALE", "FEMAL~
#> $ birth_year        <dbl> 1954, 1940, 1963, 1947, 1958, 1956, 1965, 1939, 1932~
#> $ dialect_area      <chr> "SOUTH MIDLAND", "WESTERN", "SOUTHERN", "NORTH MIDLA~
#> $ education         <dbl> 1, 3, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 3, 3, 2, 3~
#> $ ti                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
#> $ payment_type      <chr> "CASH", "GIFT", "GIFT", "NONE", "GIFT", "GIFT", "CAS~
#> $ amt_pd            <dbl> 15, 10, 11, 7, 11, 22, 20, 3, 11, 9, 25, 9, 1, 16, 1~
#> $ con               <chr> "N", "N", "N", "Y", "N", "Y", "N", "Y", "N", "N", "N~
#> $ remarks           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ calls_deleted     <dbl> 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0~
#> $ speaker_partition <chr> "DN2", "XP", "XP", "DN2", "XP", "ET", "DN1", "DN1", ~
\end{verbatim}

Now to join the \texttt{sdac\_disfluencies} and
\texttt{sdac\_speaker\_meta}. Let's turn to \texttt{left\_join()} again
as we want to retain all the observations (rows) from
\texttt{sdac\_disfluencies} and add the columns for
\texttt{sdac\_speaker\_meta} where the \texttt{speaker\_id} column
values match.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
  \FunctionTok{left\_join}\NormalTok{(sdac\_disfluencies, sdac\_speaker\_meta) }\CommentTok{\# join by \textasciigrave{}\textasciigrave{}speaker\_id\textasciigrave{}}

\FunctionTok{glimpse}\NormalTok{(sdac\_disfluencies)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 447,212
#> Columns: 18
#> $ doc_id            <dbl> 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325~
#> $ speaker_id        <dbl> 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519~
#> $ utterance_text    <chr> "Okay.  /", "Okay.  /", "{D So, }", "{D So, }", "[ [~
#> $ filler            <chr> "uh", "um", "uh", "um", "uh", "um", "uh", "um", "uh"~
#> $ count             <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0~
#> $ pin               <dbl> 7713, 7713, 7713, 7713, 775, 775, 7713, 7713, 775, 7~
#> $ target            <chr> "N", "N", "N", "N", "N", "N", "N", "N", "N", "N", "N~
#> $ sex               <chr> "FEMALE", "FEMALE", "FEMALE", "FEMALE", "FEMALE", "F~
#> $ birth_year        <dbl> 1962, 1962, 1962, 1962, 1971, 1971, 1962, 1962, 1971~
#> $ dialect_area      <chr> "WESTERN", "WESTERN", "WESTERN", "WESTERN", "SOUTH M~
#> $ education         <dbl> 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1~
#> $ ti                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
#> $ payment_type      <chr> "CASH", "CASH", "CASH", "CASH", "CASH", "CASH", "CAS~
#> $ amt_pd            <dbl> 10, 10, 10, 10, 4, 4, 10, 10, 4, 4, 10, 10, 4, 4, 4,~
#> $ con               <chr> "Y", "Y", "Y", "Y", "Y", "Y", "Y", "Y", "Y", "Y", "Y~
#> $ remarks           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ calls_deleted     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
#> $ speaker_partition <chr> "UNC", "UNC", "UNC", "UNC", "UNC", "UNC", "UNC", "UN~
\end{verbatim}

Now there are some metadata columns we may want to keep and others we
may want to drop as they may not be of importance for our analysis. I'm
going to assume that we want to keep \texttt{sex}, \texttt{birth\_year},
\texttt{dialect\_area}, and \texttt{education} and drop the rest.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(doc\_id}\SpecialCharTok{:}\NormalTok{count, sex}\SpecialCharTok{:}\NormalTok{education) }\CommentTok{\# subset key columns}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-td-sdac-disfluencies-meta-preview}{}
\begin{table}
\caption{\label{tbl-td-sdac-disfluencies-meta-preview}First 10 observations for the \texttt{sdac\_disfluencies} dataset with
speaker metadata. }\tabularnewline

\centering
\begin{tabular}{rrllrlrlr}
\toprule
doc\_id & speaker\_id & utterance\_text & filler & count & sex & birth\_year & dialect\_area & education\\
\midrule
4325 & 1632 & Okay.  / & uh & 0 & FEMALE & 1962 & WESTERN & 2\\
4325 & 1632 & Okay.  / & um & 0 & FEMALE & 1962 & WESTERN & 2\\
4325 & 1632 & \{D So, \} & uh & 0 & FEMALE & 1962 & WESTERN & 2\\
4325 & 1632 & \{D So, \} & um & 0 & FEMALE & 1962 & WESTERN & 2\\
4325 & 1519 & {}[ [ I guess, + & uh & 0 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
\addlinespace
4325 & 1519 & {}[ [ I guess, + & um & 0 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? / & uh & 0 & FEMALE & 1962 & WESTERN & 2\\
4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? / & um & 0 & FEMALE & 1962 & WESTERN & 2\\
4325 & 1519 & I think, ] + \{F uh, \} I wonder ] if that worked. / & uh & 1 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
4325 & 1519 & I think, ] + \{F uh, \} I wonder ] if that worked. / & um & 0 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{documentation-3}{%
\section{Documentation}\label{documentation-3}}

Documentation of the transformed dataset is just as important as the
curated dataset. Therefore we use the same process as covered in the
previous chapter. First we write the transformed dataset to disk and
then we work to provide a data dictionary for this dataset. I've
included the \texttt{data\_dic\_starter()} custom function to apply to
our dataset(s).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_dic\_starter }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, file\_path) \{}
  \CommentTok{\# Function:}
  \CommentTok{\# Creates a .csv file with the basic information}
  \CommentTok{\# to document a curated dataset}
  
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{variable\_name =} \FunctionTok{names}\NormalTok{(data), }\CommentTok{\# column with existing variable names }
       \AttributeTok{name =} \StringTok{""}\NormalTok{, }\CommentTok{\# column for human{-}readable names}
       \AttributeTok{description =} \StringTok{""}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# column for prose description}
  \FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{file =}\NormalTok{ file\_path) }\CommentTok{\# write to disk}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Let's apply our function to the \texttt{sdac\_disfluencies} dataset
using the R console (not part of our project script to avoid overwriting
our documentation!).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data\_dic\_starter}\NormalTok{(}\AttributeTok{data =}\NormalTok{ sdac\_disfluencies, }\AttributeTok{file\_path =} \StringTok{"../data/derived/sdac/sdac\_disfluencies\_data\_dictionary.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/derived/}
\ExtensionTok{└──}\NormalTok{ sdac/}
    \ExtensionTok{├──}\NormalTok{ data\_dictionary\_sdac.csv}
    \ExtensionTok{├──}\NormalTok{ sdac\_curated.csv}
    \ExtensionTok{├──}\NormalTok{ sdac\_disfluencies.csv}
    \ExtensionTok{└──}\NormalTok{ sdac\_disfluencies\_data\_dictionary.csv}
\end{Highlighting}
\end{Shaded}

Open the \texttt{data\_dictionary\_sdac\_disfluencies.csv} file in
spreadsheet software and add the relevant description of the dataset.

\hypertarget{summary-10}{%
\section*{Summary}\label{summary-10}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we covered the process of transforming datasets. The
goal is to manipulate the curated dataset to make it align better for
analysis. There are four general types of transformation steps:
normalization, recoding, generation, and merging. In any given research
project some or all of these steps will be employed --but not
necessarily in the order presented in this chapter. Furthermore there
may also be various datasets generated in at this stage each with a
distinct analysis focus in mind. In any case it is important to write
these datasets to disk and to document them according to the principles
that we have established in the previous chapter.

This chapter concludes the section on data/ dataset preparation. The
next section we turn to analyzing datasets. This is the stage where we
interrogate the datasets to derive knowledge and insight either through
inference, prediction, and/ or exploratory methods.

\hypertarget{activities-6}{%
\section*{Activities}\label{activities-6}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_8.html}{Dataset
manipulation: tokenization and joining datasets}\\
\textbf{How}: Read Recipe 8 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To work with to primary types of transformations,
tokenization and joins. Tokenization is the process of recasting textual
units as smaller textual units. The process of joining datasets aims to
incorporate other datasets to augment or filter the dataset of interest.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/lab_8}{Dataset
manipulation: tokenization and joining datasets}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 8.\\
\textbf{Why}: To gain experience working with coding strategies for
transforming datasets using tidyverse functions and regular expressions,
practice reading/ writing data from/ to disk, and implement
organizational strategies for organizing and documenting a dataset in
reproducible fashion.

\end{tcolorbox}

\hypertarget{questions-7}{%
\section*{Questions}\label{questions-7}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Conceptual questions}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Technical exercises}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{enumerate}

\end{tcolorbox}

\part{Analysis}

In this section we turn to the analysis of datasets, the evaluation of
results, and the interpretation of the findings. We will outline the
three main types of statistical analyses: Exploratory Data Analysis
(EDA), Predictive Data Analysis (PDA), and Inferential Data Analysis
(IDA). Each of these analysis types have distinct, non-overlapping aims
and therefore should be determined from the outset of the research
project and included as part of the research blueprint. The aim of this
section is to establish a clearer picture of the goals, methods, and
value of each of these approaches.

\hypertarget{sec-exploration}{%
\chapter{Exploration}\label{sec-exploration}}

\ldots{} Quote \ldots{}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  \ldots{}
\end{itemize}

\end{tcolorbox}

In this chapter we examine a wide range of strategies for deriving
insight from data in cases where the researcher does not start with a
preconceived hypothesis or prediction, but rather the researcher aims to
uncover patterns and associations from data allowing the data to guide
the trajectory of the analysis. The chapter outlines two main branches
of exploratory data analysis: 1) descriptive analysis which
statistically and/ or visually summarizes a dataset and 2) unsupervised
learning which is a machine learning approach that does not assume any
particular relationship between variables in a dataset. Either through
unsupervised learning or descriptive methods, exploratory data analysis
employs quantitative methods to summarize, reduce, and sort complex
datasets and statistically and visually interrogate a dataset in order
to provide the researcher novel perspective to be qualitatively
assessed.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Swirl}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/swirl}{Unsupervised
Learning}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To \ldots{}

\end{tcolorbox}

\hypertarget{eda-orientation}{%
\section{Orientation}\label{eda-orientation}}

The aim of this section is to provide an overview of exploratory data
analysis (EDA) for linguists, with a focus on descriptive methods such
as frequency analysis and co-occurence analysis, as well as unsupervised
learning approaches such as clustering, topic modeling, and vector space
modeling. It will also include use cases of these methods.

\hypertarget{eda-research-goals}{%
\subsection{Research goals}\label{eda-research-goals}}

The goal of exploratory data analysis is to discover, describe, and
posit new hypotheses. The researcher does not start with a preconceived
hypothesis or prediction, but rather the researcher aims to uncover
patterns and associations from data allowing the data to guide the
trajectory of the analysis. This analysis approach is best-suited for
research where the literature on a research question is limited, or
where the researcher is interested in exploring a new research question.
Since the researcher does not start with a preconceived hypothesis, the
researcher is not able to test a hypothesis and generalize to a
population, but rather the researcher is able to describe the data and
provide a new perspective to be qualitatively assessed. This is achieved
through an iterative and inductive process of data exploration, where
the researcher uses quantitative methods to summarize, reduce, and sort
complex datasets and statistically and visually interrogate a dataset
letting the data guide the analysis.

\hypertarget{eda-approaches}{%
\subsection{Approaches}\label{eda-approaches}}

\begin{itemize}
\tightlist
\item
  There is no fixed outcome variable rather there is only a set of
  predictors or covariates.
\item
  The data is mutable, meaning that the data can be changed or modified
  as needed to address the research question.
\end{itemize}

\hypertarget{eda-analysis-types}{%
\subsubsection{Analysis types}\label{eda-analysis-types}}

There are two main types of exploratory analysis: 1) descriptive
analysis which statistically and/ or visually summarizes a dataset and
2) unsupervised learning which is a machine learning approach that does
not assume any particular relationship between variables in a dataset.
Either or both of these approaches can be used to explore a dataset.

\hypertarget{eda-workflow}{%
\subsection{Workflow}\label{eda-workflow}}

Prerequisites: - A working research question - A dataset which aligns
with the research question or hypothesis in terms of its sampling frame
and the variables it contains or can be derived from the text - A set of
preliminary interests and/ or linguistic variables to explore in the
dataset that align with the research question

Process: - Identify and extract the variables of interest in the dataset
- Interrogate the dataset using descriptive analysis and/ or
unsupervised learning -

\hypertarget{eda-identify}{%
\subsubsection{Identify}\label{eda-identify}}

\begin{itemize}
\tightlist
\item
  With the research question in mind, identify the variables of interest
  in the dataset
\item
  Identify the linguistic variables that can be derived from the text
  (i.e.~liguistic units: words, n-grams, sentences, etc.)
\item
  Consider the operational measures of the variables derived from the
  text (i.e.~frequency, dispersion, co-occurrence, etc.)
\item
  Consider the other variables in the dataset that may be target for
  grouping or filtering the dataset (i.e.~speaker information, document
  information, linguistic unit information, etc.)
\end{itemize}

\hypertarget{eda-interrogate}{%
\subsubsection{Interrogate}\label{eda-interrogate}}

Descriptive analysis:

\begin{itemize}
\tightlist
\item
  Frequency analysis
\item
  Co-occurrence analysis
\end{itemize}

Unsupervised learning:

\begin{itemize}
\tightlist
\item
  Clustering
\item
  Topic modeling
\item
  Word embedding
\end{itemize}

\hypertarget{eda-interpret}{%
\subsubsection{Interpret}\label{eda-interpret}}

Exploratory methods will produce a set of statistical and/ or visual
results. The researcher must interpret these results to determine if
they are meaningful and if they provide a new perspective on the
research question. Many times the results from one method will lead to
new questions which can be explored with other methods. In some cases,
the results may not be meaningful and the researcher may need to return
to the data preparation stage to modify the dataset or the variables of
interest. As the aim of exploratory analysis is jus that, to explore,
the researcher can pivot the approach to explore new questions and new
variables. Ultimately, what is meaningful is determined by the
researcher in the light of the research question and the potential
insight obtained from the results.

\hypertarget{eda-analysis}{%
\section{Analysis}\label{eda-analysis}}

\hypertarget{eda-descriptive}{%
\subsection{Descriptive analysis}\label{eda-descriptive}}

\hypertarget{eda-frequency}{%
\subsubsection{Frequency analysis}\label{eda-frequency}}

Frequency analysis is a descriptive method that counts the number of
times a linguistic unit (i.e.~word, n-gram, sentence, etc.) occurs in a
dataset. The results of frequency analysis can be used to describe the
dataset and to identify linguistic units that are distinctive to a
particular group or sub-group in the dataset.

\begin{itemize}
\tightlist
\item
  Raw frequency (counting)

  \begin{itemize}
  \tightlist
  \item
    Linguistic units (characters, words, n-grams, sentences, etc.)
  \item
    Raw frequency (absolute frequency)
  \item
    Zipf's law (rank-frequency)
  \item
    Issues with raw frequency (\(f\)):

    \begin{itemize}
    \tightlist
    \item
      Incomparable across sub-corpora, corpora, and time
    \end{itemize}
  \end{itemize}
\item
  Term frequency (normalization)

  \begin{itemize}
  \tightlist
  \item
    Relative frequency (proportional frequency)
  \item
    Makes samples (more) comparable (divergence of corpus sizes can
    influences validity of comparison)
  \item
    Issues with term frequency (\(tf\)):

    \begin{itemize}
    \tightlist
    \item
      Does not reveal the distribution of terms across a text or set of
      texts
    \item
      Does not highlight distinctive words

      \begin{itemize}
      \tightlist
      \item
        One way to address this is to filter out `stop words' (i.e.~the,
        a, an, etc.)
      \end{itemize}
    \end{itemize}
  \end{itemize}
\item
  Adjusted frequency (relevance)

  \begin{itemize}
  \tightlist
  \item
    Dispersion (distribution)

    \begin{itemize}
    \tightlist
    \item
      \(idf\) (inverse document frequency)
    \item
      \ldots{} Juilland's \(D\) (equal/ non-equal parts)
    \item
      Gries' \(DP\) Deviation of Proportions (DP)
    \end{itemize}
  \item
    Weights

    \begin{itemize}
    \tightlist
    \item
      \(tf-idf\) (term frequency-inverse document frequency)

      \begin{itemize}
      \tightlist
      \item
        Distinctive words (Used to identify the most relevant keywords
        in a given text.)
      \item
        Issues with \(tf-idf\): Does not reveal the context of terms,
        Does not reveal the relationship between terms
      \end{itemize}
    \item
      weighted log odds
      (\href{https://juliasilge.r-universe.dev/articles/tidylo/tidylo.html}{\texttt{tidylo}
      package})

      \begin{itemize}
      \tightlist
      \item
        Distinctive words (Used to identify the most relevant keywords
        in a given text.)
      \item
        Advantage over \(tf-idf\): does a better job dealing with
        different combinations of words and documents having different
        counts.
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{eda-co-occurrence}{%
\subsubsection{Co-occurrence analysis}\label{eda-co-occurrence}}

\begin{itemize}
\tightlist
\item
  Concordance

  \begin{itemize}
  \tightlist
  \item
    KWIC (keyword in context) This section will discuss keyword in
    context (KWIC) analyses, which is used to identify meaningful
    keywords in a given text. It will discuss various ways to analyse a
    text and extract keywords, as well as discuss various practical
    applications of KWIC in linguistics.
  \end{itemize}
\item
  Collocation

  \begin{itemize}
  \tightlist
  \item
    PMI (pointwise mutual information)
  \item
    Dice's coefficient
  \item
    \ldots{}
  \end{itemize}
\end{itemize}

\hypertarget{eda-unsupervised}{%
\subsection{Unsupervised learning}\label{eda-unsupervised}}

\hypertarget{eda-clustering}{%
\subsubsection{Clustering}\label{eda-clustering}}

This section will discuss clustering techniques, which are used to
partition data into clusters based on similarity. It will discuss
various approaches to clustering, such as k-means and hierarchical
clustering, as well as discuss their use cases in linguistics.

\begin{itemize}
\tightlist
\item
  K-means (pre-defined number of clusters)
\item
  Hierarchical clustering (dendrogram)
\end{itemize}

\hypertarget{eda-dimensionality-reduction}{%
\subsubsection{Dimensionality
reduction}\label{eda-dimensionality-reduction}}

\begin{itemize}
\tightlist
\item
  PCA (principal component analysis)
\item
  MDS (multidimensional scaling)
\end{itemize}

\hypertarget{eda-topic-modeling}{%
\subsubsection{Topic modeling}\label{eda-topic-modeling}}

This section will discuss topic modeling techniques, which are used to
identify and group semantically similar topics in unstructured data. It
will discuss various approaches to topic modelling, such as Latent
Dirichlet Allocation (LDA), and discuss their applications in
linguistics.

\begin{itemize}
\tightlist
\item
  LDA (latent Dirichlet allocation)
\item
  LSA (latent semantic analysis)
\end{itemize}

\hypertarget{eda-word-embedding}{%
\subsubsection{Word embedding}\label{eda-word-embedding}}

This section will discuss word embedding techniques, which are used to
represent words in a vector space. It will discuss various approaches to
word embedding, such as Word2Vec and GloVe, and discuss their
applications in linguistics.

\begin{itemize}
\tightlist
\item
  Word2vec (skip-gram)
\item
  GloVe (global vectors for word representation)
\end{itemize}

\hypertarget{summary-11}{%
\section{Summary}\label{summary-11}}

Exploratory data analysis is a set of methods that can be used to
explore a dataset and to identify new questions and new variables of
interest. The methods can be used to describe a dataset and to identify
linguistic units that are distinctive to a particular group or sub-group
in the dataset. The methods can also be used to identify semantically
similar topics in unstructured data. The results of exploratory analysis
can be used to inform the development of a hypothesis or to inform the
design of a machine learning model.

\hypertarget{activities-7}{%
\section*{Activities}\label{activities-7}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_11.html}{Exploratory
methods: descriptive and unsupervised learning analysis methods}\\
\textbf{How}: Read Recipe 10 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To illustrate how to prepare a dataset for descriptive and
unsupervised machine learning methods and evaluate the results for
exploratory data analysis.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/lab_11}{Exploratory Data
Analysis}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 9.\\
\textbf{Why}: To gain experience working with coding strategies to
prepare, feature engineer, explore, and evaluate results from
exploratory data analyses, practice transforming datasets into new
object formats and visualizing relationships, and implement
organizational strategies for organizing and reporting results in a
reproducible fashion.

\end{tcolorbox}

\hypertarget{questions-8}{%
\section*{Questions}\label{questions-8}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Conceptual questions}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is exploratory data analysis?
\item
  How can exploratory data analysis be used to uncover patterns and
  associations?
\item
  Describe the workflow of exploratory data analysis?
\item
  What are the advantages and disadvantages of descriptive analysis?
\item
  What are the advantages and disadvantages of unsupervised learning?
\item
  What is the difference between supervised and unsupervised learning?
\item
  How does exploratory data analysis differ from traditional hypothesis
  testing?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Technical exercises}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a function in R to conduct a hierarchical cluster analysis on a
  dataset.
\item
  Implement a k-means algorithm in R to identify clusters within a
  dataset.
\item
  Implement a Principal Component Analysis (PCA) algorithm in R to
  identify patterns and associations within a dataset.
\item
  Write a function in R to produce a descriptive summary of a dataset.
\item
  Conduct a correlation analysis in R to identify relationships between
  variables in a dataset.
\item
  Load a dataset into R and conduct a frequency analysis on the dataset.
\item
  Load a dataset into R and conduct a keyword in context analysis on the
  dataset.
\item
  Load a dataset into R and conduct a keyword analysis on the dataset.
\item
  Load a dataset into R and conduct a sentiment analysis on the dataset.
\item
  Load a dataset into R and conduct a topic modelling analysis on the
  dataset.
\end{enumerate}

\end{tcolorbox}

\hypertarget{sec-prediction}{%
\chapter{Prediction}\label{sec-prediction}}

\ldots{} Quote \ldots{}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  \ldots{}
\end{itemize}

\end{tcolorbox}

In this chapter I present an introduction to approaches to data analysis
known as machine learning, specifically supervised learning. In a
nutshell, the aim of supervised learning is to leverage a potential
relationship between a target or outcome variable and a set of other
variables (features) derived from text to create a statistical
generalization (model) that can accurately predict the values of the
target variable using the values of the feature variables. We consider
practical tasks as well as theoretical applications of the statistical
learning in text analysis highlighting the standard workflow for
building predictive models, testing and evaluating models, working to
improve model performance, and how to interpret and report findings.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Swirl}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/swirl}{Supervised
Learning}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: \ldots{}

\end{tcolorbox}

\hypertarget{pda-orientation}{%
\section{Orientation}\label{pda-orientation}}

The aim of this section is to introduce the reader to the concept of
supervised learning and to provide a brief overview of the workflow for
building predictive models for text analysis.

\hypertarget{pda-research-goals}{%
\subsection{Research goals}\label{pda-research-goals}}

Supervised learning is a type of machine learning that involves training
a model on a labeled dataset where the input data and desired output are
both provided. The model is able to make predictions or classifications
based on the input data by learning the relationships between the input
and output data. Supervised machine learning is an important tool for
linguists studying language and communication, as it allows them to
analyze language data to identify patterns or trends in language use,
verify hypotheses, and prescribe actions. Supervised machine learning is
an active area of research in linguistics, with many potential
applications and areas for further exploration.

Predictive analyses are more inductive than exploratory analyses, which
are more deductive. This means that we are more interested in the
relationship between a particular outcome variable and a set of
predictor variables than we are in the relationship between the
predictor variables themselves, as we would be in an exploratory
analysis. In this sense, we have a particular outcome in mind from the
outset. On the other hand, the input variables are mutable, meaning that
they can be changed to see how they affect the outcome --which points to
the exploratory aspect of predictive analyses.

\hypertarget{pda-approaches}{%
\subsection{Approaches}\label{pda-approaches}}

\begin{itemize}
\tightlist
\item
  Outcome variable and any number of predictor variables
\item
  Predictor variables are features derived from text and are mutable.
\end{itemize}

\hypertarget{pda-analysis-types}{%
\subsubsection{Analysis types}\label{pda-analysis-types}}

There are two main types of supervised machine learning algorithms:
classification, which is used to predict a categorical outcome such as
the genre of a text, and regression, which is used to predict a
continuous outcome such as the sentiment of a text.

\begin{itemize}
\tightlist
\item
  Supervised learning

  \begin{itemize}
  \tightlist
  \item
    Classification

    \begin{itemize}
    \tightlist
    \item
      Categorical outcome variable
    \end{itemize}
  \item
    Regression

    \begin{itemize}
    \tightlist
    \item
      Continuous outcome variable
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{pda-workflow}{%
\subsection{Workflow}\label{pda-workflow}}

Prerequisites: - A working research question or hypothesis - A dataset
which aligns with the research question or hypothesis in terms of its
sampling frame and the variables it contains or can be derived from the
text and a target variable to be predicted. - A set of preliminary
features to be derived from the text that are used to predict the target
variable

\hypertarget{pda-identify}{%
\subsubsection{Identify}\label{pda-identify}}

Data cleaning and feature extraction are the first steps in the process
of preparing data for supervised machine learning.

In order to use supervised machine learning, linguists must first
identify measurable properties of the text use use as the input
variables or features that are most likely to produce a model that
performs well (i.e.~that when used make accurate predictions). Once the
feature types are identified, the data is processed to clean any
extraneous elements and format the structure of the dataset given the
requirements of the algorithm that will be used in subsequent steps.

\hypertarget{pda-interrogate}{%
\subsubsection{Interrogate}\label{pda-interrogate}}

Model training is the next step towards building a predictive model.

In this step, the data is split into training and testing sets. The
training set is used to train the model, and the testing set is used to
evaluate the model's performance. The testing set is reserved and not
used to train the model, so that the model's performance can be
evaluated on data that it has not seen before.

The model is then trained on the training data and evaluated on the
testing data. The results are then evaluated and the hyperparameters of
the model may be adjusted to optimize its performance.

and the hyperparameters of the model may be adjusted to optimize its
performance.

Hyperparameters are variables that are set prior to running a machine
learning algorithm whose values influence the final result. In
supervised machine learning, hyperparameters are typically used to
control the learning process such as the learning rate, momentum, and
batch size.

Some applications of supervised machine learning in linguistics include
text classification, part-of-speech tagging, and language
identification. Supervised machine learning is an active area of
research in linguistics, with many potential applications and areas for
further exploration.

\hypertarget{pda-interpret}{%
\subsubsection{Interpret}\label{pda-interpret}}

To either evaluate the training or testing set, the model is used to
make predictions on the data in the set. The predictions are then
compared to the actual values of the target variable in the set to
evaluate the model's performance. So how is the model's performance
evaluated?

\hypertarget{pda-workflow-classification}{%
\paragraph{Classification}\label{pda-workflow-classification}}

For classification, there are a number of metrics that can be used to
evaluate the performance of a model, including accuracy, precision,
recall, and F1 score. To understand these measures it is helpful to
consider a confusion matrix, which is a table that describes the
performance of a classification model on data for which the true values
are known. The confusion matrix is a two-by-two matrix that shows the
number of true positives (TP), false positives (FP), true negatives
(TN), and false negatives (FN), as seen in
Table~\ref{tbl-pda-confusion-matrix}.

\hypertarget{tbl-pda-confusion-matrix}{}
\begin{table}
\caption{\label{tbl-pda-confusion-matrix}A labeled confusion matrix }\tabularnewline

\centering
\begin{tabular}{l|l|l}
\hline
  & Predicted positive & Predicted negative\\
\hline
Actual positive & TP & FP\\
\hline
Actual negative & FN & TN\\
\hline
\end{tabular}
\end{table}

Now let's fill this confusion matrix with hypothetical values, as seen
in Table~\ref{tbl-pda-confusion-matrix-example} to see how the metrics
are calculated.

\hypertarget{tbl-pda-confusion-matrix-example}{}
\begin{table}
\caption{\label{tbl-pda-confusion-matrix-example}Confusion matrix for a hypothetical model's performance on a test set }\tabularnewline

\centering
\begin{tabular}{l|r|r}
\hline
  & Predicted positive & Predicted negative\\
\hline
Actual positive & 100 & 10\\
\hline
Actual negative & 20 & 50\\
\hline
\end{tabular}
\end{table}

\begin{itemize}
\tightlist
\item
  Accuracy is defined as the proportion of correct predictions made by
  the model.
\end{itemize}

\begin{equation}\protect\hypertarget{eq-pda-accuracy}{}{
\text{Accuracy} = \frac{\text{Number of correct predictions}}{\text{Total number of predictions}}
}\label{eq-pda-accuracy}\end{equation}

The number of correct predictions is the sum of true positives and true
negatives. So in our case this is 100 + 50 = 150. The total number of
predictions is the sum of all four cells in the confusion matrix, so in
our case this is 100 + 10 + 20 + 50 = 180. So the accuracy of our
hypothetical model is 150/180 = 0.833.

\begin{itemize}
\tightlist
\item
  Precision is defined as the proportion of positive predictions that
  are correct.
\end{itemize}

\begin{equation}\protect\hypertarget{eq-pda-precision}{}{
\text{Precision} = \frac{\text{Number of true positives}}{\text{Number of true positives + false positives}}
}\label{eq-pda-precision}\end{equation}

\begin{itemize}
\tightlist
\item
  Recall is defined as the proportion of actual positives that are
  correctly identified.
\end{itemize}

\begin{equation}\protect\hypertarget{eq-pda-recall}{}{
\text{Recall} = \frac{\text{Number of true positives}}{\text{Number of true positives + false negatives}}
}\label{eq-pda-recall}\end{equation}

\begin{itemize}
\tightlist
\item
  The F1 score is the harmonic mean of precision and recall.
\end{itemize}

\begin{equation}\protect\hypertarget{eq-pda-f1}{}{
\text{F1 score} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
}\label{eq-pda-f1}\end{equation}

Area under the curve (AUC) is the area under the ROC (Receiver Operating
Characteristic) curve, which is a graph of true positives (TPR) and
false positives (FPR). The AUC is a measure of the model's performance
across all possible classification thresholds. The AUC is a number
between 0 and 1, where 0.5 represents a model that is no better than
random guessing, and 1 represents a perfect model.

\textbf{Feature importance}

\begin{itemize}
\tightlist
\item
  parallel coordinate visualization
\end{itemize}

In a supervised text classification task, you can use parallel
coordinate plots to visualize the distribution of class labels across
different feature dimensions. This can help identify which features are
most informative for distinguishing between classes and inform feature
selection or dimensionality reduction techniques.

::: \{.cell layout-align=``center''
hash=`prediction\_cache/pdf/fig-pda-parallel-coordinates\_ad3563d97c3119e8837f53d62d1881c6'\}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Libraries}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tidytext)}

\CommentTok{\# Faux data}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
 \AttributeTok{text =} \FunctionTok{c}\NormalTok{(}\StringTok{"I love cats"}\NormalTok{, }\StringTok{"Cats are amazing"}\NormalTok{, }\StringTok{"I hate dogs"}\NormalTok{, }\StringTok{"Dogs are annoying"}\NormalTok{),}
 \AttributeTok{class =} \FunctionTok{c}\NormalTok{(}\StringTok{"positive"}\NormalTok{, }\StringTok{"positive"}\NormalTok{, }\StringTok{"negative"}\NormalTok{, }\StringTok{"negative"}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Tokenize data}
\NormalTok{tokenized\_data }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{unnest\_tokens}\NormalTok{(word, text) }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{count}\NormalTok{(class, word) }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{spread}\NormalTok{(class, n, }\AttributeTok{fill =} \DecValTok{0}\NormalTok{)}

\CommentTok{\# Normalize the term frequencies}
\NormalTok{normalized\_data }\OtherTok{\textless{}{-}}
\NormalTok{ tokenized\_data }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{mutate}\NormalTok{(}
   \AttributeTok{positive =}\NormalTok{ positive }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(positive),}
   \AttributeTok{negative =}\NormalTok{ negative }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(negative)}
\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
 \FunctionTok{gather}\NormalTok{(class, frequency, }\SpecialCharTok{{-}}\NormalTok{word)}

\CommentTok{\# Generate the parallel coordinate plot}
\FunctionTok{ggplot}\NormalTok{(normalized\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ class, }\AttributeTok{y =}\NormalTok{ frequency, }\AttributeTok{group =}\NormalTok{ word, }\AttributeTok{color =}\NormalTok{ word)) }\SpecialCharTok{+}
 \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
 \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
 \FunctionTok{labs}\NormalTok{(}
   \AttributeTok{title =} \StringTok{"Parallel Coordinate Visualization"}\NormalTok{,}
   \AttributeTok{subtitle =} \StringTok{"Text Classification Model Using Words as Features"}\NormalTok{,}
   \AttributeTok{x =} \StringTok{"Class"}\NormalTok{,}
   \AttributeTok{y =} \StringTok{"Normalized Term Frequency"}
\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

::: \{.cell-output-display\}
\includegraphics{prediction_files/figure-pdf/fig-pda-parallel-coordinates-1.pdf}
::: :::

We can look at the parallel coordinate plot in
\textbf{?@fig-fig-pda-parallel-coordinates} to see that the words
``cats'' and ``love'' are more common in the positive class, while the
words ``dogs'' and ``hate'' are more common in the negative class. This
suggests that these words are good features for distinguishing between
the two classes.

\hypertarget{pda-workflow-regression}{%
\paragraph{Regression}\label{pda-workflow-regression}}

For regression, the most common metric is the root mean squared error
(RMSE). The RMSE is the square root of the mean of the squared
differences between the predicted values and the actual values. The
lower the RMSE, the better the model fits the data.

Supervised machine learning algorithms for regression are typically
evaluated using measures of error such as mean squared error (MSE), root
mean squared error (RMSE), and mean absolute error (MAE). MSE is used to
measure the average of the squares of the errors, MAE is the average of
the absolute differences between the prediction and the actual data, and
RMSE is the square root of the mean squared error. For each of these
statistics, the lower the value, the better the model fits the data. The
differences between these statistics are shown in
Table~\ref{tbl-pda-error-metrics}.

\hypertarget{tbl-pda-error-metrics}{}
\begin{table}
\caption{\label{tbl-pda-error-metrics}A table showing the differences between mean squared error, root mean
squared error, and mean absolute error }\tabularnewline

\centering
\begin{tabular}{l|l|l}
\hline
error & formula & description\\
\hline
MSE & \$\$\textbackslash{}frac\{1\}\{n\} \textbackslash{}sum\_\{i=1\}\textasciicircum{}\{n\} (y\_i - \textbackslash{}hat\{y\}\_i)\textasciicircum{}2\$\$ & The average of the squared differences between the prediction and the actual data\\
\hline
RMSE & \$\$\textbackslash{}sqrt\{\textbackslash{}frac\{1\}\{n\} \textbackslash{}sum\_\{i=1\}\textasciicircum{}\{n\} (y\_i - \textbackslash{}hat\{y\}\_i)\textasciicircum{}2\}\$\$ & The square root of the mean squared error\\
\hline
MAE & \$\$\textbackslash{}frac\{1\}\{n\} \textbackslash{}sum\_\{i=1\}\textasciicircum{}\{n\} \textbackslash{}vert y\_i - \textbackslash{}hat\{y\}\_i \textbackslash{}vert\$\$ & The average of the absolute differences between the prediction and the actual data\\
\hline
\end{tabular}
\end{table}

The main advantages of using MSE, RMSE, and MAE are that they are all on
the same scale as the dependent variable, and they are all
differentiable, which makes them useful for optimization algorithms. MSE
is the most commonly used metric for regression, but RMSE and MAE are
also used. MSE is more sensitive to outliers than RMSE and MAE, so it is
more useful when the data has outliers. RMSE and MAE are more useful
when the data does not have outliers.

Plot the actual and predicted values to see how well the model fits the
data.

\begin{figure}[h]

{\centering \includegraphics{prediction_files/figure-pdf/pda-regression-plot-1.pdf}

}

\caption{A plot of the actual and predicted values for a regression
model}

\end{figure}

We can now apply our error metrics to the \texttt{results} data to see
how well the model fits the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#|}
\CommentTok{\# Calculate the error metrics for the \textasciigrave{}results\textasciigrave{} data}
\NormalTok{results }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{error =}\NormalTok{ actual }\SpecialCharTok{{-}}\NormalTok{ predicted) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# calculate the error}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{mse =} \FunctionTok{mean}\NormalTok{(error}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), }\CommentTok{\# calculate the MSE}
    \AttributeTok{rmse =} \FunctionTok{sqrt}\NormalTok{(mse), }\CommentTok{\# calculate the RMSE}
    \AttributeTok{mae =} \FunctionTok{mean}\NormalTok{(}\FunctionTok{abs}\NormalTok{(error)), }\CommentTok{\# calculate the MAE}
    \AttributeTok{n =} \FunctionTok{n}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# calculate the number of observations}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{mse =}\NormalTok{ mse }\SpecialCharTok{*} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ n, }\CommentTok{\# multiply by 1/n to get the MSE for n observations}
    \AttributeTok{rmse =}\NormalTok{ rmse }\SpecialCharTok{*} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ n, }\CommentTok{\# multiply by 1/n to get the RMSE for n observations}
    \AttributeTok{mae =}\NormalTok{ mae }\SpecialCharTok{*} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ n}
\NormalTok{  ) }\CommentTok{\# multiply by 1/n to get the MAE for n observations}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 1 x 4
#>      mse   rmse     mae     n
#>    <dbl>  <dbl>   <dbl> <int>
#> 1 0.0113 0.0106 0.00947   100
\end{verbatim}

Formula for calculating the MSE:

\begin{equation}\protect\hypertarget{eq-pda-mse}{}{
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
}\label{eq-pda-mse}\end{equation}

So we can implement this in R subtracting the actual values from the
predicted values, squaring the differences, then taking the mean of the
all the squared differences, and finally multiplying by \(1/n\) to get
the MSE for \(n\) observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\SpecialCharTok{|\textgreater{}} \CommentTok{\# use the \textasciigrave{}results\textasciigrave{} data}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{error =}\NormalTok{ actual }\SpecialCharTok{{-}}\NormalTok{ predicted) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# calculate the error}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{mse =} \FunctionTok{mean}\NormalTok{(error}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), }\CommentTok{\# calculate the MSE}
    \AttributeTok{n =} \FunctionTok{n}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# calculate the number of observations}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{mse =}\NormalTok{ mse }\SpecialCharTok{*} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ n) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# multiply by 1/n to get the MSE for n observations}
  \FunctionTok{pull}\NormalTok{(mse) }\CommentTok{\# pull the MSE value out of the tibble}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 0.0113
\end{verbatim}

Formula for calculating the RMSE:

\begin{equation}\protect\hypertarget{eq-pda-rmse}{}{
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
}\label{eq-pda-rmse}\end{equation}

\hypertarget{pda-analysis}{%
\section{Analysis}\label{pda-analysis}}

Recap and introduction to the structure of the analysis subsection.

\begin{itemize}
\tightlist
\item
  Introduce an algorithm
\item
  Build a model

  \begin{itemize}
  \tightlist
  \item
    Preprocessing (tokenization, lemmatization, etc.)
  \item
    Feature extraction (TF-IDF, word embeddings, etc.)
  \item
    Model selection (logistic regression, SVM, etc.)
  \item
    Model training
  \end{itemize}
\item
  Evaluate (and adjust) the model on the training data

  \begin{itemize}
  \tightlist
  \item
    Cross-validation
  \item
    Evaluation metrics
  \item
    Compare to null and/ or other models
  \item
    Adjust the model (hyperparameters, regularization, etc.) as
    necessary
  \end{itemize}
\item
  Evaluate the model on the test data

  \begin{itemize}
  \tightlist
  \item
    Evaluation metrics
  \item
    Evaluate feature importance
  \item
    Evaluate the features of correct and incorrect predictions
  \end{itemize}
\end{itemize}

\hypertarget{pda-classification}{%
\subsection{Classification}\label{pda-classification}}

We will first start with classification which is by far the most common
text analysis approach in supervised machine learning. Again,
classification is the task of predicting a categorical variable from a
set of features. The features we use will be derived from the text but
can take many forms. For example, we can use the raw text, the word
counts, the TF-IDF values, or the word embeddings. We also will take
into account the number of features we use. There is a trade-off,
however, to the number of features: a) the more features we use, the
more complex the model will be, and the more likely it will overfit the
training data and b) the less features we use, the less complex the
model will be, and the more likely it will underfit the training data.
To find the optimal number of features we can use a technique called
cross-validation.

The most common text classification algorithms are logistic regression,
k-nearest neighbors, Naive Bayes, and support vector machines. We will
start with logistic regression and k-nearest neighbors as they are the
simplest to understand and implement. We will then move on to Naive
Bayes and support vector machines as they are more complex and require
more explanation.

Building a null model for classification we simply predict the most
common class in the training data. This makes sense as we have seen
earlier, with categorical data the central tendency is estimated by the
mode --i.e.~the most common value.

\hypertarget{pda-k-nearest-neighbors}{%
\subsubsection{K-nearest neighbors}\label{pda-k-nearest-neighbors}}

K-nearest neighbors is a simple supervised machine learning method for
classification. It is a non-parametric method, which means that it does
not make any assumptions about the underlying distribution of the data.
It is a lazy learning method, which means that it does not learn a
discriminant function from the training data but instead stores the
training data. It is a distance-based method, which means that it uses a
distance metric to find the \(k\) nearest neighbors to a new
observation. It is a simple method, which means that it is easy to
understand and implement.

\hypertarget{pda-logistic-regression}{%
\subsubsection{Logistic regression}\label{pda-logistic-regression}}

Logistic regression is a supervised machine learning method for
classification. It is a parametric method, which means that it makes
assumptions about the underlying distribution of the data. It is an
iterative method, which means that it uses an iterative algorithm to
find the optimal parameters. To avoid overfitting it uses regularization
such as ridge regression or lasso regression. These regularization
methods penalize the model for having too many parameters. However, how
does one know what the optimal number of parameters is? This is where
cross-validation comes in.

\hypertarget{pda-naive-bayes}{%
\subsubsection{Naive Bayes}\label{pda-naive-bayes}}

Naive Bayes is a supervised machine learning method for classification.
It is a parametric method, which means that it makes assumptions about
the underlying distribution of the data. It is a probabilistic method,
which means that it uses Bayes' theorem to calculate the probability of
a class given the predictor variables. It is a generative method, which
means that it learns the joint probability distribution of the predictor
variables and the outcome variable. It is a simple method, which means
that it makes the assumption that the predictor variables are
independent of each other. This assumption is called the naive
assumption. Now this assumption does not theoretically hold for language
data as words are not independent of each other. However, in practice,
Naive Bayes' models still perform well on many text classification
tasks.

\hypertarget{pda-decision-trees}{%
\subsubsection{Decision trees}\label{pda-decision-trees}}

Decision trees for text classification are a supervised machine learning
method for classification. They are a non-parametric method, which means
that they do not make any assumptions about the underlying distribution
of the data. They are a greedy method, which means that they use a
greedy algorithm to find the optimal split of the predictor variables.
They are a simple method, which means that they are easy to understand
and implement.

In practical terms using decision trees for text classification can be
very useful as they are easy to interpret. For example, we can see which
words are most important for the classification of a text. However, they
are prone to overfitting the training data. To avoid this we can use a
technique called bagging. Bagging is a technique that uses multiple
decision trees to make a prediction. The prediction is then the mode of
the predictions of the individual decision trees. This is called a
random forest.

\hypertarget{pda-regression}{%
\subsection{Regression}\label{pda-regression}}

In supervised machine learning regression tasks contrast to
classification tasks as the outcome variable is continuous. A typical
example outside of language would be to predict the price of a house
given the number of bedrooms, the number of bathrooms, the size of the
house, etc. For language this means that the labled outcome variable is
a number, not a class. For example, we can predict the number of words
in a text given the number of characters in the text, the number of
sentences in the text, etc. Other applications of regression in text
analysis are sentiment analysis (where the outcome is a scalar value)
and topic modeling (where the outcome is a probability distribution over
topics).

\hypertarget{pda-linear-regression}{%
\subsubsection{Linear regression}\label{pda-linear-regression}}

Linear regression can be used to predict a continuous outcome variable
from a set of features. It is a parametric method, which means that it
makes assumptions about the underlying distribution of the data. It is
an iterative method, which means that it uses an iterative algorithm to
find the optimal parameters. To avoid overfitting it uses regularization
such as ridge regression or lasso regression. These regularization
methods penalize the model for having too many parameters. However, how
does one know what the optimal number of parameters is? This is where
cross-validation comes in.

\hypertarget{pda-decision-trees-regression}{%
\subsubsection{Decision trees
(regression)}\label{pda-decision-trees-regression}}

Decision trees for regression are a supervised machine learning method
for regression. They are a non-parametric method, which means that they
do not make any assumptions about the underlying distribution of the
data. They are a greedy method, which means that they use a greedy
algorithm to find the optimal split of the predictor variables. They are
a simple method, which means that they are easy to understand and
implement.

\hypertarget{pda-neural-networks}{%
\subsubsection{Neural networks}\label{pda-neural-networks}}

Neural networks are a supervised machine learning method for regression
and classification. They are a non-parametric method, which means that
they do not make any assumptions about the underlying distribution of
the data. They are an iterative method, which means that they use an
iterative algorithm to find the optimal parameters. They are a complex
method, which means that they are difficult to understand and implement.
However, they are very powerful and can be used to solve a wide range of
problems. However, they are expensive to train and require a lot of
data. It is often the case that a simpler method will perform just as
well as a neural network in certain contexts.

\hypertarget{pda-reporting}{%
\section{Reporting}\label{pda-reporting}}

When reporting the results of a supervised machine learning model it is
important to report the evaluation metrics that are most relevant to the
problem at hand. For example, if the problem is to predict a continuous
outcome, then the most relevant evaluation metric is the mean squared
error (MSE). It is often useful to report the root mean squared error
(RMSE) as well.

However, if the problem is to predict the class, then the most relevant
evaluation metric is the accuracy. Other evaluation metrics that are
often reported are the precision, recall, and F1-score. It can also be
useful to report the confusion matrix. The confusion matrix shows the
number of true positives, false positives, true negatives, and false
negatives.

If the research goal is focused on prediction accuracy, then these
statistics are the most relevant. But in other cases, the supervised
learning alogrithm is a means to guage a relationship between the
outcome and the predictor variables, namely to guage the most important
predictor variables. The model can then be used to identify those
predictor variables that support accurate predictions and even to
identify those predictor variables that do not support accurate
predictions. Note, however, that some supervised learning algorithms are
not able to identify the most important predictor variables. For
example, neural networks are not able to identify the most important
predictor variables. These `black box' algorithms may lead to accurate
predictions, but they do not provide any insight into the underlying
relationship between the outcome and the predictor variables.

\hypertarget{pda-summary}{%
\section{Summary}\label{pda-summary}}

In this chapter we have learned about supervised machine learning. We
have learned about the different types of supervised machine learning
methods and how they can be used to predict and classify. We have also
learned about the different types of data structures that are used in
supervised machine learning. Finally, we have learned about the
different types of evaluation metrics that are used to evaluate the
performance of supervised machine learning models.

\hypertarget{activities-8}{%
\section*{Activities}\label{activities-8}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_10.html}{Predictive
models: prep, train, test, and evaluate}\\
\textbf{How}: Read Recipe 10 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To illustrate some common coding strategies for preparing
datasets for inferential data analysis, as well as the steps conduct
descriptive assessment, statistical interrogation, and evaluation and
reporting of results.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/lab_10}{Predictive Data
Analysis}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 10.\\
\textbf{Why}: To gain experience working with coding strategies to
prepare, feature engineer, train and test a predictive model, and
evaluate results from a predictive data analysis, practice transforming
datasets into new object formats and visualizing relationships, and
implement organizational strategies for organizing and reporting results
in a reproducible fashion.

\end{tcolorbox}

\hypertarget{questions-9}{%
\section*{Questions}\label{questions-9}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Conceptual questions}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the difference between a continuous and a categorical
  variable?
\item
  What is the difference between a regression and a classification
  model?
\item
  What is the difference between a training set and a testing set?
\item
  What is the difference between a hyperparameter and a parameter?
\item
  What is the difference between a supervised and an unsupervised
  machine learning model?
\item
  What advantages and disadvantages do supervised machine learning
  models have over traditional methods of text analysis?
\item
  What are some potential applications of supervised machine learning in
  linguistics?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Technical exercises}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a program to build a classification model which uses a set of
  collected text features to predict a target variable.
\item
  Use the classification model to classify a series of documents and
  assess the accuracy of the model.
\item
  Develop a regression model which uses text features to predict a
  continuous target variable.
\item
  Create a text mining application to analyze a large body of text and
  discover correlations between variables.
\item
  Use a clustering algorithm to discover clusters in a large dataset,
  and create a visualization to present the identified clusters.
\item
  Analyze the structure of a text corpus and identify patterns in word
  usage and feature distributions.
\item
  Build a predictive model using text as an input and binary or
  categorical outcomes as the target.
\item
  Develop a natural language processing application which classifies
  text into predefined categories using a supervised learning algorithm.
\item
  Use a supervised learning algorithm to build a predictive model which
  classifies a set of unseen texts into predefined categories.
\item
  Develop a web application which allows users to easily explore a set
  of text documents, visualize the content of the documents, and
  generate predictive models from the text.
\end{enumerate}

\end{tcolorbox}

\hypertarget{sec-inference}{%
\chapter{Inference}\label{sec-inference}}

\begin{quote}
People generally see what they look for, and hear what they listen for.

--- Harper Lee, To Kill a Mockingbird
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  what are the three main types of inferential analysis approaches?
\item
  how does the informational value of the dependent variable relate to
  the statistical approach adopted?
\item
  how to descriptive, statistical, and evaluative steps work together to
  produce the reliable results?
\end{itemize}

\end{tcolorbox}

In this chapter we consider approaches to deriving knowledge from
information which can be generalized to the population from which the
data is sampled. This process is known as statistical inference. The
discussion here implements descriptive assessments, statistical tests,
and model evaluation procedures for a series of contexts which are
common in the analysis of corpus-based data. The chapter is structured
into three main sections which correspond to the number of variables
included in the statistical procedure. Each of these sections includes a
subsection dedicated to the informational value of the dependent
variable; the variable whose variation is to be explained.

For this discussion two datasets will be used as the base to pose
various questions to submit for interrogation. It is of note that the
questions in the subsequent sections are posited to highlight various
descriptive, statistic, and evaluation procedures and do not reflect the
standard approach to hypothesis testing which assumes that the null and
alternative hypotheses are developed at the outset of the research
project.

The process for each inferential data analysis in this section will
include three steps: (1) descriptive assessment, (2) statistical
interrogation, and (3) evaluation of the results.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Swirl}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/swirl}{Significance
testing}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To learn how to implement various statistical tests in
common significance testing scenarios.

\end{tcolorbox}

\hypertarget{ida-orientation}{%
\section{Orientation}\label{ida-orientation}}

The aim of this section is to provide an overview of inferential data
analysis (IDA) and to introduce the three main types of inferential
analysis approaches that are most common in text analysis research.

\hypertarget{ida-research-goals}{%
\subsection{Research goals}\label{ida-research-goals}}

The goal of IDA is to detect, explain, and generalize. The relationship
to detect is predtermined by the hypothesis. In this situation the
researcher will have identified an outcome variable (often known as a
dependent variable) and often a predictor or set of predictor variables
(independent variables) that are directly linked to the hypothesis in a
way that the results of the statistical analysis allows to either
confirm that null hypothesis or reject it. Explaining the results of the
statistical analysis is key to summarizing the findings and how they
relate to the hypothesis. To the extent that the text sample used is
representative of the population from which the data is sampled, the
results can be generalized to the population. IDA is not an exploratory
endeavor and as such that anlysis is performed on the data in a much
more conservative manner than is the case in exploratory data analysis
(EDA) or predictive data analysis (PDA).

\hypertarget{ida-approaches}{%
\subsection{Approaches}\label{ida-approaches}}

\begin{itemize}
\tightlist
\item
  The dependent variable and predictor variables are fixed (tied to
  hypothesis)
\item
  Descriptive statistics and visualizations (plots or tables) are used
  to summarize the data and provide a preliminary assessment of the data
\item
  Inferential statistics are used to test the hypothesis
\end{itemize}

\hypertarget{ida-analysis-types}{%
\subsubsection{Analysis types}\label{ida-analysis-types}}

The type of analysis that is performed depends most heavily on the
informational value of the dependent variable. The informational value
of the dependent variable is determined by the type of data that is
collected. Secondly, the number and informational types of the
independent variables (predictor variables) also play a role in
determining the type of analysis that is performed.

\begin{itemize}
\tightlist
\item
  Categorical dependent variable

  \begin{itemize}
  \tightlist
  \item
    Descriptive statistics

    \begin{itemize}
    \tightlist
    \item
      Frequency
    \item
      Proportion
    \item
      Confidence intervals
    \end{itemize}
  \item
    Inferential statistics

    \begin{itemize}
    \tightlist
    \item
      Chi-square
    \item
      Logistic regression
    \end{itemize}
  \end{itemize}
\item
  Continuous dependent variable

  \begin{itemize}
  \tightlist
  \item
    Descriptive statistics

    \begin{itemize}
    \tightlist
    \item
      Mean
    \item
      Standard deviation
    \item
      Confidence intervals
    \end{itemize}
  \item
    Inferential statistics

    \begin{itemize}
    \tightlist
    \item
      Correlation
    \item
      Linear regression
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{ida-workflow}{%
\subsection{Workflow}\label{ida-workflow}}

Prerequisites: - A testable hypothesis (covering the outcome space,
i.e.~null and alternative hypotheses). - A data set that aligns with the
population targeted to generalize to. - A operationalized dependent and
independent variable(s) that are tied to the hypothesis.

\hypertarget{ida-identify}{%
\subsubsection{Identify}\label{ida-identify}}

\begin{itemize}
\tightlist
\item
  Identify the informational value of the dependent and independent
  variable(s).
\item
  Assess the distribution of the independent and dependent variables
  with the appropriate descriptive statistics and visualizations.
\item
  Choose an appropriate statistical test based on the informational
  value and distribution of the dependent and independent variables.
\item
  Apply transformations to the data as needed to meet the assumptions of
  the statistical tests.
\end{itemize}

\hypertarget{ida-interrogate}{%
\subsubsection{Interrogate}\label{ida-interrogate}}

\begin{itemize}
\tightlist
\item
  Apply the appropriate statistical test to the data:

  \begin{itemize}
  \tightlist
  \item
    Categorical dependent variable

    \begin{itemize}
    \tightlist
    \item
      Chi-square (dependent and one independent variable)
    \item
      Logistic regression (dependent and one or more independent
      variables)
    \end{itemize}
  \item
    Continuous dependent variable

    \begin{itemize}
    \tightlist
    \item
      Correlation (dependent and one independent variable)
    \item
      Linear regression (dependent and one or more independent
      variables)
    \end{itemize}
  \end{itemize}
\item
  Assess the results of the statistical test (p-value, confidence
  intervals, effect size)
\end{itemize}

\hypertarget{ida-interpret}{%
\subsubsection{Interpret}\label{ida-interpret}}

Review the results of the statistical test and interpret the results in
the context of the hypothesis.

\hypertarget{ida-analysis}{%
\section{Analysis}\label{ida-analysis}}

Recap and introduction to the structure of the analysis subsection.

\begin{itemize}
\tightlist
\item
  Categorical dependent variable

  \begin{itemize}
  \tightlist
  \item
    Categorical/ continuous independent variable(s)

    \begin{itemize}
    \tightlist
    \item
      Descriptive assessment

      \begin{itemize}
      \tightlist
      \item
        0-1 categorical independent variable: Tables summary statistics
        (contingency table)
      \item
        Continuous independent variable(s): Plots and summary statistics
        (boxplots, histograms, etc.)
      \end{itemize}
    \item
      Statistical interrogation

      \begin{itemize}
      \tightlist
      \item
        Chi-square (dependent and one independent variable)
      \item
        Logistic regression (dependent and one or more independent
        variables)
      \end{itemize}
    \item
      Evaluation of results

      \begin{itemize}
      \tightlist
      \item
        p-value
      \item
        Confidence intervals
      \item
        Effect size
      \end{itemize}
    \end{itemize}
  \end{itemize}
\item
  Continuous dependent variable

  \begin{itemize}
  \tightlist
  \item
    Continuous/ categorical independent variable(s)

    \begin{itemize}
    \tightlist
    \item
      Descriptive assessment

      \begin{itemize}
      \tightlist
      \item
        Tables, plots, and summary statistics
      \end{itemize}
    \item
      Statistical interrogation

      \begin{itemize}
      \tightlist
      \item
        Correlation
      \item
        Linear regression
      \end{itemize}
    \item
      Evaluation of results

      \begin{itemize}
      \tightlist
      \item
        p-value
      \item
        Confidence intervals
      \item
        Effect size
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{ida-categorical-dependent-variable}{%
\subsection{Categorical dependent
variable}\label{ida-categorical-dependent-variable}}

\hypertarget{ida-chi-square}{%
\subsubsection{Chi-square}\label{ida-chi-square}}

Chi-square tests can be used for frequencies of categorical variables.
The chi-square goodness of fit test is used to test whether the observed
frequencies of a categorical variable match the expected frequencies of
a single variable. The chi-square test of independence is used to test
whether the observed frequencies of two categorical variables are
independent of each other. For hypothesis tests that include more than
two variables, the chi-square test is not appropriate. Instead, logistic
regression is used.

\hypertarget{ida-logistic-regression}{%
\subsubsection{Logistic regression}\label{ida-logistic-regression}}

Logistic regression can handle more than one independent variable, and
these variables need not be categorical. The dependent variable is a
binary variable, and the independent variables can be continuous or
categorical. The logistic regression model is used to predict the
probability of the dependent variable being 1. The logistic regression
model is used to test whether the independent variables are associated
with the dependent variable.

Using the \texttt{infer} package, a logistic regression can be performed
using the \texttt{specify()} and \texttt{generate()} functions. The
\texttt{specify()} function is used to specify the dependent and
independent variables. The \texttt{generate()} function is used to
generate the model. The \texttt{calculate()} function is used to
calculate the p-value and confidence intervals.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(infer) }\CommentTok{\# for inferential statistics}

\CommentTok{\# load the \textasciigrave{}dative\textasciigrave{} data from the \textasciigrave{}languageR\textasciigrave{} package}
\FunctionTok{data}\NormalTok{(}\AttributeTok{package =} \StringTok{"languageR"}\NormalTok{, }\AttributeTok{data =} \StringTok{"dative"}\NormalTok{)}

\CommentTok{\# specify the dependent \textasciigrave{}RealizationOfRecipient\textasciigrave{} and independent variables \textasciigrave{}LengthOfTheme\textasciigrave{} and \textasciigrave{}AnimacyOfTheme\textasciigrave{} for a logistic regression model}
\NormalTok{log\_reg }\OtherTok{\textless{}{-}} 
\NormalTok{  dative }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{specify}\NormalTok{(RealizationOfRecipient }\SpecialCharTok{\textasciitilde{}}\NormalTok{ LengthOfTheme }\SpecialCharTok{+}\NormalTok{ AnimacyOfTheme) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{response =} \StringTok{"logistic"}\NormalTok{, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"p.value"}\NormalTok{, }\AttributeTok{order =} \StringTok{"descending"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{ida-continuous-dependent-variable}{%
\subsection{Continuous dependent
variable}\label{ida-continuous-dependent-variable}}

\hypertarget{ida-correlation}{%
\subsubsection{Correlation}\label{ida-correlation}}

\hypertarget{ida-linear-regression}{%
\subsubsection{Linear regression}\label{ida-linear-regression}}

\hypertarget{ida-reporting}{%
\section{Reporting}\label{ida-reporting}}

\hypertarget{summary-12}{%
\section{Summary}\label{summary-12}}

\hypertarget{activities-9}{%
\section*{Activities}\label{activities-9}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_9.html}{Statistical
inference: prep, assess, interrogate, evaluate, and report}\\
\textbf{How}: Read Recipe 9 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To illustrate some common coding strategies for preparing
datasets for inferential data analysis, as well as the steps conduct
descriptive assessment, statistical interrogation, and evaluation and
reporting of results.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/lab_9}{Statistical
inference: prep, assess, interrogate, evaluate, and report}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 9.\\
\textbf{Why}: To gain experience working with coding strategies to
prepare, assess, interrogate, evaluate, and report results from an
inferential data analysis, practice transforming datasets and
visualizing relationships, and implement organizational strategies for
organizing and reporting results in a reproducible fashion.

\end{tcolorbox}

\hypertarget{questions-10}{%
\section*{Questions}\label{questions-10}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Conceptual questions}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the difference between a descriptive and inferential analysis?
\item
  What are the steps involved in conducting a descriptive analysis?
\item
  What are the steps involved in conducting an inferential analysis?
\item
  What are the steps involved in preparing data for inferential
  analysis?
\item
  What are the steps involved in conducting a statistical interrogation?
\item
  What are the steps involved in evaluating the results of an
  inferential analysis?
\item
  What are the steps involved in reporting the results of an inferential
  analysis?
\item
  Would word freqency differences between two groups of words be better
  assessed using a t-test or a chi-squared distribution?
\item
  Would word lengths between two groups of words be better assessed
  using a t-test or a chi-squared distribution?
\item
  What type of visualization would be best for exploring the
  relationship between two categorical variables?
\item
  What type of visualization would be best for exploring the
  relationship between two non-categorical variables?
\item
  What type of visualization would be best for exploring the
  relationship between a categorical and a non-categorical variable?
\item
  What is the role of confidence intervals in inferential data analysis?
  How is this similar or differnt to the role of p-values?
\item
  What is the role of effect size in inferential data analysis? How is
  this similar or differnt to the role of p-values?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Technical exercises}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use the lm() function to create a linear model, assess the summary
  statistics, and evaluate the results.
\item
  Use the glm() function to assess the relationship between two
  categorical variables and evaluate the results.
\item
  Apply a chi-squared distribution to explore categorical dependent
  variables and evaluate the results.
\item
  Calculate correlation coefficients between two non-categorical
  variables and evaluate the results.
\item
  Read in a dataset and transform it to prepare it for inferential
  analysis.
\item
  Decide which type of visualization is most appropriate for the dataset
  and then implement it using ggplot2.
\item
  Use the effectsize() function to calculate effect size and confidence
  intervals.
\end{enumerate}

\end{tcolorbox}

\hypertarget{preparation}{%
\section{Preparation}\label{preparation}}

At this point let's now get familiar with the datasets and prepare them
for analysis. The first dataset to consider is the \texttt{dative}
dataset. This dataset can be loaded from the languageR package (R. H.
Baayen and Shafaei-Bajestan 2019).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dative }\OtherTok{\textless{}{-}} 
\NormalTok{  languageR}\SpecialCharTok{::}\NormalTok{dative }\SpecialCharTok{|\textgreater{}} \CommentTok{\# load the \textasciigrave{}dative\textasciigrave{} dataset  }
  \FunctionTok{as\_tibble}\NormalTok{() }\CommentTok{\# convert the data frame to a tibble object}
  
\FunctionTok{glimpse}\NormalTok{(dative) }\CommentTok{\# preview structure }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 3,263
#> Columns: 15
#> $ Speaker                <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~
#> $ Modality               <fct> written, written, written, written, written, wr~
#> $ Verb                   <fct> feed, give, give, give, offer, give, pay, bring~
#> $ SemanticClass          <fct> t, a, a, a, c, a, t, a, a, a, a, a, t, a, c, a,~
#> $ LengthOfRecipient      <int> 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1,~
#> $ AnimacyOfRec           <fct> animate, animate, animate, animate, animate, an~
#> $ DefinOfRec             <fct> definite, definite, definite, definite, definit~
#> $ PronomOfRec            <fct> pronominal, nonpronominal, nonpronominal, prono~
#> $ LengthOfTheme          <int> 14, 3, 13, 5, 3, 4, 4, 1, 11, 2, 3, 3, 5, 2, 8,~
#> $ AnimacyOfTheme         <fct> inanimate, inanimate, inanimate, inanimate, ina~
#> $ DefinOfTheme           <fct> indefinite, indefinite, definite, indefinite, d~
#> $ PronomOfTheme          <fct> nonpronominal, nonpronominal, nonpronominal, no~
#> $ RealizationOfRecipient <fct> NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP,~
#> $ AccessOfRec            <fct> given, given, given, given, given, given, given~
#> $ AccessOfTheme          <fct> new, new, new, new, new, new, new, new, accessi~
\end{verbatim}

From \texttt{glimpse()} we can see that this dataset contains 3,263
observations and 15 columns.

The R Documentation can be consulted using \texttt{?dative} in the R
Console. The description states:

\begin{quote}
Data describing the realization of the dative as NP or PP in the
Switchboard corpus and the Treebank Wall Street Journal collection.
\end{quote}

For a bit more context, a dative is the phrase which reflects the entity
that takes the recipient role in a ditransitive clause. In English, the
recipient (dative) can be realized as either a noun phrase (NP) as seen
in (1) or as a prepositional phrase (PP) as seen in (2) below.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  They give {[}you \textsubscript{NP}{]} a drug test.
\item
  They give a drug test {[}to you \textsubscript{PP}{]}.
\end{enumerate}

Together these two syntactic options are known as the Dative
Alternation.

The observational unit for this dataset is
\texttt{RealizationOfRecipient} variable which is either `NP' or `PP'.
For the purposes of this chapter I will select a subset of the key
variables we will use in the upcoming analyses and drop the others.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dative }\OtherTok{\textless{}{-}} 
\NormalTok{  dative }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(RealizationOfRecipient, Modality, LengthOfRecipient, LengthOfTheme) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# select key variables}
\NormalTok{  janitor}\SpecialCharTok{::}\FunctionTok{clean\_names}\NormalTok{() }\CommentTok{\# normalize variable names}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\textbf{?(caption)}}

\end{table}

\begin{tabular}{llrr}
\toprule
realization\_of\_recipient & modality & length\_of\_recipient & length\_of\_theme\\
\midrule
NP & written & 1 & 14\\
NP & written & 2 & 3\\
NP & written & 1 & 13\\
NP & written & 1 & 5\\
NP & written & 2 & 3\\
\addlinespace
NP & written & 2 & 4\\
NP & written & 2 & 4\\
NP & written & 1 & 1\\
NP & written & 1 & 11\\
NP & written & 1 & 2\\
\bottomrule
\end{tabular}

In Table~\ref{tbl-i-dative-dictionary} I've created a data dictionary
describing the variables in our new \texttt{dative} dataset based on the
variable descriptions in the \texttt{languageR::dative} documentation.

\hypertarget{tbl-i-dative-dictionary}{}
\begin{table}
\caption{\label{tbl-i-dative-dictionary}Data dictionary for the \texttt{dative} dataset. }\tabularnewline

\centering
\begin{tabular}{lll}
\toprule
variable\_name & name & description\\
\midrule
realization\_of\_recipient & Realization of Recipient & A factor with levels NP and PP coding the realization of the dative.\\
modality & Language Modality & A factor with levels *spoken*, *written*.\\
length\_of\_recipient & Length of Recipient & A numeric vector coding the number of words comprising the recipient.\\
length\_of\_theme & Length of Theme & A numeric vector coding the number of words comprising the theme.\\
\bottomrule
\end{tabular}
\end{table}

The second dataset that we will use in this chapter is the
\texttt{sdac\_disfluencies} dataset that we worked to derived in the
previous chapter. Let's read in the dataset and preview the structure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/derived/sdac/sdac\_disfluencies.csv"}\NormalTok{) }\CommentTok{\# read transformed dataset}

\FunctionTok{glimpse}\NormalTok{(sdac\_disfluencies) }\CommentTok{\# preview structure}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 447,212
#> Columns: 9
#> $ doc_id         <dbl> 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4~
#> $ speaker_id     <dbl> 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519, 1~
#> $ utterance_text <chr> "Okay.  /", "Okay.  /", "{D So, }", "{D So, }", "[ [ I ~
#> $ filler         <chr> "uh", "um", "uh", "um", "uh", "um", "uh", "um", "uh", "~
#> $ count          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0~
#> $ sex            <chr> "FEMALE", "FEMALE", "FEMALE", "FEMALE", "FEMALE", "FEMA~
#> $ birth_year     <dbl> 1962, 1962, 1962, 1962, 1971, 1971, 1962, 1962, 1971, 1~
#> $ dialect_area   <chr> "WESTERN", "WESTERN", "WESTERN", "WESTERN", "SOUTH MIDL~
#> $ education      <dbl> 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1~
\end{verbatim}

We prepared a data dictionary that reflects this transformed dataset.
Let's read that file and then view it in
Table~\ref{tbl-i-sdac-disfluencies-dictionary}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies\_dictionary }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/derived/sdac/sdac\_disfluencies\_data\_dictionary.csv"}\NormalTok{) }\CommentTok{\# read data dictionary}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-i-sdac-disfluencies-dictionary}{}
\begin{table}
\caption{\label{tbl-i-sdac-disfluencies-dictionary}Data dictionary for the \texttt{sdac\_disfluencies} dataset. }\tabularnewline

\centering
\begin{tabular}{lll}
\toprule
variable\_name & name & description\\
\midrule
doc\_id & Document ID & Unique identifier for each conversation file.\\
speaker\_id & Speaker ID & Unique identifier for each speaker in the corpus.\\
utterance\_text & Utterance Text & Transcribed utterances for each conversation. Includes disfluency annotation tags.\\
filler & Filler & Filler type either uh or um.\\
count & Count & Number of fillers for each utterance.\\
\addlinespace
sex & Sex & Sex for each speaker either male or female.\\
birth\_year & Birth Year & The year each speaker was born.\\
dialect\_area & Dialect Area & Region from the US where the speaker spent first 10 years.\\
education & Education & Highest educational level attained: values 0, 1, 2, 3, and 9.\\
\bottomrule
\end{tabular}
\end{table}

For our analysis purposes we will reduce this dataset, as we did for the
\texttt{dative} dataset, retaining only the variables of interest for
the upcoming analyses.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(speaker\_id, filler, count, sex, birth\_year, education) }\CommentTok{\# select key variables}
\end{Highlighting}
\end{Shaded}

Let's preview this simplified \texttt{sdac\_disfluencies} dataset.

\hypertarget{tbl-i-sdac-disfluencies-preview}{}
\begin{table}
\caption{\label{tbl-i-sdac-disfluencies-preview}First 10 observations of simplified \texttt{sdac\_disfluencies} dataset. }\tabularnewline

\centering
\begin{tabular}{rlrlrr}
\toprule
speaker\_id & filler & count & sex & birth\_year & education\\
\midrule
1632 & uh & 0 & FEMALE & 1962 & 2\\
1632 & um & 0 & FEMALE & 1962 & 2\\
1632 & uh & 0 & FEMALE & 1962 & 2\\
1632 & um & 0 & FEMALE & 1962 & 2\\
1519 & uh & 0 & FEMALE & 1971 & 1\\
\addlinespace
1519 & um & 0 & FEMALE & 1971 & 1\\
1632 & uh & 0 & FEMALE & 1962 & 2\\
1632 & um & 0 & FEMALE & 1962 & 2\\
1519 & uh & 1 & FEMALE & 1971 & 1\\
1519 & um & 0 & FEMALE & 1971 & 1\\
\bottomrule
\end{tabular}
\end{table}

Now the \texttt{sdac\_disfluencies} dataset needs some extra
transformation to better prepare it for statistical interrogation. On
the one hand the variables \texttt{birth\_year} and \texttt{education}
are not maximally informative. First it would be more ideal if
\texttt{birth\_year} would reflect the age of the speaker at the time of
the conversation(s) and furthermore the coded values of
\texttt{education} are not explicit as far what the numeric values refer
to.

The second issue has to do with preparing the
\texttt{sdac\_disfluencies} dataset for statistical analysis. This
involves converting our column types to the correct vector types for
statistical methods. Specifically we need to convert our categorical
variables to the R type `factor' (fct). This includes of our current
variables which are character vectors, but also the \texttt{speaker\_id}
and \texttt{education} which appear as numeric but do not reflect a
continuous variables; one is merely a code which uniquely labels each
speaker and the other is an ordinal list of educational levels.

This will be a three step process, first we will normalize the
\texttt{birth\_year} to reflect the age of the speaker, second we will
convert all the relevant categorical variables to factors, and third we
will convert the \texttt{education} variable to a factor adding
meaningful labels for the levels of this factor.

Consulting the
\href{https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1_manual.txt}{online
manual for this corpus}, we see that the recording date for these
conversations took place in 1992, so we can simply subtract the
\texttt{birth\_year} from 1992 to get each participant's age. We'll
rename this new column \texttt{age} and drop the \texttt{birth\_year}
column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age =}\NormalTok{ (}\DecValTok{1992} \SpecialCharTok{{-}}\NormalTok{ birth\_year)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# calculate age}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{birth\_year) }\CommentTok{\# drop \textasciigrave{}birth\_year\textasciigrave{} column}
\end{Highlighting}
\end{Shaded}

Now let's convert all the variables which are character vectors. We can
do this using the the \texttt{factor()} function; first on
\texttt{speaker\_id} and then, with the help of \texttt{mutate\_if()},
to all the other variables which are character vectors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{speaker\_id =} \FunctionTok{factor}\NormalTok{(speaker\_id)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# convert numeric to factor}
  \FunctionTok{mutate\_if}\NormalTok{(is.character, factor) }\CommentTok{\# convert all character to factor}
\end{Highlighting}
\end{Shaded}

We know from the data dictionary that the \texttt{education} column
contains four values (0, 1, 2, 3, and 9). Again, consulting the corpus
manual we can see what these values mean.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{EDUCATION    COUNT}
\NormalTok{{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\NormalTok{0            14      less than high school}
\NormalTok{1            39      less than college}
\NormalTok{2            309     college}
\NormalTok{3            176     more than college}
\NormalTok{9            4       unknown}
\end{Highlighting}
\end{Shaded}

So let's convert \texttt{education} to a factor adding these
descriptions as factor level labels. The function \texttt{factor()} can
take an argument \texttt{labels\ =} which we can manually assign the
label names for the factor levels in the order of the factor levels.
Since the original values were numeric, the factor level ordering
defaults to ascending order.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_disfluencies }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{education =} \FunctionTok{factor}\NormalTok{(education, }
                            \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"less than high school"}\NormalTok{, }\CommentTok{\# value 0}
                                       \StringTok{"less than college"}\NormalTok{, }\CommentTok{\# value 1}
                                       \StringTok{"college"}\NormalTok{, }\CommentTok{\# value 2}
                                       \StringTok{"more than college"}\NormalTok{, }\CommentTok{\# value 3 }
                                       \StringTok{"unknown"}\NormalTok{))) }\CommentTok{\# value 9}
\end{Highlighting}
\end{Shaded}

So let's take a look at the \texttt{sdac\_disfluencies} dataset we've
prepared for analysis.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(sdac\_disfluencies)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 447,212
#> Columns: 6
#> $ speaker_id <fct> 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519, 1519,~
#> $ filler     <fct> uh, um, uh, um, uh, um, uh, um, uh, um, uh, um, uh, um, uh,~
#> $ count      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,~
#> $ sex        <fct> FEMALE, FEMALE, FEMALE, FEMALE, FEMALE, FEMALE, FEMALE, FEM~
#> $ education  <fct> college, college, college, college, less than college, less~
#> $ age        <dbl> 30, 30, 30, 30, 21, 21, 30, 30, 21, 21, 30, 30, 21, 21, 21,~
\end{verbatim}

Now the datasets \texttt{dative} and \texttt{sdac\_disfluencies} are
ready to be statistically interrogated.

\hypertarget{univariate-analysis}{%
\section{Univariate analysis}\label{univariate-analysis}}

In what follows I will provide a description of inferential data
analysis when only one variable is to be interrogated. This is known as
a univariate analysis, or one-variable analysis. We will consider a case
when the variable is categorical and the other continuous.

\hypertarget{categorical}{%
\subsection{Categorical}\label{categorical}}

As an example of a univariate analysis where the variable used in the
analysis is categorical we will look at the \texttt{dative} dataset. In
this analysis we may be interested in knowing whether the recipient role
in a ditransitive construction is realized more as an `NP' or `PP'.

\textbf{Descriptive assessment}

The \texttt{realization\_of\_recipient} variable contains the relevant
information. Let's take a first look using the skimr package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dative }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(realization\_of\_recipient) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# select the variable}
  \FunctionTok{skim}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# get data summary}
  \FunctionTok{yank}\NormalTok{(}\StringTok{"factor"}\NormalTok{) }\CommentTok{\# only show factor{-}oriented information}
\end{Highlighting}
\end{Shaded}

\textbf{Variable type: factor}

\begin{tabular}{l|r|r|l|r|l}
\hline
skim\_variable & n\_missing & complete\_rate & ordered & n\_unique & top\_counts\\
\hline
realization\_of\_recipient & 0 & 1 & FALSE & 2 & NP: 2414, PP: 849\\
\hline
\end{tabular}

The output from \texttt{skim()} produces various pieces of information
that can be helpful. On the one hand we get diagnostics that tell us if
there are missing cases (\texttt{NA} values), what the proportion of
complete cases is, if the the factor is ordered, how many distinct
levels the factor has, as well as the level counts.

Looking at the \texttt{top\_counts} we can see that of the 3,263
observations, in 2,414 the dative is expressed as an `NP' and 849 as
`PP'. Numerically we can see that there is a difference between the use
of the alternation types. A visualization is often helpful for
descriptive purposes in statistical analysis. In this particular case,
however, we are considering a single categorical variable with only two
levels (values) so a visualization is not likely to be more informative
than the numeric values we have already obtained. But for demonstration
purposes and to get more familiar with building plots, let's create a
visualization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dative }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ realization\_of\_recipient)) }\SpecialCharTok{+} \CommentTok{\# mapping}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Realization of recipient"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{) }\CommentTok{\# labels}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h]

{\centering \includegraphics{inference_files/figure-pdf/figi-uni-cat-visual-dative-1.pdf}

}

\caption{Barplot visualizing the realization of recipient}

\end{figure}

The question we want to address, however, is whether this numerical
difference is in fact a statistical difference.

\textbf{Statistical interrogation}

To statistical assess the distribution for a categorical variable, we
will turn to the Chi-squared test. This test aims to gauge whether the
numerical differences between `NP' and `PP' counts observed in the data
is greater than what would be expected by chance. Chance in the case
where there are only two possible outcome levels is 50/50. For our
particular data where there are 3,263 observations half would be `NP'
and the other half `PP' --specifically 1631.5 for each.

To run this test we first will need to create a cross-tabulation of the
variable. We will use the \texttt{xtabs()} function to create the table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ror\_table }\OtherTok{\textless{}{-}} 
  \FunctionTok{xtabs}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ realization\_of\_recipient, }\CommentTok{\# formula selecting the variable}
        \AttributeTok{data =}\NormalTok{ dative) }\CommentTok{\# dataset}

\NormalTok{ror\_table }\CommentTok{\# preview}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> realization_of_recipient
#>   NP   PP 
#> 2414  849
\end{verbatim}

No new information here, but the format (i.e.~an object of class
`table') is what is important for the input argument for the
\texttt{chisq.test()} function we will use to run the test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c1 }\OtherTok{\textless{}{-}} \FunctionTok{chisq.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ ror\_table) }\CommentTok{\# apply the chi{-}squared test to \textasciigrave{}ror\_table\textasciigrave{}}

\NormalTok{c1 }\CommentTok{\# preview the test results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#>  Chi-squared test for given probabilities
#> 
#> data:  ror_table
#> X-squared = 751, df = 1, p-value <2e-16
\end{verbatim}

The preview of the \texttt{c1} object reveals the main information of
interest including the Chi-squared statistic, the degrees of freedom,
and the \(p\)-value (albeit in scientific notation). However, the
\texttt{c1} is an `htest' object an includes a number of other pieces
information about the test.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(c1) }\CommentTok{\# preview column names}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "statistic" "parameter" "p.value"   "method"    "data.name" "observed" 
#> [7] "expected"  "residuals" "stdres"
\end{verbatim}

For our purposes let's simply confirm that the \(p\)-value is lower than
the standard .05 threshold for statistical significance.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c1}\SpecialCharTok{$}\NormalTok{p.value }\SpecialCharTok{\textless{}}\NormalTok{ .}\DecValTok{05} \CommentTok{\# confirm p{-}value below .05}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] TRUE
\end{verbatim}

Other information can be organized in a more readable format using the
broom package's \texttt{augment()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c1 }\SpecialCharTok{|\textgreater{}} \CommentTok{\# statistical result}
  \FunctionTok{augment}\NormalTok{() }\CommentTok{\# view detailed statistical test information}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 2 x 6
#>   realization_of_recipient .observed .prop .expected .resid .std.resid
#>   <fct>                        <int> <dbl>     <dbl>  <dbl>      <dbl>
#> 1 NP                            2414 0.740     1632.   19.4       27.4
#> 2 PP                             849 0.260     1632.  -19.4      -27.4
\end{verbatim}

Here we can see the observed and expected counts and the proportions for
each level of \texttt{realization\_of\_recipient}. We also get
additional information concerning residuals, but we will leave these
aside.

\textbf{Evaluation}

At this point we may think we are done. We have statistically
interrogated the \texttt{realization\_of\_recipient} variable and found
that the difference between `NP' and `PP' realization in the datives in
this dataset is statistically significant. However, we need to evaluate
the size (`effect size') and the reliability of the effect (`confidence
interval'). The effectsize package provides a function
\texttt{effectsize()} that can provide us both the effect size and
confidence interval.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effects }\OtherTok{\textless{}{-}} 
  \FunctionTok{effectsize}\NormalTok{(c1) }\CommentTok{\# evaluate effect size and generate a confidence interval (fei type given 2x1 contingency table)}

\NormalTok{effects }\CommentTok{\# preview effect size and confidence interval}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Fei  |       95% CI
#> -------------------
#> 0.48 | [0.45, 1.00]
#> 
#> - Adjusted for uniform expected probabilities.
#> - One-sided CIs: upper bound fixed at [1.00].
\end{verbatim}

\texttt{effectsize()} recognizes the type of test results in \texttt{c1}
and calculates the appropriate effect size measure and generates a
confidence interval. Since the effect statistic (``Fei'') falls between
the 95\% confidence interval this suggests the results are reliably
interpreted (chances of Type I (false positive) or Type II (false
negative) are low).

Now, the remaining question is to evaluate whether the significant
result here is a strong effect or not. To do this we can pass the effect
size measure to the \texttt{interpret\_r()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{interpret\_r}\NormalTok{(effects}\SpecialCharTok{$}\NormalTok{Fei) }\CommentTok{\# interpret the effect size }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "very large"
#> (Rules: funder2019)
\end{verbatim}

Turns out we have a strong effect; the realization of dative alternation
heavily favors the `NP' form in our data. The potential reasons why are
not considered in this univariate analysis, but we will return to this
question later as we add independent variables to the statistical
analysis.

\hypertarget{continuous}{%
\subsection{Continuous}\label{continuous}}

Now let's turn to a case when the variable we aim to interrogate is
non-categorical. For this case we will turn to the
\texttt{sdac\_disfluencies} dataset. Specifically we will aim to test
whether the use of fillers is normally distributed across speakers.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

This is an important step when working with numeric dependent variables
as the type of distribution will dictate decisions about whether we will
use parametric or non-parametric tests if we consider the extent to
which an independent variable (or variables) can explain the variation
of the dependent variable.

\end{tcolorbox}

Since the dataset is currently organized around fillers as the
observational unit, I will first transform this dataset to sum the use
of fillers for each speaker in the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_speaker\_fillers }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{group\_by}\NormalTok{(speaker\_id) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# group by each speaker}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{sum =} \FunctionTok{sum}\NormalTok{(count)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# add up all fillers used}
  \FunctionTok{ungroup}\NormalTok{() }\CommentTok{\# remove grouping parameter}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-i-uni-cont-sdac-transform-preview}{}
\begin{table}
\caption{\label{tbl-i-uni-cont-sdac-transform-preview}First 10 observations of \texttt{sdac\_speaker\_fillers} dataset. }\tabularnewline

\centering
\begin{tabular}{lr}
\toprule
speaker\_id & sum\\
\midrule
155 & 28\\
1000 & 45\\
1001 & 264\\
1002 & 54\\
1004 & 45\\
\addlinespace
1005 & 129\\
1007 & 0\\
1008 & 27\\
1010 & 2\\
1011 & 54\\
\bottomrule
\end{tabular}
\end{table}

\textbf{Descriptive assessment}

Let's perform some descriptive assessement of the variable of interest
\texttt{sum}. First let's apply the \texttt{skim()} function and
retrieve just the relevant numeric descriptors with \texttt{yank()}. One
twist here, however, is that I've customized the \texttt{skim()}
function using the \texttt{skim\_with()} to remove the default histogram
and add the Interquartile Range (IQR) to the output. This new skim
function \texttt{num\_skim()} will take the place of \texttt{skim()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num\_skim }\OtherTok{\textless{}{-}} 
  \FunctionTok{skim\_with}\NormalTok{(}\AttributeTok{numeric =} \FunctionTok{sfl}\NormalTok{(}\AttributeTok{hist =} \ConstantTok{NULL}\NormalTok{, }\CommentTok{\# remove hist skim}
                                   \AttributeTok{iqr =}\NormalTok{ IQR)) }\CommentTok{\# add IQR to skim}

\NormalTok{sdac\_speaker\_fillers }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(sum) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# variable of interest}
  \FunctionTok{num\_skim}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# get custom data summary}
  \FunctionTok{yank}\NormalTok{(}\StringTok{"numeric"}\NormalTok{) }\CommentTok{\# only show numeric{-}oriented information}
\end{Highlighting}
\end{Shaded}

\textbf{Variable type: numeric}

\begin{tabular}{l|r|r|r|r|r|r|r|r|r|r}
\hline
skim\_variable & n\_missing & complete\_rate & mean & sd & p0 & p25 & p50 & p75 & p100 & iqr\\
\hline
sum & 0 & 1 & 87.1 & 108 & 0 & 16 & 45 & 114 & 668 & 98\\
\hline
\end{tabular}

We see here that the mean use of fillers is 87.1 across speakers.
However, the standard deviation and IQR are large relative to this mean
which indicates that the dispersion is quite large, in other words this
suggests that there are large differences between speakers. Furthermore,
since the median (p50) is smaller than the mean, the distribution is
right skewed.

Let's look a couple visualizations of this distribution to appreciate
these descriptives. A histogram will provide us a view of the
distribution using the counts of the values of \texttt{sum} and a
density plot will provide a smooth curve which represents the scaled
distribution of the observed data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_speaker\_fillers }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(sum)) }\SpecialCharTok{+} \CommentTok{\# mapping}
  \FunctionTok{geom\_histogram}\NormalTok{() }\SpecialCharTok{+}  \CommentTok{\# geometry}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Fillers"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}

\NormalTok{p2 }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_speaker\_fillers }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(sum)) }\SpecialCharTok{+} \CommentTok{\# mapping}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{geom\_rug}\NormalTok{() }\SpecialCharTok{+}  \CommentTok{\# visualize individual observations}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Fillers"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Density"}\NormalTok{)}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2 }\SpecialCharTok{+} \FunctionTok{plot\_annotation}\NormalTok{(}\StringTok{"Filler count distributions."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h]

{\centering \includegraphics{inference_files/figure-pdf/fig-i-uni-cont-sdac-visual-1.pdf}

}

\caption{\label{fig-i-uni-cont-sdac-visual}\textbf{?(caption)}}

\end{figure}

From the plots in Figure~\ref{fig-i-uni-cont-sdac-visual} we see that
our initial intuitions about the distribution of \texttt{sum} are
correct. There is large dispersion between speakers and the data
distribution is right skewed.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white]

Note that I've used the patchwork package for organizing the display of
plots and including a plot annotation label.

\end{tcolorbox}

Since our aim is to test for normality, we can generate a
Quantile-Quantile plots (QQ Plot).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_speaker\_fillers }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{sample =}\NormalTok{ sum)) }\SpecialCharTok{+} \CommentTok{\# mapping}
  \FunctionTok{stat\_qq}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# calculate expected quantile{-}quantile distribution}
  \FunctionTok{stat\_qq\_line}\NormalTok{() }\CommentTok{\# plot the qq{-}line}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h]

{\centering \includegraphics{inference_files/figure-pdf/i-uni-cont-sdac-qq-plot-1.pdf}

}

\end{figure}

Since many points do not fall on the expected normal distribution line
we have even more evidence to support the notion that the distribution
of \texttt{sum} is non-normal.

\textbf{Statistical interrogation}

Although the descriptives and visualizations strongly suggest that we do
not have normally distributed data let's run a normality test. For this
we turn to the \texttt{shapiro.test()} function which performs the
Shapiro-Wilk test of normality. We pass the \texttt{sum} variable to
this function to run the test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s1 }\OtherTok{\textless{}{-}} \FunctionTok{shapiro.test}\NormalTok{(sdac\_speaker\_fillers}\SpecialCharTok{$}\NormalTok{sum) }\CommentTok{\# apply the normality test to \textasciigrave{}sum\textasciigrave{}}

\NormalTok{s1 }\CommentTok{\# preview the test results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#>  Shapiro-Wilk normality test
#> 
#> data:  sdac_speaker_fillers$sum
#> W = 0.8, p-value <2e-16
\end{verbatim}

As we saw with the results from the \texttt{chisq.test()} function, the
\texttt{shapiro.test()} function produces an object with information
about the test including the \(p\)-value. Let's run our logical test to
see if the test is statistically significant.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s1}\SpecialCharTok{$}\NormalTok{p.value }\SpecialCharTok{\textless{}}\NormalTok{ .}\DecValTok{05} \CommentTok{\# }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] TRUE
\end{verbatim}

\textbf{Evaluation}

The results from the Shapiro-Wilk Normality Test tell us that the
distribution of \texttt{sum} is statistically found to differ from the
normal distribution. So in this case, statistical significance suggests
that \texttt{sum} cannot be used as a parametric dependent variable. For
our aims this is all the evaluation required. Effect size and confidence
intervals are not applicable.

It is of note, however, that the expectation that the variable
\texttt{sum} would conform to the normal distribution was low from the
outset as we are working with count data. Count data, or frequencies,
are in a strict sense not continuous, but rather discrete --meaning that
they are real numbers (whole numbers which are always positive). This is
a common informational type to encounter in text analysis.

\hypertarget{bivariate-analysis}{%
\section{Bivariate analysis}\label{bivariate-analysis}}

A more common scenario in statistical analysis is the consideration of
the relationship between two-variables, known as bivariate analysis.

\hypertarget{categorical-1}{%
\subsection{Categorical}\label{categorical-1}}

Let's build on our univariate analysis of
\texttt{realization\_of\_recipient} and include an explanatory, or
independent variable which we will explore to test whether it can
explain our earlier finding that `NP' datives are more common that `PP'
datives. The question to test, then, is whether modality explains the
distribution of the \texttt{realization\_of\_recipient}.

\textbf{Descriptive assessment}

Both the \texttt{realization\_of\_recipient} and \texttt{modality}
variables are categorical, specifically nominal as we can see by using
\texttt{skim()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dative }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(realization\_of\_recipient, modality) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# select key variables}
  \FunctionTok{skim}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# get custom data summary}
  \FunctionTok{yank}\NormalTok{(}\StringTok{"factor"}\NormalTok{) }\CommentTok{\# only show factor{-}oriented information}
\end{Highlighting}
\end{Shaded}

\textbf{Variable type: factor}

\begin{tabular}{l|r|r|l|r|l}
\hline
skim\_variable & n\_missing & complete\_rate & ordered & n\_unique & top\_counts\\
\hline
realization\_of\_recipient & 0 & 1 & FALSE & 2 & NP: 2414, PP: 849\\
\hline
modality & 0 & 1 & FALSE & 2 & spo: 2360, wri: 903\\
\hline
\end{tabular}

For this reason measures of central tendency are not applicable and we
will turn to a contingency table to summarize the relationship. The
janitor package has a set of functions, the primary function being
\texttt{tabyl()}. Other functions used here are to adorn the contingency
table with totals, percentages, and to format the output for
readability.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dative }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{tabyl}\NormalTok{(realization\_of\_recipient, modality) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# cross{-}tabulate}
  \FunctionTok{adorn\_totals}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"row"}\NormalTok{, }\StringTok{"col"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# provide row and column totals}
  \FunctionTok{adorn\_percentages}\NormalTok{(}\StringTok{"col"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# add percentages to the columns}
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{(}\AttributeTok{rounding =} \StringTok{"half up"}\NormalTok{, }\AttributeTok{digits =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# round the digits}
  \FunctionTok{adorn\_ns}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# add observation number}
  \FunctionTok{adorn\_title}\NormalTok{(}\StringTok{"combined"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# add a header title}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{) }\CommentTok{\# pretty table)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-i-bi-cat-contingency-table}{}
\begin{table}
\caption{\label{tbl-i-bi-cat-contingency-table}Contingency table for \texttt{realization\_of\_recipient} and
\texttt{modality}. }\tabularnewline

\centering
\begin{tabular}{llll}
\toprule
realization\_of\_recipient/modality & spoken & written & Total\\
\midrule
NP & 79\% (1859) & 61\% (555) & 74\% (2414)\\
PP & 21\%  (501) & 39\% (348) & 26\%  (849)\\
Total & 100\% (2360) & 100\% (903) & 100\% (3263)\\
\bottomrule
\end{tabular}
\end{table}

To gain a better appreciation for this relationship let's generate a
couple plots one which shows cross-tabulated counts and the other
calculated proportions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} 
\NormalTok{  dative }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ realization\_of\_recipient, }\AttributeTok{fill =}\NormalTok{ modality)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Count"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Realization of recipient"}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p2 }\OtherTok{\textless{}{-}} 
\NormalTok{  dative }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ realization\_of\_recipient, }\AttributeTok{fill =}\NormalTok{ modality)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# geometry, with fill for proportion plot}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"Proportion"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Realization of recipient"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"Modality"}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p1 }\OtherTok{\textless{}{-}}\NormalTok{ p1 }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\CommentTok{\# remove legend from left plot}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2 }\SpecialCharTok{+} \FunctionTok{plot\_annotation}\NormalTok{(}\StringTok{"Relationship between Realization of recipient and Modality."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h]

{\centering \includegraphics{inference_files/figure-pdf/i-bi-cat-visual-1.pdf}

}

\end{figure}

Looking at the count plot (in the left pane) we see that large
difference between the realization of the dative as an `NP' or `PP'
obscures to some degree our ability to see to what degree modality is
related to the realization of the dative. So, a proportion plot (in the
right pane) standardizes each level of
\texttt{realization\_of\_recipient} to provide a more comparable view.
From the proportion plot we see that there appears to be a trend towards
more use of `PP' than `NP' in the written modality.

\textbf{Statistical interrogation}

Although the proportion plot is visually helpful, we use the raw counts
to statistically analyze this relationship. Again, as we are working
with categorical variables, now for a dependent and independent
variable, we use the Chi-squared test. And as before we need to create
the cross-tabulation table to pass to the \texttt{chisq.test()} to
perform the test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ror\_mod\_table }\OtherTok{\textless{}{-}} 
  \FunctionTok{xtabs}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ realization\_of\_recipient }\SpecialCharTok{+}\NormalTok{ modality, }\CommentTok{\# formula }
        \AttributeTok{data =}\NormalTok{ dative) }\CommentTok{\# dataset}

\NormalTok{c2 }\OtherTok{\textless{}{-}} \FunctionTok{chisq.test}\NormalTok{(ror\_mod\_table) }\CommentTok{\# apply the chi{-}squared test to \textasciigrave{}ror\_mod\_table\textasciigrave{}}

\NormalTok{c2 }\CommentTok{\# \# preview the test results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#>  Pearson's Chi-squared test with Yates' continuity correction
#> 
#> data:  ror_mod_table
#> X-squared = 101, df = 1, p-value <2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c2}\SpecialCharTok{$}\NormalTok{p.value }\SpecialCharTok{\textless{}}\NormalTok{ .}\DecValTok{05} \CommentTok{\# confirm p{-}value below .05}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] TRUE
\end{verbatim}

We can preview the result and provide a confirmation of the \(p\)-value.
This evidence suggests that there is a difference between the
distribution of dative realization according to modality.

We can also see more details about the test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c2 }\SpecialCharTok{|\textgreater{}} \CommentTok{\# statistical result}
  \FunctionTok{augment}\NormalTok{() }\CommentTok{\# view detailed statistical test information}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # A tibble: 4 x 9
#>   realization_of_~1 modal~2 .obse~3 .prop .row.~4 .col.~5 .expe~6 .resid .std.~7
#>   <fct>             <fct>     <int> <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>
#> 1 NP                spoken     1859 0.570   0.770   0.788   1746.   2.71    10.1
#> 2 PP                spoken      501 0.154   0.590   0.212    614.  -4.56   -10.1
#> 3 NP                written     555 0.170   0.230   0.615    668.  -4.37   -10.1
#> 4 PP                written     348 0.107   0.410   0.385    235.   7.38    10.1
#> # ... with abbreviated variable names 1: realization_of_recipient, 2: modality,
#> #   3: .observed, 4: .row.prop, 5: .col.prop, 6: .expected, 7: .std.resid
\end{verbatim}

\textbf{Evaluation}

Now we want to calculate the effect size and the confidence interval to
provide measures of assurance that our finding is robust.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effects }\OtherTok{\textless{}{-}} \FunctionTok{effectsize}\NormalTok{(c2) }\CommentTok{\# evaluate effect size and generate a confidence interval}

\NormalTok{effects }\CommentTok{\# preview effect size and confidence interval}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Cramer's V (adj.) |       95% CI
#> --------------------------------
#> 0.18              | [0.15, 1.00]
#> 
#> - One-sided CIs: upper bound fixed at [1.00].
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{interpret\_r}\NormalTok{(effects}\SpecialCharTok{$}\NormalTok{Cramers\_v) }\CommentTok{\# interpret the effect size}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "small"
#> (Rules: funder2019)
\end{verbatim}

We get effect size and confidence interval information. Note that the
effect size, reflected by Cramer's V, for this relationship is weak.
This points out an important aspect to evaluation of statistical tests.
The fact that a test is significant does not mean that it is meaningful.
A small effect size suggests that we should be cautious about the extent
to which this significant finding is robust in the population from which
the data is sampled.

\hypertarget{continuous-1}{%
\subsection{Continuous}\label{continuous-1}}

For a bivariate analysis in which the dependent variable is not
categorical, we will turn to the \texttt{sdac\_disfluencies} dataset.
The question we will pose to test is whether the use of fillers is
related to the type of filler (`uh' or `um').

\textbf{Descriptive assessment}

The key variables to assess in this case are the variables
\texttt{count} and \texttt{filler}. But before we start to explore this
relationship we will need to transform the dataset such that each
speaker's use of the levels of \texttt{filler} is summed. We will use
\texttt{group\_by()} to group \texttt{speaker\_id} and \texttt{filler}
combinations and then use \texttt{summarize()} to then sum the counts
for each filler type for each speaker

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_fillers }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{group\_by}\NormalTok{(speaker\_id, filler) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# grouping parameters}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{sum =} \FunctionTok{sum}\NormalTok{(count)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# summed counts for each speaker{-}filler combination}
  \FunctionTok{ungroup}\NormalTok{() }\CommentTok{\# remove the grouping parameters}
\end{Highlighting}
\end{Shaded}

Let's preview this transformation.

\hypertarget{tbl-i-bi-cont-sdac-fillers-preview}{}
\begin{table}
\caption{\label{tbl-i-bi-cont-sdac-fillers-preview}First 10 observations from \texttt{sdac\_fillers} dataset. }\tabularnewline

\centering
\begin{tabular}{llr}
\toprule
speaker\_id & filler & sum\\
\midrule
155 & uh & 28\\
155 & um & 0\\
1000 & uh & 37\\
1000 & um & 8\\
1001 & uh & 262\\
\addlinespace
1001 & um & 2\\
1002 & uh & 34\\
1002 & um & 20\\
1004 & uh & 30\\
1004 & um & 15\\
\bottomrule
\end{tabular}
\end{table}

Let's take a look at them together by grouping the dataset by
\texttt{filler} and then using the custom skim function
\texttt{num\_skim()} for the numeric variable\texttt{count}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_fillers }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{group\_by}\NormalTok{(filler) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# grouping parameter}
  \FunctionTok{num\_skim}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# get custom data summary}
  \FunctionTok{yank}\NormalTok{(}\StringTok{"numeric"}\NormalTok{) }\CommentTok{\# only show numeric{-}oriented information}
\end{Highlighting}
\end{Shaded}

\textbf{Variable type: numeric}

\begin{tabular}{l|l|r|r|r|r|r|r|r|r|r|r}
\hline
skim\_variable & filler & n\_missing & complete\_rate & mean & sd & p0 & p25 & p50 & p75 & p100 & iqr\\
\hline
sum & uh & 0 & 1 & 71.4 & 91.5 & 0 & 14 & 39 & 91 & 661 & 77\\
\hline
sum & um & 0 & 1 & 15.7 & 31.0 & 0 & 0 & 4 & 16 & 265 & 16\\
\hline
\end{tabular}

We see here that the standard deviation and IQR for both `uh' and `um'
are relatively large for the respective means (71.4 and 15.7) suggesting
the distribution is quite dispersed. Let's take a look at a boxplot to
visualize the counts in \texttt{sum} for each level of \texttt{filler}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_fillers }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ filler, }\AttributeTok{y =}\NormalTok{ sum)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Filler"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Counts"}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p2 }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_fillers }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ filler, }\AttributeTok{y =}\NormalTok{ sum)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# scale the y axis to trim outliers}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Filler"}\NormalTok{, }\AttributeTok{y =} \StringTok{""}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h]

{\centering \includegraphics{inference_files/figure-pdf/i-bi-cont-visual-1.pdf}

}

\end{figure}

In the plot in the left pane we see a couple things. First, it appears
that there is in fact quite a bit of dispersion as there are quite a few
outliers (dots) above the lines extending from the boxes. Recall that
the boxes represent the first and third quantile, that is the IQR and
that the notches represent the confidence interval. Second, when we
compare the boxes and their notches we see that there is little overlap
(looking horizontally). In the right pane I've zoomed in a bit trimming
some outliers to get a better view of the relationship between the
boxes. Since the overlap is minimal and in particular the notches do not
overlap at all, this is a good indication that there is a significant
trend.

From the descriptive statistics and the visual summary it appears that
the filler `uh' is more common than `um'. It's now time to submit this
to statistical interrogation.

\textbf{Statistical interrogation}

In a bivariate (and multivariate) analysis where the dependent variable
is non-categorical we apply Linear Regression Modeling (LM). The default
assumption of linear models, however, is that the dependent variable is
normally distributed. As we have seen our variable \texttt{sum} does not
conform to the normal distribution. We know this because of our tests in
the univariate case, but as mentioned at the end of that section, we are
working with count data which by nature is understood as discrete and
not continuous in a strict technical sense. So instead of using the
linear model for our regression analysis we will use the Generalized
Linear Model (GLM) (R. Harald Baayen 2008; Gries 2013).

The function \texttt{glm()} implements generalized linear models. In
addition to the formula (\texttt{sum\ \textasciitilde{}\ filler}) and
the dataset to use, we also include an appropriate distribution family
for the dependent variable. For count and frequency data the appropriate
family is the ``Poisson'' distribution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 }\OtherTok{\textless{}{-}} 
  \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ sum }\SpecialCharTok{\textasciitilde{}}\NormalTok{ filler, }\CommentTok{\# formula}
      \AttributeTok{data =}\NormalTok{ sdac\_fillers, }\CommentTok{\# dataset}
      \AttributeTok{family =} \StringTok{"poisson"}\NormalTok{) }\CommentTok{\# distribution family}

\FunctionTok{summary}\NormalTok{(m1) }\CommentTok{\# preview the test results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Call:
#> glm(formula = sum ~ filler, family = "poisson", data = sdac_fillers)
#> 
#> Deviance Residuals: 
#>    Min      1Q  Median      3Q     Max  
#> -11.95   -5.61   -3.94    0.80   41.99  
#> 
#> Coefficients:
#>             Estimate Std. Error z value Pr(>|z|)    
#> (Intercept)  4.26794    0.00564     757   <2e-16 ***
#> fillerum    -1.51308    0.01327    -114   <2e-16 ***
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#> (Dispersion parameter for poisson family taken to be 1)
#> 
#>     Null deviance: 72049  on 881  degrees of freedom
#> Residual deviance: 55071  on 880  degrees of freedom
#> AIC: 58524
#> 
#> Number of Fisher Scoring iterations: 6
\end{verbatim}

Let's focus on the coefficients, specifically for the `fillerum' line.
Since our factor \texttt{filler} has two levels one level is used as the
reference to contrast with the other level. In this case by default the
first level is used as the reference. Therefore the coefficients we see
in `fillerum' are `um' in contrast to `uh'. Without digging into the
details of the other parameter statistics, let's focus on the last
column which contains the \(p\)-value. A convenient aspect of the
\texttt{summary()} function when applied to regression model results is
that it provides statistical significance codes. In this case we can see
that the contrast between `uh' and `um' is signficant at \(p < .001\)
which of course is lower than our standard threshold of \(.05\).

Therefore we can say with some confidence that the filler `uh' is more
frequent than `um'.

\textbf{Evaluation}

Given we have found a significant effect for \texttt{filler}, let's look
at evaluating the effect size and the confidence interval. Again, we use
the \texttt{effectsize()} function. We then can preview the
\texttt{effects} object. Note that effect size of interest is in the
second row of the coefficient (\texttt{Std\_Coefficient}) so we subset
this column to extract only the effect coefficient for the
\texttt{filler} contrast.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effects }\OtherTok{\textless{}{-}} \FunctionTok{effectsize}\NormalTok{(m1) }\CommentTok{\# evaluate effect size and generate a confidence interval}

\NormalTok{effects }\CommentTok{\# preview effect size and confidence interval}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # Standardization method: refit
#> 
#> Parameter   | Std. Coef. |         95% CI
#> -----------------------------------------
#> (Intercept) |       4.27 | [ 4.26,  4.28]
#> fillerum    |      -1.51 | [-1.54, -1.49]
#> 
#> - Response is unstandardized.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{interpret\_r}\NormalTok{(effects}\SpecialCharTok{$}\NormalTok{Std\_Coefficient[}\DecValTok{2}\NormalTok{]) }\CommentTok{\# interpret the effect size}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "very large"
#> (Rules: funder2019)
\end{verbatim}

The coefficient statistic falls within the confidence interval and the
effect size is strong so we can be confident that our findings are
reliable given this data.

\hypertarget{multivariate-analysis}{%
\section{Multivariate analysis}\label{multivariate-analysis}}

The last case to consider is when we have more than one independent
variable we want to use to assess their potential relationship to the
dependent variable. Again we will consider a categorical and
non-categorical dependent variable. But, in this case the implementation
methods are quite similar, as we will see.

\hypertarget{categorical-2}{%
\subsection{Categorical}\label{categorical-2}}

For the categorical multivariate case we will again consider the
\texttt{dative} dataset and build on the previous analyses. The question
to be posed is whether modality in combination with the length of the
recipient (\texttt{length\_of\_recipient}) together explain the
distribution of the realization of the recipient
(\texttt{realization\_of\_recipient}).

\textbf{Descriptive assessment}

Now that we have three variables, there is more to summarize to get our
descriptive information. Luckily, however, the same process can be
applied to three (or more) variables using the \texttt{group\_by()}
function and then passed to \texttt{skim()}. In this case we have two
categorical variables and one numeric variable. So we will group by both
the categorical variables and then pass the numeric variable to the
custom \texttt{num\_skim()} function --pulling out only the relevant
descriptive information for numeric variables with \texttt{yank()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dative }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(realization\_of\_recipient, modality, length\_of\_recipient) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# select key variables}
  \FunctionTok{group\_by}\NormalTok{(realization\_of\_recipient, modality) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# grouping parameters}
  \FunctionTok{num\_skim}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# get custom data summary}
  \FunctionTok{yank}\NormalTok{(}\StringTok{"numeric"}\NormalTok{) }\CommentTok{\# only show numeric{-}oriented information}
\end{Highlighting}
\end{Shaded}

\textbf{Variable type: numeric}

\begin{tabular}{l|l|l|r|r|r|r|r|r|r|r|r|r}
\hline
skim\_variable & realization\_of\_recipient & modality & n\_missing & complete\_rate & mean & sd & p0 & p25 & p50 & p75 & p100 & iqr\\
\hline
length\_of\_recipient & NP & spoken & 0 & 1 & 1.14 & 0.60 & 1 & 1 & 1 & 1 & 12 & 0\\
\hline
length\_of\_recipient & NP & written & 0 & 1 & 1.95 & 1.59 & 1 & 1 & 2 & 2 & 17 & 1\\
\hline
length\_of\_recipient & PP & spoken & 0 & 1 & 2.30 & 2.04 & 1 & 1 & 2 & 3 & 15 & 2\\
\hline
length\_of\_recipient & PP & written & 0 & 1 & 4.75 & 4.10 & 1 & 2 & 4 & 6 & 31 & 4\\
\hline
\end{tabular}

There is much more information now that we are considering multiple
independent variables, but if we look over the measures of dispersion we
can see that the median and the IQR are relatively similar to their
respective means suggesting that there are fewer outliers and relativley
little skew.

Let's take a look at a visualization of this information. Since we are
working with a categorical dependent variable and there is one
non-categorical variable we can use a boxplot. The addition here is to
include a \texttt{color} mapping which will provide a distinct box for
each level of modality (`written' and `spoken').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} 
\NormalTok{  dative }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ realization\_of\_recipient, }\AttributeTok{y =}\NormalTok{ length\_of\_recipient, }\AttributeTok{color =}\NormalTok{ modality)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Realization of recipient"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Length of recipient (in words)"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Modality"}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p2 }\OtherTok{\textless{}{-}} 
\NormalTok{  dative }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ realization\_of\_recipient, }\AttributeTok{y =}\NormalTok{ length\_of\_recipient, }\AttributeTok{color =}\NormalTok{ modality)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# scale the y axis to trim outliers}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Realization of recipient"}\NormalTok{, }\AttributeTok{y =} \StringTok{""}\NormalTok{, }\AttributeTok{color =} \StringTok{"Modality"}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p1 }\OtherTok{\textless{}{-}}\NormalTok{ p1 }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\CommentTok{\# remove the legend from the left pane plot}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h]

{\centering \includegraphics{inference_files/figure-pdf/i-multi-cat-visual-1.pdf}

}

\end{figure}

In the left pane we see the entire visualization including all outliers.
From this view it appears that there is a potential trend that the
length of the recipient is larger when the realization of the recipient
is `PP'. There is also a potential trend for modality with written
language showing longer recipient lengths overall. The pane on the right
is scaled to get a better view of the boxes by scaling the y-axis down
and as such trimming the outliers. This plot shows more clearly that the
length of the recipient is longer when the recipient is realized as a
`PP'. Again, the contrast in modality is also a potential trend, but the
boxes (of the same color), particularly for the spoken modality overlap
to some degree.

So we have some trends in mind which will help us interpret the
statistical interrogation so let's move there next.

\textbf{Statistical interrogation}

Once we involve more than two variables, the choice of statistical
method turns towards regression. In the case that the dependent variable
is categorical, however, we will use Logistic Regression. The workhorse
function \texttt{glm()} can be used for a series of regression models,
including logistic regression. The requirement, however, is that we
specify the family of the distribution. For logistic regression the
family is ``binomial''. The formula includes the dependent variable as a
function of our other two variables, each are separated by the
\texttt{+} operator.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ realization\_of\_recipient }\SpecialCharTok{\textasciitilde{}}\NormalTok{ modality }\SpecialCharTok{+}\NormalTok{ length\_of\_recipient, }\CommentTok{\# formula}
          \AttributeTok{data =}\NormalTok{ dative, }\CommentTok{\# dataset}
          \AttributeTok{family =} \StringTok{"binomial"}\NormalTok{) }\CommentTok{\# distribution family}

\FunctionTok{summary}\NormalTok{(m1) }\CommentTok{\# preview the test results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Call:
#> glm(formula = realization_of_recipient ~ modality + length_of_recipient, 
#>     family = "binomial", data = dative)
#> 
#> Deviance Residuals: 
#>    Min      1Q  Median      3Q     Max  
#> -4.393  -0.598  -0.598   0.132   1.924  
#> 
#> Coefficients:
#>                     Estimate Std. Error z value Pr(>|z|)    
#> (Intercept)          -2.3392     0.0797  -29.35   <2e-16 ***
#> modalitywritten      -0.0483     0.1069   -0.45     0.65    
#> length_of_recipient   0.7081     0.0420   16.86   <2e-16 ***
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#> (Dispersion parameter for binomial family taken to be 1)
#> 
#>     Null deviance: 3741.1  on 3262  degrees of freedom
#> Residual deviance: 3104.7  on 3260  degrees of freedom
#> AIC: 3111
#> 
#> Number of Fisher Scoring iterations: 5
\end{verbatim}

The results from the model again provide a wealth of information. But
the key information to focus on is the coefficients. In particular the
coefficients for the independent variables \texttt{modality} and
\texttt{length\_of\_recipient}. What we notice, is that the \(p\)-value
for \texttt{length\_of\_recipient} is significant, but the contrast
between `written' and `spoken' for \texttt{modality} is not. If you
recall, we used this same dataset to explore \texttt{modality} as a
single indpendent variable earlier --and it was found to be significant.
So why now is it not? The answer is that when multiple variables are
used to explain the distribution of a measure (dependent variable) each
variable now adds more information to explain the dependent variable
--each has it's own contribution. Since \texttt{length\_of\_recipient}
is significant, this suggests that the explanatory power of
\texttt{modality} is weak, especially when compared to
\texttt{length\_of\_recipient}. This make sense as we saw in the earlier
model the fact that the effect size for \texttt{modality} was not strong
and that is now more evident that the \texttt{length\_of\_recipient} is
included in the model.

\textbf{Evaluation}

Now let's move on and gauge the effect size and calculate the confidence
interval for \texttt{length\_of\_recipient} in our model. We apply the
\texttt{effectsize()} function to the model and then use
\texttt{interpret\_r()} on the coefficient of interest (which is in the
fourth row of the \texttt{Std\_Coefficients} column).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effects }\OtherTok{\textless{}{-}} \FunctionTok{effectsize}\NormalTok{(m1) }\CommentTok{\# evaluate effect size and generate a confidence interval}

\NormalTok{effects }\CommentTok{\# preview effect size and confidence interval}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # Standardization method: refit
#> 
#> Parameter           | Std. Coef. |         95% CI
#> -------------------------------------------------
#> (Intercept)         |      -1.03 | [-1.15, -0.92]
#> modalitywritten     |      -0.05 | [-0.26,  0.16]
#> length_of_recipient |       1.46 | [ 1.30,  1.64]
#> 
#> - Response is unstandardized.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{interpret\_r}\NormalTok{(effects}\SpecialCharTok{$}\NormalTok{Std\_Coefficient[}\DecValTok{4}\NormalTok{]) }\CommentTok{\# interpret the effect size}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] NA
#> (Rules: funder2019)
\end{verbatim}

We see we have a coefficient that falls within the confidence interval
and the effect size is large. So we can saw with some confidence that
the length of the recipient is a significant predictor of the use of
`PP' as the realization of the recipient in the dative alternation.

\hypertarget{continuous-2}{%
\subsection{Continuous}\label{continuous-2}}

The last case we will consider here is when the dependent variable is
non-categorical and we have more than one independent variable. The
question we will pose is whether the type of filler and the sex of the
speaker can explain the use of fillers in conversational speech.

We will need to prepare the data before we get started as our current
data frame \texttt{sdac\_fillers} has filler and the sum count for each
filler grouped by speaker --but it does not include the \texttt{sex} of
each speaker. The \texttt{sdac\_disfluencies} data frame does have the
\texttt{sex} column, but it has not been grouped by speaker. So let's
transform the \texttt{sdac\_disfluencies} summarizing it to only get the
\texttt{speaker\_id} and \texttt{sex} combinations. This should result
in a data frame with 441 observations, one observation for each speaker
in the corpus.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_speakers\_sex }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_disfluencies }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{distinct}\NormalTok{(speaker\_id, sex) }\CommentTok{\# summarize for distinct \textasciigrave{}speaker\_id\textasciigrave{} and \textasciigrave{}sex\textasciigrave{} values}
\end{Highlighting}
\end{Shaded}

Let's preview the first 10 observations form this transformation.

\hypertarget{tbl-i-multi-cont-transform-sdac-preview}{}
\begin{table}
\caption{\label{tbl-i-multi-cont-transform-sdac-preview}First 10 observations of the \texttt{sdac\_speakers\_sex} data frame. }\tabularnewline

\centering
\begin{tabular}{ll}
\toprule
speaker\_id & sex\\
\midrule
155 & NA\\
1000 & FEMALE\\
1001 & MALE\\
1002 & FEMALE\\
1004 & FEMALE\\
\addlinespace
1005 & FEMALE\\
1007 & FEMALE\\
1008 & FEMALE\\
1010 & MALE\\
1011 & FEMALE\\
\bottomrule
\end{tabular}
\end{table}

Great, now we have each \texttt{speaker\_id} and \texttt{sex} for all
441 speakers. One thing to note, however, is that speaker `155' does not
have a value for \texttt{sex} --this seems to be an error in the
metadata that we will need to deal with before we proceed in our
analysis. Let's move on to join our new \texttt{sdac\_speakers\_sex}
data frame and the \texttt{sdac\_fillers} data frame.

Now that we have a complete dataset with \texttt{speaker\_id} and
\texttt{sex} we will now join this dataset with our
\texttt{sdac\_fillers} dataset effectively adding the column
\texttt{sex}. We want to keep all the observations in
\texttt{sdac\_fillers} and add the column \texttt{sex} for observations
that correspond between each data frame for the column
\texttt{speaker\_id} so we will use a left join with the function
\texttt{left\_join()} with the \texttt{sdac\_fillers} dataset on the
left.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_fillers\_sex }\OtherTok{\textless{}{-}} 
  \FunctionTok{left\_join}\NormalTok{(sdac\_fillers, sdac\_speakers\_sex) }\CommentTok{\# join}
\end{Highlighting}
\end{Shaded}

Now let's preview the first observations in this new
\texttt{sdac\_fillers\_sex} data frame.

\hypertarget{tbl-i-multi-cont-sdac-fillers-sex-preview}{}
\begin{table}
\caption{\label{tbl-i-multi-cont-sdac-fillers-sex-preview}First 10 observations of the \texttt{sdac\_fillers\_sex} data frame. }\tabularnewline

\centering
\begin{tabular}{llrl}
\toprule
speaker\_id & filler & sum & sex\\
\midrule
155 & uh & 28 & NA\\
155 & um & 0 & NA\\
1000 & uh & 37 & FEMALE\\
1000 & um & 8 & FEMALE\\
1001 & uh & 262 & MALE\\
\addlinespace
1001 & um & 2 & MALE\\
1002 & uh & 34 & FEMALE\\
1002 & um & 20 & FEMALE\\
1004 & uh & 30 & FEMALE\\
1004 & um & 15 & FEMALE\\
\bottomrule
\end{tabular}
\end{table}

At this point let's drop this speaker from the
\texttt{sdac\_speakers\_sex} data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_fillers\_sex }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_fillers\_sex }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{filter}\NormalTok{(speaker\_id }\SpecialCharTok{!=} \StringTok{"155"}\NormalTok{) }\CommentTok{\# drop speaker\_id 155}
\end{Highlighting}
\end{Shaded}

We are now ready to proceed in our analysis.

\textbf{Descriptive assessment}

The process by now should be quite routine for getting our descriptive
statistics: select the key variables, group by the categorical
variables, and finally pull the descriptives for the numeric variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdac\_fillers\_sex }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(sum, filler, sex) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# select key variables}
  \FunctionTok{group\_by}\NormalTok{(filler, sex) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# grouping parameters}
  \FunctionTok{num\_skim}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# get custom data summary}
  \FunctionTok{yank}\NormalTok{(}\StringTok{"numeric"}\NormalTok{) }\CommentTok{\# only show numeric{-}oriented information}
\end{Highlighting}
\end{Shaded}

\textbf{Variable type: numeric}

\begin{tabular}{l|l|l|r|r|r|r|r|r|r|r|r|r}
\hline
skim\_variable & filler & sex & n\_missing & complete\_rate & mean & sd & p0 & p25 & p50 & p75 & p100 & iqr\\
\hline
sum & uh & FEMALE & 0 & 1 & 63.22 & 76.5 & 0 & 12.0 & 39.0 & 81.8 & 509 & 69.8\\
\hline
sum & uh & MALE & 0 & 1 & 78.74 & 102.6 & 0 & 15.2 & 37.5 & 101.5 & 661 & 86.2\\
\hline
sum & um & FEMALE & 0 & 1 & 22.38 & 36.3 & 0 & 1.0 & 9.0 & 28.0 & 265 & 27.0\\
\hline
sum & um & MALE & 0 & 1 & 9.92 & 24.2 & 0 & 0.0 & 1.0 & 8.0 & 217 & 8.0\\
\hline
\end{tabular}

Looking at these descriptives, it seems like there is quite a bit of
variability for some combinations and not others. In short, it's a mixed
bag. Let's try to make sense of these numbers with a boxplot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_fillers\_sex }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ filler, }\AttributeTok{y =}\NormalTok{ sum, }\AttributeTok{color =}\NormalTok{ sex)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Filler"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Counts"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Sex"}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p2 }\OtherTok{\textless{}{-}} 
\NormalTok{  sdac\_fillers\_sex }\SpecialCharTok{|\textgreater{}} \CommentTok{\# dataset}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ filler, }\AttributeTok{y =}\NormalTok{ sum, }\AttributeTok{color =}\NormalTok{ sex)) }\SpecialCharTok{+} \CommentTok{\# mappings}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# geometry}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{200}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# scale the y axis to trim outliers}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Filler"}\NormalTok{, }\AttributeTok{y =} \StringTok{""}\NormalTok{, }\AttributeTok{color =} \StringTok{"Sex"}\NormalTok{) }\CommentTok{\# labels}

\NormalTok{p1 }\OtherTok{\textless{}{-}}\NormalTok{ p1 }\SpecialCharTok{+} \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\CommentTok{\# drop the legend from the left pane plot}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\begin{figure}[h]

{\centering \includegraphics{inference_files/figure-pdf/i-mulit-cont-visual-1.pdf}

}

\end{figure}

We can see that `uh' is used more than `um' overall. But that whereas
men and women use `uh' in similar ways, women use more `um' than men.
This is known as an interaction. So we will approach our statistical
analysis with this in mind.

\textbf{Statistical interrogation}

We will again use a generalized linear model with the \texttt{glm()}
function to conduct our test. The distribution family will be the same
has we are again using the \texttt{sum} as our dependent variable which
contains discrete count values. The formula we will use, however, is
new. Instead of adding a new variable to our independent variables, we
will test the possible interaction between \texttt{filler} and
\texttt{sex} that we noted in the descriptive assessment. To encode an
interaction the \texttt{*} operator is used. So our formula will take
the form \texttt{sum\ \textasciitilde{}\ filler\ *\ sex}. Let's generate
the model and view the summary of the test results as we have done
before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 }\OtherTok{\textless{}{-}} 
  \FunctionTok{glm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ sum }\SpecialCharTok{\textasciitilde{}}\NormalTok{ filler }\SpecialCharTok{*}\NormalTok{ sex, }\CommentTok{\# formula}
      \AttributeTok{data =}\NormalTok{ sdac\_fillers\_sex, }\CommentTok{\# dataset}
      \AttributeTok{family =} \StringTok{"poisson"}\NormalTok{) }\CommentTok{\# distribution family}

\FunctionTok{summary}\NormalTok{(m1) }\CommentTok{\# preview the test results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 
#> Call:
#> glm(formula = sum ~ filler * sex, family = "poisson", data = sdac_fillers_sex)
#> 
#> Deviance Residuals: 
#>    Min      1Q  Median      3Q     Max  
#> -12.55   -6.21   -3.64    1.08   40.60  
#> 
#> Coefficients:
#>                  Estimate Std. Error z value Pr(>|z|)    
#> (Intercept)       4.14660    0.00876   473.2   <2e-16 ***
#> fillerum         -1.03827    0.01714   -60.6   <2e-16 ***
#> sexMALE           0.21955    0.01145    19.2   <2e-16 ***
#> fillerum:sexMALE -1.03344    0.02791   -37.0   <2e-16 ***
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#> (Dispersion parameter for poisson family taken to be 1)
#> 
#>     Null deviance: 71956  on 879  degrees of freedom
#> Residual deviance: 53543  on 876  degrees of freedom
#> AIC: 56994
#> 
#> Number of Fisher Scoring iterations: 6
\end{verbatim}

Again looking at the coefficients we something new. First we see that
there is a row for the \texttt{filler} contrast and the \texttt{sex}
contrast but also the interaction between \texttt{filler} and
\texttt{sex} (`fillerum:sexMALE'). All rows show significant effects. It
is important to note that when an interaction is explored and it is
found to be significant, the other simple effects, known as main effects
(`fillerum' and `sexMALE'), are ignored. Only the higer-order effect is
considered significant.

Now what does the `fillerum:sexMALE' row mean. It means that there is an
interaction between \texttt{filler} and \texttt{sex}. the directionality
of that interaction should be interpreted using our descriptive
assessment, in particular the visual boxplots we generated. In sum,
women use more `um' than men or stated another way men use `um' less
than women.

\textbf{Evaluation}

We finalize our analysis by looking at the effect size and confidence
intervals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effects }\OtherTok{\textless{}{-}} \FunctionTok{effectsize}\NormalTok{(m1) }\CommentTok{\# evaluate effect size and generate a confidence interval}

\NormalTok{effects }\CommentTok{\# preview effect size and confidence interval}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> # Standardization method: refit
#> 
#> Parameter        | Std. Coef. |         95% CI
#> ----------------------------------------------
#> (Intercept)      |       4.15 | [ 4.13,  4.16]
#> fillerum         |      -1.04 | [-1.07, -1.00]
#> sexMALE          |       0.22 | [ 0.20,  0.24]
#> fillerum:sexMALE |      -1.03 | [-1.09, -0.98]
#> 
#> - Response is unstandardized.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{interpret\_r}\NormalTok{(effects}\SpecialCharTok{$}\NormalTok{Std\_Coefficient[}\DecValTok{4}\NormalTok{]) }\CommentTok{\# interpret the effect size}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "very large"
#> (Rules: funder2019)
\end{verbatim}

We can conclude, then, that there is a strong interaction effect for
\texttt{filler} and \texttt{sex} and that women use more `um' than men.

\hypertarget{summary-13}{%
\section{Summary}\label{summary-13}}

In this chapter we have discussed various approaches to conducting
inferential data analysis. Each configuration, however, always includes
a descriptive assessment, statistical interrogation, and an evaluation
of the results. We considered univariate, bivariate, and multivariate
analyses using both categorical and non-categorical dependent variables
to explore the similarities and differences between these approaches.

\part{Communication}

In this section I cover the steps in presenting the findings of the
research both as a research document and as a reproducible research
project. Both research documents and reproducible projects are
fundamental components of modern scientific inquiry. On the one hand a
research document provides readers a detailed summary of the main import
of the research study. On the other hand making the research project
available to interested readers ensures that the scientific community
can gain insight into the process implemented in the research and thus
enables researchers to vet and extend this research to build a more
robust and verifiable research base.

\hypertarget{sec-reporting}{%
\chapter{Reporting}\label{sec-reporting}}

\begin{quote}
..quote..

--- Author
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  \ldots{}
\end{itemize}

\end{tcolorbox}

In this chapter, we first examine the role of research presentations in
academic and professional settings, highlighting the benefits of
presenting one's work as a means to develop and refine ideas and
conclusions. We then discuss the purpose of a research document,
focusing on how its structure effectively conveys the project's
rationale, goals, procedures, results, and findings. Additionally, we
explore the integration of citations, references, figures, and tables in
research documents using Quarto. Lastly, we investigate the application
of field-specific and publishing house formats to a variety of document
formats, such as Word, PDF, HTML, and ePub.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Swirl}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/swirl}{Rendering
Quarto}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To \ldots{}

\end{tcolorbox}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Introduction

  \begin{itemize}
  \tightlist
  \item
    Importance of clear and effective communication in research

    \begin{itemize}
    \tightlist
    \item
      Enhances understanding of the research
    \item
      Facilitates collaboration and interdisciplinary work
    \end{itemize}
  \item
    Overview of the chapter content

    \begin{itemize}
    \tightlist
    \item
      Research presentations
    \item
      Research document structure
    \item
      Citations and references
    \item
      Figures and tables
    \item
      Field-specific and publishing house formats
    \end{itemize}
  \end{itemize}
\item
  Research Presentations

  \begin{itemize}
  \tightlist
  \item
    Importance of research presentations in academia and professional
    contexts

    \begin{itemize}
    \tightlist
    \item
      Sharing research findings
    \item
      Building a professional reputation
    \end{itemize}
  \item
    Benefits of presenting research work

    \begin{itemize}
    \tightlist
    \item
      Developing and refining ideas and conclusions through feedback and
      discussion
    \item
      Receiving feedback and suggestions from peers and experts

      \begin{itemize}
      \tightlist
      \item
        Conducting peer review can help ensure that the research is
        conducted appropriately and transparently. Peer review can also
        identify potential errors or biases in the research design or
        analysis, and can provide suggestions for improving the study's
        reproducibility
      \end{itemize}
    \item
      Enhancing communication and presentation skills
    \item
      Building professional network and fostering collaboration
    \end{itemize}
  \item
    Key elements of an effective research presentation

    \begin{itemize}
    \tightlist
    \item
      Structure and organization: logical flow, clear sections, concise
      points
    \item
      Visual design and aesthetics: use of color, fonts, visuals, and
      layout
    \item
      Delivery and engagement: pacing, tone, body language, and audience
      interaction
    \end{itemize}
  \item
    Using R for creating research presentations

    \begin{itemize}
    \tightlist
    \item
      RMarkdown and its integration with presentation formats (e.g.,
      xaringan, slidify)
    \item
      Incorporating data visualization, tables, and other R-generated
      content seamlessly
    \end{itemize}
  \item
    Tips for delivering engaging and memorable presentations

    \begin{itemize}
    \tightlist
    \item
      Practice and preparation
    \item
      Storytelling and relatability
    \item
      Addressing diverse audience backgrounds
    \end{itemize}
  \end{itemize}
\item
  Research Documents

  \begin{itemize}
  \tightlist
  \item
    Purpose of a research document

    \begin{itemize}
    \tightlist
    \item
      Communicate research methodology and findings
    \item
      Contribute to the body of knowledge
    \end{itemize}
  \item
    Benefits of writing a research document

    \begin{itemize}
    \tightlist
    \item
      Clarifying and organizing thoughts and ideas
    \item
      Providing a comprehensive record of research work
    \item
      Enhancing writing and argumentation skills
    \item
      Contributing to the body of knowledge in a field
    \item
      Establishing professional credibility and visibility
    \end{itemize}
  \item
    Components of a research document

    \begin{itemize}
    \tightlist
    \item
      Title: concise and informative
    \item
      Abstract: brief summary of the research
    \item
      Introduction: background, research question, and objectives
    \item
      Methodology: description of the data, tools, and analysis
      techniques
    \item
      Results: presentation of the findings
    \item
      Discussion: interpretation and implications of the results
    \item
      Conclusion: summary and future directions
    \item
      References: list of cited sources
    \end{itemize}
  \item
    Tips for organizing and structuring research documents

    \begin{itemize}
    \tightlist
    \item
      Outlining the main sections
    \item
      Maintaining logical flow and consistency
    \item
      Using clear and concise language
    \end{itemize}
  \end{itemize}
\item
  Citations and References

  \begin{itemize}
  \tightlist
  \item
    Importance of proper citation and referencing

    \begin{itemize}
    \tightlist
    \item
      Acknowledge the work of others
    \item
      Demonstrate the foundation of your research
    \end{itemize}
  \item
    Different citation styles (APA, MLA, Chicago, etc.)

    \begin{itemize}
    \tightlist
    \item
      Overview of common styles
    \item
      Choosing the appropriate style for your discipline
    \end{itemize}
  \item
    Using R and Quarto for managing citations and references

    \begin{itemize}
    \tightlist
    \item
      Integration with reference managers (e.g., Zotero, Mendeley)
    \item
      Automating citation formatting
    \end{itemize}
  \item
    Integration of citations and references in research documents

    \begin{itemize}
    \tightlist
    \item
      In-text citations
    \item
      Reference list or bibliography
    \end{itemize}
  \end{itemize}
\item
  Figures and Tables

  \begin{itemize}
  \tightlist
  \item
    Importance of visual representations in research reporting

    \begin{itemize}
    \tightlist
    \item
      Aid in understanding complex data
    \item
      Enhance the readability of the document
    \end{itemize}
  \item
    Creating and customizing figures and tables using R and Quarto

    \begin{itemize}
    \tightlist
    \item
      ggplot2 for creating visualizations
    \item
      kable and gt for generating tables
    \end{itemize}
  \item
    Guidelines for labeling, formatting, and presenting figures and
    tables

    \begin{itemize}
    \tightlist
    \item
      Clear and informative titles and labels
    \item
      Consistent formatting
    \item
      Appropriate use of color, fonts, and layout
    \item
      Proper referencing and integration in the text
    \item
      Accessibility considerations for diverse readers
    \end{itemize}
  \end{itemize}
\item
  Field-specific and Publishing House Formats

  \begin{itemize}
  \tightlist
  \item
    Importance of adhering to field-specific and publisher guidelines

    \begin{itemize}
    \tightlist
    \item
      Consistency and professionalism
    \item
      Meeting submission requirements for journals, conferences, and
      other venues
    \end{itemize}
  \item
    Overview of common format requirements

    \begin{itemize}
    \tightlist
    \item
      Manuscript formatting: margins, line spacing, headings, etc.
    \item
      Citation and reference style
    \item
      Figure and table formatting
    \end{itemize}
  \item
    Using R and Quarto to apply formatting guidelines

    \begin{itemize}
    \tightlist
    \item
      Customizing document templates
    \item
      Applying and managing style files
    \end{itemize}
  \item
    Exporting research documents to various formats

    \begin{itemize}
    \tightlist
    \item
      Word: .docx files for collaboration and editing
    \item
      PDF: .pdf files for sharing and printing
    \item
      HTML: web-based documents for online publication
    \item
      ePub: e-book format for digital reading
    \end{itemize}
  \end{itemize}
\item
  Conclusion

  \begin{itemize}
  \tightlist
  \item
    Recap of the main topics covered in the chapter
  \item
    Importance of clear and effective communication in research
    reporting
  \item
    Encouragement to practice and refine reporting skills through
    various research projects
  \end{itemize}
\end{enumerate}

\hypertarget{questions-11}{%
\section*{Questions}\label{questions-11}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Conceptual questions}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Technical exercises}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{enumerate}

\end{tcolorbox}

\hypertarget{sec-collaboration}{%
\chapter{Collaboration}\label{sec-collaboration}}

\begin{quote}
..quote..

--- Author
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  \ldots{}
\end{itemize}

\end{tcolorbox}

Whether for other researchers or for your future self, creating research
that is well-documented and reproducible is a fundamental part of
conducting modern scientific inquiry. In this chapter we will emphasize
the importance of this endeavor and outline strategies for ensuring your
research project is reproducible. This will include directory and file
structure, key documentation files as well as how to effectively use
existing software resources and frameworks for publishing your research
(either for private use, journal requirements, or general public
consumption) on popular repositories such as GitHub and Open Science
Framework (OSF).

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Swirl}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colback=white]

\textbf{What}: \href{https://github.com/lin380/swirl}{Compiling Research
Projects}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To \ldots{}

\end{tcolorbox}

Outline\ldots{}

This is what we will cover in this chapter: \ldots{}

\hypertarget{questions-12}{%
\section*{Questions}\label{questions-12}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Conceptual questions}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Technical exercises}, breakable, left=2mm, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, opacityback=0, toprule=.15mm, leftrule=.75mm, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colback=white]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{enumerate}

\end{tcolorbox}

\bookmarksetup{startatroot}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-Ackoff1989}{}}%
Ackoff, Russell L. 1989. {``From Data to Wisdom.''} \emph{Journal of
Applied Systems Analysis} 16 (1): 3--9.

\leavevmode\vadjust pre{\hypertarget{ref-Adel2020}{}}%
Ädel, Annelie. 2020. {``Corpus Compilation.''} In \emph{A Practical
Handbook of Corpus Linguistics}, edited by Magali Paquot and Stefan Th.
Gries, 3--24. Switzerland: Springer.

\leavevmode\vadjust pre{\hypertarget{ref-Almeida2011a}{}}%
Almeida, Tiago A, José María Gómez Hildago, and Akebo Yamakami. 2011.
{``Contributions to the Study of SMS Spam Filtering: New Collection and
Results.''} In \emph{Proceedings of the 2011 ACM Symposium on Document
Engineering (DOCENG'11)}, 4. Mountain View, CA.

\leavevmode\vadjust pre{\hypertarget{ref-Baayen2004}{}}%
Baayen, R. Harald. 2004. {``Statistics in Psycholinguistics: A Critique
of Some Current Gold Standards.''} \emph{Mental Lexicon Working Papers}
1 (1): 1--47.

\leavevmode\vadjust pre{\hypertarget{ref-Baayen2008a}{}}%
---------. 2008. \emph{Analyzing Linguistic Data: A Practical
Introduction to Statistics Using r}. Cambridge Univ Pr.

\leavevmode\vadjust pre{\hypertarget{ref-Baayen2011}{}}%
---------. 2011. {``Corpus Linguistics and Naive Discriminative
Learning.''} \emph{Revista Brasileira de
Lingu\textbackslash'\textbackslash istica Aplicada} 11 (2): 295--328.

\leavevmode\vadjust pre{\hypertarget{ref-R-languageR}{}}%
Baayen, R. H., and Elnaz Shafaei-Bajestan. 2019. \emph{languageR:
Analyzing Linguistic Data: A Practical Introduction to Statistics}.
\url{https://CRAN.R-project.org/package=languageR}.

\leavevmode\vadjust pre{\hypertarget{ref-Bao2019}{}}%
Bao, Wang, Ning Lianju, and Kong Yue. 2019. {``Integration of
Unsupervised and Supervised Machine Learning Algorithms for Credit Risk
Assessment.''} \emph{Expert Systems with Applications} 128 (August):
301--15. \url{https://doi.org/10.1016/j.eswa.2019.02.033}.

\leavevmode\vadjust pre{\hypertarget{ref-R-quanteda.corpora}{}}%
Benoit, Kenneth. 2020. \emph{Quanteda.corpora: A Collection of Corpora
for Quanteda}. \url{http://github.com/quanteda/quanteda.corpora}.

\leavevmode\vadjust pre{\hypertarget{ref-Broman2018}{}}%
Broman, Karl W., and Kara H. Woo. 2018. {``Data Organization in
Spreadsheets.''} \emph{The American Statistician} 72 (1): 2--10.
\url{https://doi.org/10.1080/00031305.2017.1375989}.

\leavevmode\vadjust pre{\hypertarget{ref-Brown2005}{}}%
Brown, Keith. 2005. \emph{Encyclopedia of Language and Linguistics}.
Vol. 1. Elsevier.

\leavevmode\vadjust pre{\hypertarget{ref-Bychkovska2017}{}}%
Bychkovska, Tetyana, and Joseph J. Lee. 2017. {``At the Same Time:
Lexical Bundles in L1 and L2 University Student Argumentative
Writing.''} \emph{Journal of English for Academic Purposes} 30
(November): 38--52. \url{https://doi.org/10.1016/j.jeap.2017.10.008}.

\leavevmode\vadjust pre{\hypertarget{ref-Chambers2020}{}}%
Chambers, John M. 2020. {``S, r, and Data Science.''} \emph{Proceedings
of the ACM on Programming Languages} 4 (HOPL): 1--17.
\url{https://doi.org/10.1145/3386334}.

\leavevmode\vadjust pre{\hypertarget{ref-Chan2014}{}}%
Chan, Sin-wai. 2014. \emph{Routledge Encyclopedia of Translation
Technology}. Routledge.

\leavevmode\vadjust pre{\hypertarget{ref-Conway2012}{}}%
Conway, Lucian Gideon, Laura Janelle Gornick, Chelsea Burfeind, Paul
Mandella, Andrea Kuenzli, Shannon C. Houck, and Deven Theresa Fullerton.
2012. {``Does Complex or Simple Rhetoric Win Elections? An Integrative
Complexity Analysis of u.s. Presidential Campaigns.''} \emph{Political
Psychology} 33 (5): 599--618.
\url{https://doi.org/10.1111/j.1467-9221.2012.00910.x}.

\leavevmode\vadjust pre{\hypertarget{ref-Cross2006}{}}%
Cross, Nigel. 2006. {``Design as a Discipline.''} \emph{Designerly Ways
of Knowing}, 95--103.

\leavevmode\vadjust pre{\hypertarget{ref-DataNeverSleeps08-2021}{}}%
{``Data Never Sleeps 7.0 Infographic.''} 2019.
https://www.domo.com/learn/infographic/data-never-sleeps-7.

\leavevmode\vadjust pre{\hypertarget{ref-Deshors2016}{}}%
Deshors, Sandra C, and Stefan Th. Gries. 2016. {``Profiling Verb
Complementation Constructions Across New Englishes.''}
\emph{International Journal of Corpus Linguistics.} 21 (2): 192--218.

\leavevmode\vadjust pre{\hypertarget{ref-Desjardins2019}{}}%
Desjardins, Jeff. 2019. {``How Much Data Is Generated Each Day?''}
\emph{Visual Capitalist}.

\leavevmode\vadjust pre{\hypertarget{ref-Donoho2017}{}}%
Donoho, David. 2017. {``50 Years of Data Science.''} \emph{Journal of
Computational and Graphical Statistics} 26 (4): 745--66.
\url{https://doi.org/10.1080/10618600.2017.1384734}.

\leavevmode\vadjust pre{\hypertarget{ref-Dubnjakovic2010}{}}%
Dubnjakovic, Ana, and Patrick Tomlin. 2010. \emph{A Practical Guide to
Electronic Resources in the Humanities}. Elsevier.

\leavevmode\vadjust pre{\hypertarget{ref-Eisenstein2012}{}}%
Eisenstein, Jacob, Brendan O'Connor, Noah A Smith, and Eric P Xing.
2012. {``Mapping the Geographical Diffusion of New Words.''}
\emph{Computation and Language}, 1--13.
\url{https://doi.org/10.1371/journal.pone.0113114}.

\leavevmode\vadjust pre{\hypertarget{ref-Gandrud2015}{}}%
Gandrud, Christopher. 2015.
\emph{\href{https://www.ncbi.nlm.nih.gov/pubmed/17811671}{Reproducible
Research with r and r Studio}}. Second edition. CRC Press.

\leavevmode\vadjust pre{\hypertarget{ref-Gentleman2007}{}}%
Gentleman, Robert, and Duncan Temple Lang. 2007. {``Statistical Analyses
and Reproducible Research.''} \emph{Journal of Computational and
Graphical Statistics} 16 (1): 1--23.

\leavevmode\vadjust pre{\hypertarget{ref-Gilquin2009}{}}%
Gilquin, Gaëtanelle, and Stefan Th Gries. 2009. {``Corpora and
Experimental Methods: A State-of-the-Art Review.''} \emph{Corpus
Linguistics and Linguistic Theory} 5 (1): 1--26.
\url{https://doi.org/10.1515/CLLT.2009.001}.

\leavevmode\vadjust pre{\hypertarget{ref-Gomez-Uribe2015}{}}%
Gomez-Uribe, Carlos A., and Neil Hunt. 2015. {``The Netflix Recommender
System: Algorithms, Business Value, and Innovation.''} \emph{ACM
Transactions on Management Information Systems (TMIS)} 6 (4): 1--19.

\leavevmode\vadjust pre{\hypertarget{ref-Gries2013a}{}}%
Gries, Stefan Th. 2013. \emph{Statistics for Linguistics with r. A
Practical Introduction}. 2nd revise.

\leavevmode\vadjust pre{\hypertarget{ref-Grieve2018}{}}%
Grieve, Jack, Andrea Nini, and Diansheng Guo. 2018. {``Mapping Lexical
Innovation on American Social Media.''} \emph{Journal of English
Linguistics} 46 (4): 293--319.

\leavevmode\vadjust pre{\hypertarget{ref-Head2015}{}}%
Head, Megan L., Luke Holman, Rob Lanfear, Andrew T. Kahn, and Michael D.
Jennions. 2015. {``The Extent and Consequences of p-Hacking in
Science.''} \emph{PLOS Biology} 13 (3): e1002106.
\url{https://doi.org/10.1371/journal.pbio.1002106}.

\leavevmode\vadjust pre{\hypertarget{ref-HowMakeData10-2021}{}}%
{``How to Make a Data Dictionary.''} 2021. \emph{OSF Guides}.
https://help.osf.io/hc/en-us/articles/360019739054-How-to-Make-a-Data-Dictionary.

\leavevmode\vadjust pre{\hypertarget{ref-Hu2004}{}}%
Hu, Minqing, and Bing Liu. 2004. {``Mining and Summarizing Customer
Reviews.''} In \emph{Proceedings of the Tenth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining}, 168--77.

\leavevmode\vadjust pre{\hypertarget{ref-Ignatow2017}{}}%
Ignatow, Gabe, and Rada Mihalcea. 2017. \emph{An Introduction to Text
Mining: Research Design, Data Collection, and Analysis}. Sage
Publications.

\leavevmode\vadjust pre{\hypertarget{ref-Jaeger2007}{}}%
Jaeger, T Florian, and Neal Snider. 2007. {``Implicit Learning and
Syntactic Persistence: Surprisal and Cumulativity.''} \emph{University
of Rochester Working Papers in the Language Sciences} 3 (1).

\leavevmode\vadjust pre{\hypertarget{ref-Jurafsky2020}{}}%
Jurafsky, Daniel, and James H. Martin. 2020. \emph{Speech and Language
Processing}.

\leavevmode\vadjust pre{\hypertarget{ref-R-rtweet}{}}%
Kearney, Michael W., Lluís Revilla Sancho, and Hadley Wickham. 2023.
\emph{Rtweet: Collecting Twitter Data}.
\url{https://CRAN.R-project.org/package=rtweet}.

\leavevmode\vadjust pre{\hypertarget{ref-Kerr1998}{}}%
Kerr, Norbert L. 1998. {``HARKing: Hypothesizing After the Results Are
Known.''} \emph{Personality and Social Psychology Review} 2 (3):
196--217.

\leavevmode\vadjust pre{\hypertarget{ref-Kloumann2012}{}}%
Kloumann, IM, CM Danforth, KD Harris, and CA Bliss. 2012. {``Positivity
of the English Language.''} \emph{PloS One}.

\leavevmode\vadjust pre{\hypertarget{ref-Kowsari2019}{}}%
Kowsari, Kamran, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana
Mendu, Laura E. Barnes, and Donald E. Brown. 2019. {``Text
Classification Algorithms: A Survey.''} \emph{Information} 10 (4): 150.
\url{https://doi.org/10.3390/info10040150}.

\leavevmode\vadjust pre{\hypertarget{ref-Kucera1967}{}}%
Kucera, H, and W N Francis. 1967. \emph{Computational Analysis of
Present Day American English}. Brown University Press Providence.

\leavevmode\vadjust pre{\hypertarget{ref-Lantz2013}{}}%
Lantz, Brett. 2013. \emph{Machine Learning with r}. Birmingham: Packt
Publishing.

\leavevmode\vadjust pre{\hypertarget{ref-Lewis2004}{}}%
Lewis, Michael. 2004. \emph{Moneyball: The Art of Winning an Unfair
Game}. WW Norton \& Company.

\leavevmode\vadjust pre{\hypertarget{ref-Lozano2009}{}}%
Lozano, Cristóbal. 2009. {``CEDEL2: Corpus Escrito Del Español L2.''}
\emph{Applied Linguistics Now: Understanding Language and Mind/La
Lingüística Aplicada Hoy: Comprendiendo El Lenguaje y La Mente. Almería:
Universidad de Almería}, 197--212.

\leavevmode\vadjust pre{\hypertarget{ref-Marwick2018}{}}%
Marwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. {``Packaging
Data Analytical Work Reproducibly Using r (and Friends).''} \emph{The
American Statistician} 72 (1): 80--88.

\leavevmode\vadjust pre{\hypertarget{ref-Millikan1923}{}}%
Millikan, Robert A. 1923. \emph{The Electron and the Light-Quant from
the Experimental Point of View}. \emph{Nobel Prize Acceptance Speech}.

\leavevmode\vadjust pre{\hypertarget{ref-Mosteller1963}{}}%
Mosteller, Frederick, and David L Wallace. 1963. {``Inference in an
Authorship Problem.''} \emph{Journal of the American Statistical
Association} 58 (302): 275--309.
\url{https://www.jstor.org/stable/2283270}.

\leavevmode\vadjust pre{\hypertarget{ref-Olohan2008}{}}%
Olohan, Maeve. 2008. {``Leave It Out! Using a Comparable Corpus to
Investigate Aspects of Explicitation in Translation.''} \emph{Cadernos
de Tradução}, 153--69.

\leavevmode\vadjust pre{\hypertarget{ref-Paquot2020a}{}}%
Paquot, Magali, and Stefan Th. Gries, eds. 2020. \emph{A Practical
Handbook of Corpus Linguistics}. Switzerland: Springer.

\leavevmode\vadjust pre{\hypertarget{ref-Roediger2000}{}}%
Roediger, H. L. L, and K. B. B McDermott. 2000. {``Distortions of
Memory.''} \emph{The Oxford Handbook of Memory}, 149--62.

\leavevmode\vadjust pre{\hypertarget{ref-Saxena2020}{}}%
Saxena, Shweta, and Manasi Gyanchandani. 2020. {``Machine Learning
Methods for Computer-Aided Breast Cancer Diagnosis Using Histopathology:
A Narrative Review.''} \emph{Journal of Medical Imaging and Radiation
Sciences} 51 (1): 182--93.

\leavevmode\vadjust pre{\hypertarget{ref-Talarico2003}{}}%
Talarico, Jennifer M., and David C. Rubin. 2003. {``Confidence, Not
Consistency, Characterizes Flashbulb Memories.''} \emph{Psychological
Science} 14 (5): 455--61. \url{https://doi.org/10.1111/1467-9280.02453}.

\leavevmode\vadjust pre{\hypertarget{ref-Voigt2017}{}}%
Voigt, Rob, Nicholas P. Camp, Vinodkumar Prabhakaran, William L.
Hamilton, Rebecca C. Hetey, Camilla M. Griffiths, David Jurgens, Dan
Jurafsky, and Jennifer L. Eberhardt. 2017. {``Language from Police Body
Camera Footage Shows Racial Disparities in Officer Respect.''}
\emph{Proceedings of the National Academy of Sciences} 114 (25):
6521--26.

\leavevmode\vadjust pre{\hypertarget{ref-R-rsyntax}{}}%
Welbers, Kasper, and Wouter van Atteveldt. 2022. \emph{Rsyntax: Extract
Semantic Relations from Text by Querying and Reshaping Syntax}.
\url{https://CRAN.R-project.org/package=rsyntax}.

\leavevmode\vadjust pre{\hypertarget{ref-Wickham2014a}{}}%
Wickham, Hadley. 2014. {``Tidy Data.''} \emph{Journal of Statistical
Software} 59 (10). \url{https://doi.org/10.18637/jss.v059.i10}.

\leavevmode\vadjust pre{\hypertarget{ref-R-rvest}{}}%
---------. 2022. \emph{Rvest: Easily Harvest (Scrape) Web Pages}.
\url{https://CRAN.R-project.org/package=rvest}.

\leavevmode\vadjust pre{\hypertarget{ref-Wulff2007}{}}%
Wulff, S, A Stefanowitsch, and Stefan Th. Gries. 2007. {``Brutal Brits
and Persuasive Americans.''} \emph{Aspects of Meaning}.

\end{CSLReferences}

\cleardoublepage
\phantomsection
\addcontentsline{toc}{part}{Appendices}
\appendix

\hypertarget{data-appendix}{%
\chapter{Data}\label{data-appendix}}

\ldots{}



\printindex

\end{document}
