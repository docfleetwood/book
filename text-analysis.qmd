---
execute: 
  echo: false
---

# Text analysis in context {#sec-text-analysis-in-context}

```{r}
#| label: setup-options
#| child: "_common.qmd"
#| cache: false
```

::: {.callout-tip title="Draft"}
Ready for review.
:::

<!--
- [ ] Revise second draft (~6k words) to make it more concise. Bring it within 5k words.
- [ ] Consider making the first two sections ('Making sense...' and 'Data Analysis') more concise/ collapse and language-centered (word frequency, grammatical frequency in written and spoken language?)
-->

> Everything about science is changing because of the impact of information technology and the data deluge. 
>
> --- Jim Gray

::: {.callout}
**{{< fa regular list-alt >}} Outcomes**

<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

- Understand the role and goals of data analysis both within and outside of academia.
- Describe the various approaches to quantitative language research.
- Identify the applications of text analysis in different contexts.
:::

<!-- [ ] Need to add more on how this chapter provides a foundation for the rest of the textbook. -->

In this chapter I will aim to introduce the topic of text analysis and provide the context needed to understand how text analysis fits in a larger universe of science and the ever-ubiquitous methods of data science, with attention to how linguistics and language-related studies employ data analysis down to the particular area of text analysis. 

::: {.callout}
**{{< fa terminal >}} Lessons**

<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ --> 

**What**: [Variables and vectors, Workspace](https://github.com/qtalr/lessons)\
**How**: In an R console  load `swirl`, run `swirl()`, and follow prompts to select the lesson.\
**Why**: To explore some key building blocks of the R programming language and to examine your local workspace in R and understand the relationship between your R workspace and the file system of your computing environment.
:::

## Making sense of a complex world {#text-making-sense-of-a-complex-world}

<!-- [ ] Collapse this and following main sectios  -->

### Heuristic Understanding

<!-- 
- Introduce the concept of heuristics and their role in simplifying complex information
- Discuss cognitive shortcuts and potential biases
-->

The world around us is full of actions and interactions so numerous that it is difficult to really comprehend. As each individual sees and experiences this world, we gain knowledge and build up heuristic understanding about how it works and how we can interact with it. This happens regardless of your educational background. As humans we are built for this. Our minds process countless sensory inputs. They underlie skills and abilities that we take for granted like being able to predict what will happen if you see someone about to knock a wine glass off a table and onto a concrete floor. You've never seen this object before and this is the first time you've been to this winery, but somehow and from somewhere you 'instinctively' make an effort to warn the would-be-glass-breaker before it is too late. You most likely have not stopped to consider where this predictive knowledge comes from, or if you have, you may have just chalked it up to 'common sense'. As common as it may be, it is an incredible display of the brain's capacity to monitor your environment, relate the events and observations that take place, and store that information all the time not making a big fuss to tell your conscious mind what it's up to.

So wait, this is a textbook on text analysis, right? So what does all this have to do with that? Well, there are two points to make that are relevant for framing our journey: (1) the world is constantly churning out data in real-time at a scale that is daunting and (2) for all the power of the brain that works so efficiently behind the scene making sense of the world, we are one individual living one life that has a limited view of the world at large. Let me expand on these two points a little more.

First let's be clear. There is no way for anyone to experience all things at all times. But even extremely reduced slices of reality are still vastly outside of our experiential capacity, at least in real-time. One can make the point that since the inception of the internet an individual's ability to experience larger slices of the world has increased. But could you imagine reading, watching, and listening to every file that is currently accessible on the web? Or has been? (See the [Wayback Machine](https://web.archive.org/).) Scale this down even further; let's take Wikipedia, the world's largest encyclopedia. Can you imagine reading every wiki entry? As large as a resource such as Wikipedia is [^text-analysis-1], it is still a small fragment of the written language that is produced on the web, just the web [^text-analysis-2]. Consider that for a moment.

[^text-analysis-1]: As of 22 July 2021, there are 6,341,359 articles in the [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) containing over 3.9 billion words occupying around 19 gigabytes of information.

[^text-analysis-2]: For reference, [Common Crawl](https://commoncrawl.org/big-picture/) has millions of gigabytes collected since 2008.

To my second framing point, which is actually two points in one. I underscored the efficiency of our brain's capacity to make sense of the world. That efficiency comes from some clever evolutionary twists that lead our brain to take in the world but it makes some shortcuts that compress the raw experience into heuristic understanding. What that means is that the brain is not a supercomputer. It does not store every experience in raw form, we do not have access to the records of our experience like we would imagine a computer would have access to the records logged in a database. Where our brains do excel is in making associations and predictions that help us (most of the time) navigate the complex world we inhabit. This point is key --our brains are doing some amazing work, but that work can give us the impression that we understand the world in more detail that we actually do. Let's do a little thought experiment. Close your eyes and think about the last time you saw your best friend. What were they wearing? Can you remember the colors? If your like me, or any other human, you probably will have a pretty confident feeling that you know the answers to these questions and there is a chance you a right. But it has been demonstrated in numerous experiments on human memory that our confidence does not correlate with accuracy [@Talarico2003; @Roediger2000]. You've experienced an event, but there is no real reason that we should bet our lives on what we experienced. It's a little bit scary, for sure, but the magic is that it works 'good enough' for practical purposes.

So here's the deal: as humans we are (1) clearly unable to experience large swaths of experience by the simple fact that we are individuals living individual lives and (2) the experiences we do live are not recorded in memory with perfect precision and therefore we cannot 'trust' our intuitions, at least not in an absolute sense.

::: {.callout}
**{{< fa regular lightbulb >}} Consider this**

How might your own experiences and biases influence your understanding of the world? 
What are some ways that you can mitigate these biases? Is ever possible to be completely objective? How might biases influence the way you approach text analysis?
:::

### Science to advance understanding

<!-- 
- Explain the scientific method and its importance in advancing human knowledge
- Describe the iterative nature of scientific inquiry
-->

What does that mean for our human curiosity about the world around us and our ability to reliably make sense of it? In short it means that we need to approach understanding our world with the tools of science. Science starts with a question, identifies and collects data, careful selected slices of the complex world, submits this data to analysis through clearly defined and reproducible procedures, and reports the results for others to evaluate. This process is repeated, modifying, and manipulating the procedures, asking new questions and positing new explanations, all in an effort to make inroads to bring the complex into tangible view.

In essence what science does is attempt to subvert our inherent limitations in understanding by drawing on carefully and purposefully collected slices of observable experience and letting the analysis of these observations speak, even if it goes against our intuitions (those powerful but sometime spurious heuristics that our brains use to make sense of the world).

## Data analysis

<!-- The science of data -->

### Emergence of data science

<!-- 
- Brief history of data science and its interdisciplinary roots
- Importance of data-driven decision making in various fields
-->

At this point I've sketched an outline strengths and limitations of humans' ability to make sense of the world and why science is used to address these limitations. This science I've described is the one you are familiar with and it has been an indispensable tool to make sense of the world. If you are like me, this description of science may be associated with visions of white coats, labs, and petri dishes. While science's foundation still stands strong in the 21st century, a series of intellectual and technological events mid-20th century set in motion changes that have changed aspects about how science is done, not why it is done. We could call this Science 2.0, but let's use the more popularized term \index{data science}**data science**. The recognized beginnings of data science are attributed to work in the "Statistics and Data Analysis Research" department at Bell Labs during the 1960s. Although primarily conceptual and theoretic at the time, a framework for quantitative data analysis took shape that would anticipate what would come: sizable datasets which would "\[...\] require advanced statistical and computational techniques \[...\] and the software to implement them." [@Chambers2020] This framework emphasized both the inference-based research of traditional science, but also embraced exploratory research and recognized the need to address practical considerations that would arise when working with and deriving insight from an abundance of machine-readable data.

Fast-forward to the 21st century a world in which machine-readable data is truly in abundance. With increased computing power, the emergence of the world wide web, and wide adoption of mobile devices electronic communication skyrocketed around the globe. To put this in perspective, in 2019 it was estimated that every minute 511 thousand tweets were posted, 18.1 million text messages were sent, and 188 million emails were sent [@DataNeverSleeps08-2021]. The data flood has not been limited to language, there are more sensors and recording devices than ever before which capture evermore swaths of the world we live in [@Desjardins2019]. Where increased computing power gave rise to the influx of data, it is also one of the primary methods for gathering, preparing, transforming, analyzing, and communicating insight derived from this data [@Donoho2017]. The vision laid out in the 1960s at Bell Labs had come to fruition.

### Computing skills, statistical knowledge, and domain knowledge

<!--  
- Explain the three key components of data science expertise
- Discuss the need for domain-specific knowledge in applied data analysis
-->

Data science is not predicated on data alone. Turning data into insight takes **computing skills** (i.e. programming), **statistical knowledge**, and **domain expertise**. This triad has been popularly represented as a Venn diagram such as in @fig-intro-data-science-venn.

```{r}
#| label: fig-intro-data-science-venn
#| fig-cap: 'Data Science Venn Diagram adapted from [Drew Conway](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram).'
#| fig-alt: 'A Venn diagram with three overlapping circles. The left circle is labeled "Computing" and the right circle is labeled "Statistics". The bottom circle is labeled "Domain". The intersection of all three circles is labeled "Data Science".'
#| out-width: 50%
#| echo: false

knitr::include_graphics("figures/ta-ds-venn.drawio.png")
```


The **computing skills** component of data science is the ability to write code to perform the data analysis process. This is the primary approach for working with data at scale. The **statistical knowledge** component of data science is the ability to apply statistical methods to data to derive insight. **Domain expertise** provides researchers insight at key junctures in the development of a research project and aid researchers in evaluating results.

This triad of skills in combination with reproducible research practices is the foundational toolbelt of data science [@Hicks2019]. **Reproducible research**\index{reproducible research} entails the use of computational tools to automate the process of data analysis. This automation is achieved by writing code that can be executed to replicate the data analysis. This code can then be shared through code sharing repositories, such as GitHub, where it can be viewed, downloaded, and executed by others. This adds transparency to the process and allows others to build on previous work. This is in contrast to traditional approaches where data analysis is performed (semi-)manually, results are reported in a static document such as a report or journal article, and the data analysis process is not shared. This approach is not reproducible because the data analysis process is not transparent and cannot be replicated. This is problematic because it is difficult to evaluate the results and build on previous work. Reproducible research practices are a key component of data science and are emphasized throughout this book.

### Applications of data science

Equipped with the data science toolbelt, the interest in deriving insight from the available data is now almost ubiquitous. The science of data has now reached deep into all aspects of life where making sense of the world is sought. Predicting whether a loan applicant will get a loan [@Bao2019], whether a lump is cancerous [@Saxena2020], what films to recommend based on your previous viewing history [@Gomez-Uribe2015], what players a sports team should sign [@Lewis2004] all now incorporate a common set of data analysis tools.

The data science toolbelt also underlies well-known public-facing language applications. From the language-capable chat applications, plagiarism detection software, machine translation algorithms, and search engines, tangible results of quantitative approaches to language are becoming standard fixtures in our lives, as seen in @fig-intro-language-applications.

<!-- Highly visible applications in language -->

```{r}
#| label: fig-intro-language-applications
#| fig-cap: 'Well-known public-facing language applications'
#| fig-alt: 'A list of well-known language applications which draw on the data science toolbelt. The list includes chat applications, plagiarism detection software, machine translation algorithms, and search engines.'
#| out-width: 100%
#| echo: false

knitr::include_graphics("figures/ta-lang-venn.drawio.png")
```

The spread of quantitative data analysis too has taken root in academia. Even in areas that on first blush don't appear readily approachable in a quantitative manner, such as fields in the social sciences and humanities, data science is making important and sometimes disciplinary changes to the way that academic research is conducted. This textbook focuses in on a domain that cuts across many of these fields; namely language. At this point let's turn to quantitative approaches to language analysis as we work closer to contextualizing text analysis.

## Language analysis

Language is a defining characteristic of our species. Since antiquity, language has attracted interest across disciplines and schools of thought. In the early 20th century, the development of the rigorous approach to study of language as a field in its own right took root [@Campbell2001], yet a plurality of theoretical views and methodological approaches remained. Contemporary linguistics bares this complex history and is far from theoretically and methodologically unified. 

Either based on the tenets of theoretical frameworks and/or the objects of study of particular fields, approaches to language research vary. On the one hand some language research commonly applies qualitative assessment of language structure and/ or use. **Qualitative approaches** describe and account for characteristics, or "qualities", that can be observed, but not measured (*e.g.* introspective methods, ethnographic methods, *etc.*)

On the other hand other language research programs employ quantitative research methods either out of necessity given the object of study (phonetics, psycholinguistics, *etc.*) or based on theoretical principles (Cognitive Linguistics, Connectionism, *etc.*). **Quantitative approaches** involve measurements of properties of language that can be observed and measured (*e.g.* frequency of use, reaction time, *etc.*).

These latter research areas and theoretical paradigms employ methods that share much of the common data analysis toolbox described in the previous section. In effect, this establishes a common methodological language between other language research fields but also with research outside of linguistics.

However, there is never a one-size-fits all approach to anything --much less data analysis. And even in quantitative language analysis there is a key methodological distinction that has downstream effects in terms of procedure but also in terms of interpretation. The key distinction that we need to make at this point, which will provide context for our introduction to quantitative text analysis, comes down to the approach to collecting language data and the nature of that data. This distinction is between **experimental data**\index{experimental data} and **observational data**\index{observational data}. 

Experimental approaches start with a intentionally designed hypothesis and lay out a research methodology with appropriate instruments and a plan to collect data that shows promise for shedding light on the validity of the hypothesis. Experimental approaches are conducted under controlled contexts, usually a lab environment, in which participants are recruited to perform a language related task with stimuli that have been carefully curated by researchers to elicit some aspect of language behavior of interest. Experimental approaches to language research are heavily influenced by procedures adapted from psychology. This link is logical as language is a central area of study in cognitive psychology. This approach looks  much like the white-coat science that we made reference to earlier but, as in most quantitative research, has now taken advantage of the data analysis toolbelt to collect and organize much larger quantities of data and conduct statistically more robust analysis procedures and communicate findings more efficiently.

Observational approaches are a bit more of a mixed bag in terms of the rationale for the study; they may either start with a testable hypothesis or in other cases may start with a more open-ended research question to explore. But a more fundamental distinction between the two is drawn in the amount of control the researcher has on contexts and conditions in which the language behavior data to be collected is produced. Observational approaches seek out records of language behavior that is produced by language speakers for communicative purposes in natural(istic) contexts. This may take place in labs (language development, language disorders, *etc.*), but more often than not, language is collected from sources where speakers are performing language as part of their daily lives --whether that be posting on social media, speaking on the telephone, making political speeches, writing class essays, reporting the latest news for a newspaper, or crafting the next novel destined to be a New York Times best-seller. What is more, data collected from the 'wild' varies more in structure relative to data collected in experimental approaches and requires a number of steps to prepare the data to sync up with the data analysis toolbelt.

I liken this distinction between experimental and observational data collection to the difference between farming and foraging. Experimental approaches are like farming; the groundwork for a research plan is designed, much as a field is prepared for seeding, then the researcher performs as series of tasks to produce data, just as a farmer waters and cares for the crops, the results of the process bear fruit, data in our case, and this data is harvested. Observational approaches are like foraging; the researcher scans the available environmental landscape for viable sources of data from all the naturally existing sources, these sources are assessed as to their usefulness and value to address the research question, the most viable is selected, and then the data is collected.

The data acquired from both of these approaches have their trade-offs, just as farming and foraging. Experimental approaches directly elicit language behavior in highly controlled conditions. This directness and level of control has the benefit of allowing researchers to precisely track how particular experimental conditions effect language behavior. As these conditions are an explicit part of the design and therefore the resulting language behavior can be more precisely attributed to the experimental manipulation. The primary shortcoming of experimental approaches is that there is a level of artificialness to this directness and control. Whether it is the language materials used in the task, the task itself, or the fact that the procedure takes place under supervision the language behavior elicited can diverge quite significantly from language behavior performed in natural communicative settings. Observational approaches show complementary strengths and shortcomings. 

Whereas experimental approaches may diverge from natural language use, observational approaches strive to identify and collected language behavior data in natural, uncontrolled, and unmonitored contexts, as seen in @fig-data-collection-methods. In this way observational approaches do not have to question to what extent the language behavior data is or is not performed as a natural communicative act. On the flipside, the contexts in which natural language communication take place are complex relative to experimental contexts. Language collected from natural contexts are nested within the complex workings of a complex world and as such inevitably include a host of factors and conditions which can prove challenging to disentangle from the language phenomenon of interest but must be addressed in order to draw reliable associations and conclusions.

```{r}
#| label: fig-data-collection-methods
#| fig-cap: "Trade-offs between experimental and observational data collection methods."

# Drawio image: trade-off hierarchy
knitr::include_graphics("figures/ta-data-collection-methods.drawio.png")
```

The upshot, then, is twofold: (1) data collection methods matter for research design and interpretation and (2) there is no single best approach to data collection, each have their strengths and shortcomings. In the ideal, a robust science of language will include insight from both experimental and observational approaches [@Gilquin2009]. And evermore there is greater appreciation for the complementary nature of experimental and observational approaches and a growing body of research which highlights this recognition. 

::: {.callout}
**{{< fa regular file-alt >}} Case study**
<!-- theoretical syntax -->

@Manning2003 discusses the use of probabilistic models in syntax to account for the variability in language usage and the presence of both hard and soft constraints in grammar. The paper touches on the statistical methods in text analysis, the importance of distinguishing between external and internal language, and the limitations of Generative Grammar. Overall, the paper suggests that usage-based and formal syntax can learn from each other to better understand language variation and change.
:::

Given their particular trade-offs observational data is often used as an exploratory starting point to help build insight and form predictions that can then be submitted to experimental conditions. In this way, studies based on observational data serve as an exploratory tool to gather a better and more externally valid view of language use which can then serve to make prediction that can be explored with more precision in an experimental paradigm. However, this is not always the case; observational data is also often used in hypothesis-testing contexts as well. And furthermore, some in some language-related fields, a hypothesis-testing is not the approach for deriving knowledge and insight.

## Text analysis

In a nutshell, **text analysis**\index{Text analysis} is the process of leveraging the data science toolbelt to derive insight from textual data collected through observational methods. In the next subsections, I will unpack this definition and discuss the primary components that make up text analysis including research appoaches and technical implementation, as well as practical applications.

### Approaches

Text analysis is a multifacited research methodology. It can be used use facilitate the qualitative exploration of smaller, human-digestable textual information, but is more often employed quantitatively to bring to the surface patterns and relationships in large samples of textual data that would be otherwise difficult, if not impossible, to identify manually.

Text being text, there are a series of **data prepration** steps that must be taken to ready the data for analysis. In addition to collecting the data, the data must be organized, cleaned, and transformed into a format that is amenable to statistical analysis. 

The statistical and evaluative approach employed in the analysis is dependent on the aim of the research. For research aimed at exploring and uncovering patterns and relationships in a data-driven manner, **Exploratory Data Analysis** (EDA) is employed. EDA combines descriptive statistics, visualizations, and statistical learning methods in an iterative and interactive way to provide the researcher the ability to identify patterns and relationships and to evaluate whether and why they are meaningful. 

**Predictive Data Analysis** (PDA), applied in research for outcome prediction, is a supervised machine learning task. It uses feature sets to predict an outcome variable. Its primary evaluation metric is the prediction accuracy on new data. However, for many text analysis tasks, human interpretation is crucial to provide context and assess the significance of the results.

Research aimed at explaining relationships between variables and the population from which the sample was drawn will adopt an **Inferential Data Analysis** (IDA) approach. IDA is a theory-driven process that employs statistical models for hypothesis testing. The extent to which the results can be confidently generalized to the population is the primary evaluation metric. 

As we see, text analysis can be used for a variety of purposes; from data-driven exploration and discovery to hypothesis testing and generalization.

### Implementation 

To ensure that the results of text analysis projects are replicable and transparent, programming strategies play an integral role at each stage of the implementation of a research project. While there are a number of programming languages that can be used for text analysis, R widely adopted in linguistics research. R is a free and open-source programming language that is specifically designed for statistical computing and graphics. It has a large and active community of users and developers, and a robust ecosystem of packages which make it a powerful and flexible language that is well-suited for core text analysis tasks: data collection, organization, transformation, analysis, and visualization. When combined with Quarto for literate programming and GitHub for version control and collaboration, R provides a robust and reproducible workflow for text analysis. 

### Applications
  
So what are some applications of text analysis? Most public facing applications stem from Computational Linguistic research, often known as **Natural Language Processing** (NLP) by practitioners. Whether it be using search engines, online translators, submitting your paper to plagiarism detection software, *etc.* many of the text analysis methods we will cover are at play. 

<!-- Public-facing text analysis applications (drawio) -->

::: {.callout}
**{{< fa regular lightbulb >}} Consider this**

What are some other public facing applications of text analysis that you are aware of?
:::

In academia the use of quantitative text analysis is even more widespread, despite the lack of public fanfare. In linguistics, text analysis is applied to a wide range of topics and research questions in both theoretical and applied subfields, as seen in @exm-linguistic-theory and @exm-linguistic-applied. 

::: {#exm-linguistic-theory}
Theoretical linguistics

- @Hay2002 use a corpus study to investigate the role of frequency and phonotatics in affix ordering in English.
- @Riehemann2001 explores the extent to which idiomatic expressions (*e.g.* 'raise hell') are lexical or syntactic units.
- @Bresnan2007a investigate the claim that possessed deverbal nouns in English (*e.g.* 'the city's destruction') are subject to a syntactic constraint that requires the possessor to be affected by the action denoted by the deverbal noun.
:::

::: {#exm-linguistic-applied}
Applied linguistics

<!-- Language variation/ dialectology -->
- @Wulff2007 explore differences between British and American English at the lexico-syntactic level in the *into*-causative construction (*e.g.* 'He tricked me into employing him.').
<!-- Language variation and change -->
- @Eisenstein2012 track the geographic spread of neologisms (*e.g.* 'bruh', 'af', '-__-') from city to city in the United States using Twitter data collected between 6/2009 and 5/2011.
<!-- L2 academic writing -->
- @Bychkovska2017 investigates possible differences between L1-English and L1-Chinese undergraduate students' use of lexical bundles, multiword sequences which are extended collocations (*e.g.* 'as the result of'), in argumentative essays.
<!-- Psycholinguistics -->
- @Jaeger2007 use a corpus study to investigate the phenomenon of syntactic persistence, the increased tendency for speakers to use a particular syntactic form over an alternate when the syntactic form has been recently processed.
<!-- Sociolinguistics -->
- @Voigt2017 explore potential racial disparities in officer respect in police body camera footage.
<!-- Translation studies -->
- @Olohan2008 investigate the extent to which translated texts differ from native texts do to 'explicitation'. 
:::

So too, text analysis is used in a variety of other fields where insight from language is sought, as seen in @exm-other-fields.

::: {#exm-other-fields}
Language-related fields

<!-- Psychology -->
- @Kloumann2012 explore the extent to which languages are positively, neutrally, or negatively biased.
<!-- Authorship attribution -->
- @Mosteller1963 provide a method for solving the authorship debate surrounding The Federalist papers.
<!-- Political science -->
- @Conway2012 investigate whether the established drop in language complexity of rhetoric in election seasons is associated with election outcomes.
:::


::: {.callout}
**{{< fa regular lightbulb >}} Consider this**

Language is a key component of human communication and interaction. What are some other areas of research in and outside linguistics that you think could be explored using text analysis methods?
:::

These studies in Examples [-@exm-linguistic-theory], [-@exm-linguistic-applied], and [-@exm-other-fields] are just a few illustrations of the contributions of text analysis as the primary method to gain a deeper understanding of language structure, function, variation, and acquisition. As a method, however, text analysis can also be used to support other research methods. For example, text analysis can be used collect data, generate authentic materials, provide linguistic annotation, to generate hypotheses, for either qualitative and/ or quantitative approaches. Together these efforts contribute to a more robust language science by incorporating externally valid data and providing methodological triangulation [@Francom2022].

In sum, the applications highlighted in this section underscore the versatility of text analysis as a research method. Whether it be in the public sphere or in academia, text analysis methods furnish a set of powerful tools for gaining insight from language data.

## Summary {.unnumbered}

In this chapter I started with some general observations about the difficulty of making sense of a complex world. The standard approach to overcoming inherent human limitations in sense making is science. In the 21st century the toolbelt for doing scientific research and exploration has grown in terms of the amount of data available, the statistical methods for analyzing the data, and the computational power to manage, store, and share the data, methods, and results from quantitative research. The methods and tools for deriving insight from data have made significant inroads in and outside academia, and increasingly figure in the quantitative investigation of language. Text analysis is a particular branch of this enterprise based on observational data from real-world language and is used in a wide variety of fields.

In the end I hope that you enjoy this exploration into text analysis. Although the learning curve at times may seem steep --the experience you will gain will not only improve your data literacy, research skills, and programmings skills but also enhance your appreciation for the richness of human language and its important role in our everyday lives.

<!-- This textbook aims to introduce you to methods and applications of using text as data. My goal is to inspire you to either employ quantitative text analysis in your research and/ or to raise your awareness of the advantages of text analysis for making sense of language-related and linguistic-based phenomenon. -->

## Actitivies {.unnumbered}

The following activities build on your introduction to R and Quarto in the previous chapter. In these activities you will uncover more features offered by Quarto which will enhance your ability to produce comprehensive reproducible research documents. You will apply the capabilities of Quarto in a practical context conveying the objectives and key discoveries from a primary research article.

::: {.callout}
**{{< fa regular file-code >}} Recipe**

<!-- Understand, apply, and analyze verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

**What**: [Academic writing with Quarto](https://qtalr.github.io/qtalrkit/articles/recipe-.html)\
**How**: Read Recipe 1 and participate in the Hypothes.is online social annotation.\
**Why**: To explore additional functionality in Quarto: numbered sections, table of contents, in-line citations and a document-final references list, and cross-referenced tables and figures.
:::

::: {.callout}
**{{< fa flask >}} Lab**

<!-- Analyze, evaluate, and create verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- [ ] update name of lab to something more lively -->

**What**: [Literate programming II](https://github.com/qtalr/lab-1)\
**How**: Clone, fork, and complete the steps in Lab 1.\
**Why**: To put into practice Quarto functionality to communicate the aim(s) and main finding(s) from a primary research article and to interpret a related plot.
:::

## Questions {.unnumbered}

<!-- Digital literacy, Research, Programming -->

::: {.callout}
**Conceptual questions**

- How has scientific research and exploration changed in the 21st century?
- What are the three basic skill sets that make up the data science toolbelt?
- What are the benefits of reproducible research in data science?
- Explain the trade-offs between experimental and observational data collection methods.
- What is text analysis and how is it used in various fields?
- Identify research in an area of interest in linguistics that has taken a quantitative approach to text analysis.
- In your own words, define literate programming?
- What are the benefits of literate programming?
- What are the benefits of using R and Quarto for literate programming?
:::

::: {.callout}
**Technical exercises**

- Create a literate programming document in Quarto. Edit the yaml header to reflect details of the work and add your work with the data types in R to code chunks. Add, commit, and push the project to GitHub.
- In the Quarto document, explore using R to create vectors and explore their properties.
- Explore the following resources and with the goal to identify a quantitative text analysis project. [Rpubs](https://rpubs.com/), [GitHub](https://github.com), [DataCamp](https://datacamp.com), [Kaggle](https://kaggle.com), [R-bloggers](https://r-bloggers.com).
- {{< fa wrench >}} ... more to come ...
:::

<!--
Other potential questions:

- What is the main function of the human brain in processing sensory inputs and making sense of the world?
- How does the vast amount of information available in the world present a challenge for individuals to fully comprehend and experience it?
- How does the human brain's ability to make associations and predictions help us navigate the complex world we inhabit?
- What are some limitations of the human brain's ability to understand and remember experiences and events?
- How does the use of heuristics in the brain's processing of information lead to potential biases and errors in our understanding of the world?
- How does the field of quantitative text analysis aim to address some of the challenges and limitations of individual understanding and experience in the world?
- What are some examples of the types of data and language phenomena that can be studied using quantitative text analysis?
- How can quantitative text analysis be used to uncover patterns and trends in large datasets of text data?
- What are some ethical considerations that need to be taken into account when using quantitative text analysis to study language and communication?

-->


<!-- RESOURCES

translation studies

@Olohan2008 investigate the extent to which translated texts differ from native texts. In particular the author explores the notion of explicitation in translated texts (the tendency to make information in the source text explicit in the target translation). The study makes use of the Translational English Corpus (TEC) for translation samples and comparable sections of the British National Corpus (BNC) for the native samples. The results suggest that there is a tendency for syntactic explicitation in the translational corpus (TEC) which is assumed to be a subconscious process employed unwittingly by translators.

L2 academic writing

@Bychkovska2017 investigates possible differences between L1-English and L1-Chinese undergraduate students' use of lexical bundles, multiword sequences which are extended collocations (i.e. as the result of), in argumentative essays. The authors used the Michigan Corpus of Upper-Level Student Papers (MICUSP) corpus using the argumentative essay section for L1-English and the Corpus of Ohio Learner and Teacher English (COLTE) for the L1-Chinese English essays. They found that L1-Chinese writers used more than 2 times as many bundle types than L1-English peers which they attribute to L1-Chinese writers attempt to avoid uncommon expressions and/or due to their lack of register awareness (conversation has more bundles than writing generally).

Language variation/ dialectology

@Wulff2007 explore differences between British and American English at the lexico-syntactic level in the *into*-causative construction (ex. 'He tricked me into employing him.'). The analysis uses newspaper text (The Guardian and LA Times) and the findings suggest that American English uses this construction in verbal persuasion verbs whereas British English uses physical force verbs.

language variation and change

@Eisenstein2012 track the geographic spread of neologisms from city to city in the United States using Twitter data collected between 6/2009 and 5/2011. They only used tweets with geolocation data and then associated each tweet with a zip code using the US Census. The most populous metropolitan areas were used. They also used the demographics from these areas to make associations between lexical innovations and demographic attributes. From this analysis they are able to reconstruct a network of linguistic influence. One of the main findings is that demographically-similar cities are more likely to share linguistic influence. At the individual level, there is a strong, potentially stronger role of demographics than geographical location.

sociolinguistics

@Voigt2017 explore potential racial disparities in officer respect in police body camera footage. The dataset is based on body camera footage from the Oakland Police Department during April 2014. At total of 981 stops by 245 different officers were included (black 682, white 299) and resulted in 36,738 officer utterances. The authors found evidence for racial disparities in respect but not formality of utterances, with less respectful language used with the black community members.

psycholinguistics

@Jaeger2007 use a corpus study to investigate the phenomenon of syntactic persistence, the increased tendency for speakers to use a particular syntactic form over an alternate when the syntactic form is recently processed. The authors attempt to distinguish between two competing explanations for the phenomenon: (1) transient activation, where the increased tendency is short-lived and time-bound and (2) implicit learning, where the increased tendency is a reflect of learning mechanisms. The use of a speech corpora (Switchboard and spoken BNC) were used to avoid the artificialness that typically occurs in experimental settings. The authors investigated the ditransitive alternation (NP PP/ NP NP), voice alternation (active/ passive), and complementizer/ relativizer omission. In these alternations structural bias was established by measuring the probability for a verb form to appear in one of the two syntactic forms. Then the probability that that form (target) would change given previous exposure to the alternative form (prime) was calculated; what the authors called surprisal. Distance between the prime structure and the target verb were considered in the analysis. In these alternations, the less common structure was used in the target more often when the when it corresponded to the prime form (higher surprisal) suggesting that implicit learning underlies syntactic persistence effects.

-->
