---
execute: 
  echo: false
---

# Understanding data {#sec-understanding-data}

```{r}
#| label: setup-options
#| child: "_common.qmd"
#| cache: false
```

<!--

Content:

- [x] Move the data sources section to the online companion website (qtalr.com) and link to it from here. Note: I will want to specify here and for all links that the LaTeX version should add links as footnotes and the HTML version should add links as hyperlinks.

Exercises:

Formatting:

-->

> The goal is to turn data into information, and information into insight.
>
> --- Carly Fiorina

::: {.callout}
**{{< fa regular list-alt >}} Outcomes**

- Analyze the distinct types of data and compare their differences.
- Define information and describe format it takes.
- Evaluate the importance of documentation in promoting reproducible research.
:::

```{r}
#| label: understanding-data-packages

pacman::p_load(tidyverse, knitr, jsonlite, tadr, qtalrkit, tidytext, readtext)
```

In this chapter, the groundwork is laid for deriving insights from data by focusing on Data and Information. The concepts of populations and samples are introduced, highlighting their similarities and key differences. Connecting these topics to text analysis, language samples or corpora are explored, discussing their types, sources, formats, and ethical considerations. Subsequently, key concepts in information, such as organization and transformation, are examined. The 'tidy' approach to organizing data is discussed, including structural characteristics and semantic attributes. Moreover, data transformation techniques are introduced, which can enhance the quality and usability of data in subsequent analysis steps. The importance of documentation in quantitative research is emphasized through addressing data origin and data dictionaries. Methods for documenting sources and processes are provided to ensure a comprehensive understanding of extracting valuable insights from data.

::: {.callout}
**{{< fa terminal >}} Swirl lesson**

**What**: [Objects, Packages and functions](https://github.com/qtalr/swirl)\
**How**: In the R Console pane load `swirl`, run `swirl()`, and follow prompts to select the lesson.\
**Why**: To introduce you to the main types of objects in R and to understand the role and use of functions and packages in R programming.
:::

## Data

Data is data, right? The term 'data' is so common in popular vernacular it is easy to assume we know what we mean when we say 'data'. But as in most things, where there are common assumptions there are important details the require more careful consideration. Let's turn to the first key distinction that we need to make to start to break down the term 'data': the difference between populations and samples.

<!-- When we talk about data, we often assume that it is objective and unbiased. However, this is not always the case. In fact, there are many important details about data that require more careful consideration. Let's explore one of these details: the difference between populations and samples -->

### Populations

<!--  
- Definition and examples
- Relation to research question
-->

The first thing that comes to many people's mind when the term population is used is human populations (derived from Latin 'populus'). Say for example we pose the question --What's the population of Milwuakee? When we speak of a population in these terms we are talking about the total sum of individuals living within the geographical boundaries of Milwaukee. In concrete terms, a \index{population}**population** an idealized set of objects or events in reality which share a common characteristic or belong to a specific category. The term to highlight here is idealized. Although we can look up the US Census report for Milwaukee and retrieve a figure for the population, this cannot truly be the population. Why is that? Well, whatever method that was used to derive this numerical figure was surely incomplete. If not incomplete, by the time someone recorded the figure some number of residents of Milwaukee moved out, moved in, were born, or passed away. In either case, this example serves to point out that populations are not fixed and are subject to change over time.

Likewise when we talk about populations in terms of language we dealing with an idealized aspect of reality. Let's take the words of the English language as an analog to our previous example population. In this case the words are the people and English is the grouping characteristic. Just as people, words move out, move in, are born, and pass away. Any compendium of the words of English at any moment is almost instananeously incomplete. This is true for all populations, save those relatively rare cases in which the grouping characteristics select a narrow slice of reality which is objectively measurable and whose membership is fixed (the complete works of Shakespeare, for example).

In sum, (most) populations are amorphous moving targets. We subjectively hold them to exist, but in practical terms we often cannot nail down the specifics of populations. So how do researchers go about studying populations if they are theoretically impossible to access directly? The strategy employed is called sampling.

### Samples

<!--  
- Definition and examples
- Sampling techniques (e.g., size, random, stratified)
-->

A \index{sample}**sample** is the product of a subjective process of selecting a finite set of observations from an idealized population with the goal of capturing the relevant characteristics of the target population. The **degree of representativeness** of a sample is the extent to which the sample reflects the characteristics of the population. The degree of representativeness is crucial form research as it directly impacts of any findings based on the sample. 

<!-- Although there are strategies to minimize the mismatch between the characteristics of the sample and population, it is important to note that it is almost certainly true that any given sample diverges from the population it aims to represent to some degree. The aim, however, is to employ a series of sampling decisions, which are collectively known as a sampling frame, that maximize the chance of representing the population. -->

To maximize the representativeness of a sample, researchers employ a variety of strategies. One of the first and sometimes the easiest strategy is to increase the \index{sample size}**sample size**. A larger sample will always be more representative than a smaller sample. Sample size, however, is often not enough. It is not hard to imagine a large sample which by chance captures only a subset of the features of the population. Another step to enhance sample representativeness is to apply **random sampling**. Together a large random sample has an even better chance of reflecting the main characteristics of the population better than a large or random sample. But, random as random is, we still run the risk of acquiring a skewed sample (*i.e.* a sample with low representativeness).

To help mitigate these issues, there are two more strategies that can be applied to improve sample representativeness. Note, however, that while size and random samples can be applied to any sample with little information about internal characteristics of the population, these next two strategies require decisions depend on the presumed internal characteristics of the population. The first of these more informed sampling strategies is called **stratified sampling**. Stratified samples make (educated) assumptions about sub-components within the population of interest. With these sub-populations in mind, large random samples are acquired for each sub-population, or strata. At a minimum, stratified samples can be no less representative than random sampling alone, but the chances that the sample is better increases. Can there be problems in the approach? Yes, and on two fronts. First knowledge of the internal components of a population are often based on a limited or incomplete knowledge of the population (remember populations are idealized). In other words, strata are selected subjectively by researchers using various heuristics some of which are based on some sense of 'common knowledge'. 

The second front on which stratified sampling can err concerns the relative sizes of the sub-components relative to the whole population. Even if the relevant sub-components are identified, their relative size adds another challenge which researchers must address in order to maximize the representativeness of a sample. To attempt to align, or **balance**, the relative sizes of the samples for the strata is the second population-informed sampling strategy.

### Corpora

A key feature of a sample is that it is purposely selected to model a target population. In text analysis, a purposely sampled collection of texts, of the type defined here, is known as a **corpus** (pl. corpora). A set of texts or documents which have not been selected purposely selected lack a **sampling frame** is not a corpus but rather a collection of documents. The sampling frame, and therefore the populations modeled, in any given corpus will vary. It is key to vet corpora to ensure that the resource's sampling frame and the research project's target populations align as closely as possible to safeguard the integrity of research findings later in the research process.

::: {.callout}
**{{< fa regular lightbulb >}} Consider this**

The 'Standard Sample of Present-Day American English' (known commonly as the Brown Corpus) is widely recognized as one of the first large, machine-readable corpora. Compiled by @Kucera1967, the corpus is comprised of 1,014,312 words from edited English prose published in the United States in 1961. 

Given the sampling frame for this corpus visualized in @fig-ud-brown-distribution:

```{r}
#| label: fig-ud-brown-distribution
#| fig-cap: "Overview of the sampling frame of the Brown Corpus."
#| fig-alt: "The Brown Corpus sampling frame is shown as a bar plot comprised of 15 main categories (strata) of text on the y-axis. The relative size of each category is shown as a percentage of the total number of words in the corpus on the x-axis. The largest categories are 'Fiction' (23%) and 'Press' (18%) and the smallest categories are 'Religion' (0.3%) and 'Learned' (0.2%)."
#| fig-pos: H
#| fig-align: left
#| out.width: 70%

brown <- readr::read_csv("data/understanding-data/brown.csv")

brown_categories_docs <-
  brown |>
  distinct(category, category_description, document_id) |>
  separate(category_description,
    into = c("main_category", "sub_category"),
    sep = ": ",
    fill = "right"
  )

brown_categories_docs |>
  count(main_category) |>
  ggplot(aes(y = reorder(main_category, n), x = n / sum(n))) +
  geom_col() +
  scale_x_continuous(labels = scales::percent) +
  labs(
    x = "Percentage of the corpus (balance)",
    y = "Main category (strata)",
    title = "Brown Corpus Sampling Frame"
  )
```

Can you determine what language population this corpus aims to represent?
What types of research might this corpus support or not support?
:::

#### Types

Let's take a look at some key characteristics, attributes, and features that distinguish corpora. 

##### Reference

The least common and most ambitious corpus resources are those which aim to model the characteristics of a language population. These are known as **reference corpora**. These are projects designed with wide sampling frames, and require significant investments of time in corpus design and implementation (and continued development) that are usually undertaken by research teams [@Adel2020].

The [American National Corpus (ANC)](https://www.anc.org/) or the [British National Corpus (BNC)](http://www.natcorp.ox.ac.uk/) are corpora which aim to model (represent/ reflect) the general characteristics of a variety of the English language, the former of American English and the later British English. Reference corpora exist for other languages as well: Spanish [Reference Corpus of Present-Day Spanish (CREA)](http://corpus.rae.es/creanet.html), German [The German Reference Corpus (DeReKo)](https://www.ids-mannheim.de/digspra/kl/projekte/korpora/), Turkish [Turkish National Corpus (TNC)](https://www.tnc.org.tr/), and many others. 


::: {.callout}
**{{< fa regular lightbulb >}} Consider this**

Of note is the fact that, at present, most of the world's languages lack reference corpus resources, or any corpus resources whatsoever. "Low-resourced" languages are often less studied, resource scarce, less available in born-digital formats, etc. [@Magueresse2020].

Visit the [Clarin overview](https://www.clarin.eu/resource-families/reference-corpora) on reference corpora and then visit [LRE Map](https://lremap.elra.info/). Can you find a reference corpus for a language you speak or are interested in studying? If not, consider what can be done to address this gap in the research community.
:::


##### Specialized

<!-- 
- Modality (e.g., written, spoken, signed)
- Genre (e.g., news, fiction, academic, social media)
- Population (age, region, language background, etc.)
- Comparision (e.g., parallel, comparable)
-->

**Specialized corpora** aim to represent more specific populations. The population may be defined either by modality, genre, time, location, or speaker-oriented characteristics, or some combination thereof. What specialized corpora lack in breadth of coverage, they make up for in depth of coverage by providing a more targeted representation of specific language populations.

The [Santa Barbara Corpus of Spoken American English (SBCSAE)](https://www.linguistics.ucsb.edu/research/santa-barbara-corpus), as you can imagine from the name of the resource, aims to model spoken American English. No claim to written English is included. There are even more specific types of corpora which attempt to model other types of sub-populations such as [academic writing](https://www.coventry.ac.uk/research/research-directories/current-projects/2015/british-academic-written-english-corpus-bawe/), [computer-mediated communication (CMC)](https://www.clarin.eu/resource-families/cmc-corpora), language use in specific [regions of the world](http://ice-corpora.net/ice/index.html), a [country](https://www.wgtn.ac.nz/lals/resources/corpora-default/corpora-wsc), a [region of a country](https://cesa.arizona.edu), *etc*.

<!-- <img src="figures/understanding-data/word-mapper.png" width="200" align="right"/>  -->

::: {.callout}
**{{< fa regular lightbulb >}} Consider this**

@Grieve2018 compiled a 8.9 billion-word corpus of geotagged posts from Twitter between 2013-2014 in the United States. The authors provide a [search interface](https://isogloss.shinyapps.io/isogloss/) to explore relationship between lexical usage and geographic location. Explore this corpus searching for terms related to slang ("hella", "wicked"), geographical ("mountain", "river"), meteorological ("snow", "rain"), and/ or any other term types. What types of patterns do you find? What are the benefits and/ or limitations of this type of data and/ or interface?
:::

Another set of specialized corpora are resources which aim to compile texts from different languages or different language varieties for direct or indirect comparison. Corpora that are directly comparable, that is they include source and translated texts, are called **parallel corpora**. Parallel corpora include different languages or language varieties that are indexed and aligned at some linguistic level (*i.e.* word, phrase, sentence, paragraph, or document), see [OPUS](https://opus.nlpl.eu/). Corpora that are compiled with different languages or language varieties but are not directly aligned are called **comparable corpora**. The comparable language or language varieties are sampled with the same or similar sampling frame, for example [Brown](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/0402) and [LOB](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/0167) corpora.

The aim of the quantitative text researcher is to select the corpus, or corpora, which best align with the purpose of the research. For example, a general corpus such as the American National Corpus may be better suited to address a question dealing with the way American English works, but this general resource may lack detail in certain areas, such as [medical language](https://mtsamples.com/index.asp), that may be vital for a research project aimed at understanding changes in medical terminology. Furthermore, a researcher studying spoken language might collect a corpus of transcribed conversations from a particular community or region, such as the SBCSAE. While this would not include every possible spoken utterance produced by members of that group, it could be considered a representative sample of the population of speech in that context.

#### Sources

<!--  
- Publicly available datasets and repositories
- Custom-built corpora (web scraping, APIs, OCR, etc.)
- Ethical considerations for data collection
-->

##### Published

The most common source of data used in contemporary quantitative research is the internet. On the web an investigator can access corpora published for research purposes. Many organizations exist around the globe that provide access to published corpora in browsable catalogs, or **repositories**. There are repositories dedicated to language research, in general, such as the [Language Data Consortium](https://www.ldc.upenn.edu/) or that specialize in specific domains, such as the spoken language repository [TalkBank](http://talkbank.org/). It is always advisable to start looking for the available language data in a repository. The advantage of beginning your data search in repositories is that a repository, especially those geared towards the linguistic community, will make identifying language corpora faster than through a general web search. Furthermore, repositories often require certain standards for corpus format and documentation for publication.

Repositories are by no means the only source of published corpora on the web. Researchers from around the world provide access to corpora and datasets on their own sites or through data sharing platforms. Corpora of various sizes and scopes will often be accessible on a dedicated homepage or appear on the homepage of a sponsoring institution. These resources may be available for download or via search inferaces. Finding these resources is often a matter of doing a web search with the word 'corpus' and a list of desired attributes, including language, modality, register, *etc*. 

As part of a general movement towards reproducibility, more corpora are available on **data sharing platforms** such as [GitHub](https://github.com/), [Zenodo](https://zenodo.org/), [Re3data](http://www.re3data.org/), [OSF](https://osf.io/), *etc*. These platforms enable researchers to securely store, manage, and share data with others. Support is provided for various types of data, including documents and code, and as such they are a good place to look as they often include reproducible research projects as well.

##### Custom-built {.unnumbered}

Language corpora prepared by researchers and research groups listed on repositories or hosted by the researchers themselves is often the first place to look for data. The web, however, contains a wealth of language and language-related data that can be accessed by researcher to compile their own corpus. There are two primary ways to attain language data from the web. The first is through the process of web scraping. **Web scraping** is the process of harvesting data from the public-facing web. Language texts may be found on sites as uploaded files, such as pdf or doc (Word) documents, or found displayed as the primary text of a site. Given the wide variety of documents uploaded and language behavior recorded daily on news sites, blogs and the like, compiling a corpus has never been easier. Having said that, how the data is structured and how much data needs to be retrieved can pose practical obstacles to collecting data from the web, particularly if the approach is to acquire the data by manually instead of automating the task.

The second way to acquire data from the web is through an **Application Programming Interface** (API). APIs are, as the title suggests, programming interfaces which allow access, under certain conditions, to information that a website or database accessible via the web contains.

::: {.callout}
**{{< fa medal >}} Dive deeper**

The process of corpus development is a topic in and of itself. For a more in-depth discussion of the process, see @Adel2020.
:::

::: {.callout}
**{{< fa regular lightbulb >}} Consider this**

<!--  
- [x] Add link to Guide 5 on companion website
-->
Explore some of the resources listed on the [qtalrkit compansion site](https://qtalr.github.io/qtalrkit/articles/guide-5.html) and consider their sampling frames. Can you think of a research question or questions that this resource may be well-suited to support research into? What types of questions would be less-than-adequate for a given resource?
:::

##### Ethical considerations

<!--  
- [x] Revise this section
  - [x] adding web scraping, APIs.
  - [x] adding examples of licenses, permissions, and citation/ attribution
-->

Just because data is available on the web does not mean it is free to use. Repositories, APIs, and individual data resources often have licensing agreements and terms of use, ranging from public domain to proprietary licenses. Public domain licenses, such as those found in Project Gutenberg, allow anyone to use the data for any purpose. [Creative Commons licenses](https://creativecommons.org/about/cclicenses/), like those used by the American National Corpus, Wikipedia, and TalkBank, span from public domain to more restrictive uses, including requirements for attribution or prohibiting commercial use. Even more restrictive licenses, such as those for the Corpus of Contemporary American English and the British National Corpus, may require a fee to access and use the data, even for research purposes.

Respecting intellectual property rights is crucial when working with corpus data. Violating these rights can lead to legal and ethical issues, including lawsuits, fines, and damage to one's professional reputation. To avoid these problems, researchers must ensure they have the necessary permissions to use copyrighted works in their corpora. Obtaining permissions involves contacting the author or publisher and requesting consent to use their work for research purposes. Documenting all obtained permissions and providing attribution and/ or citation is essential respecting the intellectual property rights of others.

#### Formats

Whether you are using a published corpus or developing your own, it is important to understand how the data you want to work with is formatted. When referring to the format of a corpus, this includes the folder and file structure, the file types, the internal structure of the files themselves, and how file content is encoded electronically. 

##### Folder and file structure

Some corpus resources are contained in a single file, such as a spreadsheet or a text file, but more often than not a corpus will be comprised of multiple files and folders. The folder and file structure will reflect the organization of the corpus and may include sub-folders for different types or groupings of data. In addition to the corpus data itself, metadata and documentation will often be included in the corpus folder structure. The corpus data may be grouped by language, modality, register, or other attributes such as types of linguistic annotation. 

To illustrate, in @exm-toy-corpus we have the file and folder structure of a toy corpus.

::: {#exm-toy-corpus}
**Toy corpus structure**

```{r}
#| label: toy-corpus-structurre
#| comment: ""
#| linewidth: .95

cat("corpus/
├── documentation/
│   ├── README.md
│   ├── LICENSE
├── metadata/
│   ├── speakers.csv
├── data/
│   ├── spoken/
│   │   ├── inter-09-a.xml
│   │   ├── inter-09-b.xml
│   │   ├── convo-09-a.xml
│   │   ├── ...
│   ├── written/
│   │   ├── essay-09-a.xml
│   │   ├── essay-09-b.xml
│   │   ├── respo-09-a.xml
│   │   ├── ...")
```
:::

In this example, we have a corpus folder with three sub-folders: *documentation/*, *metadata/*, and *data/*. The *data/* folder contains two sub-folders: *spoken/* and *written/*. Each folder contains the relevant data files. Where a single file is easy to download from the web, a corpus with a more complex folder structure can be more difficult to access. For that reason, many corpus resources are packaged into and made into a single compressed file. **File compression** has two benefits: it preserves the folder structure in a format which is contained in a single file and it also reduces the overall storage size. Common file compression formats are *.zip* and *.tar.gz*. So a compressed corpus file for the example above may be named something like *corpus.zip* or *corpus.tar.gz*. To access the original data within a compressed file, one must use a decompression tool or software to extract the contents after downloading it.

##### File types

In our toy corpus example, you may have noticed that each of the filenames appear with either *.md*, *.csv*, *.xml*, or nothing appended. These are examples of **file extensions**. File extensions a short sequence of characters, usually preceded by a period (*.*)  which are used to indicate the type or format of file. File extensions help both users and software programs to identify the content and purpose of a file.

::: {.callout}
**{{< fa exclamation-triangle >}} Warning**

If you are working on your own desktop computer, you may not see the file extensions. This is because the file explorer is configured to hide them by default. To see the file extensions, you will need to change the settings in your file explorer. Use a search engine to find instructions for your operating system.
:::

In addition to those listed above, other file extensions often encountered when working with data for text analysis include *.txt*, *.pdf*, *.docx*, *.xlsx*, *.json*, and *.html*. Common file extensions will often be associated with specific software programs on your computer, especially those which are directly associated with proprietary software such as *.docx* for Microsoft Word or *.xlsx* for Microsoft Excel. However, many file extensions are not directly associated with any specific software program and can be opened and edited with any text editor. 

It is important to note that file extensions are helpful conventions, but they are not a guarantee of the file type or structure of the file content. Furthermore, corpus developers may create their own file extensions to signal the unique structure of their data. For example, the *.utt* file extension used in the Switchboard Dialogue Act Corpus (SWDA) or the *.cha* extension used for TalkBank resource transcripts. In either case, it is recommended to open the file in a text editor to inspect the structure of the file content before processing the data contained therein.

##### File content

The internal structure of the content of corpus data files is an important aspect of any corpus both in terms of what data is included and how to approach accessing and processing the data. A corpus may include various types of linguistic (*e.g.* part of speech, syntactic structure, named entities, etc.) or non-linguistic (*e.g.* source, dates, speaker information, etc.) attributes. These attributes are known as **metadata**. As a general rule, files which include more metadata tend to be more internally structured. Internal file structure refers to the degree to which the content is easy to query and analyze by a computer. Let's review characteristics of the three main types of file structure types and associate common file extensions that files in each have.

**Unstructured data** is data which does not have a machine-readable internal structure. This is the case for plain text files (*.txt*), which are simply a sequence of characters. For example, in @exm-masc-text we see a snippet of a plain text file from the the Manually Annotated Sub-Corpus of American English (MASC) [@Ide2008]: 

::: {#exm-masc-text}
**MASC plain text**

```{r}
#| label: formats-masc-txt
#| comment: ""
#| linewidth: 0.95

readtext::readtext("data/understanding-data/formats_masc_txt.txt") |>
  pull(text) |>
  cat(fill = TRUE)
```
:::

Other examples of files which often contain unstructured data include *.pdf* and *.docx* files. While these file types may contain data which appears structured to the human eye, the structure is not designed to be machine-readable. As such the data would typically be read into R as a vector of **character strings**. It is possible to peform only the most rudimentary queries on this type of data, such as string matches. For anything more informative, it is necessary to further process this data. 

On the other end of the spectrum, **structured data** is data which conforms to a tabular format in which elements in tables and relationships between tables are defined. This makes querying and analyzing easy and efficient. Relational databases (*e.g.* MySQL, PostgreSQL, etc.) are designed to store and query structured data. The data frame object in R is also a structured data format. In each case, the data is stored in a tabular format in which each row represents a single observation and each column represents a single attribute whose values are of the same type.

In @exm-masc-df we see an example of an R data frame object which overlaps with the data in the plain text file above:

::: {#exm-masc-df}
**MASC data frame**

```{r}
#| label: formats-masc-df
#| linewidth: 0.95

masc_exm <- 
  readr::read_rds("data/understanding-data/formats_masc_df.rds")
masc_exm |> 
  slice(1:11) |> 
  kable()
```
:::

Here we see that the data is stored in a tabular format with each row representing a single observation (`word`) and each column representing a single attribute. Internally, R applies a schema to ensure the values in each column are of the same type (*e.g.* `<chr>`, `<dbl>`, `<fct>`, *etc.*). This structured format is designed to be easy to query and analyze and as such is the primary format for data analysis in R. 

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

It is conventional to work with column names for datasets in R using the same conventions that are used for naming objects. It is a matter of taste which convention is used, but I have adopted [snake case](https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/names.html#snake_case) as my personal preference. There are also [alternatives](https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/names.html). Regardless of the convention you choose, it is good practice to be consistent.

It is also of note that the column names should be balanced for meaningfulness and brevity. This brevity is of practical concern but can be somewhat opaque. For questions into the meaning of the column and is values consult the resource's dataset documentation, consult @sec-ud-documentation.
:::

**Semi-structured data** falls between unstructured and structured data. This covers a wide range of file structuring approaches. For example, a otherwise plain text file with part-of-speech tags appended to each word is minimally structured (@exm-exm-masc-pos). 

::: {#exm-masc-pos}
**MASC plain text with part-of-speech tags**

```{r}
#| label: formats-masc-pos
#| comment: ""
#| linewidth: 0.95

masc_exm |> 
  slice(1:50) |> 
  glue::glue_data("{word}/{pos}") |> 
  cat()
```
:::

Towards the more structured end, many file formats including *.xml* and *.json* contain highly structured, hierarchical data. For example, in @exm-masc-xml shows a snippet from a *.xml* file from the MASC corpus.

::: {#exm-masc-xml}
**MASC XML**

```{r}
#| label: formats-masc-xml
#| comment: ""
#| linewidth: 0.95

readr::read_lines("data/understanding-data/formats_masc_xml.txt") |> # read in lines
  _[1:17] |> # select lines 1-17
  cat(sep = "\n") # print with line breaks
```
:::

The format of semi-structured data is often influenced by characteristics of the data or reflect an author's individual preferences. It is sometimes the case that data will be semi-structured in a less-standard format. For example, the SWDA corpus includes a *.utt* file extension for files which contain utterances annotated with dialogue act tags.

::: {#exm-masc-xml}
**SWDA *.utt* file**

```{r}
#| label: formats-swda-utt
#| comment: ""
#| linewidth: 0.95

readr::read_lines("data/understanding-data/formats_swda_sample.txt") |> # read in lines
  _[20:30] |> # select lines 20-30
  cat(sep = "\n") # print with line breaks
```
:::

Whether standard or not, semi-structured data is often designed to be machine-readable. As with unstructured data, the ultimate goal is to convert the data into a structured format and augment the data where necessary to prepare it for a particular research analysis. 

##### File encoding

The last aspect to consider about corpus formats is **file encoding**. For a computer to display and process text characters, it must be encoded in a way that the computer can understand (*i.e.* 1's and 0's). Historically, character encoding schemes were developed to represent characters from specific character script sets (*e.g.* ASCII only includes characters from the English alphabet). However, as the need for a consistent and more inclusive way to encode characters from multiple languages and scripts became apparent, the Unicode standard, Unicode Transformation Format (UTF), was developed in the early 1990s. UTF encodings (UTF-8, UTF-16, and UTF-32) are now the most common way to encode text data and modern computers typically use them by default. Although other more script-specific encoding schemes can still be found in older data (e.g. ISO-8859, Windows-1252, Shift JIS). 

When working with corpus data, it is important to know if the encoding scheme used for the data is compatible with your computing environment's default (most likely UTF). If it is not, you will need to convert the data to a compatible encoding scheme. Rest assured, there is support in R for converting between different encoding schemes if the need arises.

## Information

<!--  
- [x] Add a summary of the Transformation section.
-->

Identifying an adequate corpus resource, in terms of content, licensing, and formatting, for the target research question is the first step in moving a quantitative text research project forward. The next step is to select the components or characteristics of this resource that are relevant for the research and then move to organize the attributes of this data into a more informative format. This is the process of converting corpus data into a \index{dataset}**dataset** --a tabular representation of particular attributes of the data as the basis for generating information. Once the data is in a dataset, it can be further manipulated and transformed adjusting and augmenting the data such that it better aligns with the research question and the analytical approach. 

### Organization {#sec-ud-organization}

<!-- 
- [x] Modify this section to remove redundancy with the previous section. 
-->

Data alone is not informative. Only through explicit organization of the data in a way that makes relationships and meaning explicit does data become information. In this form, our data is called a dataset. This is a particularly salient hurdle in text analysis research. Many textual sources are unstructured or semi-structured, that is relationships that will be used in the analysis have yet to be purposefully drawn and organized from the data.

#### Tidy Data {#sec-ud-tidy-data}

The selection of the attributes from a corpus and the juxtaposition of these attributes in a relational format, or dataset, that converts data into information is known as **data curation**. The process of data curation minimally involves creating a base dataset, or *curated dataset*, which establishes the main informational associations according to philosophical approach outlined by @Wickham2014a. 

In this work, a **tidy dataset** refers both to the structural (physical) and informational (semantic) organization of the dataset. Physically, a tidy dataset is a tabular data structure, illustrated in @fig-ud-tidy-format-image, where each *row* is an observation and each *column* is a variable that contains measures of a feature or attribute of each observation. Each cell where a given row-column intersect contains a *value* which is a particular attribute of a particular observation for the particular observation-feature pair also known as a *data point*.

```{r}
#| label: fig-ud-tidy-format-image
#| fig-cap: 'Visual summary of the tidy format.'

knitr::include_graphics("figures/understanding-data/ud-tidy.drawio.png")
```

In terms of semantics, columns and rows both contribute to the informational value of the dataset. Let's start with columns.  In a tidy dataset, each column is a variable, an attribute that can take on a number of values. Although variables vary in terms of values, they do not in type. A variable is of one and only one informational type. Statistically speaking, informational types are defined as **levels of measurement**, a classification system used to semantically distiguish between types of variables. There are four levels (or types) in this system: nominal, ordinal, interval, and ratio. In practice, however, researchers often group these levels into three main informational types: categorical, ordinal, and numeric [@Gries2021a]. What do these informational types represent? **Categorical data** is for labeled data or classes that answer the question "what?" **Ordinal data** is categorical data with rank order that answers the question "what order?" **Numeric data** is ordinal data with equal intervals between values that answers the question "how much or how many?"  

Let's look at a realistic example of a tidy dataset. Using the criteria just described, let's see if we can identify the informational values (categorical, ordinal, or numeric) of the variables that appear in a snippet from the MASC corpus in dataset form in @tbl-ud-info-values-masc.

```{r}
#| label: tbl-ud-info-values-masc
#| tbl-cap: "MASC dataset variables."

masc_exm |> 
  slice_head(n = 10) |> 
  mutate(num_letters = nchar(word)) |> 
  select(title, modality, date, ref_num, word, pos, num_letters) |> 
  kable()
```

We have seven variables listed as headers for each of the columns. We could go one-by-one left-to-right but let's take another tack. Instead, let's identify all those variables that cannot be numeric --these are all the non-numeral variables: `title`, `modality`, `word`, and `pos`. The question to ask of these variables is whether they represent an order or rank. Since titles, modalities, words, and parts-of-speech are not ordered values, they are all categorical. 

Now let's come back to `date`, `ref_num`, and `num_letters`. All three are numerals, so they could be numeric. But they could also be numeral representations of ordinal data. 

Before we can move forward, we need to make sure we understand what each variable means and how it is measured, or **operationalized**. The variable name and the values can be helpful in this respect. `date` is what it sounds like, a date, and is operationalized as  a year in the Gregorian calendar. And `num_letters` seems quite descriptive as well, number of letters, appearing as a letter count. But in some cases is may be opaque as to what is being measured by the variable name alone, for example `ref_num`,  and one will have to refer to the dataset documentation. In this case `ref_num` is a reference number operationalized as a unique identifier for each word per document in the corpus. This identifier is not random, but rather indexes the order in which each word appears in the original text as an integer. 

With this in mind, let's return to the question of whether `date`, `ref_num`, and `num_letters` are numeric or ordinal. Starting with the trickiest one, `date`, we can ask the question to identify numeric data: "how much or how many?". In the case of `date`, the answer is neither. A date is a point in time, not a quantity. So `date` is not numeric. But it does provide information about order. Hence, `date` is ordinal. `ref_num` is also ordinal because the question "what order?" can be asked of it. Finally, `num_letters` is numeric because it answers the question "how many?".

Let's turn to the second semantic value of a tidy dataset. In a tidy dataset, each row is an observation. But an observation of what? This depends on what the unit of observation is. That sounds circular, but its not. The **unit of observation** is simply the primary entity that is being observed. Without context, it can it can be identified in a dataset by looking at the level of specificity of the variable values and asking what each variable describes. When one variable appears to be the most individualized and other variables appear to describe that variable, then the most individualized variable is likely the unit of observation of the dataset, *i.e.* the meaning of each observation.

Applying these strategies to the Table in [-@tbl-ud-info-values-masc], we can see that each observation at its core is a word. We see that the values of each observation are the attributes of each word. `word` is the most individualized variable and the `pos` (part-of-speech), `num_letters`, and `ref_num` all describe the word. 

The other variables `title`, `modality`, and `date` are not direct attributes of the word. Instead, they are attributes of the document in which the word appears. Together, however, they all provide information about the word. 

As we will see, these row-based and column-based classifications are meaningful for appropriately choosing visualization types, statistical analysis methods, and evaluation techniques.

<!--

- [ ] Incorporate the role of unit of analysis and unit of observation into the discussion of dataset organization (under Information section).
  - [ ] What role does the unit of analysis and the unit of observation play in structuring the dataset?
      - Define unit of analysis: the levels of observation that are the focus of the analysis (e.x. speaker, language, text genre, etc.)
      - Define unit of observation: the types of data that is collected for each unit of analysis (e.x. text, utterances, n-grams, words, lemmas, morphemes, etc.)
  - [ ] The unit of analysis and the unit of observation are derived from the research question or hypothesis.
    - [ ] Add examples of research questions and hypotheses that would lead to different units of analysis and units of observation. Such as "A what rate does lexical diversity increase for second language learners of Spanish whose first language is English?" Unit of analysis: learners. Unit of observation: words at distinct levels. 


---- 

NOTES:

The *unit of analysis refers to the entity that is studied and analyzed* in a research study or corpus linguistics, while the *unit of observation refers to the specific item or element that is actually observed or measured* within the selected unit of analysis. In simpler terms, the unit of analysis is the larger context while the unit of observation is the subset within that context.

Examples of primary unit of analysis:

* Words: The most common unit of analysis in corpus linguistics is the word. Researchers often examine the use of different types of words, such as nouns or verbs, or the frequency of specific individual words.

* Phrases: Phrases can also be a useful unit of analysis in corpus linguistics. For example, researchers might examine the frequency of certain collocations or the use of particular grammatical structures.

* Sentences: Sentences can provide valuable insights into how language is used, particularly in terms of syntax and grammar. Researchers might analyze sentence length or the use of specific sentence structures.

* Discourse: Discourse refers to larger units of language use, such as conversations, speeches, or written texts. Researchers might analyze patterns in discourse, such as how speakers use language to convey social status or power.

* Speakers/Writers: Another important unit of analysis in corpus linguistics is the individual speaker or writer. Researchers might analyze differences between male and female speakers, regional dialects, or the language use of different age groups.

* Language and language varieties: Researchers might also analyze differences between languages or language varieties. For example, researchers might compare the use of different types of words in English and Spanish, or the use of different grammatical structures in British and American English.

Examples of primary unit of observation:

* Tokens
* Types
* Lemmas
* Part-of-speech tags
* Sentences
* Paragraphs
* Documents
* Speakers
* Languages
* Genres
* Years/time periods
* Social media posts
* Comments/reviews
* Text messages


Note that the unit of observation is the entity or object on which data is collected or measured and there can be multiple units of observation for a given unit of analysis. 

-->

It is important to make clear that data in tabular format in itself does not constitute a dataset, in the tidy sense we will be using. Data can be organized in many ways which do not make relationships between variables and observations explicit.

<!-- *Consider adding some 'messy' data and/ or summary tables which do not reflect the relational structure we are aiming to create to base our research on.* -->

::: {.callout}
**{{< fa regular lightbulb >}} Consider this**

All tabular data does not have the 'tidy' format that I have described here. Can you think of examples of tabular information that would not be in a tidy format? What would be the implications of this for data analysis?
:::

### Transformation

At this point have introduced the first step creating a dataset ready for analysis, data curation. However, a curated dataset is rarely the final organizational step before proceeding to statistical analysis. Many times, if not always, the curated dataset requires **data transformation** to derive or generate new data for the dataset to align with the statistical analysis to be performed.

Common types of transformation include cleaning variables (normalization), separating or aggregating variables (recoding), creating new variables (generation), or incorporating others datasets which integrate with the existing variables (merging). The results of these transformations build on and manipulate the curated dataset and produce *derived dataset*. Let's discuss these transformation in turn.

<!--
- [ ] Highlight the changes in the datasets visually so that it is easier for the reader to see the changes.
-->

#### Normalization

The process of normalization aims to *sanitize* the values within a variable or set of variables. This may include removing whitespace, punctuation, numerals, or special characters or substituting uppercase for lowercase characters, numerals for word versions, acronyms for their full forms, irregular or incorrect spelling for accepted forms, or removing common words (**stopwords**), *etc*.


- [ ] Pick up here on Monday (2023-06-19) ...


<!-- - Remove non-speech -->

On inspecting the Europarle dataset (@tbl-structure-europarle) we will see that there are sentence lines which do not represent actual parliament speeches. In @tbl-normalize-non-speech-identify-europarle we see these lines.

```{r}
#| label: tbl-normalize-non-speech-identify-europarle
#| tbl-cap: 'Non-speech lines in the Europarle dataset.'
#| eval: false
# europarle_example: identify non-speech
europarle_example |>
  dplyr::filter(str_detect(sentence, "^\\(") | str_detect(sentence, "[^.]$")) |>
  kable(booktabs = TRUE)
```
  

A research project aiming to analyze speech would want to normalize this dataset removing these lines, as seen in @tbl-normalize-non-speech-remove-europarle.

```{r}
#| label: tbl-normalize-non-speech-remove-europarle
#| tbl-cap: 'The Europarle dataset with non-speech lines removed.'
#| eval: false
# europarle_example: remove non-speech
europarle_example_normalized <-
  europarle_example |>
  dplyr::filter(!str_detect(sentence, "^\\(") &
    !str_detect(sentence, "[^.]$"))

europarle_example_normalized |>
  kable(booktabs = TRUE)
```
  
<!-- - Remove whitespace (possessives) -->

Another feature of this dataset which may require attention is the fact that the English lines include whitespace between possessive nouns.

```{r}
#| label: tbl-normalize-whitespace-identify-europarle
#| tbl-cap: 'Lines with possessives with extra whitespace in the Europarle dataset.'
#| eval: false
# Europarle: identify whitespace
europarle_example_normalized |>
  dplyr::filter(str_detect(sentence, "'\\ss")) |>
  kable(booktabs = TRUE)
```
  
This may affect another transformation process or subsequent analysis, so it may be a good idea to normalize these forms by removing the extra whitespace.

```{r }
#| label: tbl-normalize-whitespace-remove-europarle
#| tbl-cap: 'The Europarle dataset with whitespace from possessives removed.'
#| eval: false
# Europarle: remove whitespace
europarle_example_normalized <-
  europarle_example_normalized |>
  mutate(sentence = str_replace_all(sentence, "'\\ss", "'s"))

europarle_example_normalized |>
  dplyr::filter(str_detect(sentence, "'s")) |>
  kable(booktabs = TRUE)
```
  
<!-- - lowercase text -->

A final normalization case scenario involves changing converting all the text to lowercase. If the goal for the research is to count words at some point the fact that a word starts a sentence and by convention the first letter is capitalized will result distinct counts for words that are in essence the same (*i.e.* "In" vs. "in").

```{r}
#| label: tbl-normalize-lowercase-europarle
#| tbl-cap: 'The Europarle dataset with lowercasing applied.'
#| eval: false
# europarle: lowercase
europarle_example_normalized |>
  mutate(sentence = str_to_lower(sentence)) |>
  kable(booktabs = TRUE)
```
  
Note that lowercasing text, and normalization steps in general, can come at a cost. For example, lowercasing the Europarle dataset sentences means we lose potentially valuable information; namely the ability to identify proper names (*i.e.* "Mr Kumar Ponnambalam") and titles (*i.e.* "European Parliament") directly from the orthographic forms. There are, however, transformation steps that can be applied which aim to recover 'lost' information in situations such as this and others.

#### Recoding

The process of recoding aims to *recast* the values of a variable or set of variables to a new variable or set of variables to enable more direct access. This may include extracting values from a variable, stemming or lemmatization of words, tokenization of linguistic forms (words, ngrams, sentences, *etc*.), calculating the lengths of linguistic units, removing variables that will not be used in the analysis, *etc*.

<!-- - Stemming/ lemmatization  -->

Words that we intuitively associate with a 'base' word can take many forms in language use. For example the word forms 'investigation', 'investigation', 'investigate', 'investigated', *etc*. are intuitively linked. There are two common methods that can be applied to create a new variable to facilitate the identification of these associations. The first is stemming. **Stemming** is a rule-based heuristic to reduce word forms to their stem or root form.

```{r}
#| label: tbl-recoding-stemming-brown-example
#| tbl-cap: 'Results for stemming the first words in the Brown Corpus.'
#| eval: false
# Stemming
brown_example |>
  mutate(word_stems = SnowballC::wordStem(words)) |>
  kable(booktabs = TRUE)
```
  
A few things to note here. First there are a number of stemming algorithms both for individual languages and distinct languages [^understanding-data-1]. Second not all words can be stemmed as they do not have alternate morphological forms (*i.e.* "The", "of", *etc*.). This generally related to the distinction between closed-class (articles, prepositions, conjunctions, *etc*.) and open-class (nouns, verbs, adjectives, *etc*.) grammatical categories. Third the stem generated for those words that can be stemmed result in forms that are not words themselves. Nonetheless, stems can be very useful for more easily extracting a set of related word forms.

[^understanding-data-1]: https://snowballstem.org/algorithms/

As an example, let's identify all the word forms for the stem 'investig'.

```{r}
#| label: tbl-recoding-stemming-brown-search
#| tbl-cap: 'Results for filter word stems for "investig" in the Brown Corpus.'
#| eval: false
# Stemming
brown |>
  select(-category_description) |>
  mutate(word_stems = SnowballC::wordStem(words)) |>
  dplyr::filter(word_stems == "investig") |>
  slice_head(n = 10) |>
  kable(booktabs = TRUE)
```
  
We can see from the results in @tbl-recoding-stemming-brown-search that searching for `word_stems` that match 'investig' returns a set of stem-related forms. But it is worth noting that these forms cut across a number of grammatical categories. If instead you want to draw a distinction between grammatical categories, we can apply **lemmatization.** This process is distinct from stemming in two important ways: (1) inflectional forms are grouped by grammatical category and (2) the resulting forms are lemmas or 'base' forms of words.

```{r}
#| label: tbl-recoding-lemmatization-brown-example
#| tbl-cap: 'Results for lemmatization of the first words in the Brown Corpus.'
#| eval: false
# Lemmatization
brown_example |>
  mutate(word_lemmas = textstem::lemmatize_words(words)) |>
  kable(booktabs = TRUE)
```
  
To appreciate the difference between stemming and lemmatization, let's compare a filter for `word_lemmas` which match 'investigation'.

```{r}
#| label: tbl-recoding-lemmatization-brown-investigation
#| tbl-cap: 'Results for filter word stems for "investigation" in the Brown Corpus.'
#| eval: false
# Lemmatization: investigation
brown |>
  select(-category_description) |>
  mutate(word_lemmas = textstem::lemmatize_words(words)) |>
  dplyr::filter(word_lemmas == "investigation") |>
  slice_head(n = 10) |>
  kable(booktabs = TRUE)
```
  
Only lemma forms of 'investigate' which are nouns appear. Let's run a similar search but for the lemma 'be'.

```{r}
#| label: tbl-recoding-lemmatization-brown-be
#| tbl-cap: 'Results for filter word stems for "be" in the Brown Corpus.'
#| eval: false
# Lemmatization: be
brown |>
  select(-category_description) |>
  mutate(word_lemmas = textstem::lemmatize_words(words)) |>
  dplyr::filter(word_lemmas == "be") |>
  slice_head(n = 10) |>
  kable(booktabs = TRUE)
```
  
Again only words of the same grammatical category are returned. In this case the verb 'be' has many more inflectional forms than 'investigate'.

<!-- - Match and extract -->

Another form of recoding is to detect a pattern in the values of an existing variable and create a new variable whose values are the extracted pattern or register that the pattern occurs and/ or how many times it occurs. As an example, let's count the number of disfluencies ('uh' or 'um') that occur in each utterance in `utterance_text` from the Switchboard Dialog Act Corpus. *Note I've simplified the dataset dropping the non-relevant variables for this example.*

```{r}
#| label: tbl-recoding-extract-switchboard
#| tbl-cap: 'Disfluency counts in the first 10 utterance text values from the Switchboard Corpus.'
#| eval: false
# Extraction:
swda_disfluencies <-
  swda |>
  mutate(disfluency_count = str_count(utterance_text, "\\{F"))

swda_disfluencies |>
  select(utterance_text, disfluency_count) |>
  slice_head(n = 10) |>
  kable(booktabs = TRUE)
```
  
<!-- - Tokenization -->

One of the most common forms of recoding in text analysis is tokenization. **Tokenization** is the process of recasting the text into smaller linguistic units. When working with text that has not been linguistically annotated, the most feasible linguistic tokens are words, ngrams, and sentences. While word and sentence tokens are easily understandable, ngram tokens need some explanation. An **ngram** is a sequence of either characters or words where *n* is the length of this sequence. The ngram sequences are drawn incrementally, so the bigrams (two-word sequences) for the sentence "This is an input sentence." are:

`r tokenizers::tokenize_ngrams(c("This is an input sentence."), n = 2) |>unlist() |>str_c(collapse = ", ")`

We've already seen word tokenization exemplified with the Europarle Corpus in subsection [Structure] in @tbl-tidy-words-europarle, so let's create (word) bigram tokens for this corpus.

```{r}
#| label: tbl-recoding-tokenization-europarle-bigram-words
#| tbl-cap: 'The first 10 word bigrams of the Europarle Corpus.'
#| eval: false
# Tokenization: bigram
europarle_example_normalized |>
  unnest_tokens(word_bigrams, sentence, token = "ngrams", n = 2) |>
  slice_head(n = 10) |>
  kable(booktabs = TRUE)
```
  
As I just mentioned, ngrams sequences can be formed of characters as well. Here are character trigram (three-character) sequences.

```{r}
#| label: tbl-recoding-tokenization-europarle-trigram-chars
#| tbl-cap: 'The first 10 character trigrams of the Europarle Corpus.'
#| eval: false
# Tokenization: bigram
europarle_example_normalized |>
  unnest_tokens(char_trigrams, sentence, token = "character_shingles", n = 3) |>
  slice_head(n = 10) |>
  kable(booktabs = TRUE)
```
  

#### Generation

The process of generation aims to *augment* a variable or set of variables. In essence this aims to make implicit attributes explicit to that they are directly accessible. This often targeted at the automatic generation of linguistic annotations such as grammatical category (part-of-speech) or syntactic structure.

<!-- - Linguistic annotation (part of speech, syntactic structure) -->

```{r}
#| label: functions-load-udpipe-model
#| eval: false
load_model_udpipe <- function(model_lang) {
  cleanNLP::cnlp_init_udpipe(model_lang) # to download the model, if not downloaded
  base_path <- system.file("extdata", package = "cleanNLP") # get the base path
  model_name <- # extract the model_name
    base_path |> # extract the base path
    dir() |>
    stringr::str_subset(pattern = paste0("^", model_lang))

  model <-
    udpipe::udpipe_load_model(file = file.path(base_path, model_name, fsep = "/"))

  return(model)
}
```

In the examples below I've added linguistic annotation to a target (English) and source (Spanish) example sentence from the Europarle Parallel Corpus. First, note the variables that are added to our dataset that correspond to grammatical category. In addition to the `type` and `sentence_id` we have an assortment of variables which replace the `sentence` variable. As part of the process of annotation the input text to be annotated `sentence` is tokenized `token` and indexed `token_id`. Then `upos` contains the Universal Part of Speech tags[^understanding-data-2], and a detailed list of features is included in `feats`. The syntactic annotation is reflected in the `token_id_source` and `syntactic_relation` variables. These variables correspond to the type of syntactic parsing that has been done, in this case Dependency Parsing (using the [Universal Dependencies](https://universaldependencies.org/) framework). Another common syntactic parsing framework is phrase constituency parsing [@Jurafsky2020].

[^understanding-data-2]: [Descriptions of the UPOS tagset](https://universaldependencies.org/u/pos/)

```{r}
#| label: tbl-generation-europarle-en-example
#| tbl-cap: 'Automatic linguistic annotation for grammatical category and syntactic structure for an example English sentence from the Europarle Corpus.'
#| eval: false
eng_model <- load_model_udpipe("english")

annotation <-
  europarle_example_normalized |>
  dplyr::filter(type == "Target" & sentence_id == 6) |>
  cleanNLP::cnlp_annotate(text_name = "sentence", doc_name = "sentence_id")

europarle_en_annotation <-
  left_join(annotation$document, annotation$token) |>
  select(type, sentence_id = doc_id, token_id = tid, token, upos, feats, token_id_source = tid_source, syntactic_relation = relation)

europarle_en_annotation |>
  kable(booktabs = TRUE)
```
  
Now compare the English example sentence dataset in @tbl-generation-europarle-en-example with the parallel sentence in Spanish. Note that the grammatical features are language specific. For example, Spanish has gender which is apparent when scanning the `feats` variable.

```{r}
#| label: tbl-generation-europarle-es-example
#| tbl-cap: 'Automatic linguistic annotation for grammatical category and syntactic structure for an example Spanish sentence from the Europarle Corpus.'
#| eval: false
spa_model <- load_model_udpipe("spanish")

annotation <-
  europarle_example_normalized |>
  dplyr::filter(type == "Source" & sentence_id == 6) |>
  cleanNLP::cnlp_annotate(text_name = "sentence", doc_name = "sentence_id")

europarle_es_annotation <-
  left_join(annotation$document, annotation$token) |>
  select(type, sentence_id = doc_id, token_id = tid, token, upos, feats, token_id_source = tid_source, syntactic_relation = relation)

europarle_es_annotation |>
  kable(booktabs = TRUE)
```
  
There is much more to explore with linguistic annotation, and syntactic parsing in particular, but at this point it will suffice to note that it is possible to augment a dataset with grammatical information automatically.

There are strengths and shortcomings with automatic linguistic annotation that a research should be aware of. First, automatic linguistic annotation provides quick access to rich and highly reliable linguistic information for a large number of languages. However, part-of-speech taggers and syntactic parsers are not magic. They are resources that are built by training a computational algorithm to recognize patterns in manually annotated datasets producing a language model. This model is then used to predict the linguistic annotations for new language (as we just did in the previous examples). The shortcomings of automatic linguistic annotation is first, not all languages have trained language models and second, the data used to train the model inevitably reflect a particular variety, register, modality, *etc*. The accuracy of the linguistic annotation is highly dependent on alignment between the language sampling frame of the trained data and the language data to be automatically annotated. Many (most) of the language models available for automatic linguistic annotation are based on language that is most readily available and for most languages this has traditionally been newswire text. It is important to be aware of these characteristics when using linguistic annotation tools.

<!-- *Consider adding 'creating measures' here* -->

#### Merging

The process of merging aims to *join* a variable or set of variables with another variable or set of variables from another dataset. The option to merge two (or more) datasets requires that there is a shared variable that indexes and aligns the datasets.

To provide an example let's look at the Switchboard Diaglog Act Corpus. Our existing, disfluency recoded, version includes the following variables.

<!-- - Join other information with the dataset -->

```{r}
#| label: merging-swda-disfluencies
#| eval: false
swda_disfluencies |>
  slice_head(n = 5) |>
  select(doc_id, speaker_id, topic_num:utterance_text, disfluency_count) |>
  glimpse()
```
  
It turns out that on the [corpus website](https://catalog.ldc.upenn.edu/docs/LDC97S62/) a number of meta-data files are available, including files pertaining to speakers and the topics of the conversations.

The speaker meta-data for this corpus is in a the `caller_tab.csv` file and contains a `speaker_id` variable which corresponds to each speaker in the corpus and other potentially relevant variables for a language research project including `sex`, `birth_year`, `dialect_area`, and `education`.

```{r}
#| label: tbl-merging-swda-speaker
#| tbl-cap: 'Speaker meta-data for the Switchboard Dialog Act Corpus.'
#| eval: false
swda |>
  slice_head(n = 5) |>
  select(speaker_id:education) |>
  kable(booktabs = TRUE)
```
  
Since both datasets contain a shared index, `speaker_id` we can merge these two datasets. The result is found in @tbl-merging-swda-speaker-added.

```{r}
#| label: tbl-merging-swda-speaker-added
#| tbl-cap: 'Merged conversations and speaker meta-data for the Switchboard Dialog Act Corpus.'
#| eval: false
swda_disfluencies |>
  slice_head(n = 5) |>
  select(doc_id, speaker_id:education, topic_num:utterance_text, disfluency_count) |>
  kable(booktabs = TRUE)
```
  
In this example case the dataset that was merged was already in a structured format (.csv). Many corpus resources contain meta-data in stand-off files that are structured.

In some cases a researcher would like to merge information that does not already accompany the corpus resource. This is possible as long as a dataset can be created that contains a variable that is shared. Without a shared variable to index the datasets the merge cannot take place.

In sum, the transformation steps described here collectively aim to produce higher quality datasets that are relevant in content and structure to submit to analysis. The process may include one or more of the previous transformations but is rarely linear and is most often iterative. It is typical to do some normalization then generation, then recoding, and then return to normalizing, and so forth. This process is highly idiosyncratic given the characteristics of the derived dataset and the ultimate goals for the analysis dataset.

::: callout-warning
## Tip

Note in some cases we may convert our tidy tabular dataset to other data formats that may be required for some particular statistic approaches but at all times the relationship between the variables should be maintained in line with our research purpose. We will touch on examples of other types of data formats (*e.g.* Corpus and Document-Term Matrix (DTM) objects in R) when we dive into particular statistical approaches that require them later in the textbook.
:::

## Documentation {#sec-ud-documentation}

### Data origin information

<!--  
- [ ] Include data origin information: source, date, version, license, permissions, attribution, etc.
- [ ] Emphasize 'read-only' nature of acquired corpus data/ datasets
-->

### Data dictionaries

<!--  
- [ ] Point out curation and transformation steps are documented in code and in prose (README.md, or other?)
- [ ] Add citation for data dictionary @HowMakeData10-2021 (from OSF)
-->

As we have seen in this chapter that acquiring data and converting that data into information involves a number of conscious decisions and implementation steps. As a favor to ourselves as researchers and to the research community, it is crucial to document these decisions and steps. This makes it both possible to retrace our own steps and also provides a guide for future researchers that want to reproduce and/ or build on your research. A programmatic approach to quantitative research helps ensure that the implementation steps are documented and reproducible but it is also vital that the decisions that are made are documented as well. This includes the creation/ selection of the corpus data, the description of the variables chosen from the corpus for the derived dataset, and the description of the variables created from the derived dataset for the analysis dataset.

For an existing corpus sample acquired from a repository (*e.g.* [Switchboard Dialog Act Corpus](https://catalog.ldc.upenn.edu/LDC97S62), Language Data Consortium), a research group (*e.g.* [CEDEL2](http://cedel2.learnercorpora.com/)), or an individual researcher (*e.g.* [SMS Spam Collection](https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/)), there is often documentation provided describing key attributes of the resource. This documentation should be included with the acquisition of the corpus and added to the research project. For a corpus that a researcher compiles themselves, they will need to generate this documentation.

The curation and transformation steps conducted on the original corpus data to produce the datasets should also be documented. The steps themselves can be included in the programming scripts as code comments (or in prose if using a literate programming strategy (*e.g.* R Markdown)). The structure of each resulting dataset should include what is called a **data dictionary**. This is a table which includes the variable names, the values they contain, and a short prose description of each variable (*e.g.* [ACTIV-ES Corpus](https://osf.io/9jafz/)).

<!-- 
- Selecting or developing a corpus
  - if you develop a corpus you should document your procedure, including the sampling frame
  - by the same token, selecting a corpus requires that you consult the corpus documentation and vet the resources for its potential to be a viable research resource
  - include the __corpus documentation__
  
- Depending on the original structure of a corpus and the goals of the research, the data will need human intervention to create a tabular structure which provides the base physical form which makes explicit the semantic relationships between the variables of primary interest
  - this is a derived dataset, it serves as the base dataset for a research project. 
  - include the __derived dataset documentation__
  
- With an eye towards the method of analysis many times there will additional transformations necessary
  - this may include variable normalization, variable recoding, or the creation or incorporation of new variables (linguistic and/ or non-linguistic) which bring the dataset inline with the goals of the research
  - include the __analysis dataset documentation__


- Acquired data: original corpus (reference to the source)
- Curated data: derived dataset (description of the relational structure/ variables)
- Transformed data: analysis dataset (description of the procedures to prepare for analysis)
-->


## Summary {.unnumbered}

In this chapter we have focused on data and information --the first two components of DIKI Hierarchy. This process is visualized in @fig-understanding-data-vis-sum.

```{r}
#| label: fig-understanding-data-vis-sum
#| fig-cap: 'Understanding data: visual summary'
#| echo: false

knitr::include_graphics("figures/understanding-data/ud-diki.drawio.png")
```

First a distinction is made between populations and samples, the latter being a intentional and subjective selection of observations from the world which attempt to represent the population of interest. The result of this process is known as a corpus. Whether developing a corpus or selecting an existing a corpus it is important to vet the sampling frame for its applicability and viability as a resource for a given research project.

Once a viable corpus is identified, then that corpus is converted into a derived dataset which adopts the tidy dataset format where each column is a variable, each row is an observation, and the intersection of columns and rows contain values. This derived dataset serves to establish the base informational relationships from which your research will stem.

The derived dataset will most likely require transformations including normalization, recoding, generation, and/ or merging to enhance the usefulness of the information to analysis. An analysis dataset is the result of this process.

Finally, documentation should be implemented at each stage of the analysis project process. Employing a programmatic approach establishes documentation of the implementation steps but the motivation behind the decisions taken and the content of the corpus data and datasets generated also need documentation to ensure transparent and reproducible research.

## Activities {.unnumbered}

- [ ] Add summary of the purpose, goals, outcomes of these activities

::: {.callout}
**{{< fa regular file-code >}} Recipe**

**What**: [Reading, inspecting, and writing data](https://qtalr.github.io/qtalrkit/articles/recipe-2.html)\
**How**: Read Recipe 2 and participate in the Hypothes.is online social annotation.\
**Why**: To use literate programming in Quarto to work with R coding strategies for reading, inspecting, and writing datasets.
:::

::: {.callout}
**{{< fa flask >}} Lab**

**What**: [Reading, inspecting, and writing data](https://github.com/qtalr/lab-2)\
**How**: Clone, fork, and complete the steps in Lab 2.\
**Why**: To read datasets from packages and from plain-text files, inspect and report characteristics of datasets, and write datasets to plain-text files.
:::

## Questions {.unnumbered}

::: {.callout}
**Conceptual questions**


1. What is the difference between a population and a sample in linguistic research?
2. Why is it important to vet a corpus before using it in a research project?
3. How does the tidy table format help with linguistic analysis?  
4. What is a curated dataset in the context of linguistic research?
5. What is the difference between a variable, an observation, and a value?
6. Why is it important to identify the levels of measurement of variables in a dataset?
7. What kinds of transformations may be performed on a curated dataset to enhance its usefulness for analysis?
8. What is an analysis dataset and why is it important in linguistic research?
9. Why is documentation important in the process of conducting linguistic analysis?
10. How does a programmatic approach enhance documentation in linguistic research?
11. Why is it important to document the motivation behind decisions taken in linguistic analysis projects?
12. How does documenting the corpus data and generated datasets contribute to transparent and reproducible research in linguistics?

---

To add:

- What is the primary unit of observation in this dataset? What are the variables? What are the values for each variable? What is the semantic value of this dataset?
:::


::: {.callout}
**Technical questions**

<!-- Compare with the swirl, recipe, and lab exercises. -->

1. Creating a Sample Corpus:
2. Writing a Corpus Documentation
3. Converting a Corpus to a Derived Dataset:
4. Writing a Data Dictionary
5. Transforming a Derived Dataset:
6. Merging Datasets:
7. Writing a dataset to disk
8. Consider (an example dataset) and its data dictionary, write a script to read the dataset, inspect it, and write it to disk.
9.  Consider a dataset and its data dictionary what appears to be the unit of analysis and the unit of observation?
:::