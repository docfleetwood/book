% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
  \setmonofont[Scale=0.8]{PT Mono}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{3}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{255,255,255}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% wrap code blocks
\usepackage{fvextra}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\},breaklines,breaknonspaceingroup,breakanywhere}

% https://github.com/hadley/r-pkgs/blob/main/latex/preamble.tex
% No widow lines
\widowpenalty=10000 % avoid leaving lines behind
\clubpenalty=10000  % avoid creating orphans

% create index
% \usepackage{makeidx}
% \makeindex
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{File}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[chapter]
\newtheorem{refsolution}{Solution}[chapter]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}{}
\makeatother
\makeatletter
\@ifundefined{codebgcolor}{\definecolor{codebgcolor}{HTML}{f9f9f9}}{}
\makeatother
\makeatletter
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[enhanced, boxrule=0pt, breakable, frame hidden, colback={codebgcolor}, sharp corners]}{\end{tcolorbox}}\fi
\makeatother
\makeatletter
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
% Make links footnotes instead of hotlinks:
\DeclareRobustCommand{\href}[2]{#2\footnote{\url{#1}}}
\hypersetup{
  pdftitle={An Introduction to Quantitative Text Analysis for Linguistics},
  pdfauthor={Jerid Francom},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{An Introduction to Quantitative Text Analysis for Linguistics}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Reproducible Research using R}
\author{Jerid Francom}
\date{February 19, 2024}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\chapter*{Welcome}\label{welcome}

\markboth{Welcome}{Welcome}

The goal of this textbook is to provide readers with foundational
knowledge and practical skills in quantitative text analysis using the R
programming language.

By the end of this textbook, readers will be able to identify, interpret
and evaluate data analysis procedures and results to support research
questions within language science. Additionally, readers will gain
experience in designing and implementing research projects that involve
processing and analyzing textual data employing modern programming
strategies. This textbook aims to instill a strong sense of reproducible
research practices, which are critical for promoting transparency,
verification, and sharing of research findings.

This textbook is geared towards advanced undergraduates, graduate
students, and researchers looking to expand their methodological
toolbox. It assumes no prior knowledge of programming or quantitative
methods and prioritizes practical application and intuitive
understanding over technical details.

\textbf{About the author}

Dr.~Jerid Francom is Associate Professor of Spanish and Linguistics at
Wake Forest University. His research focuses on the use of language
corpora from a variety of sources (news, social media, and other
internet sources) to better understand the linguistic and cultural
similarities and differences between language varieties for both
scholarly and pedagogical projects. He has published on topics including
the development, annotation, and evaluation of linguistic corpora and
analyzed corpora through corpus, psycholinguistic, and computational
methodologies. He also has experience working with and teaching
statistical programming with R.

\section*{License}\label{license}
\addcontentsline{toc}{section}{License}

\markright{License}

This work by \href{https://francojc.github.io/}{Jerid C. Francom} is
licensed under a Creative Commons
Attribution-NonCommercial-NoDerivatives 4.0 International License.

\section*{Credits}\label{credits}
\addcontentsline{toc}{section}{Credits}

\markright{Credits}

\href{https://fontawesome.com/}{Font Awesome Icons} are SIL OFL 1.1
Licensed

\section*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

\markright{Acknowledgements}

The development of this book has benefited from the generous feedback
from the following people: Andrea Bowling, Caroline Brady, Declan
Golsen, Asya Little, Claudia Valdez, Laura Aull, Jack Nelson, \emph{(add
your name here!)}. As always, any errors or omissions are my own.

\section*{Build information}\label{build-information}
\addcontentsline{toc}{section}{Build information}

\markright{Build information}

This textbook was built with the \texttt{quarto} package
(\citeproc{ref-R-quarto}{Allaire 2023}) and the \texttt{bookdown}
package (\citeproc{ref-R-bookdown}{Xie 2023a}) for R. The source code
for this book is available on
\href{https://github.com/qtalr/book}{GitHub}.

This version of the textbook was built with R version 4.3.2 (2023-10-31)
on macOS Ventura 13.6.3 with the following packages:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1413}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1087}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7391}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
package
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
version
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
source
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
dplyr & 1.1.4 & CRAN (R 4.3.2) \\
ggplot2 & 3.4.4 & CRAN (R 4.3.2) \\
here & 1.0.1 & CRAN (R 4.3.0) \\
kableExtra & 1.4.0 & CRAN (R 4.3.2) \\
knitr & 1.45 & CRAN (R 4.3.2) \\
qtalrkit & 0.9.2 & Github
(qtalr/qtalrkit@e6c0ee871482c503bd8b159c5ea4dd562383fa66) \\
readr & 2.1.5 & CRAN (R 4.3.2) \\
reprex & 2.1.0 & CRAN (R 4.3.2) \\
rmarkdown & 2.25 & CRAN (R 4.3.1) \\
rstudioapi & 0.15.0 & CRAN (R 4.3.1) \\
scales & 1.3.0 & CRAN (R 4.3.2) \\
stringr & 1.5.1 & CRAN (R 4.3.2) \\
tibble & 3.2.1 & CRAN (R 4.3.2) \\
tidyr & 1.3.1 & CRAN (R 4.3.2) \\
tinytable & 0.0.5 & CRAN (R 4.3.2) \\
\end{longtable}

\bookmarksetup{startatroot}

\chapter*{Preface}\label{sec-preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

\begin{quote}
The journey of a thousand miles begins with one step.

--- Lao Tzu
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Comprehend the book's rationale, learning goals, and pedagogical
  approach.
\item
  Navigate and engage with the book's structure and content effectively.
\item
  Set up the computing environment and utilize textbook and support
  resources for an optimal learning experience.
\end{itemize}

\end{tcolorbox}

The purpose of this chapter is to present the rationale behind this
textbook, outline the key learning objectives, describe the pedagogical
approach, and identify the intended audience. Additionally, this chapter
will provide readers with a guide to the book's structure and the scope
of its content, as well as instructions for the instructor and a summary
of supporting resources available. Finally, this chapter will provide
readers with information on setting up their computing environment and
where to seek support.

\section*{Rationale}\label{sec-p-rationale}
\addcontentsline{toc}{section}{Rationale}

\markright{Rationale}

\index{data science}\textbf{Data science}, an interdisciplinary field
that combines knownledge and skills from statistics, computer science,
and domain-specific expertise to extract meaningful insight from
structured and unstructured data, has emerged as an exciting and rapidly
growing field in recent years, driven in large part by the increase in
computing power available to the average individual and the abundance of
electronic data now available through the internet. These advances have
become an integral part of the modern scientific landscape, with
data-driven insights now being used to inform decision-making in a wide
variety of academic fields, including linguistics and language-related
disciplines.

This textbook seeks to meet this growing demand by providing an
introduction to the fundamental concepts and practical programming
skills from data science applied to the task of quantitative text
analysis. It is intended primarily for undergraduate students, but may
also be useful for graduates and researchers seeking to expand their
methodological toolbox. The textbook takes a pedagogical approach which
assumes no prior experience with statistics or programming, making it an
accessible resource for novices beginning their exploration of
quantitative text analysis methods.

\section*{Aims}\label{sec-p-aims}
\addcontentsline{toc}{section}{Aims}

\markright{Aims}

The overarching goal of this textbook is to provide readers with
foundational knowledge and practical skills to conduct and evaluate
quantitative text analysis using the R programming language and other
open source tools and technologies. The specific aims are to develop the
reader's proficiency in three main areas:

\begin{itemize}
\tightlist
\item
  \index{data literacy}\textbf{Data literacy}: Identify, interpret and
  evaluate data analysis procedures and results
\end{itemize}

\begin{quote}
Throughout this textbook we will explore topics which will help you
understand how data analysis methods derive insight from data. In this
process you will be encouraged to critically evaluate connections across
linguistic and language-related disciplines using data analysis
knowledge and skills. Data literacy is an invaluable skillset for
academics and professionals but also is an indispensable aptitude for in
the 21st century citizens to navigate and actively participate in the
`Information Age' in which we live (\citeproc{ref-Carmi2020}{Carmi et
al. 2020}).
\end{quote}

\begin{itemize}
\tightlist
\item
  \textbf{Research skills}: Design, implement, and communicate
  quantitative research
\end{itemize}

\begin{quote}
This aim does not differ significantly, in spirit, from common learning
outcomes in a research methods course. However, working with text will
incur a series of key steps in the selection, collection, and
preparation of the data that are unique to text analysis projects. In
addition, I will stress the importance of research documentation and
creating reproducible research as an integral part of modern scientific
inquiry (\citeproc{ref-Buckheit1995}{Buckheit and Donoho 1995}).
\end{quote}

\begin{itemize}
\tightlist
\item
  \textbf{Programming skills}: Apply programmatic strategies to develop
  and collaborate on reproducible research projects
\end{itemize}

\begin{quote}
Modern data analysis, and by extension, text analysis is conducted using
programming. There are various key reasons for this: a programming
approach (1) affords researchers unlimited research freedom --if you can
envision it, you can program it, (2) underlies well-documented and
reproducible research (\citeproc{ref-Gandrud2015}{Gandrud 2015}), and
(3) invites researchers to engage more intimately with the data and the
methods for analysis.
\end{quote}

These aims are important for linguistics students because they provide a
foundation for concepts and in the skills required to succeed in the
rapidly evolving landscape of 21st-century research. These abilities
enable researchers to evaluate and conduct high-quality empirical
investigation across linguistic fields on a wide variety of topics.
Moreover, these skills go beyond linguistics research; they are widely
applicable across many disciplines where quantitative data analysis and
programming are becoming increasingly important. Thus, this textbook
provides students with a comprehensive introduction to quantitative text
analysis that is relevant to linguistics research and that equips them
with valuable skills for their future careers.

\section*{Approach}\label{sec-p-approach}
\addcontentsline{toc}{section}{Approach}

\markright{Approach}

The approach taken in this textbook is designed to accomodate
linguistics students and researchers with little to no prior experience
with programming or quantitative methods. With this in mind the
objective is connect conceptual understanding with practical
application. Real-world data and research tasks relevant to linguistics
are used thoughtout the book to provide context and to motivate the
learning process\footnote{Research data and questions are primarily
  based on English for wide accessibility as it is the \emph{de facto}
  language of academics and research. However, the methods and
  techniques presented in this textbook are applicable to many other
  languages.}. Furthermore, as an introduction to the field, the
textbook focuses on the most common and fundamental methods and
techniques for quantitative text analysis and prioritizes breadth over
depth and intuitive understanding over technical explanations. On the
programming side, the
\index{r!tidyverse|see{tidyverse}}\textbf{Tidyverse} approach to
programming in R will be adopted. This approach provides a consistent
syntax across different packages and is known for its legibility, making
it easier for readers to understand and write code. Together, these
strategies form an approach that is intended to provide readers with an
accessible resource to gain a foothold in the field and to equip them
with the knowledge and skills to apply quantitative text analysis in
their own research.

\section*{Structure}\label{sec-p-structure}
\addcontentsline{toc}{section}{Structure}

\markright{Structure}

The aims and approach described above is reflected in the overall
structure of the book and each chapter.

\subsection*{Book level}\label{sec-p-structure-book}
\addcontentsline{toc}{subsection}{Book level}

At the book level, there are five interdependent parts:

Part I ``Orientation'' provides the necessary background knowledge to
situate quantitative text analysis in the wider context of data analysis
and linguistic research and to provide a clearer picture of what text
analysis entails and its range of applications.

The subsequent parts are directly aligned with the data analysis
process. The building blocks of this process are reflected in
\index{Data to Insight Hierarchy}\textbf{`Data to Insight Hierarchy
(DIKI)'} visualized in Figure 1\footnote{Adapted from Ackoff
  (\citeproc{ref-Ackoff1989}{1989}) and Rowley
  (\citeproc{ref-Rowley2007}{2007}).}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.75\textwidth,height=\textheight]{figures/p-diki.drawio.png}

}

\caption{\label{fig-diki-hierarchy}Data to Insight Hierarchy (DIKI)}

\end{figure}%

The DIKI Hierarchy highlights the stages and intermediate steps required
to derive insight from data. Part II ``Foundations'' provides a
conceptual introduction to the DIKI Hierarchy and establishes
foundational knowledge about data, information, knowledge, and insight
which is fundamental to developing a viable research plan.

Parts III ``Preparation'' and IV ``Analysis'' focus on the
implementation process. Part III covers the steps involved in preparing
data for analysis, including data acquisition, curation, and
transformation. Part IV covers the steps involved in conducting
analysis, including exploratory, predictive, and inferential data
analysis.

The final part, Part V ``Communication'', covers the final stage of the
data analysis process, which is to communicate the results of the
analysis. This includes the structure and content of research reports as
well as the process of publishing, sharing, and collaborating on
research.

\subsection*{Chapter level}\label{sec-p-structure-chapter}
\addcontentsline{toc}{subsection}{Chapter level}

At the chapter level, both conceptual and programming skills are
developed in stages\footnote{These stages attempt to capture the general
  progression of learning reflected in Bloom's Taxonomy (see Krathwohl
  (\citeproc{ref-Krathwohl2002}{2002}) for a description and revision).}.
The chapter-level structure is consistent across chapters and can be
seen in Table 1.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}@{}}
\caption{The general structure of a
chapter}\label{tbl-structure-approach}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Resource
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Stage
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Resource
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Stage
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Outcomes & Identify the learning objectives for the chapter & Textbook &
Introduction \\
Overview & Provide a brief introduction to the chapter topic & Textbook
& Introduction \\
Coding Lessons & Teach programming techniques with hands-on interactive
exercises & GitHub & Skills \\
Content & Combine conceptual discussions and programming skills,
incorporating thought-provoking questions, relevant studies, and
advanced topic references & Textbook & Knowledge \\
Recipes & Offer step-by-step programming examples related to the chapter
and relevant for the upcoming lab & Resources website & Comprehension \\
Labs & Allow readers to apply chapter-specific concepts and techniques
to practical tasks & GitHub & Application \\
Summary & Review the key concepts and skills covered in the chapter &
Textbook & Review \\
\end{longtable}

Each chapter will begin with a list of key learning outcomes followed by
a brief introduction to the chapter's content. The goal is to orient the
reader to the chapter. Next there will be a prompt to complete the
interactive coding lesson(s) to introduce readers to key programming
concepts related to the chapter though hands-on experience and then the
main content of the chapter will follow. The content will be a
combination of conceptual discussions and programming skills,
incorporating thought-provoking questions (`\faIcon{lightbulb} Consider
this'), relevant studies (`\faIcon{file-alt} Case study'), and advanced
topic references (`\faIcon{medal} Dive deeper'). Together these
components form the skills and knowledge phase.

The next phase is the application phase. This phase will include
step-by-step programming demonstrations related to the chapter (Recipes)
and lab exercises that allow readers to apply their knowledge and skills
chapter-related tasks. Finally, the chapters conclude with a summary of
the key concepts and skills covered in the chapter and in the associated
activites.

\section*{Resources}\label{sec-p-resources}
\addcontentsline{toc}{section}{Resources}

\markright{Resources}

The description and location of the available resources to support the
aims and approach of this textbook appear in Table 2.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2000}}@{}}
\caption{Resources available to support the aims and approach of this
textbook}\label{tbl-resources}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Resource
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Location
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Resource
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Location
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Textbook & Prose discussion, figures/ tables, R code, case studies, and
thought and practical exercises & Physical/
\href{https://qtalr.github.io/book/}{Online} \\
\texttt{qtalrkit}\footnote{The \texttt{qtalrkit} package
  (\citeproc{ref-R-qtalrkit}{Francom 2023}) is not yet available on CRAN
  but will be in the future.} & R package with functions for accessing
data and datasets, as well as various useful functions developed
specifically for this textbook & CRAN/
\href{https://github.com/qtalr/qtalrkit}{GitHub} \\
Resources Kit & Programming tutorials (Recipes), guides to enhance the
reader's recognition of how programming strategies are implemented, and
other supplementary materials\footnote{Including Instructor materials
  and Errata for the textbook.} &
\href{https://qtalr.github.io/qtalrkit/}{Online} \\
Lessons & A set of interactive R programming lessons (Swirl) &
\href{https://github.com/qtalr/}{GitHub} \\
Labs & A set of lab exercises designed to guide the reader through
practical hands-on programming applications &
\href{https://github.com/qtalr/}{GitHub} \\
\end{longtable}

\section*{Getting started}\label{sec-p-getting-started}
\addcontentsline{toc}{section}{Getting started}

\markright{Getting started}

Before jumping in to this and subsequent chapter's textbook activities,
it is important to prepare your computing environment and understand how
to take advantage of the resources available, both those directly and
indirectly associated with the textbook.

\subsection*{R and IDEs}\label{sec-p-r-ides}
\addcontentsline{toc}{subsection}{R and IDEs}

Programming is the backbone for modern quantitative research. Among the
many programming languages available, R is a popular open-source
language and software environment for statistical computing. R is
popular with statisticians and has been adopted as the \emph{de facto}
language by many other fields in natural and social sciences, including
linguistics. It is freely downloadable from
\href{https://www.r-project.org/}{The R Project for Statistical
Programming} website and is available for
\href{https://cloud.r-project.org/}{macOS, Linux, and Windows} operating
systems.

Successfully installing R is rarely the last step in setting up your
R-enabled computing environment. The majority of R users also install an
\textbf{integrated development environment} (IDE). An IDE, such as
\href{https://posit.co/products/open-source/rstudio/}{RStudio} or
\href{https://code.visualstudio.com/}{Visual Studio Code}, provide a
\textbf{graphical user interface} (GUI) for working with R. In effect,
IDEs provide a dashboard for working with R and are designed to make it
easier to write and execute R code. IDEs also provide a number of other
useful features such as syntax highlighting, code completion, and
debugging. IDEs are not required to work with R but they are
\emph{highly} recommended.

Choosing to install R and an IDE directly on your personal computer,
which is know as your \textbf{local environment}, is not the only option
to work with R. Other options include working with R in a \textbf{remote
environment} or a \textbf{virtual environment}.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Guides}

For more information and instructions on setting up an R environment
consult the
\href{https://qtalr.github.io/qtalrkit/articles/guide-1.html}{Setting up
an R environment} guide.

\end{tcolorbox}

There are trade-offs in terms of cost, convenience, and flexibility when
choosing to work with R in a local, remote, or virtual environment. The
choice is yours and you can always change your mind later. The important
thing is to get started and begin learning R. Furthermore, any of the
approaches described here will be compatible with this textbook.

\subsection*{R packages}\label{sec-p-r-packages}
\addcontentsline{toc}{subsection}{R packages}

As you progress in your R programming experience, you'll find yourself
leveraging code from other R users, which is typically provided as
packages. Packages are sets of functions and/ or datasets that are
freely accessible for download, designed to perform a specific set of
interrelated tasks. They enhance the capabilities of R. Official R
packages can be found in repositories like
\href{https://cran.r-project.org/}{CRAN} (Comprehensive R Archive
Network), while other packages can be obtained from code-sharing
platforms such as \href{https://github.com/}{GitHub}.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

The Comprehensive R Archive Network (CRAN) includes groupings of popular
packages related to a given applied programming task called
\href{https://cran.r-project.org/web/views/}{Task Views}. Explore the
available CRAN Task Views listings. Note the variety of areas (tasks)
that are covered in this listing. Now explore in more detail one of the
following task views which are directly related to topics covered in
this textbook noting the associated packages and their descriptions: (1)
Cluster, (2) MachineLearning, (3) NaturalLanguageProcessing, or (4)
ReproducibleResearch.

\end{tcolorbox}

You will download a number of packages at different stages of this
textbook, but there is a set of packages that will be key to have from
the get go. Once you have access to a working R/ RStudio environment,
you can proceed to install the following packages.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Guides}

For instructions on how to install the \texttt{qtalrkit} package from
GitHub and download and use the interactive R programming lessons for
this textbook, see the
\href{https://qtalr.github.io/qtalrkit/articles/qtalrkit.html}{Getting
started} guide.

\end{tcolorbox}

Install the following packages from CRAN.

\begin{itemize}
\tightlist
\item
  \texttt{tidyverse} (\citeproc{ref-R-tidyverse}{Wickham 2023b})
\item
  \texttt{remotes} (\citeproc{ref-R-remotes}{Csárdi et al. 2023})
\item
  \texttt{tinytex} (\citeproc{ref-R-tinytex}{Xie 2023b})
\item
  \texttt{swirl} (\citeproc{ref-R-swirl}{Kross et al. 2020})
\end{itemize}

You can do this by running the following code in an R console:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install key packages from CRAN}
\FunctionTok{install.packages}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{, }\StringTok{"remotes"}\NormalTok{, }\StringTok{"tinytex"}\NormalTok{, }\StringTok{"swirl"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsection*{Git and GitHub}\label{sec-p-git-github}
\addcontentsline{toc}{subsection}{Git and GitHub}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Guides}

For more information and instructions on setting up version control and
interacting with GitHub consult the
\href{https://qtalr.github.io/qtalrkit/articles/guide-2.html}{Setting up
Git and GitHub} guide.

\end{tcolorbox}

\href{https://github.com/}{GitHub} is a code sharing website. Modern
computing is highly collaborative and GitHub is a very popular platform
for sharing and collaborating on coding projects. The
\href{https://github.com/stars/francojc/lists/labs}{lab exercises for
this textbook} are shared on GitHub. To access and complete these
exercises you will need to
\href{https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=\%2F&source=header-home}{sign
up for a (free) GitHub account} and then set up the version control
software \texttt{git} on your computing environment, if it is not
already.

\subsection*{Getting help}\label{sec-p-getting-help}
\addcontentsline{toc}{subsection}{Getting help}

The technologies employed in this approach to text analysis will include
a somewhat steep learning curve. And in all honesty, the learning never
stops! Both seasoned programmers and beginners alike need assistance.
Fortunately there is a very large community of programmers who have
developed many official support resources and who actively contribute to
official and unofficial discussion forums. Together these resources
provide many avenues for overcoming challenges.

In Table 3, I provide a list of steps for seeking help with R.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6500}}@{}}
\caption{Recommended order for seeking help with
R}\label{tbl-support-resources}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Order
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Resource
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Order
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Resource
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Official R Documentation & Access the official documentation by
running \texttt{help(package\ =\ "package\_name")} in an R console. Use
the \texttt{?} operator followed by the package or function name. Check
out available Vignettes by running
\texttt{browseVignettes("package\_name")}. \\
2 & Web Search & Look for package documentation and vignettes on the
web. A popular site for this is R-Universe. \\
3 & RStudio IDE Help Toolbar & If you're using RStudio IDE, use the
``Help'' toolbar menu. It provides links to help resources, guides, and
manuals. \\
4 & Online Discussion Forums & Sites like Stack Overflow and RStudio
Community are great platforms where the programming community asks and
answers questions related to real-world issues. \\
5 & Post Questions with Reprex & When posting a question, especially
those involving coding issues or errors, provide enough background and
include a reproducible example (reprex) - a minimal piece of code that
demonstrates your issue. This helps others understand and answer your
question effectively. \\
\end{longtable}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Guides}

For information on how to create a minimal reproducible example with the
\texttt{reprex} package (\citeproc{ref-R-reprex}{Bryan et al. 2024}),
consult the
\href{https://qtalr.github.io/qtalrkit/articles/guide-3.html}{Creating
reproducible examples} guide.

\end{tcolorbox}

The take-home message here is that you are not alone. There are many
people world-wide that are learning to program and/ or contribute to the
learning of others. The more you engage with these resources and
communities the more successful your learning will be. As soon as you
are able, pay it forward. Posting questions and offering answers helps
the community and engages and refines your skills --a win-win.

\section*{Conventions}\label{sec-p-conventions}
\addcontentsline{toc}{section}{Conventions}

\markright{Conventions}

To facilitate the learning process, this textbook will employ a number
of conventions. These conventions are intended to help the reader
navigate the text and to signal the reader's attention to important
concepts and information.

\subsection*{Prose}\label{sec-p-prose}
\addcontentsline{toc}{subsection}{Prose}

The following typographic conventions are used throughout the text:

\begin{itemize}
\tightlist
\item
  \emph{Italics}

  \begin{itemize}
  \tightlist
  \item
    Filenames, file extensions, directory paths, and URLs.
  \end{itemize}
\item
  \texttt{Fixed-width}

  \begin{itemize}
  \tightlist
  \item
    Package names, function names, variable names, and in-line code
    including expressions and operators.
  \end{itemize}
\item
  \textbf{Bold}

  \begin{itemize}
  \tightlist
  \item
    Key concepts when first introduced.
  \end{itemize}
\item
  \href{https://qtalr.github.io/qtalrkit/}{Linked text}

  \begin{itemize}
  \tightlist
  \item
    Links to internal and external resources, footnotes, and citations
    including references to R packages when first introduced.
  \end{itemize}
\end{itemize}

\subsection*{Code blocks}\label{sec-p-code-blocks}
\addcontentsline{toc}{subsection}{Code blocks}

More lengthy code will be presented in code blocks, as seen in Example
0.1.

\begin{example}[]\protect\hypertarget{exm-code-block}{}\label{exm-code-block}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# A function that takes a name and returns a greeting}
\NormalTok{greetings }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(name) \{}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Hello"}\NormalTok{, name)}
\NormalTok{\}}

\FunctionTok{greetings}\NormalTok{(}\AttributeTok{name =} \StringTok{"Jerid"}\NormalTok{) }\CommentTok{\# apply function to a name}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] "Hello Jerid"
\end{verbatim}

\end{example}

There are a couple of things to note about the code in Example 0.1.
First, it shows the code that is run in R as well as the ouput that is
returned. The code will appear in a box and the output will appear below
the box. Both code and output will appear in fixed-width font. Output
which is text will be prefixed with \texttt{\textgreater{}}. Second, the
\texttt{\#} symbol is used to signal a \textbf{code comment}, a
human-facing description. Everything right of a \texttt{\#} is not run
as code. In this textbook you will see code comments above code on a
separate line and to the right of code on the same line. It is good
practice to comment your code to enhance readability and to help others
understand what your code is doing.

All figures, tables, and images in this textbook are generated by code
blocks but only code for those elements that are relevant for discussion
will be shown. However, if you wish to see the code for any element in
this textbook, you can visit the GitHub repository
\url{https://qtalr.github.io/book/}.

When a reference to a file and its contents is made, it will appear as
in File 0.1.

\begin{codelisting}

\caption{\label{lst-r-code}\texttt{example.R} R script}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load libraries}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Add 1 and 1}
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\subsection*{Callouts}\label{sec-p-callouts}
\addcontentsline{toc}{subsection}{Callouts}

Callouts are used to signal the reader's attention to content, activity,
and other important sections. The following callouts are used in this
textbook:

\textbf{Content}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{list-alt} Outcomes}

Learning outcomes for the chapter appear here.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

Points for you to consider and questions to explore appear here.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-alt} Case study}

Case studies for applying conceptual knowledge and coding skills covered
in the chapter appear here.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

Links to additional resources for diving deeper into the topic appear
here.

\end{tcolorbox}

\textbf{Activities}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{terminal} Lessons}

Links to swirl lessons for practicing coding skills for the chapter
appear here.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Recipe}

Links to demonstration programming tasks on the qtalr site for the
chapter appear here.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{flask} Lab}

Links to lab exercises for applying conceptual knowledge and coding
skills on the qtalr GitHub repository for the chapter appear here.

\end{tcolorbox}

\textbf{Other}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{hand-point-up} Tip}

Tips for using R and related tools appear here.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{exclamation-triangle} Warning}

Warnings for using R and related tools appear here.

\end{tcolorbox}

\section*{Activities}\label{sec-p-activities}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

At this point you should have a working R environment with the core
packages including \texttt{qtalrkit} installed. You should also have
verified that you have a working Git environment and that you have a
GitHub account. If you have not completed these tasks, return to the
guides listed above in ``\hyperref[sec-p-getting-started]{Getting
started}'' of this Preface and complete them before proceeding.

The following activities are designed to help you become familiar with
the tools and resources that you will be using throughout this textbook.
These and subsequent activities are designed to be completed in the
order that they are presented in this textbook.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{terminal} Lessons}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Intro to Swirl}\\
\textbf{How}: In the R console load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To familiarize you with navigating, selecting, and
completing swirl lessons for interactive R programming tutorials.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-0.html}{Literate
Programming and Quarto}\\
\textbf{How}: Read Recipe 0, complete comprehension check, and prepare
for Lab 0.\\
\textbf{Why}: To introduce the concept of Literate Programming and how
to create literate documents using R and Quarto.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-00}{Writing with
code}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 0.\\
\textbf{Why}: To put literate programming techniques covered in Recipe 0
into practice. Specifically, you will create and edit a Quarto document
and render a report in PDF format.

\end{tcolorbox}

\section*{Summary}\label{sec-p-summary}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

This prefaces outlines the textbook's underlying principles, learning
goals, teaching methods, and target audience. The chapter also offers
advice on how to navigate the book's layout, comprehend its subject
matter, and make use of supplementary materials. With this foundation,
you're now prepared to delve into quantitative text analysis. I hope you
enjoy the journey!

\section*{To the instructor}\label{sec-p-instructor}
\addcontentsline{toc}{section}{To the instructor}

\markright{To the instructor}

For recommendations on how to use this textbook in your course and to
access additional resources, visit the
\href{https://qtalr.github.io/qtalrkit/articles/instructor-guide.html}{Instructor
Guide} on the companion website.

\part{Orientation}

In this part, I introduce fundamental concepts that are essential for
understanding text analysis in its research and methodological context.
We start by pointing to the limitations of human cognition in processing
vast amounts of information and that underscore the need for scientific
methods to objectively analyze data. A brief history of quantitative
data analysis highlights the commonalities between text analysis and
other quantitative methods. We then discuss the role of quantitative
methods in language research, and how text analysis contributes to the
field. As we transition into the subsequent parts of the textbook, these
initial insights will serve as a backbone for the entire text analysis
process.

\chapter{Text analysis}\label{sec-text-analysis}

\begin{quote}
Everything about science is changing because of the impact of
information technology and the data deluge.

--- Jim Gray
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Understand the role and goals of data analysis both within and outside
  of academia.
\item
  Describe the various approaches to quantitative language research.
\item
  Identify the applications of text analysis in different contexts.
\end{itemize}

\end{tcolorbox}

In this chapter, I will aim to introduce the topic of text analysis and
provide the context needed to understand how text analysis fits in a
larger universe of science and the ever-ubiquitous methods of data
science, with attention to how linguistics and language-related studies
employ data analysis down to the particular area of text analysis.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{terminal} Lessons}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Workspace,
Vectors}\\
\textbf{How}: In an R console load \texttt{swirl}, run \texttt{swirl()},
and follow prompts to select the lesson.\\
\textbf{Why}: To examine your local workspace in RStudio and understand
the relationship between your R workspace and the file system of your
computing environment. To explore the key building blocks of the R
programming language.

\end{tcolorbox}

\section{Enter science}\label{sec-enter-science}

The world around us is full of actions and interactions so numerous that
it is difficult to truly comprehend. As individuals, we see and
experience this world, gaining knowledge and building heuristic
understanding of how it works and how we can interact with it. This
happens regardless of one's educational background. As humans, we are
designed for this. Our minds process countless sensory inputs, which
enable skills and abilities that we often take for granted, such as
predicting what will happen if someone is about to knock a wine glass
off a table and onto a concrete floor. Even if we have never encountered
this specific situation before, our minds somehow instinctively make an
effort to warn the potential glass-breaker before it is too late.

You have most likely not stopped to consider where this predictive
knowledge comes from. If you have, you may have simply attributed it to
`common sense'. Despite its commonality, it is an incredible display of
the brain's capacity to monitor the environment, make connections
between events and observations, and store that information without
making a big fuss to inform our conscious mind about its processes.

Our brains are efficient but not infallible along to key fronts. First,
our brains are not supercomputers. They do not store every experience in
raw form; we do not have access to the records of our experience like we
would imagine a computer would have access to the records logged in a
database. Where our brains do excel is in making associations and
predictions that help us (most of the time) navigate the complex world
we inhabit.

Second, our brains are prone to bias. Given our particular and limited
experiences and the way our brains process information, we are prone to
biases that can influence our understanding of the world. For example,
we are prone to confirmation bias, the tendency to seek out information
that confirms our beliefs and ignore information that contradicts them.
We are also prone to the availability heuristic, the tendency to
overestimate the likelihood of events that are more easily recalled.
These are just two of many biases that can influence our understanding
of the world.

These points are key -- our brains are doing some amazing work, but that
work can give us the impression that we understand the world better and
in more detail than we actually do. In this way, what we think the world
is like and what the world is actually like can be two different things.
This is where science comes in.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

How might your own experiences and biases influence your understanding
of the world? What are some ways that you can mitigate these biases? Is
ever possible to be completely objective? How might biases influence the
way you approach text analysis?

\end{tcolorbox}

Science starts with a question, identifies and collects data, careful
selected slices of the complex world, submits this data to analysis
through clearly defined and reproducible procedures, and reports the
results for others to evaluate. This process is repeated, modifying, and
manipulating the procedures, asking new questions and positing new
explanations, all in an effort to make inroads to bring the complex into
tangible view.

In essence what science does is attempt to subvert our inherent
limitations by drawing on carefully and purposefully collected samples
of observable experience and letting the analysis of these observations
speak, even if it goes against our intuitions (those powerful but
sometime spurious heuristics that our brains use to make sense of the
world).

\section{Data analysis}\label{data-analysis}

\subsection{Emergence of data science}\label{emergence-of-data-science}

This science I've described is the one you are likely quite familiar
with and, if you are like me, this description of science conjure
visions of white coats, labs, and petri dishes. While science's
foundation still stands strong in the 21st century, a series of
intellectual and technological events mid-20th century set in motion
changes that have changed aspects about how science is done, not why it
is done. We could call this Science 2.0, but let's use the more
popularized term \index{data science}\textbf{data science}.

The recognized beginnings of data science are attributed to work in the
``Statistics and Data Analysis Research'' department at Bell Labs during
the 1960s. Although primarily conceptual and theoretic at the time, a
framework for quantitative data analysis took shape that would
anticipate what would come: sizable datasets which would ``{[}\ldots{]}
require advanced statistical and computational techniques {[}\ldots{]}
and the software to implement them.''
(\citeproc{ref-Chambers2020}{Chambers 2020}) This framework emphasized
both the inference-based research of traditional science, but also
embraced exploratory research and recognized the need to address
practical considerations that would arise when working with and deriving
insight from an abundance of machine-readable data.

Fast-forward to the 21st century, a world in which machine-readable data
is truly in abundance. With increased computing power, the emergence of
the world wide web, and wide adoption of mobile devices electronic
communication skyrocketed around the globe. To put this in perspective,
in 2019 it was estimated that every minute 511 thousand tweets were
posted, 18.1 million text messages were sent, and 188 million emails
were sent (\citeproc{ref-DataNeverSleeps08-2021}{{``Data Never Sleeps
7.0 Infographic''} 2019}). The data flood has not been limited to
language, there are more sensors and recording devices than ever before
which capture evermore swaths of the world we live in
(\citeproc{ref-Desjardins2019}{Desjardins 2019}).

Where increased computing power gave rise to the influx of data, it is
also one of the primary methods for gathering, preparing, transforming,
analyzing, and communicating insight derived from this data
(\citeproc{ref-Donoho2017}{Donoho 2017}). The vision laid out in the
1960s at Bell Labs had come to fruition.

\subsection{Data science toolbelt}\label{data-science-toolbelt}

Data science is not predicated on data alone. Turning data into insight
takes computing skills, statistical knowledge, and domain expertise.
This triad has been popularly represented as a Venn diagram such as in
Figure~\ref{fig-intro-data-science-venn}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.5\textwidth,height=\textheight]{figures/ta-ds-venn.drawio.png}

}

\caption{\label{fig-intro-data-science-venn}Data Science Venn Diagram
adapted from
\href{http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram}{Drew
Conway}.}

\end{figure}%

The \index{computing skills}\textbf{computing skills} component of data
science is the ability to write code to perform the data analysis
process. This is the primary approach for working with data at scale.
The \index{statistical knowledge}\textbf{statistical knowledge}
component of data science is the ability to apply statistical methods to
data to derive insight by bringing patterns and relationships in the
data into view. \index{Domain expertise}\textbf{Domain expertise}
provides researchers insight at key junctures in the development of a
research project and aid researchers in evaluating results.

This triad of skills in combination with reproducible research practices
is the foundational toolbelt of data science
(\citeproc{ref-Hicks2019}{Hicks and Peng 2019}). \textbf{Reproducible
research}\index{reproducible research} entails the use of computational
tools to automate the process of data analysis. This automation is
achieved by writing code that can be executed to replicate the data
analysis. This code can then be shared through code sharing
repositories, where it can be viewed, downloaded, and executed by
others. This adds transparency to the process and allows others to build
on previous work enhacing scientific progress along the way
(\citeproc{ref-Baker2016}{Baker 2016}).

\subsection{Quant everywhere}\label{quant-everywhere}

Equipped with the data science toolbelt, the interest in deriving
insight from data is now almost ubiquitous. The science of data has now
reached deep into all aspects of life where making sense of the world is
sought. Predicting whether a loan applicant will get a loan
(\citeproc{ref-Bao2019}{Bao, Lianju, and Yue 2019}), whether a lump is
cancerous (\citeproc{ref-Saxena2020}{Saxena and Gyanchandani 2020}),
what films to recommend based on your previous viewing history
(\citeproc{ref-Gomez-Uribe2015}{Gomez-Uribe and Hunt 2015}), what
players a sports team should sign (\citeproc{ref-Lewis2004}{Lewis
2004}), now all incorporate a common set of data analysis tools.

The data science toolbelt also underlies well-known public-facing
language applications. From the language-capable chat applications,
plagiarism detection software, machine translation algorithms, and
search engines, tangible results of quantitative approaches to language
are becoming standard fixtures in our lives.

The spread of quantitative data analysis too has taken root in academia.
Even in areas that on first blush don't appear readily approachable in a
quantitative manner, such as fields in the social sciences and
humanities, data science is making important and sometimes disciplinary
changes to the way that academic research is conducted.

This textbook focuses in on a domain that cuts across many of these
fields; namely language. At this point let's turn to quantitative
approaches to language analysis as we work closer to contextualizing
text analysis in the field of linguistics.

\section{Language analysis}\label{language-analysis}

\subsection{Qualities and quantities}\label{qualities-and-quantities}

Language is a defining characteristic of our species. Since antiquity,
language has attracted interest across disciplines and schools of
thought. In the early 20th century, the development of the rigorous
approach to study of language as a field in its own right took root
(\citeproc{ref-Campbell2001}{Campbell 2001}), yet a plurality of
theoretical views and methodological approaches remained. Contemporary
linguistics bares this complex history and even today, it is far from
theoretically and methodologically unified discipline.

Either based on the tenets of theoretical frameworks and/ or the objects
of study of particular fields, methodological approaches to language
research vary. On the one hand, some language research commonly applies
qualitative assessment of language structure and/ or use.
\textbf{Qualitative approaches} describe and account for
characteristics, or ``qualities'', that can be observed, but not
measured (\emph{e.g.} introspective methods, ethnographic methods,
\emph{etc.})

On the other hand, other language research programs employ quantitative
research methods either out of necessity given the object of study
(phonetics, psycholinguistics, \emph{etc.}) or based on theoretical
principles (Cognitive Linguistics, Connectionism, \emph{etc.}).
\textbf{Quantitative approaches} involve ``quantities'' of properties
that can be observed and measured (\emph{e.g.} frequency of use,
reaction time, \emph{etc.}).

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-alt} Case study}

Manning (\citeproc{ref-Manning2003}{2003}) discusses the use of
probabilistic models in syntax to account for the variability in
language usage and the presence of both hard and soft constraints in
grammar. The paper touches on the statistical methods in text analysis,
the importance of distinguishing between external and internal language,
and the limitations of Generative Grammar. Overall, the paper suggests
that usage-based and formal syntax can learn from each other to better
understand language variation and change.

\end{tcolorbox}

These latter research areas and theoretical paradigms employ methods
that share much of the common data analysis toolbox described in the
previous section. In effect, this establishes a common methodological
language between other language-based research fields but also with
research outside of linguistics.

\subsection{The nature of data}\label{the-nature-of-data}

In quantitative language analysis there is a key methodological
distinction that has downstream effects in terms of procedure but also
in terms of interpretation. The key distinction that we need to make at
this point, which will provide context for our introduction to
quantitative text analysis, comes down to the approach to collecting
language data and the nature of that data. This distinction is between
experimental data and observational data.

\index{experimental data}\textbf{Experimental approaches} start with a
intentionally designed hypothesis and lay out a research methodology
with appropriate instruments and a plan to collect data that shows
promise for shedding light on the likelihood of the hypothesis.
Experimental approaches are conducted under controlled contexts, usually
a lab environment, in which participants are recruited to perform a
language related task with stimuli that have been carefully curated by
researchers to elicit some aspect of language behavior of interest.
Experimental approaches to language research are heavily influenced by
procedures adapted from psychology. This link is logical as language is
a central area of study in cognitive psychology. This approach looks
much like the white-coat science that we made reference to earlier but,
as in most quantitative research, has now taken advantage of the data
analysis toolbelt to collect and organize much larger quantities of data
and conduct statistically more robust analysis procedures and
communicate findings more efficiently.

\index{observational data}\textbf{Observational approaches} are a bit
more of a mixed bag in terms of the rationale for the study; they may
either start with a testable hypothesis or in other cases may start with
a more open-ended research question to explore. But a more fundamental
distinction between the two approaches is drawn in the amount of control
the researcher exerts on the contexts and conditions in which the
language behavior data to be collected is produced. Observational
approaches seek out records of language behavior that is produced by
language speakers for communicative purposes in natural(-istic)
contexts. This may take place in labs (language development, language
disorders, \emph{etc.}), but more often than not, language is collected
from sources where speakers are performing language as part of their
daily lives --whether that be posting on social media, speaking on the
telephone, making political speeches, writing class essays, reporting
the latest news for a newspaper, or crafting the next novel destined to
be a New York Times best-seller. What is more, data collected from the
`wild' varies more in structure relative to data collected in
experimental approaches and requires a number of organizational steps to
prepare the data to sync up with the data analysis toolbelt.

I liken this distinction between experimental and observational data
collection to the difference between farming and foraging. Experimental
approaches are like farming; the groundwork for a research plan is
designed, much as a field is prepared for seeding, then the researcher
performs as series of tasks to produce data, just as a farmer waters and
cares for the crops, the results of the process bear fruit, data in our
case, and this data is harvested. Observational approaches are like
foraging; the researcher scans the available environmental landscape for
viable sources of data from all the naturally existing sources, these
sources are assessed as to their usefulness and value to address the
research question, the most viable is selected, and then the data is
collected.

The data acquired from both of these approaches have their trade-offs,
just as farming and foraging. Experimental approaches directly elicit
language behavior in highly controlled conditions. This directness and
level of control has the benefit of allowing researchers to precisely
track how particular experimental conditions effect language behavior.
As these conditions are an explicit part of the design and therefore the
resulting language behavior can be more precisely attributed to the
experimental manipulation.

The primary shortcoming of experimental approaches is that there is a
level of artificialness to this directness and control. Whether it is
the language materials used in the task, the task itself, or the fact
that the procedure takes place under supervision the language behavior
elicited can diverge quite significantly from language behavior
performed in natural communicative settings.

Observational approaches show complementary strengths and shortcomings.
Whereas experimental approaches may diverge from natural language use,
observational approaches strive to identify and collected language
behavior data in natural, uncontrolled, and unmonitored contexts, as
seen in Figure~\ref{fig-data-collection-methods}. In this way,
observational approaches do not have to question to what extent the
language behavior data is or is not performed as a natural communicative
act.

On the flipside, the contexts in which natural language communication
take place are complex relative to experimental contexts. Language
collected from natural contexts are nested within the complex workings
of a complex world and as such inevitably include a host of factors and
conditions which can prove challenging to disentangle from the language
phenomenon of interest but must be addressed in order to draw reliable
associations and conclusions.

\begin{figure}[H]

\centering{

\includegraphics[width=4.04in,height=\textheight]{figures/ta-data-collection-methods.drawio.png}

}

\caption{\label{fig-data-collection-methods}Trade-offs between
experimental and observational data collection methods.}

\end{figure}%

The upshot, then, is two-fold: (1) data collection methods matter for
research design and interpretation and (2) there is no single best
approach to data collection, each have their strengths and shortcomings.

In the ideal, a robust science of language will include insight from
both experimental and observational approaches
(\citeproc{ref-Gilquin2009}{Gilquin and Gries 2009}). And evermore there
is greater appreciation for the complementary nature of experimental and
observational approaches and a growing body of research which highlights
this recognition.

\section{Text analysis}\label{text-analysis}

In a nutshell, \textbf{text analysis}\index{Text analysis} is the
process of leveraging the data science toolbelt to derive insight from
textual data collected through observational methods. In the next
subsections, I will unpack this definition and discuss the primary
components that make up text analysis including research appoaches and
technical implementation, as well as practical applications.

\subsection{Aims}\label{aims}

Text analysis is a multifacited research methodology. It can be used use
facilitate the qualitative exploration of smaller, human-digestable
textual information, but is more often employed quantitatively to bring
to the surface patterns and relationships in large samples of textual
data that would be otherwise difficult, if not impossible, to identify
manually.

The aims of text analysis are as varied as the research questions that
can be asked of language data. Some research questions are data-driven,
where the researcher is interested in exploring and uncovering patterns
and relationships in the data. Other research questions are
theory-driven, where the researcher is interested in testing a
hypothesis or evaluating a theory. In either case, the researcher is
interested in gaining insight from the data.

The relationship(s) of interest in text analysis may be language
internal, where the researcher is interested in the patterns and
relationships between linguistic features or language external, where
the researcher is interested in the patterns and relationships between
linguistic features and some external variable.

\subsection{Approaches}\label{approaches}

Text being text, there are a series of \textbf{data prepration} steps
that must be taken to ready the data for analysis, whatever the aim may
be. In addition to collecting the data, the data must be organized,
cleaned, and transformed into a format that is amenable to statistical
analysis.

The statistical and evaluative approach employed in the analysis is
dependent on the aim of the research. For research aimed at exploring
and uncovering patterns and relationships in a data-driven manner,
\textbf{Exploratory Data Analysis} (EDA) is employed. EDA combines
descriptive statistics, visualizations, and statistical learning methods
in an iterative and interactive way to provide the researcher the
ability to identify patterns and relationships and to evaluate whether
and why they are meaningful.

\textbf{Predictive Data Analysis} (PDA), applied in research for outcome
prediction, is a supervised machine learning task. It uses feature sets
to predict an outcome variable. Its primary evaluation metric is the
prediction accuracy on new data. However, for many text analysis tasks,
human interpretation is crucial to provide context and assess the
significance of the results.

Research aimed at explaining relationships between variables and the
population from which the sample was drawn will adopt an
\textbf{Inferential Data Analysis} (IDA) approach. IDA is a
theory-driven process that employs statistical models for hypothesis
testing. The extent to which the results can be confidently generalized
to the population is the primary evaluation metric.

As we see, text analysis can be used for a variety of purposes; from
data-driven exploration and discovery to hypothesis testing and
generalization.

\subsection{Implementation}\label{implementation}

Text analysis branch of data science. As such, it takes advantage of the
data science toolbelt to derive insight from data. It is important to
note, that while all of the components of the data science toolbelt are
present in text analysis, the relative importance of each varies with
the stage research. Computing skills being the most important at the
data and information stages, statistical knowledge being the most
important to derive knowledge, and domain knowledge leading the way
towards insight.

\begin{figure}[H]

\centering{

\includegraphics[width=0.75\textwidth,height=\textheight]{figures/p-diki.drawio.png}

}

\caption{\label{fig-diki-hierarchy}Data to Insight Hierarchy (DIKI)}

\end{figure}%

Text is a rather raw form of data. It is more often that not
unstructured, meaning that it is not organized in a way that such that
it can be easily analyzed. The collection, organization, and
transformation of text data is a key component of text analysis and
computers are well-suited for this task, as we will see in
Chapter~\ref{sec-understanding-data} and Part III ``Preparation''.

Once text is transformed to a dataset that can be analyzed, we lean on
statistical methods to gain perspective on the relationship(s) of
interest. By and large, these methods are the same as those used in
other areas of data science. We will provide an overview of these
methods in Chapter~\ref{sec-approaching-analysis} and do a deeper dive
in Part IV ``Analysis''.

With the results of the analysis in hand, the researcher must interpret
the results and evaluate their significance in disciplinary context.
This is where domain knowledge comes to the fore. The researcher must be
able to interpret the results in light of the research question and the
context in which the data was collected and communicate the value of the
results to the broader community. Domain knowledge also plays a vital
role in framing the research question and designing the research
methodology, as we will see in Chapter~\ref{sec-framing-research}. Then
we will return to the role of domain knowledge in interpreting and
communicating the results in Part V ``Communication''.

To ensure that the results of text analysis projects are replicable and
transparent, programming strategies and documentation play an integral
role at each stage of the implementation of a research project. While
there are a number of programming languages that can be used for text
analysis, R widely adopted in linguistics research and is the language
of choice for this textbook.

R is a free and open-source programming language that is specifically
designed for statistical computing and graphics. It has a large and
active community of users and developers, and a robust ecosystem of
packages which make it a powerful and flexible language that is
well-suited for core text analysis tasks: data collection, organization,
transformation, analysis, and visualization.

When combined with Quarto for literate programming and GitHub for
version control and collaboration, R provides a robust and reproducible
workflow for text analysis projects. We will scaffold your R and general
computing skills in the first parts of this textbook and then apply and
develop these skills as we move into the steps of implementing text
analysis projects.

\subsection{Applications}\label{applications}

So what are some applications of text analysis? Most public facing
applications stem from Computational Linguistic research, often known as
\textbf{Natural Language Processing} (NLP) by practitioners. Whether it
be using search engines, online translators, submitting your paper to
plagiarism detection software, \emph{etc.} many of the text analysis
methods we will cover are at play.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

What are some other public facing applications of text analysis that you
are aware of?

\end{tcolorbox}

In academia the use of quantitative text analysis is even more
widespread, despite the lack of public fanfare. In linguistics, text
analysis research is often falls under
\index{corpus linguistics}\textbf{Corpus Linguistics} (CL). And this
approach is applied to a wide range of topics and research questions in
both theoretical and applied linguistics fields and subfields, as seen
in Example~\ref{exm-linguistic-theory} and
Example~\ref{exm-linguistic-applied}.

\begin{example}[]\protect\hypertarget{exm-linguistic-theory}{}\label{exm-linguistic-theory}

Theoretical linguistics

\begin{itemize}
\tightlist
\item
  Hay (\citeproc{ref-Hay2002}{2002}) use a corpus study to investigate
  the role of frequency and phonotatics in affix ordering in English.
\item
  Riehemann (\citeproc{ref-Riehemann2001}{2001}) explores the extent to
  which idiomatic expressions (\emph{e.g.} `raise hell') are lexical or
  syntactic units.
\item
  Bresnan (\citeproc{ref-Bresnan2007a}{2007}) investigate the claim that
  possessed deverbal nouns in English (\emph{e.g.} `the city's
  destruction') are subject to a syntactic constraint that requires the
  possessor to be affected by the action denoted by the deverbal noun.
\end{itemize}

\end{example}

\begin{example}[]\protect\hypertarget{exm-linguistic-applied}{}\label{exm-linguistic-applied}

Applied linguistics

\begin{itemize}
\tightlist
\item
  Wulff, Stefanowitsch, and Gries (\citeproc{ref-Wulff2007}{2007})
  explore differences between British and American English at the
  lexico-syntactic level in the \emph{into}-causative construction
  (\emph{e.g.} `He tricked me into employing him.').
\item
  Eisenstein et al. (\citeproc{ref-Eisenstein2012}{2012}) track the
  geographic spread of neologisms (\emph{e.g.} `bruh', `af', '-\_\_-')
  from city to city in the United States using Twitter data collected
  between 6/2009 and 5/2011.
\item
  Bychkovska and Lee (\citeproc{ref-Bychkovska2017}{2017}) investigates
  possible differences between L1-English and L1-Chinese undergraduate
  students' use of lexical bundles, multiword sequences which are
  extended collocations (\emph{e.g.} `as the result of'), in
  argumentative essays.
\item
  Jaeger and Snider (\citeproc{ref-Jaeger2007}{2007}) use a corpus study
  to investigate the phenomenon of syntactic persistence, the increased
  tendency for speakers to use a particular syntactic form over an
  alternate when the syntactic form has been recently processed.
\item
  Voigt et al. (\citeproc{ref-Voigt2017}{2017}) explore potential racial
  disparities in officer respect in police body camera footage.
\item
  Olohan (\citeproc{ref-Olohan2008}{2008}) investigate the extent to
  which translated texts differ from native texts do to `explicitation'.
\end{itemize}

\end{example}

So too, text analysis is used in a variety of fields outside of
linguistics where insight from language is sought, as seen in
Example~\ref{exm-other-fields}.

\begin{example}[]\protect\hypertarget{exm-other-fields}{}\label{exm-other-fields}

Language-related fields

\begin{itemize}
\tightlist
\item
  Kloumann et al. (\citeproc{ref-Kloumann2012}{2012}) explore the extent
  to which languages are positively, neutrally, or negatively biased.
\item
  Mosteller and Wallace (\citeproc{ref-Mosteller1963}{1963}) provide a
  method for solving the authorship debate surrounding The Federalist
  papers.
\item
  Conway et al. (\citeproc{ref-Conway2012}{2012}) investigate whether
  the established drop in language complexity of rhetoric in election
  seasons is associated with election outcomes.
\end{itemize}

\end{example}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

Language is a key component of human communication and interaction. What
are some other areas of research in and outside linguistics that you
think could be explored using text analysis methods?

\end{tcolorbox}

These studies in Examples \ref{exm-linguistic-theory},
\ref{exm-linguistic-applied}, and \ref{exm-other-fields} are just a few
illustrations of the contributions of text analysis as the primary
method to gain a deeper understanding of language structure, function,
variation, and acquisition.

As a method, however, text analysis can also be used to support other
research methods. For example, text analysis can be used collect data,
generate authentic materials, provide linguistic annotation, and to
generate hypotheses, for either qualitative and/ or quantitative
approaches. Together these efforts contribute to a more robust language
science by incorporating externally valid data and providing
methodological triangulation (\citeproc{ref-Francom2022}{Francom 2022}).

In sum, the applications highlighted in this section underscore the
versatility of text analysis as a research method. Whether it be in the
public sphere or in academia, text analysis methods furnish a set of
powerful tools for gaining insight into the nature of language.

\section*{Actitivies}\label{actitivies}
\addcontentsline{toc}{section}{Actitivies}

\markright{Actitivies}

The following activities build on your introduction to R and Quarto in
the preface. In these activities you will uncover more features offered
by Quarto which will enhance your ability to produce comprehensive
reproducible research documents. You will apply the capabilities of
Quarto in a practical context conveying the objectives and key
discoveries from a primary research article.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-1.html}{Academic
writing with Quarto}\\
\textbf{How}: Read Recipe 1, complete comprehension check, and prepare
for Lab 1.\\
\textbf{Why}: To explore additional functionality in Quarto: numbered
sections, table of contents, in-line citations and a document-final
references list, and cross-referenced tables and figures.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-01}{Crafting scholarly
documents}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 1.\\
\textbf{Why}: To put into practice Quarto functionality to communicate
the aim(s) and main finding(s) from a primary research article.

\end{tcolorbox}

\section*{Summary}\label{summary}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter, I started with some general observations about the
difficulty of making sense of a complex world. The standard approach to
overcoming inherent human limitations in sense making is science. In the
21st century the toolbelt for doing scientific research and exploration
has grown in terms of the amount of data available, the statistical
methods for analyzing the data, and the computational power to manage,
store, and share the data, methods, and results from quantitative
research. The methods and tools for deriving insight from data have made
significant inroads in and outside academia, and increasingly figure in
the quantitative investigation of language. Text analysis is a
particular branch of this enterprise based on observational data from
real-world language and is used in a wide variety of fields.

In the end I hope that you enjoy this exploration into text analysis.
Although the learning curve at times may seem steep --the experience you
will gain will not only improve your data literacy, research skills, and
programmings skills but also enhance your appreciation for the richness
of human language and its important role in our everyday lives.

\part{Foundations}

Before working on the specifics of a data project, it is important to
establish a fundamental understanding of the characteristics of each of
the levels in the ``Data, Information, Knowledge, and Insight Hierarchy
(DIKI)'' (see Figure~\ref{fig-diki-hierarchy}) and the roles each of
these levels have in deriving insight from data. In
Chapter~\ref{sec-understanding-data} we will explore the Data and
Information levels drawing a distinction between two main types of data
(populations and samples) and then cover how data is structured and
transformed to generate information (datasets) that is fit for
statistical analysis. In Chapter~\ref{sec-approaching-analysis} I will
outline the importance and distinct types of statistical procedures
(descriptive and analytic) that are commonly used in text analysis.
Chapter~\ref{sec-framing-research} aims to tie these concepts together
and cover the required steps for preparing a research blueprint to guide
the implementation of a text analysis project.

\chapter{Data}\label{sec-understanding-data}

\begin{quote}
The goal is to turn data into information, and information into insight.

--- Carly Fiorina
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Describe the difference between data and information.
\item
  Understand how the tidy approach to data organization can enhance the
  quality and usability of data.
\item
  Articulate the importance of documentation in promoting reproducible
  research.
\end{itemize}

\end{tcolorbox}

In this chapter, the groundwork is laid for deriving insights from text
analysis by focusing on content and structure of data and information.
The concepts of populations and samples are introduced, highlighting
their similarities and key differences. Connecting these topics to text
analysis, language samples, or corpora, are explored, discussing their
types, sources, formats, and ethical considerations. Subsequently, key
concepts in creating information from corpus data, such as organization
and transformation, are introduced. Documentation in quantitative
research is emphasized addressing the importance of data origin files
and data dictionaries.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{terminal} Lessons}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Objects, Packages
and functions}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To introduce you to the main types of objects in R and to
understand the role and use of functions and packages in R programming.

\end{tcolorbox}

\section{Data}\label{sec-ud-data}

Data is data, right? The term `data' is so common in popular vernacular
it is easy to assume we know what we mean when we say `data'. But as in
most things in science, where there are common assumptions there are
important details that require more careful consideration. Let's turn to
the first key distinction that we need to make to start to break down
the term `data': the difference between populations and samples.

\subsection{Populations and samples}\label{populations-and-samples}

The first thing that comes to many people's mind when the term
population is used is human populations (derived from Latin `populus').
Say for example we pose the question --What's the population of
Milwuakee? When we speak of a population in these terms we are talking
about the total sum of individuals living within the geographical
boundaries of Milwaukee. In concrete terms, a
\index{population}\textbf{population} an idealized set of objects or
events in reality which share a common characteristic or belong to a
specific category. The term to highlight here is idealized. Although we
can look up the US Census report for Milwaukee and retrieve a figure for
the population, this cannot truly be the population. Why is that? Well,
whatever method that was used to derive this numerical figure was surely
incomplete. If not incomplete, by the time someone recorded the figure
some number of residents of Milwaukee moved out, moved in, were born, or
passed away. In either case, this example serves to point out that
populations are not fixed and are subject to change over time.

Likewise when we talk about populations in terms of language we dealing
with an idealized aspect of linguistic reality. Let's take the words of
the English language as an analog to our previous example population. In
this case the words are the people and English is the grouping
characteristic. Just as people, words move out, move in, are born, and
pass away. Any compendium of the words of English at any moment is
almost instananeously incomplete. This is true for all populations, save
those relatively rare cases in which the grouping characteristics select
a narrow slice of reality which is objectively measurable and whose
membership is fixed (the complete works of Shakespeare, for example).

Therefore, (most) populations are amorphous moving targets. We
subjectively hold them to exist, but in practical terms we often cannot
nail down the specifics of populations. So how do researchers go about
studying populations if they are theoretically impossible to access
directly? The strategy employed is called sampling.

A \index{sampling}\textbf{sample} is the product of a subjective process
of selecting a finite set of observations from an idealized population
with the goal of capturing the relevant characteristics of this
population. When we talk about data in data science, we are talking
about samples.

Whether selecting a sample for your research or evaluating a sample used
in someone else's research, there are two key characteristics to
consider: the sampling frame and the representativeness. The
\textbf{sampling frame} is the set of characteristics that define the
population of interest. The \textbf{representativeness} is the degree to
which the sample reflects the characteristics of the population. Both of
these concern bias, albeit in different ways. By defining the
population, a sampling frame sets the boundaries of the population and
therefore the scope of research based on the sample. This bias is not a
bad thing, in fact, the more clearly defined the sampling frame the
better. Low representativeness, on the other hand, is a type of bias we
would like to avoid. Given the nature of samples, perfect
representativeness is not achievable. That said, there are a series of
sampling strategies that tend to increase the representativeness of a
sample, seen in Table~\ref{tbl-sampling-strategies}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8500}}@{}}
\caption{Sampling strategies to increase
representativeness}\label{tbl-sampling-strategies}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Strategy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Strategy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Size & Larger samples increase the likelihood of representing the
population \\
Randomized & Avoid invertently including bias in selection \\
Stratified & Divide the population into sub-populations, `strata', and
sample from each \\
Balanced & Ensure that the relative size of the strata is reflected in
the sample \\
\end{longtable}

Together, large randomly selected and balanced stratified samples set
the benchmark for sampling. However, hitting this ideal is not always
feasible. There are situations where sizeable samples are not
accessible. Alternatively, there may be instances where the population
or its strata are not well understood. In such scenarios, researchers
have to work with the most suitable sample they can obtain given the
limitations of their research project.

\subsection{Corpora}\label{corpora}

A sample, as just defined, of a language population is called a
\index{corpus}\textbf{corpus} (\emph{pl.} corpora). Corpora are often
classified into various types. These types reflect general
characteristics of the scope of the corpus sampling frame. The most
common types of corpora appear in Table~\ref{tbl-corpus-types}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8500}}@{}}
\caption{Types of corpora}\label{tbl-corpus-types}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sampling scope
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sampling scope
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Reference & General characteristics of a language population \\
Specialized & Specific populations, \emph{e.g.} spoken language,
academic writing, \emph{etc.} \\
Parallel & Directly comparable texts in different languages (\emph{i.e.}
translations) \\
Comparable & Indirectly comparable texts in different languages or
language varieties (\emph{i.e.} similar sampling frames) \\
\end{longtable}

Of the corpus types, \index{corpora!reference}\textbf{reference corpora}
are the least common and most ambitious. These resources aim to model
the characteristics of a language population.
\index{corpora!specialized}\textbf{Specialized corpora} aim to represent
more specific populations. What specialized corpora lack in breadth of
coverage, they make up for in depth of coverage by providing a more
targeted representation of specific language populations.
\index{corpora!parallel}\textbf{Parallel} and
\index{corpora!comparable}\textbf{comparable corpora} are both types of
specialized corpora which aim to model different languages or different
language varieties for direct or indirect comparison, respectively.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

\begin{figure}[H]

\begin{minipage}{0.48\linewidth}
The `Standard Sample of Present-Day American English' (known commonly as
the Brown Corpus) is widely recognized as one of the first large,
machine-readable corpora. Compiled by Kucera and Francis
(\citeproc{ref-Kucera1967}{1967}), the corpus is comprised of 1,014,312
words from edited English prose published in the United States in
1961.\\
\strut \\
Given the sampling frame and the strata and balance for this corpus
visualized in Figure~\ref{fig-ud-brown-distribution}, can you determine
what language population this corpus aims to represent? What types of
research might this corpus support or not support?\end{minipage}%
%
\begin{minipage}{0.03\linewidth}
~\end{minipage}%
%
\begin{minipage}{0.48\linewidth}

\begin{figure}[H]

\centering{

\includegraphics{understanding-data_files/figure-pdf/fig-ud-brown-distribution-1.pdf}

}

\caption{\label{fig-ud-brown-distribution}Overview of the sampling
characteristics of the Brown Corpus.}

\end{figure}%

\end{minipage}%

\end{figure}%

\end{tcolorbox}

In text analysis, corpora are the raw materials of research. The aim of
the quantitative text researcher is to select the corpus, or corpora,
which best align with the purpose of the research. For example, a
reference corpus such as the American National Corpus may be better
suited to address a question dealing with the way American English
works, but this general resource may lack detail in certain areas, such
as medical language, that may be vital for a research project aimed at
understanding changes in medical terminology. Furthermore, a researcher
studying spoken language might collect a corpus of transcribed
conversations from a particular community or region, such as the Santa
Barbara Corpus of Spoken American English. While this would not include
every possible spoken utterance produced by members of that group, it
could be considered a representative sample of the population of speech
in that context.

\subsection{Other considerations}\label{other-considerations}

In preparing and conducting research using corpora, the most primary
concerns is aligning research goals with the corpus resource. However,
there are other, more practical, considerations to keep in mind.

\subsubsection{Access}\label{access}

Ensuring access both in terms of physical access to the data and legal
access to the data should not be overlooked in the design and execution
of a project. Simply put, without access to the data, research cannot
proceed. It is better to consider access early in the research process
to avoid delays and complications later on.

The medium to acquire corpus data most used in contemporary quantitative
research is the internet. Although a general search query can lead you
to corpus data, there are a few primary sources of corpora you should be
aware of, summarized in Table~\ref{tbl-corpus-sources}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3000}}@{}}
\caption{Sources of corpus
data}\label{tbl-corpus-sources}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Language repositories & Repositories that specialize in language data &
\href{https://www.ldc.upenn.edu/}{Language Data Consortium},
\href{http://talkbank.org/}{TalkBank} \\
Data sharing platforms & Platforms that enable researchers to securely
store, manage, and share data & \href{https://github.com}{GitHub},
\href{https://zenodo.org}{Zenodo}, \href{https://osf.io/}{OSF} \\
Developed corpora & Corpora prepared by researchers for research
purposes & APIs, web scraping \\
\end{longtable}

It is always advisable to start looking for data in a \textbf{language
repository}. The advantage of beginning your data search in repositories
is that a repository, especially those geared towards the linguistic
community, will make identifying language corpora faster than through a
general web search. Furthermore, repositories often require certain
standards for corpus format and documentation for publication.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

Explore some of the resources listed on the
\href{https://qtalr.github.io/qtalrkit/articles/guide-4.html}{qtalrkit
companion site} and consider their sampling frames. Can you think of a
research question or questions that this resource may be well-suited to
support research into? What types of questions would be
less-than-adequate for a given resource?

\end{tcolorbox}

As part of a general movement towards reproducibility, more corpora are
available on \index{data!sharing platforms}\textbf{data sharing
platforms}. These platforms enable researchers to securely store,
manage, and share data with others. Support is provided for various
types of data, including documents and code, and as such they are a good
place to look as they often include reproducible research projects as
well.

Finally, if satisfactory data cannot be found in a repository or data
sharing platform, researchers may need to develop their own corpus.
There are two primary ways to attain language data from the web. The
first is through an \textbf{Application Programming Interface} (API).
APIs are, as the title suggests, programming interfaces which allow
access, under certain conditions, to information that a website or
database accessible via the web contains.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

The process of corpus development is a topic in and of itself. For a
more in-depth discussion of the process, see Ädel
(\citeproc{ref-Adel2020}{2020}).

\end{tcolorbox}

The second, more involved, way to acquire data from the web is is
through the process of web scraping. \textbf{Web scraping} is the
process of harvesting data from the public-facing web. Language texts
may be found on sites as uploaded files, such as pdf or doc (Word)
documents, or found displayed as the primary text of a site. Given the
wide variety of documents uploaded and language behavior recorded daily
on news sites, blogs and the like, compiling a corpus has never been
easier. Having said that, how the data is structured and how much data
needs to be retrieved can pose practical obstacles to collecting data
from the web, particularly if the approach is to acquire the data by
manually instead of automating the task.

Beyond physical access to the data, legal access is also a
consideration. Just because data is available on the web does not mean
it is free to use. Repositories, APIs, and individual data resources
often have licensing agreements and terms of use, ranging from public
domain to proprietary licenses. Respecting intellectual property rights
is crucial when working with corpus data. Violating these rights can
lead to legal and ethical issues, including lawsuits, fines, and damage
to one's professional reputation. To avoid these problems, researchers
must ensure they have the necessary permissions to use copyrighted works
in their research.

\subsubsection{Formats}\label{formats}

Whether you are using a published corpus or developing your own, it is
important to understand how the data you want to work with is formatted
so you can ensure that you are prepared to conduct the subsequent
processing steps. When referring to the format of a corpus, this
includes the folder and file structure, the file types, and how file
content is encoded electronically. Yet, the most important
characteristic, especially for language-based data, is the internal
structure of the files themselves. With this in mind less discuss the
difference between unstructured, semi-structured, and structured data.

A corpus may include various types of linguistic (\emph{e.g.} part of
speech, syntactic structure, named entities, \emph{etc.}) or
non-linguistic (\emph{e.g.} source, dates, speaker information,
\emph{etc.}) attributes. These attributes are known as
\textbf{metadata}, or data about data. As a general rule, files which
include more metadata tend to be more internally structured. Internal
file structure refers to the degree to which the content is easy to
query and analyze by a computer. Let's review characteristics of the
three main types of file structure types and associate common file
extensions that files in each have.

\textbf{Unstructured data} is data which does not have a
machine-readable internal structure. This is the case for plain text
files (\emph{.txt}), which are simply a sequence of characters. For
example, in Example~\ref{exm-masc-text} we see a snippet of a plain text
file from the the Manually Annotated Sub-Corpus of American English
(MASC) (\citeproc{ref-Ide2008}{Ide et al. 2008}):

\begin{example}[]\protect\hypertarget{exm-masc-text}{}\label{exm-masc-text}

MASC plain text

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Sound is a vibration. Sound travels as a mechanical wave through a medium, and in space, there is no}
\NormalTok{medium. So when my shuttle malfunctioned and the airlocks didn\textquotesingle{}t keep the air in, I heard nothing. After the}
\NormalTok{first whoosh of the air being sucked away, there was lightning, but no thunder. Eyes bulging in}
\NormalTok{panic, but no screams. Quiet and peaceful, right? Such a relief to never again hear my crewmate Jesse natter}
\NormalTok{about his girl back on Earth and that all{-}expenses{-}paid vacation{-}for{-}two she won last time he was on leave. I}
\NormalTok{swore, if I ever had to see a photo of him in a skimpy bathing suit again, giving the camera a cheesy thumbs{-}up}
\NormalTok{from a lounge chair on one of those white sandy beaches, I\textquotesingle{}d kiss a monkey. Metaphorically, of course.}
\end{Highlighting}
\end{Shaded}

\end{example}

Other examples of files which often contain unstructured data include
\emph{.pdf} and \emph{.docx} files. While these file types may contain
data which appears structured to the human eye, the structure is not
designed to be machine-readable. As such the data would typically be
read into R as a vector of \textbf{character strings}. It is possible to
perform only the most rudimentary queries on this type of data, such as
string matches. For anything more informative, it is necessary to
further process this data, as we will see in
Section~\ref{sec-ud-organization} and
Section~\ref{sec-ud-transformation}.

On the other end of the spectrum, \textbf{structured data} is data which
conforms to a tabular format in which elements in tables and
relationships between tables are defined. This makes querying and
analyzing easy and efficient. Relational databases (\emph{e.g.} MySQL,
PostgreSQL, \emph{etc}.) are designed to store and query structured
data. The data frame object in R is also a structured data format. In
each case, the data is stored in a tabular format in which each row
represents a single observation and each column represents a single
attribute whose values are of the same type.

In Example~\ref{exm-masc-df} we see an example of an R data frame object
which overlaps with the language in the plain text file in
Example~\ref{exm-masc-text}:

\begin{example}[]\protect\hypertarget{exm-masc-df}{}\label{exm-masc-df}

MASC data frame

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   title             date modality domain          ref\_num word       lemma      pos}
\NormalTok{   \textless{}}\KeywordTok{chr}\NormalTok{\textgreater{}            \textless{}}\KeywordTok{dbl}\NormalTok{\textgreater{} \textless{}}\KeywordTok{fct}\NormalTok{\textgreater{}    \textless{}}\KeywordTok{chr}\NormalTok{\textgreater{}             \textless{}}\KeywordTok{int}\NormalTok{\textgreater{} \textless{}}\KeywordTok{chr}\NormalTok{\textgreater{}      \textless{}}\KeywordTok{chr}\NormalTok{\textgreater{}      \textless{}}\KeywordTok{chr}\NormalTok{\textgreater{}}
\NormalTok{ 1 Hotel California  2008 Writing  General Fiction       1 Sound      sound      NNP}
\NormalTok{ 2 Hotel California  2008 Writing  General Fiction       2 is         be         VBZ}
\NormalTok{ 3 Hotel California  2008 Writing  General Fiction       3 a          a          DT}
\NormalTok{ 4 Hotel California  2008 Writing  General Fiction       4 vibration  vibration  NN}
\NormalTok{ 5 Hotel California  2008 Writing  General Fiction       5 .          .          .}
\NormalTok{ 6 Hotel California  2008 Writing  General Fiction       6 Sound      sound      NNP}
\NormalTok{ 7 Hotel California  2008 Writing  General Fiction       7 travels    travel     VBZ}
\NormalTok{ 8 Hotel California  2008 Writing  General Fiction       8 as         as         IN}
\NormalTok{ 9 Hotel California  2008 Writing  General Fiction       9 a          a          DT}
\NormalTok{10 Hotel California  2008 Writing  General Fiction      10 mechanical mechanical JJ}
\end{Highlighting}
\end{Shaded}

\end{example}

Here we see that the data is stored in a tabular format with each row
representing a single observation (\texttt{word}) and each column
representing a single attribute. This tabular structure supports the
increased number of metadata attributes. Internally, R applies a schema
to ensure the values in each column are of the same type (\emph{e.g.}
\texttt{\textless{}chr\textgreater{}},
\texttt{\textless{}dbl\textgreater{}},
\texttt{\textless{}fct\textgreater{}}, \emph{etc.}). This structured
format is designed to be easy to query and analyze and as such is the
primary format for data analysis in R.

\textbf{Semi-structured data} falls between unstructured and structured
data. This covers a wide range of file structuring approaches. For
example, a otherwise plain text file with part-of-speech tags appended
to each word is minimally structured (Example~\ref{exm-masc-pos}).

\begin{example}[]\protect\hypertarget{exm-masc-pos}{}\label{exm-masc-pos}

MASC plain text with part-of-speech tags

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Sound/NNP is/VBZ a/DT vibration/NN ./. Sound/NNP travels/VBZ as/IN a/DT mechanical/JJ wave/NN through/IN a/DT medium/NN ,/, and/CC in/IN space/NN ,/, there/EX is/VBZ no/DT medium/NN ./. So/RB when/WRB my/PRP$ shuttle/NN malfunctioned/JJ and/CC the/DT airlocks/NNS did/VBD n\textquotesingle{}t/RB keep/VB the/DT air/NN in/IN ,/, I/PRP heard/VBD nothing/NN ./. After/IN the/DT}
\end{Highlighting}
\end{Shaded}

\end{example}

Towards the more structured end of semi-structured data, many file
formats including \emph{.xml} and \emph{.json} contain highly
structured, hierarchical data. For example, in
Example~\ref{exm-masc-xml} shows a snippet from a \emph{.xml} file from
the MASC corpus.

\begin{example}[]\protect\hypertarget{exm-masc-xml}{}\label{exm-masc-xml}

MASC XML

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}}\KeywordTok{a}\OtherTok{ xml:id=}\StringTok{"penn{-}N264215"}\OtherTok{ label=}\StringTok{"tok"}\OtherTok{ ref=}\StringTok{"penn{-}n7345"}\OtherTok{ as=}\StringTok{"anc"}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{fs}\NormalTok{\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{f}\OtherTok{ name=}\StringTok{"base"}\OtherTok{ value=}\StringTok{"sound"}\NormalTok{/\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{f}\OtherTok{ name=}\StringTok{"msd"}\OtherTok{ value=}\StringTok{"NNP"}\NormalTok{/\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{f}\OtherTok{ name=}\StringTok{"string"}\OtherTok{ value=}\StringTok{"Sound"}\NormalTok{/\textgreater{}}
\NormalTok{  \textless{}/}\KeywordTok{fs}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{a}\NormalTok{\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{node}\OtherTok{ xml:id=}\StringTok{"penn{-}n7346"}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{link}\OtherTok{ targets=}\StringTok{"seg{-}r13152"}\NormalTok{/\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{node}\NormalTok{\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{a}\OtherTok{ xml:id=}\StringTok{"penn{-}N264243"}\OtherTok{ label=}\StringTok{"tok"}\OtherTok{ ref=}\StringTok{"penn{-}n7346"}\OtherTok{ as=}\StringTok{"anc"}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{fs}\NormalTok{\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{f}\OtherTok{ name=}\StringTok{"string"}\OtherTok{ value=}\StringTok{"is"}\NormalTok{/\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{f}\OtherTok{ name=}\StringTok{"msd"}\OtherTok{ value=}\StringTok{"VBZ"}\NormalTok{/\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{f}\OtherTok{ name=}\StringTok{"base"}\OtherTok{ value=}\StringTok{"be"}\NormalTok{/\textgreater{}}
\NormalTok{  \textless{}/}\KeywordTok{fs}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{a}\NormalTok{\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{node}\OtherTok{ xml:id=}\StringTok{"penn{-}n7347"}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{link}\OtherTok{ targets=}\StringTok{"seg{-}r13154"}\NormalTok{/\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{node}\NormalTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

\end{example}

The format of semi-structured data is often influenced by
characteristics of the data or reflect an author's individual
preferences. It is sometimes the case that data will be semi-structured
in a less-standard format. For example, the SWDA corpus includes a
\emph{.utt} file extension for files which contain utterances annotated
with dialogue act tags.

\begin{example}[]\protect\hypertarget{exm-swda-utt}{}\label{exm-swda-utt}

SWDA \emph{.utt} file

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{o          A.1 utt1: Okay.  /}
\NormalTok{qw          A.1 utt2: \{D So, \}}

\NormalTok{qy\^{}d          B.2 utt1: [ [ I guess, +}

\NormalTok{+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /}

\NormalTok{+          B.4 utt1: I think, ] + \{F uh, \} I wonder ] if that worked. /}

\NormalTok{qy          A.5 utt1: Does it say something? /}
\end{Highlighting}
\end{Shaded}

\end{example}

Whether standard or not, semi-structured data is often designed to be
machine-readable. As with unstructured data, the ultimate goal is to
convert the data into a structured format and augment the data where
necessary to prepare it for a particular research analysis.

\section{Information}\label{information}

Identifying an adequate corpus resource, in terms of content, access,
and formatting, for the target research question is the first step in
moving a quantitative text research project forward. The next step is to
select the components or characteristics of this resource that are
relevant for the research and then move to organize the attributes of
this data into a more informative format. This is the process of
converting corpus data into a \index{dataset}\textbf{dataset} --a
tabular representation of particular attributes of the data as the basis
for generating information. Once the data represented as dataset, it is
often manipulated and transformed adjusting and augmenting the data such
that it better aligns with the research question and the target
analytical approach.

\subsection{Organization}\label{sec-ud-organization}

Data alone is not informative. Only through explicit organization of the
data in a way that makes relationships and meaning explicit does data
become information. In this form, our data is called a dataset. This is
a particularly salient hurdle in text analysis research. Many textual
sources are unstructured or semi-structured. This means relationships
that will be used in the analysis have yet to be purposefully drawn and
organized as a dataset.

\subsubsection{Tidy Data}\label{sec-ud-tidy-data}

The selection of the attributes from a corpus and the juxtaposition of
these attributes in a relational format, or dataset, that converts data
into information is known as \textbf{data curation}. The process of data
curation minimally involves deriving a base dataset, or \emph{curated
dataset}, which establishes the main informational associations
according to philosophical approach outlined by Wickham
(\citeproc{ref-Wickham2014a}{2014}).

In this work, a \textbf{tidy dataset} refers both to the structural
(physical) and informational (semantic) organization of the dataset.
Physically, a tidy dataset is a tabular data structure, illustrated in
Figure~\ref{fig-ud-tidy-format-image}, where each \emph{row} is an
observation and each \emph{column} is a variable that contains measures
of a feature or attribute of each observation. Each cell where a given
row-column intersect contains a \emph{value} which is a particular
attribute of a particular observation for the particular
observation-feature pair also known as a \emph{data point}.

\begin{figure}[H]

\centering{

\includegraphics[width=2.76in,height=\textheight]{figures/ud-tidy.drawio.png}

}

\caption{\label{fig-ud-tidy-format-image}Visual summary of the tidy
format.}

\end{figure}%

In terms of semantics, columns and rows both contribute to the
informational value of the dataset. Let's start with columns. In a tidy
dataset, each column is a variable, an attribute that can take on a
number of values. Although variables vary in terms of values, they do
not in type. A variable is of one and only one informational type.
Statistically speaking, informational types are defined as
\textbf{levels of measurement}, a classification system used to
semantically distiguish between types of variables. There are four
levels (or types) in this system: nominal, ordinal, interval, and ratio.

In practice, however, text analysis researchers often group these levels
into three main informational types: categorical, ordinal, and numeric
(\citeproc{ref-Gries2021a}{S. T. Gries 2021}). What do these
informational types represent? \textbf{Categorical data} is for labeled
data or classes that answer the question ``what?'' \textbf{Ordinal data}
is categorical data with rank order that answers the question ``what
order?'' \textbf{Numeric data} is ordinal data with equal intervals
between values that answers the question ``how much or how many?''

Let's look at an example of a tidy dataset. Using the criteria just
described, let's see if we can identify the informational values
(categorical, ordinal, or numeric) of the variables that appear in a
snippet from the MASC corpus in dataset form in
Table~\ref{tbl-ud-info-values-masc}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2235}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1294}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0824}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1176}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1529}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0706}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1647}}@{}}

\caption{\label{tbl-ud-info-values-masc}MASC dataset variables.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
title
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
date
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ref\_num
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
word
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
pos
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
num\_letters
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Hotel California & Writing & 2008 & 1 & Sound & NNP & 5 \\
Hotel California & Writing & 2008 & 2 & is & VBZ & 2 \\
Hotel California & Writing & 2008 & 3 & a & DT & 1 \\
Hotel California & Writing & 2008 & 4 & vibration & NN & 9 \\
Hotel California & Writing & 2008 & 5 & . & . & 1 \\
Hotel California & Writing & 2008 & 6 & Sound & NNP & 5 \\
Hotel California & Writing & 2008 & 7 & travels & VBZ & 7 \\
Hotel California & Writing & 2008 & 8 & as & IN & 2 \\
Hotel California & Writing & 2008 & 9 & a & DT & 1 \\
Hotel California & Writing & 2008 & 10 & mechanical & JJ & 10 \\

\end{longtable}

We have seven variables listed as headers for each of the columns. We
could go one-by-one left-to-right but let's take another tack. Instead,
let's identify all those variables that cannot be numeric --these are
all the non-numeral variables: \texttt{title}, \texttt{modality},
\texttt{word}, and \texttt{pos}. The question to ask of these variables
is whether they represent an order or rank. Since titles, modalities,
words, and parts-of-speech are not ordered values, they are all
categorical.

Now in relation to \texttt{date}, \texttt{ref\_num}, and
\texttt{num\_letters}. All three are numerals, so they could be numeric.
But they could also be numeral representations of ordinal data. Before
we can move forward, we need to make sure we understand what each
variable means and how it is measured, or \textbf{operationalized}. The
variable name and the values can be helpful in this respect.
\texttt{date} is what it sounds like, a date, and is operationalized as
a year in the Gregorian calendar. And \texttt{num\_letters} seems quite
descriptive as well, number of letters, appearing as a letter count. But
in some cases it may be opaque as to what is being measured by the
variable name alone, for example \texttt{ref\_num}, and one will have to
refer to the dataset documentation. In this case \texttt{ref\_num} is a
reference number operationalized as a unique identifier for each word
per document in the corpus.

With this in mind, let's return to the question of whether
\texttt{date}, \texttt{ref\_num}, and \texttt{num\_letters} are numeric
or ordinal. Starting with the trickiest one, \texttt{date}, we can ask
the question to identify numeric data: ``how much or how many?''. In the
case of \texttt{date}, the answer is neither. A date is a point in time,
not a quantity. So \texttt{date} is not numeric. But it does provide
information about order. Hence, \texttt{date} is ordinal.
\texttt{ref\_num} is also ordinal because the question ``what order?''
can be asked of it. Finally, \texttt{num\_letters} is numeric because it
answers the question ``how many?''.

Let's turn to the second semantic value of a tidy dataset. In a tidy
dataset, each row is an observation. But an observation of what? This
depends on what the unit of observation is. That sounds circular, but
its not. The \textbf{unit of observation} is simply the primary entity
that is being observed or measured (\citeproc{ref-Sedgwick2015}{Sedgwick
2015}). Even without context, it can often be identified in a dataset by
looking at the level of specificity of the variable values and asking
what each variable describes. When one variable appears to be the most
individualized and other variables appear to describe that variable,
then the most individualized variable is likely the unit of observation
of the dataset, \emph{i.e.} the meaning of each observation.

Applying these strategies to the Table in \ref{tbl-ud-info-values-masc},
we can see that each observation at its core is a word. We see that the
values of each observation are the attributes of each word.
\texttt{word} is the most individualized variable and the \texttt{pos}
(part-of-speech), \texttt{num\_letters}, and \texttt{ref\_num} all
describe the word.

The other variables \texttt{title}, \texttt{modality}, and \texttt{date}
are not direct attributes of the word. Instead, they are attributes of
the document in which the word appears. Together, however, they all
provide information about the word.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

Data can be organized in many ways. It is important to make clear that
data in tabular format in itself does not constitute a dataset, in the
tidy sense we will be using. Can you think of examples of tabular
information that would not be in a tidy format? What would be the
implications of this for data analysis?

\end{tcolorbox}

As we round out this section on data organization, it is important to
stress that the purpose of curation is to represent the corpus data in
an informative, tidy format. A curated dataset serves as a reference
point making relationships explicit, enabling more efficient querying,
and paving the way for further processing before analysis. In the
subsequent section, we will highlight common approaches to modifying the
curated dataset, either row-wise or column-wise, to make it more
amenable to the particular aims of a given analysis leading to one or
more \emph{transformed datasets}.

\subsection{Transformation}\label{sec-ud-transformation}

At this point have introduced the first step towards creating a dataset
ready for analysis, data curation. However, a curated dataset is rarely
the final organizational step before proceeding to statistical analysis.
Many times, if not always, the curated dataset requires
\textbf{transformation} to derive or generate new data for the dataset.
This process may incur row-wise (observation) or column-wise (variable)
level changes, as illustrated in Figure~\ref{fig-ud-transformations}.

\begin{figure}[H]

\centering{

\includegraphics[width=2.65in,height=\textheight]{figures/ud-transformations.drawio.png}

}

\caption{\label{fig-ud-transformations}Visualization of row-wise and
column-wise transformation operations on a dataset.}

\end{figure}%

The results build on and manipulate the curated dataset to produce a
\emph{transformed dataset}. While there is typically one curated dataset
that serves as the base organizational dataset, there may be multiple
transformed datasets, each aligning with the informational needs of
specific analyses in the research project.

In what follows, we will group common transformation processes into two
purpose-based groupings: preparation and enrichment. The purpose of
preparation transformations is to clean, standardize, and derive the key
attributes of the dataset on which further processing will depend.
Enrichment transformations, on the other hand, are designed to add new
attributes to the dataset. These attributes may be derived from the
existing attributes or may be integrated from other datasets. Together,
the goal of these transformations is to make the dataset more
informative and more amenable to the particular aims of a given
analysis.

\subsubsection{Preparation}\label{preparation}

Common preparation transformations include text normalization and text
tokenization. \textbf{Text normalization} is the process of
standardizing text to convert the text into a uniform format and reduce
unwanted variation and noise. It is often a preliminary step in data
transformation processes which include variables with text.

Let's take a toy dataset, in Table~\ref{tbl-ud-text-dataset}, as an
example starting point. In this dataset, we have three variables,
\texttt{text\_id}, \texttt{sent\_id}, and \texttt{sentence}. It has five
observations.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.8000}}@{}}

\caption{\label{tbl-ud-text-dataset}A toy dataset with three variables,
\texttt{text\_id}, \texttt{sent\_id}, and \texttt{sentence}.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sent\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sentence
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 1 & It's a beautiful day in the US, and our group decided to visit
the famous Grand Canyon. \\
1 & 2 & As we reached the destination, Jane said, ``I can't believe
we're finally here!'' \\
1 & 3 & The breathtaking view left us speechless; indeed, it was a sight
to behold. \\
1 & 4 & During our trip, we encountered tourists from different
countries, sharing stories and laughter. \\
1 & 5 & For all of us, this experience will be cherished forever. \\

\end{longtable}

The types of transformations we apply will depend on the specific needs
of the project, but can include those found in
Table~\ref{tbl-ud-text-normalization}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4500}}@{}}
\caption{Common text normalization
tasks}\label{tbl-ud-text-normalization}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Task name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Relevant example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical purpose
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Task name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Relevant example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Lowercasing & \texttt{"Text"} to \texttt{"text"} & Minimizing case
sensitivity in subsequent analysis \\
Removal of Punctuation and Special Characters &
\texttt{"Hello,\ World!"} to \texttt{"Hello\ World"} & Removing
non-alphanumeric characters that may not carry semantic value \\
Adjustment of Forms & \texttt{"colour"} to \texttt{"color"},
\texttt{"it\textquotesingle{}s"} to \texttt{"it\ is"}, \texttt{"1"} to
\texttt{"one"} & Standardizing variations in spelling, contractions, and
numeric forms to a common format \\
\end{longtable}

These transformations are column-wise operations, meaning they preserve
the number of rows in the dataset. They also preserve the number of
columns, but \emph{do} change the values of the variables. These tasks
should be applied with an understanding of how the changes will impact
the analysis. For example, lowercasing can be useful for reducing
differences between words that are otherwise identical, yet differ in
case due to word position in a sentence (``The'' versus ``the'').
However, lowercasing can also be problematic if the case of the word
carries semantic value, such as in the case of ``US'' (United States)
and ``us'' (first person plural pronoun).

\textbf{Text tokenization} involves adapting the text such that it
reflects the target linguistic unit that will be used in the analysis.
This is a row-wise operation expanding the number of rows, if the
linguistic unit is smaller than the original variable, or reducing the
number of rows, if the linguistic unit is larger than the original
variable.

Text variables can be tokenized at any linguistic level, to the extent
we can operationalize the linguistic unit. For example, a text variable
can be tokenized into characters, words, sentences, \emph{etc}. The
choice of tokenization level often reflects the unit of observation of
the dataset. For example, if the unit of observation is a word, then the
text variable will be tokenized into words. If the unit of observation
is a sentence, then the text variable will be tokenized into sentences.

Sequential groupings of characters and words are also common
tokenization units. These are known as \textbf{n-grams}, where \emph{n}
is the number of words or characters in the grouping. For example, a
word bigram is a sequence of two words, and a character trigram is a
sequence of three characters.

In Table~\ref{tbl-ud-tokenization}, we see examples of tokenization at
word and character levels.

\begin{table}

\caption{\label{tbl-ud-tokenization}Examples of tokenization at word and
character levels.}

\begin{minipage}{0.33\linewidth}

\subcaption{\label{tbl-ud-tokenization-1}Word tokenization}

\centering{

\begin{tabular}{lll}
\toprule
text\_id & sent\_id & word\\
\midrule
1 & 1 & it's\\
1 & 1 & a\\
1 & 1 & beautiful\\
1 & 1 & day\\
1 & 1 & in\\
1 & 1 & the\\
1 & 1 & us\\
1 & 1 & and\\
1 & 1 & our\\
1 & 1 & group\\
\bottomrule
\end{tabular}

}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\subcaption{\label{tbl-ud-tokenization-2}Word bigram tokenization}

\centering{

\begin{tabular}{lll}
\toprule
text\_id & sent\_id & bigram\\
\midrule
1 & 1 & it's a\\
1 & 1 & a beautiful\\
1 & 1 & beautiful day\\
1 & 1 & day in\\
1 & 1 & in the\\
1 & 1 & the us\\
1 & 1 & us and\\
1 & 1 & and our\\
1 & 1 & our group\\
1 & 1 & group decided\\
\bottomrule
\end{tabular}

}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\subcaption{\label{tbl-ud-tokenization-3}Character trigram tokenization}

\centering{

\begin{tabular}{lll}
\toprule
text\_id & sent\_id & trigram\\
\midrule
1 & 1 & its\\
1 & 1 & tsa\\
1 & 1 & sab\\
1 & 1 & abe\\
1 & 1 & bea\\
1 & 1 & eau\\
1 & 1 & aut\\
1 & 1 & uti\\
1 & 1 & tif\\
1 & 1 & ifu\\
\bottomrule
\end{tabular}

}

\end{minipage}%

\end{table}%

At its core, tokenization is the process which enables the quantitative
analysis of text. Choosing the right tokenization level is crucial for
the success of the analysis.

\subsubsection{Enrichment}\label{enrichment}

Let's now turn to transformations that augment the variables of the
dataset. Common enrichment transformations include recoding, generation,
and integration of observations and/ or variables.

\textbf{Recoding} is the process of transforming the values of one or
more variables into new values which are more amenable to analysis. This
is a column-wise operation which can be applied to categorical or
numeric variables alike.

The aim is to simplify complex variables, making it easier to identify
patterns and trends relevant for the research question. Say for example,
we have a variable in our datataset that represents the part-of-speech
(POS) of each word token in the text. The measure is a POS tag from the
Penn Treebank tagset (\citeproc{ref-Marcus1993}{Marcus, Santorini, and
Marcinkiewicz 1993}). This tagset makes twelve major and 45 minor
grammatical class distinctions. In an analysis that aims to explore only
major class distinctions, it would be useful to recode the POS variable
into major classes only (\emph{i.e.} noun, pronoun, adjective, verb,
adverb, \emph{etc.}) to facilitate queries, summaries, and
visualizations.

Along these lines, the surface forms of words can be recoded to their
lemmatized or stemmed forms. \textbf{Stemming} is the process of
reducing inflected words to their word stem, base, or root form.
\textbf{Lemmatization} is the process of reducing inflected words to
their dictionary form, or lemma.

In Table~\ref{tbl-ud-recode-categorical-lemma}, we see an example of
recoding surface forms of words to their lemmatized forms.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1389}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1389}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1944}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1944}}@{}}

\caption{\label{tbl-ud-recode-categorical-lemma}A toy dataset
illustrating recoding of surface forms of words to their lemmatized
forms.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sent\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
word
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
lemma
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 2 & as & as \\
1 & 2 & we & we \\
1 & 2 & reached & reach \\
1 & 2 & the & the \\
1 & 2 & destination & destination \\
1 & 2 & jane & jane \\
1 & 2 & said & say \\
1 & 2 & i & i \\
1 & 2 & can & can \\
1 & 2 & not & not \\
1 & 2 & believe & believe \\
1 & 2 & we & we \\
1 & 2 & are & be \\
1 & 2 & finally & finally \\
1 & 2 & here & here \\

\end{longtable}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-alt} Case study}

Inflectional family size is the number of inflectional forms for a given
word and can be calculated from a corpus by counting the number of
surface forms for each lemma in the corpus
(\citeproc{ref-Kostic2003}{Kostić, Marković, and Baucal 2003}). R.
Harald Baayen, Feldman, and Schreuder (\citeproc{ref-Baayen2006}{2006})
found that words with larger inflectional family size are associated
with faster word recognition times in lexical processing tasks.

\end{tcolorbox}

Recoding transformations can be useful for simplifying complex
variables, making it easier to identify patterns, as we see in
Table~\ref{tbl-ud-recode-categorical-lemma}. However, it is important to
note that recoding can also lead to loss of information. For example,
recoding a variable with many levels into a binary variable can lead to
loss of information about the original levels. It is important to
consider the trade-offs of recoding and to ensure that the recoding
aligns with the research question.

Furthermore, it is important to note recoding can be used to create more
nuanced variables instead of more simplified. This draws on the
combination of level information from multiple variables creating a
variable that reflects a more complex relationship that was previously
implicit in the dataset.

\textbf{Generation} is the process of creating new variables from
existing variables, and as such it is a column-wise operation.
Generation can include applying calculations or extracting relevant
information from existing variables or enhancing text variables with
linguistic annotation. Simplifying a bit, generation helps make implicit
attributes explicit.

Let's highlight a some common calculation and extraction examples that
generate variables. First, let's look at the calculation of measures. In
text analysis, measures are often used to describe the properties of a
document or linguistic unit. For example, the number of words in a
corpus document, the lengths of sentences, the number of clauses in a
sentence, \emph{etc.}. In turn, these measures can be used to calculate
other measures, such as lexical diversity or syntactic complexity
measures.

In terms of extraction, the goal is to distill relevant information from
existing variables. For example, extracting the year from a date
variable, or extracting the first name from a full name variable. In
text analysis, extraction is often used to extract information from text
variables. Say we have a dataset with a variable containing conversation
utterances. We may want to extract some characteristic from those
utterances and capture their occurrence in a new variable.

But what if we want to extract linguistic information from a text
variable that is not explicitly present in the text? This is where
linguistic annotation comes in. Linguistic annotation is the process of
enriching text with linguistic information, such as part-of-speech tags,
morphological features, syntactic structure, \emph{etc.}. This can be
done manually by linguist coders and/ or done using natural language
processing (NLP) tools, such as part-of-speech taggers, parsers,
\emph{etc.}.

To illustrate the process of linguistic annotation, we will start with
the dataset from Table~\ref{tbl-ud-text-dataset}. Applying a pre-trained
model from the \href{https://universaldependencies.org/}{Universal
Dependencies (UD)}\footnote{The Universal Dependency project is an
  effort to develop cross-linguistically consistent treebank annotation
  for many languages. The project has developed a set of annotation
  guidelines and a set of tools for generating linguistic annotation.
  The project has also developed a set of pre-trained models for many
  languages.} project, we can generate linguistic annotation for each
word in the \texttt{sentence} variable.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0700}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0700}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.6100}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}

\caption{\label{tbl-ud-generate-annotation}Automatic linguistic
annotation for grammatical category and syntactic structure for an
example English sentence from the MASC.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
token\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
token
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
pos
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
features
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
syn\_relation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & It & PRP &
Case=Nom\textbar Gender=Neut\textbar Number=Sing\textbar Person=3\textbar PronType=Prs
& nsubj \\
2 & 's & VBZ &
Mood=Ind\textbar Number=Sing\textbar Person=3\textbar Tense=Pres\textbar VerbForm=Fin
& cop \\
3 & a & DT & Definite=Ind\textbar PronType=Art & det \\
4 & beautiful & JJ & Degree=Pos & amod \\
5 & day & NN & Number=Sing & root \\
6 & in & IN & NA & case \\
7 & the & DT & Definite=Def\textbar PronType=Art & det \\
8 & US & NNP & Number=Sing & nmod \\
9 & , & , & NA & punct \\
10 & and & CC & NA & cc \\
11 & our & PRP\$ &
Number=Plur\textbar Person=1\textbar Poss=Yes\textbar PronType=Prs &
nmod:poss \\
12 & group & NN & Number=Sing & nsubj \\
13 & decided & VBD & Mood=Ind\textbar Tense=Past\textbar VerbForm=Fin &
conj \\
14 & to & TO & NA & mark \\
15 & visit & VB & VerbForm=Inf & xcomp \\
16 & the & DT & Definite=Def\textbar PronType=Art & det \\
17 & famous & JJ & Degree=Pos & amod \\
18 & Grand & NNP & Number=Sing & compound \\
19 & Canyon & NNP & Number=Sing & obj \\
20 & . & . & NA & punct \\

\end{longtable}

The annotated dataset now includes the key variables \texttt{pos} (Penn
treebank tags), \texttt{features} (morphological features), and
\texttt{syn\_relation}. The results of this process can then be further
transformed as need be to fit the needs of the analysis. The results of
this process enables more direct access during analysis to features that
were hidden or otherwise difficult to access.

A word of caution: automated linguistic annotation offers rapid access
to abundant and highly dependable linguistic data for numerous
languages. However, linguistic annotation tools are not infallible. They
are tools developed by training computational algorithms to identify
patterns in previously annotated and verified datasets, resulting in a
language model. This model is then employed to predict linguistic
annotations for new language data (as seen in
Table~\ref{tbl-ud-generate-annotation}). The accuracy of the linguistic
annotation heavily relies on the congruence between the language
sampling frame of the trained data and that of the dataset to be
automatically annotated.

\textbf{Integration} is a transformation step which can be row-wise or
column-wise. Row-wise integration is the process of combining datasets
by appending observations from one dataset to another. Column-wise
integration is the process of combining datasets by appending variables
from one dataset to another.

To integrate in row-wise manner the datasets involved in the process
must have the same variables and variable types. This process is often
referred to as \textbf{concatenating datasets}. It can be thought of as
stacking datasets on top of each other to create a larger dataset.
Remember, having the same variables and variable types is not the same
has having the same values.

Take, for example, a case when a corpus resource contains data for two
populations. In the course of curating and transforming the datasets, it
may make more sense to work with the datasets separately. However, when
it comes time to analyze the data, it may be more convenient to work
with the datasets as a single dataset. In this case, the datasets can be
concatenated to create a single dataset.

To illustate, consider the toy datasets in
Table~\ref{tbl-ud-merge-dataset-written} and
Table~\ref{tbl-ud-merge-dataset-spoken}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5500}}@{}}

\caption{\label{tbl-ud-merge-dataset-written}Toy dataset of written text
data.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
participant\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
P1 & T1 & Written & Technology has revolutionized our lives in many
ways. It has made communication easier, faster, and more efficient. \\
P3 & T3 & Written & Climate change is a pressing issue that affects
everyone on Earth. We must take immediate action to reduce our carbon
footprint. \\
P5 & T5 & Written & Education is the key to personal and societal
growth. Investing in quality education will lead to a brighter future
for all. \\

\end{longtable}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5500}}@{}}

\caption{\label{tbl-ud-merge-dataset-spoken}Toy dataset of spoken text
data.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
participant\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
P2 & T2 & Spoken & Hello, my name is X. I am a software engineer working
at XYZ company. \\
P4 & T4 & Spoken & Hi, I'm X, and I work as a project manager. My main
responsibility is to ensure that projects are completed on time and
within budget. \\
P6 & T6 & Spoken & Hi, my name is X, and I'm a teacher. I teach English
at a local high school. \\

\end{longtable}

These datasets, in Table~\ref{tbl-ud-merge-dataset-written} and
Table~\ref{tbl-ud-merge-dataset-spoken}, contain the same variables and
variable types, but different observations --one in which the sample
contains written language and the other spoken. Conveniently, they can
be concatenated to create a single dataset that contains all of the
observations, as seen in Table~\ref{tbl-ud-merge-dataset-concat}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5500}}@{}}

\caption{\label{tbl-ud-merge-dataset-concat}Toy dataset of written and
spoken text data concatenated.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
participant\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
P1 & T1 & Written & Technology has revolutionized our lives in many
ways. It has made communication easier, faster, and more efficient. \\
P3 & T3 & Written & Climate change is a pressing issue that affects
everyone on Earth. We must take immediate action to reduce our carbon
footprint. \\
P5 & T5 & Written & Education is the key to personal and societal
growth. Investing in quality education will lead to a brighter future
for all. \\
P2 & T2 & Spoken & Hello, my name is X. I am a software engineer working
at XYZ company. \\
P4 & T4 & Spoken & Hi, I'm X, and I work as a project manager. My main
responsibility is to ensure that projects are completed on time and
within budget. \\
P6 & T6 & Spoken & Hi, my name is X, and I'm a teacher. I teach English
at a local high school. \\

\end{longtable}

Integrating datasets can be performed in a column-wise manner as well.
In this process, the datasets need not have the exact same variables and
variable types, rather it is required that the datasets share a common
variable of the same informational type that can be used to index the
datasets. This process is often referred to as \textbf{joining
datasets}.

Corpus resources often include metadata in stand-off annotation format.
That is, the metadata is not embedded in the corpus files, but rather is
stored in a separate file. The metatdata and corpus files will share a
common variable which is used to join the metadata with the corpus
files.

To exemplify, here's another toy dataset that shares the
\texttt{participant\_id} index with the previous dataset in
Table~\ref{tbl-ud-merge-dataset-concat} and includes the variables
\texttt{eng\_native}, \texttt{age}, and \texttt{gender}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2361}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1806}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1250}}@{}}

\caption{\label{tbl-ud-merge-vars-participant}Toy dataset of participant
data with a shared variable \texttt{participant\_id} to index the
datasets.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
participant\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
eng\_native
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
gender
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
P1 & Yes & 28 & M \\
P2 & No & 35 & M \\
P3 & Yes & 42 & F \\
P4 & No & 26 & F \\
P5 & Yes & 31 & M \\
P6 & No & 39 & F \\

\end{longtable}

This dataset provides additional information about each participant,
such as their English native speaker status, age, and gender.

Since the two datasets share the \texttt{participant\_id} variable, we
can merge them to create a new dataset that combines the information
from both datasets, as we see in Table~\ref{tbl-ud-merge-join}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0800}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0800}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0800}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0800}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0800}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0800}}@{}}

\caption{\label{tbl-ud-merge-join}Joining variables from two datasets
based on a shared index variable.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
participant\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
eng\_native
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
gender
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
P1 & T1 & Written & Technology has revolutionized our lives in many
ways. It has made communication easier, faster, and more efficient. &
Yes & 28 & M \\
P3 & T3 & Written & Climate change is a pressing issue that affects
everyone on Earth. We must take immediate action to reduce our carbon
footprint. & Yes & 42 & F \\
P5 & T5 & Written & Education is the key to personal and societal
growth. Investing in quality education will lead to a brighter future
for all. & Yes & 31 & M \\
P2 & T2 & Spoken & Hello, my name is X. I am a software engineer working
at XYZ company. & No & 35 & M \\
P4 & T4 & Spoken & Hi, I'm X, and I work as a project manager. My main
responsibility is to ensure that projects are completed on time and
within budget. & No & 26 & F \\
P6 & T6 & Spoken & Hi, my name is X, and I'm a teacher. I teach English
at a local high school. & No & 39 & F \\

\end{longtable}

Joining datasets is a powerful tool for enriching a dataset with
additional column-wise information. It is important to note that
integrating datasets can also remove information in a row-wise manner.
For example, when merging two datasets with a shared variable, it is
possible to remove observations that do not have a match one of the two
datasets. This process effectively filters out observations not shared
between the two datasets. On the other hand, an anti-join explicitly
removes observations that are shared between the two datasets.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

In some analyses, it may be useful to remove words with little semantic
value, such as articles, prepositions, and conjunctions or words that
are very common in the language. These are known as stopwords. There are
various predefined lists of stopwords for different languages available
on the web and through R in the \texttt{stopwords} package
(\citeproc{ref-R-stopwords}{Benoit, Muhr, and Watanabe 2021}).
Anti-joining a stopword list with a dataset of word tokens is often used
to remove stopwords from the dataset.

However, it is important to note the criteria used to determine which
words are considered stopwords in a particular resource may not fit a
researcher's needs or the characteristics of the data. Learn more about
approaches to identifying stopwords in Kaur and Buttar
(\citeproc{ref-Kaur2018}{2018}).

\end{tcolorbox}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In sum, the transformation steps described here collectively aim to
produce higher quality datasets that are relevant in content and
structure to submit to analysis. The process may include one or more of
the previous transformations but is rarely linear and is most often
iterative. It is typical to do some normalization then generation, then
recoding, and then return to normalizing, and so forth. This process is
highly idiosyncratic given the characteristics of the curated dataset
and the ultimate goal for the transformed dataset(s).

\section{Documentation}\label{sec-ud-documentation}

As we have seen in this chapter, acquiring corpus data and converting
that data into information involves a number of conscious decisions and
implementation steps. As a favor to ourselves, as researchers, and to
the research community, it is crucial to document these decisions and
steps. This makes it both possible to retrace our own steps and also
provides a guide for future researchers that want to reproduce and/ or
build on your research. A programmatic approach to quantitative research
helps ensure that the implementation steps are documented and
reproducible but it is also vital that the decisions that are made are
documented as well. This includes data origin information for the
acquired corpus data and data dictionaries for the curated and
transformed datasets.

\subsection{Data origin}\label{sec-ud-data-origin}

Data acquired from corpus resources should be accompanied by information
about the \textbf{data origin}. Table~\ref{tbl-ud-data-origin} provides
a list of the types of information that should be included in the data
origin information.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7500}}@{}}

\caption{\label{tbl-ud-data-origin}Data origin information.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Information
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Resource name & Name of the corpus resource. \\
Data source & URL, DOI, \emph{etc.} \\
Data sampling frame & Language, language variety, modality, genre,
\emph{etc.} \\
Data collection date(s) & The date or date range of the data
collection. \\
Data format & Plain text, XML, HTML, \emph{etc.} \\
Data schema & Relationships between data elements: files, folders,
\emph{etc.} \\
License & CC BY, CC BY-NC, \emph{etc.} \\
Attribution & Citation information for the data source. \\

\end{longtable}

For many corpus resources, the corpus documentation will include all or
most of this information as part of the resource download or documented
online. If this information is not present in the corpus resource or you
compile your own, it is important to document this information yourself.
This information can be documented in file, such as a plain text file or
spreadsheet, that is included with the corpus resource.

\subsection{Data dictionaries}\label{sec-ud-data-dictionaries}

The process of organizing the data into a dataset, curation, and
modifications to the dataset in preparation for analysis,
transformation, each include a number of project-specific decisions.
These decisions should be documented.

On the one hand, each dataset that is created should have a \textbf{data
dictionary} file. A data dictionary is a document, usually in a
spreadsheet format, that describes the variables in a dataset. The key
information that should be included in a data dictionary is provided in
Table~\ref{tbl-ud-data-dictionary}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7500}}@{}}

\caption{\label{tbl-ud-data-dictionary}Data dictionary information.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Information
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Variable name & The name of the variable as it appears in the dataset,
\emph{e.g.} \texttt{participant\_id}, \texttt{modality}, \emph{etc.} \\
Readable variable name & A human-readable name for the variable,
\emph{e.g.} `Participant ID', `Language modality', \emph{etc.} \\
Variable type & The type of information that the variable contains,
\emph{e.g.} `categorical', `ordinal', \emph{etc.} \\
Variable description & A prose description expanding on the readable
name and can include measurement units, allowed values, \emph{etc.} \\

\end{longtable}

Organizing this information in a tabular format, such as a spreadsheet,
can make it easy for others to read and understand your data dictionary.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{hand-point-up} Tip}

It is conventional to work with variable names for datasets in R using
the same conventions that are used for naming objects. It is a matter of
taste which convention is used, but I have adopted
\href{https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/names.html\#snake_case}{snake
case} as my personal preference (\emph{e.g} \texttt{ref\_num}). There
are also
\href{https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/names.html}{alternatives}.
Regardless of the convention you choose, it is good practice to be
consistent.

It is also of note that the variable names should be balanced for
meaningfulness and brevity. This brevity is of practical concern but can
be somewhat opaque. For questions into the meaning of the variable and
is values consult the resource's dataset documentation.

\end{tcolorbox}

On the other hand, the data curation and transformation steps should be
documented in the code that is used to create the dataset. This is one
of the valuable features of a programmatic approach to quantitative
research. The transparency of this documentation is enhanced by using
\textbf{literate programming} strategies to intermingling prose
descriptions and code the steps in the same, reproducible document.

By providing a comprehensive data dictionary and using a programmatic
approach to data curation and transformation, you ensure that others can
easily understand and work with your dataset, facilitating collaboration
and reproducibility.

\section*{Activities}\label{activities}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

In the following activities, we will be tackle a common scenario in data
analysis: to read, to inspect, and to write datasets. The recipe will
discuss the necessary packages and functions to accomplish these tasks
including \texttt{readr} and \texttt{dplyr}. The recipe will also
refresh and expand on the elements of code blocks in Quarto documents
such as the \texttt{label}, \texttt{echo}, \texttt{message}, and
\texttt{include} options.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-2.html}{Reading,
inspecting, and writing datasets}\\
\textbf{How}: Read Recipe 2, complete comprehension check, and prepare
for Lab 2.\\
\textbf{Why}: To use literate programming in Quarto to work with R
coding strategies for reading, inspecting, and writing datasets.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-02}{Dive into
datasets}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 2.\\
\textbf{Why}: To read datasets from packages and from plain-text files,
inspect and report characteristics of datasets, and write datasets to
plain-text files.

\end{tcolorbox}

\section*{Summary}\label{summary-1}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we have focused on data and information --the first two
components of DIKI Hierarchy. This process is visualized in
Figure~\ref{fig-understanding-data-vis-sum}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.75\textwidth,height=\textheight]{figures/ud-diki.drawio.png}

}

\caption{\label{fig-understanding-data-vis-sum}Understanding data:
visual summary}

\end{figure}%

First a distinction is made between populations and samples, the latter
being a intentional and subjective selection of observations from the
world which attempt to represent the population of interest. The result
of this process is known as a corpus. Whether developing a corpus or
selecting an existing a corpus it is important to vet the sampling frame
for its applicability and viability as a resource for a given research
project.

Once a viable corpus is identified, then that corpus is converted into a
curated dataset which adopts the tidy dataset format where each column
is a variable, each row is an observation, and the intersection of
columns and rows contain values. This curated dataset serves to
establish the base informational relationships from which your research
will stem.

The curated dataset will most likely require transformations which may
include normalization, tokenization, recoding, generation, and/ or
integration to enhance the usefulness of the information to analysis. A
transformed dataset or set of datasets will the result from this
process.

Finally, documentation should be implemented at the acquisition,
curation, and transformation stages of the analysis project process. The
combination of data origin, data dictionary, and literate programming
files establishes documentation of the data and implementation steps to
ensure transparent and reproducible research.

\chapter{Analysis}\label{sec-approaching-analysis}

\begin{quote}
Statistical thinking will one day be as necessary for efficient
citizenship as the ability to read and write.

--- H.G. Wells
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Recall the fundamental concepts and principles of statistics in data
  analysis.
\item
  Articulate the roles of diagnostic, analytic, and interpretive
  statistics in quantitative analysis.
\item
  Compare the similarities and differences between analytic approaches
  to data analysis.
\end{itemize}

\end{tcolorbox}

The aim of analysis is to derive knowledge from information, the next
step in the DIKI Hierarchy. Where the creation of information from data
involves human intervention and conscious decisions, as we have seen,
deriving knowledge from information involves another level of
intervention. The goal is to break down complex information into simpler
components which are more readily interpretable.

In what follows, we will cover the main steps in the process of
analysis. The first is to inspect the data to ensure its quality and
understand its characteristics. The second is to interrogate the data to
uncover patterns and relationships and interpret the findings. To
conclude this chapter, I will outline methods to and the importance of
communicating the analysis results and procedure in a transparent and
reproducible manner.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{terminal} Lessons}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Summarizing data,
Visual summaries}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To showcase methods for statistical summaries of vectors
and data frames and to create informative graphics that enhance data
interpretation and analysis.

\end{tcolorbox}

\section{Describe}\label{sec-aa-describe}

The goal of descriptive statistics is to summarize the data in order to
understand and prepare the data for the analysis approach to be
performed. This is accomplished through a combination of statistic
measures and/ or tabular or graphic summaries. The choice of descriptive
statistics is guided by the type of data, as well as the question(s)
being asked of the data.

In descriptive statistics, there are four basic questions that are asked
of each of the variables in the dataset. Each correspond to a different
type of descriptive measure.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Central Tendency}: Where do the data points tend to be
  located?
\item
  \textbf{Dispersion}: How spread out are the data points?
\item
  \textbf{Distribution}: What is the overall shape of of the data
  points?
\item
  \textbf{Association}: How are these data points related to other data
  points?
\end{enumerate}

To ground this discussion I will introduce a new dataset. This dataset
is drawn from the Barcelona English Language Corpus (BELC)
(\citeproc{ref-Munoz2006}{Muñoz 2006}), which is found in the TalkBank
repository. I've selected the ``Written composition'' task from this
corpus which contains 80 writing samples from 36 second language
learners of English at different ages. Participants were given the task
of writing for 15 minutes on the topic of ``Me: my past, present and
future''. Data was collected for participants from one to three times
over the course of seven years (at 10, 12, 16, and 17 years of age).

In Table~\ref{tbl-aa-belc-dd} we see the data dictionary for the BELC
dataset which reflects structural and transformational steps I've done
so we start with a tidy dataset with \texttt{essay\_id} as the unit of
observation.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1300}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1700}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5500}}@{}}

\caption{\label{tbl-aa-belc-dd}Data dictionary for the BELC dataset.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable\_type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
essay\_id & Essay ID & categorical & Unique identifier for each essay \\
part\_id & Participant ID & categorical & Identifier for each
participant learner \\
sex & Sex & categorical & Sex of the participant \\
group & Group & ordinal & Time group of the essay, ordered from T1 to T4
(10, 12, 16, and 17 years old) \\
tokens & Tokens & numeric & Number of word tokens in the essay \\
types & Types & numeric & Number of unique word types in the essay \\
ttr & TTR & numeric & Type-Token Ratio (TTR) of the essay \\
prop\_l2 & Proportion of L2 & numeric & Proportion of words in the essay
identified as second (target) language (L2) \\

\end{longtable}

The data dictionary provides a easily accessible overview of the
dataset. This includes a human-readable mapping from variable names to
variable descriptions. Further, it provides information about the type
of variable (e.g., categorical, ordinal, numeric). As we will see the
informational type of variables is key to descriptive measures, as well
as all other components of analysis.

Now, let's take a look a the first few observations of the BELC dataset
to get another perspective on the dataset as we view the values of the
dataset.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1300}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1300}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1300}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1300}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1300}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1300}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1300}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1300}}@{}}

\caption{\label{tbl-aa-belc-overview}First 5 observations of the BELC
dataset.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
essay\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
part\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sex
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
group
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
tokens
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
types
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ttr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
prop\_l2
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
E1 & L01 & female & T2 & 79 & 46 & 0.582 & 0.987 \\
E2 & L02 & female & T1 & 18 & 18 & 1.000 & 0.667 \\
E3 & L02 & female & T3 & 101 & 53 & 0.525 & 1.000 \\
E4 & L05 & female & T1 & 20 & 17 & 0.850 & 0.900 \\
E5 & L05 & female & T3 & 158 & 80 & 0.506 & 0.987 \\

\end{longtable}

In Table~\ref{tbl-aa-belc-overview}, each of the variable are attributes
or measures of the \texttt{essay\_id} variable. \texttt{tokens} is the
number of total words, \texttt{types} is the number of unique words,
\texttt{ttr} is the ratio of unique words to total words. This is known
as the Type-Token Ratio and it is a standard metric for measuring
lexical diversity. Finally, the proportion of L2 words (English) to the
total words (tokens) is provided in \texttt{prop\_l2}.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-alt} Case study}

Type-Token Ratio is a standard metric for measuring lexical diversity,
but it is not without its flaws. Most importantly, TTR is highly
sensitive to the word length of the text. Duran
(\citeproc{ref-Duran2004}{2004}) discuss this limitation, and the
limitations of other lexical diversity measures and propose a new
measure \(D\) which shows a stronger correlation with language
proficiency in their comparative studies.

\end{tcolorbox}

Let's now turn our attention to exploring descriptive measures using the
BELC dataset.

\subsection{Central tendency}\label{sec-aa-central-tendency}

The central tendency is measure which aims to summarize the data points
in a variable as the most representative, middle or most typical value.
There are three common measures of central tendency: the mode, mean and
median. Each differ in how they summarize the data points.

The \textbf{mode} is the value, or values, that appears most frequently
in a set of values. If there are multiple values with the highest
frequency, then the variable is said to be multimodal. The most
versatile of the central tendency measures as it can be applied to all
levels of measurement, although the mode is not often used for numeric
variables as it is not as informative as other measures.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

\begin{figure}[H]

\begin{minipage}{0.58\linewidth}
Grieve, Nini, and Guo (\citeproc{ref-Grieve2018}{2018}) compiled a 8.9
billion-word corpus of geotagged posts from Twitter between 2013-2014 in
the United States. The authors provide a
\href{https://isogloss.shinyapps.io/isogloss/}{search interface} to
explore relationship between lexical usage and geographic location.
Explore this corpus searching for terms related to slang (``hella'',
``wicked''), geographical (``mountain'', ``river''), meteorological
(``snow'', ``rain''), and/ or any other term types. What types of
patterns do you find? What are the benefits and/ or limitations of this
type of data, data summarization, and/ or interface?\end{minipage}%
%
\begin{minipage}{0.03\linewidth}
~\end{minipage}%
%
\begin{minipage}{0.39\linewidth}

\begin{figure}[H]

\centering{

\includegraphics[width=0.8\textwidth,height=\textheight]{figures/ud-word-mapper.png}

}

\caption{\label{fig-ud-word-mapper}Example distribution of the term
`Ya'll' the Word Mapper project.}

\end{figure}%

\end{minipage}%

\end{figure}%

\end{tcolorbox}

The more common measures for numeric variables are the mean and the
median. The \textbf{mean} is a summary statistic calculated by summing
all the values and dividing by the number of values. The \textbf{median}
is calculated by sorting all the values in the variable and then
selecting the middle value. Given that the mean and median are
calculated differently, they will not always yield the same result.
Differences that appear between the mean and median will be of interest
to us later in this chapter.

\subsection{Dispersion}\label{dispersion}

The mean, median, and mode provide summary information where data points
tend to be located. However, they do not provide us with any
understanding as to how representative this value is. To provide this
context, the spread of the values around the central tendency, or
\textbf{dispersion}, is calculated.

For categorical variables, the spread is framed in terms of how balanced
the values are across the levels. One way to do this is to calculate the
(normalized) entropy. \textbf{Entropy} is a measure of uncertainty. The
more balanced the values are across the levels, the closer entropy is 1.
In practice, however, proportions are often used to assess the balance
of the values across the levels. The \textbf{proportion} of each level
is the frequency of the level divided by the total number of values.

The most common measure of dispersion for numeric variables is the
\textbf{standard deviation}. The standard deviation is calculated by
taking the square root of the variance. The \textbf{variance} is the
average of the squared differences from the mean. So, more succinctly,
the standard deviation is a measure of the spread of the values around
the mean. Where the standard deviation is anchored to the mean, the
\textbf{interquartile range} (IQR) is tied to the median. The median
represents the sorted middle of the values, in other words the 50th
percentile. The IQR is the difference between the 75th percentile and
the 25th percentile. Again, just as the mean and the median, the
standard deviation and the IQR are calculated in different ways, they
are not always the same.

Let's now consider the relevant central tendency and dispersion of the
variables in the BELC dataset. In
Table~\ref{tbl-aa-belc-descriptive-stats-categorical}, we see the
categorical measures.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1528}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4583}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2083}}@{}}

\caption{\label{tbl-aa-belc-descriptive-stats-categorical}Central
tendency and dispersion of the categorical variables}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
top\_counts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
norm\_entropy
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
essay\_id & E1: 1, E10: 1, E11: 1, E12: 1 & 1.00 \\
part\_id & L05: 3, L10: 3, L11: 3, L12: 3 & 0.98 \\
sex & fem: 48, mal: 32 & 0.97 \\
group & T1: 25, T3: 24, T2: 16, T4: 15 & 0.98 \\

\end{longtable}

In Table~\ref{tbl-aa-belc-descriptive-stats-categorical}, the
\texttt{top\_counts} measure gives us a short list of the most frequent
levels of the variable. From \texttt{top\_count} we can gather whether
the variable has one mode or is multimodel. Both \texttt{essay\_id} and
\texttt{part\_id} have the same most frequent value for the levels
listed. On the other hand, \texttt{sex} and \texttt{group} have a single
mode. We can also appreciate the dispersion of these variables based on
the \texttt{norm\_entropy} of each variable. \texttt{essay\_id} is
completely balanced across the levels, so it has a normalized entropy of
1. the other variables are not as balanced, but still quite balanced as
the normalized entropy is close to 1.

In Table~\ref{tbl-aa-belc-descriptive-stats-numeric}, we see the numeric
measures.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1528}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1250}}@{}}

\caption{\label{tbl-aa-belc-descriptive-stats-numeric}Central tendency
and dispersion for numeric variables.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
mean
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sd
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
median
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
iqr
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
tokens & 67.62 & 44.20 & 56.50 & 61.250 \\
types & 41.85 & 23.03 & 38.50 & 31.500 \\
ttr & 0.68 & 0.13 & 0.66 & 0.149 \\
prop\_l2 & 0.96 & 0.10 & 0.99 & 0.027 \\

\end{longtable}

In Table~\ref{tbl-aa-belc-descriptive-stats-numeric}, we have meansures
for the mean, median, standard deviation, and IQR for each variable. The
variable \texttt{tokens} has a larger difference between the mean and
median than the other variables and the standard deviation is relatively
large suggesting that the values are more spread out around the mean. In
the case of \texttt{ttr} the mean and median are quite close and the
standard deviation is relatively small suggesting that the values are
more tightly clustered around the mean.

When interpreting these numeric summary values, it is important to only
directly compare column-wise. That is, focusing only on a single
variable, not across variables. Each variable, as is, is measured on a
different scale and only relative to itself can we make sense of the
values.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

The inability to compare summary statistics across variables is a key
reason why \textbf{standardization} is often applied before submiting a
dataset for analysis (\citeproc{ref-Johnson2008}{Johnson 2008};
\citeproc{ref-Baayen2008a}{R. Harald Baayen 2008}).

Standardization is a scale-based transformation that changes the scale
of the values to a common scale, or \emph{z-scores}. The result of this
transformation puts data points of each variable on the same scale and
allows for direct comparison. Furthermore, standardization also
mitigates the influence of variables with large values relative to other
variables. This is particularly important in multivariate analysis where
the influence of variables with large values can be magnified.

The caveat is that standardization masks the original meaning of the
data. That is, if we consider token frequency, before standardization,
we can say that a value of 1000 tokens is 1000 tokens. After
standardization, we can only say that a value of 1 is 1 standard
deviation from the mean. This is why standardization is often applied
after the descriptive phase of analysis.

\end{tcolorbox}

Another thing to note in
Table~\ref{tbl-aa-belc-descriptive-stats-numeric} is that the mean and
median are not the same. They are both measures of central tendency, but
the mean is more sensitive to extreme values than the median. The
difference between the mean and median is a numeric representation that
gives us a first glimpse of the distribution of the data. With this in
mind, let's turn to the distribution of the variables.

\subsection{Distributions}\label{sec-aa-distributions}

Summary statistics of the central tendency and dispersion of a variable
provide a sense of the most representative value and how spread out the
data is around this value. However, to gain a more comprehensive
understanding of the variable, it is key to consider the frequencies of
all the data points. The \textbf{distribution} of a variable is the
pattern or shape of the data that emerges when the frequencies of all
data points are considered. This can reveal patterns that might not be
immediately apparent from summary statistics alone.

When assessing the distribution of categorical variables, we can use a
frequency table or bar plot. A \textbf{frequency table} is a useful
method to display the frequency and proportion of each level in a
categorical variable in a clear and concise manner. In
Table~\ref{tbl-aa-belc-frequency-table} we see the frequency table for
the variable \texttt{sex}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1806}}@{}}

\caption{\label{tbl-aa-belc-frequency-table}Frequency table for the
variable \texttt{sex}.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
sex
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
frequency
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
proportion
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
female & 48 & 0.6 \\
male & 32 & 0.4 \\

\end{longtable}

A \textbf{bar plot} is a type of plot where the x-axis is a categorical
variable and the y-axis is the frequency of the values. The frequency is
represented by the height of the bar. The variables can be ordered by
frequency, alphabetically, or some other order.
Figure~\ref{fig-aa-belc-barplots} is a bar chart for the variables
\texttt{sex} and \texttt{group} ordered alphabetically.

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-barplots-1.pdf}

}

\subcaption{\label{fig-aa-belc-barplots-1}Sex}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-barplots-2.pdf}

}

\subcaption{\label{fig-aa-belc-barplots-2}Time group}

\end{minipage}%

\caption{\label{fig-aa-belc-barplots}Bar plots for categorical variables
\texttt{sex} and \texttt{group}.}

\end{figure}%

So for a frequency table or barplot, we can see the frequency of each
level of a categorical variable. This gives us some knowledge about the
BELC dataset: there are more girls in the dataset and more essays appear
in first and third time groups. If we were to see any clearly loopsided
categories, this would be a sign of imbalance in the data and we would
need to consider how this might impact our analysis.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

The goal of descriptive statistics is to summarize the data in a way
that is meaningful and interpretable. With this in mind, compare the
frequency table in \ref{tbl-aa-belc-frequency-table} and bar plot in
\ref{fig-aa-belc-barplots-1}. Does one provide a more interpretable
summary of the data? Why or why not? Are there any other ways you might
communicate this distribution more effectively?

\end{tcolorbox}

For numeric variables, a frequency table, as in
Table~\ref{tbl-aa-belc-frequency-table}, does not summarize a
distibution well. Instead, the distribution of a numeric variable is
best understood visually. Furthermore, we aim to assess the distribution
of the variable in terms of the shape of the distribution and the
presence of outliers.

The most common visualizations of the distribution of a numeric variable
are histograms and density plots. \textbf{Histograms} are a type of bar
plot where the x-axis is a numeric variable and the y-axis is the
frequency of the values falling within a determined range of values, or
bins. The frequency of values within each bin is represented by the
height of the bars.

\textbf{Density plots} are a smoothed version of histograms. The y-axis
of a density plot is the probability of the values. When frequent values
appear closely together, the plot line is higher. When the frequency of
values is lower or more spread out, the plot line is lower.

An example of these plots is show in
Figure~\ref{fig-aa-belc-histogram-density-tokens} for the variable
\texttt{tokens}.

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histogram-density-tokens-1.pdf}

}

\subcaption{\label{fig-aa-belc-histogram-density-tokens-1}Histogram}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histogram-density-tokens-2.pdf}

}

\subcaption{\label{fig-aa-belc-histogram-density-tokens-2}Density plot}

\end{minipage}%

\caption{\label{fig-aa-belc-histogram-density-tokens}Distribution plots
for the variable \texttt{tokens}.}

\end{figure}%

Both the histogram in
Figure~\ref{fig-aa-belc-histogram-density-tokens-1} and the density plot
in Figure~\ref{fig-aa-belc-histogram-density-tokens-2} show the
distribution of the variable \texttt{tokens} in slightly different ways
which translate into trade-offs in terms of interpretability.

The histogram shows the frequency of the values in bins. The number of
bins and/ or binwidth can be changed for more or less granularity. A
rough grain histogram shows the general shape of the distribution, but
it is difficult to see the details of the distribution. A fine grain
histogram shows the details of the distribution, but it is difficult to
see the general shape of the distribution. The density plot shows the
general shape of the distribution, but it hides the details of the
distribution. Given this trade-off, it is often useful explore outliers
with histograms and the overall shape of the distribution with density
plots.

In Figure~\ref{fig-aa-belc-histograms} we see both histograms and
density plots combined for the variables \texttt{tokens},
\texttt{types}, and \texttt{ttr}.

\begin{figure}[H]

\begin{minipage}{0.33\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histograms-1.pdf}

}

\subcaption{\label{fig-aa-belc-histograms-1}Number of tokens}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histograms-2.pdf}

}

\subcaption{\label{fig-aa-belc-histograms-2}Number of types}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histograms-3.pdf}

}

\subcaption{\label{fig-aa-belc-histograms-3}Type-token ratio score}

\end{minipage}%

\caption{\label{fig-aa-belc-histograms}Histograms for numeric variables
\texttt{tokens}, \texttt{types}, and \texttt{ttr}.}

\end{figure}%

Focusing on the details captured in the histogram we are better able to
detect potential outliers. Outliers can reflect valid values that are
simply extreme or they can reflect something erroneous in the data. To
distinguish between these two possibilities, it is important to know the
context of the data. Take, for example,
Figure~\ref{fig-aa-belc-histograms-3}. We see that there is a bin near
the value 1.0. Given that the type-token ratio is a ratio of the number
of types to the number of tokens, it is unlikely that the type-token
ratio would be exactly 1.0 as this would mean that every word in an
essay is unique. Another, less dramatic, example is the bin to the far
right of Figure~\ref{fig-aa-belc-histograms-1}. In this case, the bin
represents the number of tokens in an essay. An uptick in the number of
essays with a large number of tokens is not surprising and would not
typically be considered an outlier. On the other hand, consider the bin
near the value 0 in the same plot. It is unlikely that a true essay
would have 0, or near 0, words and therefore a closer look at the data
is warranted.

It is important to recognize that outliers contribute undue influence to
overall measures of central tendency and dispersion. To appreciate this,
let's consider another helpful visualization called a \textbf{boxplot}.
A boxplot is a visual representation which aims to represent the central
tendency, dispersion, and distribution of a numeric variable in one
plot.

\begin{figure}[H]

\begin{minipage}{\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histogram-boxplot-1.pdf}

}

\subcaption{\label{fig-aa-belc-histogram-boxplot-1}Histogram}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histogram-boxplot-2.pdf}

}

\subcaption{\label{fig-aa-belc-histogram-boxplot-2}Boxplot}

\end{minipage}%

\caption{\label{fig-aa-belc-histogram-boxplot}Understanding boxplots}

\end{figure}%

In Figure~\ref{fig-aa-belc-histogram-boxplot-2} we see a boxplot for
\texttt{ttr} variable. The box in the middle of the plot represents the
interquartile range (IQR) which is the range of values between the first
quartile and the third quartile. The solid line in the middle of the box
represents the median. The lines extending from the box are called
`whiskers' and provide the range of values which are within 1.5 times
the IQR. Values outside of this range are plotted as individual points.

Now let's consider boxplots from another angle. Just above in
Figure~\ref{fig-aa-belc-histogram-boxplot-1} I've plotted a histogram.
In this view, we can see that a boxplot is a simplifed histogram
augmented with central tendency and dispersion statistics. While
histograms focus on the frequency distribution of data points, boxplots
focus on the data's quartiles and potential outliers.

I've added a dashed line in Figure~\ref{fig-aa-belc-histogram-boxplot-1}
and Figure~\ref{fig-aa-belc-histogram-boxplot-2} to signal the mean in
this set of plots, but it is not typically included. I include the
dashed line to make a point: the mean is more sensitive to outliers than
the median. As I pointed out in Section~\ref{sec-aa-central-tendency},
the mean is the sum of all values divided by the number of values. If
there are extreme values, the mean will be pulled in the direction of
the extreme values. The median, however, is the middle value and a few
extreme values have less effect. So, when central tendency is reported,
if there is a sizeable difference between the mean and the median,
measures of dispersion will be larger and the direction of the
difference can be used to infer the presence of outliers.

Returning to outliers, it is important to address them to safeguard the
accuracy of the analysis. There are two main ways to address outliers:
1) transform the data and 2) eliminate observations with outliers
(\textbf{trimming}). Trimming is more extreme as it removes data but can
be the best approach for true outliers. Transforming the data is an
approach to mitigating the influence of extreme but valid values.
\textbf{Transformation} involves applying a mathematical function to the
data which changes the scale and/ or shape of the distribution, but does
not remove data nor does it change the relative order of the values.

In Figure~\ref{fig-aa-belc-boxplot-trimmed}, we see two boxplots.
Figure~\ref{fig-aa-belc-boxplot-trimmed-1} is the original \texttt{ttr}
data and Figure~\ref{fig-aa-belc-boxplot-trimmed-2} reflects the data
trimmed to remove outliers. In this case, we have removed essays with a
type-token ratio of 1.

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-boxplot-trimmed-1.pdf}

}

\subcaption{\label{fig-aa-belc-boxplot-trimmed-1}Type-token ratio score}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-boxplot-trimmed-2.pdf}

}

\subcaption{\label{fig-aa-belc-boxplot-trimmed-2}Type-token ratio score
(trimmed)}

\end{minipage}%

\caption{\label{fig-aa-belc-boxplot-trimmed}Boxplots for \texttt{ttr}
before and after trimming.}

\end{figure}%

We can now appreciate the relatively larger effect that the outliers had
on the mean value of the \texttt{ttr} variable. As outliers are removed
as the difference between the mean and median will become smaller.

The exploration the data points with histograms and boxplots has helped
us to identify outliers. Now we turn to the question of the overall
shape of the distribution. The key question is whether the observed
distribution of each variable groups around a central tendency in a
symmetrical manner or if the distribution is skewed in one direction or
another.

When values are symmetrically dispersed around the central tendency, the
distribution is said to be normal. The \textbf{Normal Distribution} is
characterized by a distribution where the mean and median are the same.
The Normal Distribution has a key role in theoretical statistics and is
the foundation for many statistical tests. This distribution is also
known as the Gaussian Distribution or the Bell Curve for the hallmark
bell shape of the distribution. In a normal distribution, extreme values
are less likely than values near the center.

When values are not symmetrically dispersed around the central tendency,
the distribution is said to be skewed. A distribution in which values
tend to disperse to the left of the central tendency is \textbf{left
skewed} and a distribution in which values tend to disperse to the right
of the central tendency is \textbf{right skewed}.

Stepping away from our BELC dataset, I've created simulated data that
fit normal and non-normal, or skewed, distributions. I present each of
these distributions as density plots with mean and median line overlays
in Figure~\ref{fig-aa-distributions}.

\begin{figure}[H]

\begin{minipage}{0.33\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-distributions-1.pdf}

}

\subcaption{\label{fig-aa-distributions-1}Left skewed distribution}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-distributions-2.pdf}

}

\subcaption{\label{fig-aa-distributions-2}Normal distribution}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-distributions-3.pdf}

}

\subcaption{\label{fig-aa-distributions-3}Right skewed distribution}

\end{minipage}%

\caption{\label{fig-aa-distributions}Mean and median for normal and
skewed distributions.}

\end{figure}%

Assessing the distribution of a variable is important for two reasons.
First, the distribution of a variable can inform the choice of
statistical test in theory-based hypothesis testing. Data that are
normally, or near-normally distributed are often analyzed using
parametric tests while data that exhibit a skewed distributed are often
analyzed using non-parametric tests. Second, highly skewed distributions
have the effect of compressing the range of values. This can lead to a
loss of information and can make it difficult to detect patterns in the
data.

Interestingly, a key feature of the distribution of linguistic units
(phonemes, words, morphemes, \emph{etc}) is that they tend to be highly
skewed. This is due to the fact that a small number of linguistic units
are used frequently and a large number of linguistic units are used
infrequently. This is known as Zipf's Law (\citeproc{ref-Zipf1949}{Zipf
1949}). \textbf{Zipf's Law} is a statistical principle that states that
the frequency of a word (other other linguistic units) is inversely
proportional to its rank in the frequency table. In other words, the
most frequent units will appear twice as often as the second most
frequent unit, three times as often as the third most frequent unit, and
so on.

The plot in Figure~\ref{fig-aa-zipf-distribution-1} is simulated data
that fits a Zipfian distribution.

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-zipf-distribution-1.pdf}

}

\subcaption{\label{fig-aa-zipf-distribution-1}Zipfian distribution}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-zipf-distribution-2.pdf}

}

\subcaption{\label{fig-aa-zipf-distribution-2}Log-transformed Zipfian
distribution}

\end{minipage}%

\caption{\label{fig-aa-zipf-distribution}Zipfian distribution}

\end{figure}%

Zipf's law describes a theoretical distribution, and the actual
distribution of units in a corpus is affected by various sampling
factors, including the size of the corpus. The larger the corpus, the
closer the distribution will be to the Zipf distribution.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

As stated above, Zipfian distributions are typical of natural language
and are observed a various linguistic levels. This is because natural
language is a complex system, and complex systems tend to exhibit
Zipfian distributions. Other examples of complex systems that exhibit
Zipfian distributions include the size of cities, the frequency of
species in ecological communities, the frequency of links in the World
Wide Web, \emph{etc.}

\end{tcolorbox}

Let's return to the BELC dataset and assess the distribution of the
variables \texttt{tokens}, \texttt{types} and \texttt{ttr} in
Figure~\ref{fig-aa-belc-histogram-density-trimmed} to the three
distributions in Figure~\ref{fig-aa-distributions}, we see that all
three numeric variables in the BELC dataset are skewed to some degree,
but not as extreme as the simulated data.

\begin{figure}[H]

\begin{minipage}{0.33\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histogram-density-trimmed-1.pdf}

}

\subcaption{\label{fig-aa-belc-histogram-density-trimmed-1}Number of
tokens}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histogram-density-trimmed-2.pdf}

}

\subcaption{\label{fig-aa-belc-histogram-density-trimmed-2}Number of
types}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histogram-density-trimmed-3.pdf}

}

\subcaption{\label{fig-aa-belc-histogram-density-trimmed-3}Type-token
ratio score}

\end{minipage}%

\caption{\label{fig-aa-belc-histogram-density-trimmed}Histogram/ Density
plots for numeric variables in the BELC dataset.}

\end{figure}%

Figure~\ref{fig-aa-belc-histogram-density-trimmed-1} and
\ref{fig-aa-belc-histogram-density-trimmed-2} for \texttt{tokens} and
\texttt{types} has a bit more right skewing than \texttt{ttr} seen in
Figure~\ref{fig-aa-belc-histogram-density-trimmed-3}. This is consistent
with the summary statistics for the central tendency in
Table~\ref{tbl-aa-belc-descriptive-stats-numeric}. The mean and median
for \texttt{tokens} and \texttt{types} are not the same and the mean is
larger than the median. For \texttt{ttr}, the mean and median is much
closer.

In the case that a variable is highly skewed, it is often useful to
attempt transform the variable to reduce the skewness. In contrast to
scale-based transformations (\emph{e.g.} centering and scaling),
shape-based transformations change the scale and the shape of the
distribution. The most common shape-based transformation is the
logarithmic transformation. The \textbf{logarithmic transformation}
(log-transformation) takes the log (typically base 10) of each value in
a variable. The log-transformation is useful for reducing the skewness
of a variable as it compresses large values and expands small values. If
the skewness is due to these factors, the log-transformation can help,
as in the case of the Zipfian distribution in
Figure~\ref{fig-aa-zipf-distribution-2}.

It is important to note, however, that if scale-based transformations
are to be applied to a variable, they should be applied after the
log-transformation as the log of negative values is undefined.

\subsection{Association}\label{association}

We have covered the first three of the four questions we are interested
in asking in a descriptive analysis. The fourth, and last, question is
whether there is an association between variables. If so, what is the
directionality and what is the apparent magnitude of the dependence?
Knowing the answers to these questions will help frame our approach to
analysis.

To assess association, the number and information types of the variables
under consideration are important. Let's start by considering two
variables. If we are working with two variables, we are dealing with a
\textbf{bivariate} relationship. Given there are three informational
types (categorical, ordinal, and numeric), there are six logical
bivariate combinations: categorical-categorical, categorical-ordinal,
categorical-numeric, ordinal-ordinal, ordinal-numeric, and
numeric-numeric.

The directionality of a relationship will take the form of a tabular or
graphic summary depending on the informational value of the variables
involved. In Table~\ref{tbl-aa-summary-types}, we see the appropriate
summary types for each of the six bivariate combinations.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1400}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2200}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2200}}@{}}
\caption{Appropriate summary types for different combinations of
variable types.}\label{tbl-aa-summary-types}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Categorical
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Ordinal
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Numeric
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Categorical
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Ordinal
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Numeric
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Categorical} & Contingency table & Contingency table/ Bar plot &
Pivot table/ Boxplot \\
\textbf{Ordinal} & - & Contingency table/ Bar plot & Pivot table/
Boxplot \\
\textbf{Numeric} & - & - & Scatterplot \\
\end{longtable}

Let's first start with the combinations that include a categorical or
ordinal variable. Categorical and ordinal variables reflect measures of
class-type information, with add meaningful ranks to ordinal variables.
To assess a relationship with these variable types, a table is always a
good place to start. When combined together, a contingency table is the
appropriate table. A \textbf{contingency table} is a cross-tabulation of
two class-type variables, basically a two-way frequency table. This
means that three of the six bivariate combinations are assessed with a
contingency table: categorical-categorical, categorical-ordinal, and
ordinal-ordinal.

In Table~\ref{tbl-aa-belc-contingency-tables} we see contingency tables
for the categorical variable \texttt{sex} and ordinal variable
\texttt{group} in the BELC dataset.

\begin{table}

\caption{\label{tbl-aa-belc-contingency-tables}Contingency tables for
categorical variable \texttt{sex} and ordinal variable \texttt{group} in
the BELC dataset.}

\begin{minipage}{0.50\linewidth}

\subcaption{\label{tbl-aa-belc-contingency-tables-1}Counts}

\centering{

\begin{tabular}{llll}
\toprule
group & female & male & Total\\
\midrule
T1 & 7 & 9 & 16\\
T2 & 11 & 4 & 15\\
T3 & 13 & 10 & 23\\
T4 & 9 & 5 & 14\\
Total & 40 & 28 & 68\\
\bottomrule
\end{tabular}

}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\subcaption{\label{tbl-aa-belc-contingency-tables-2}Percentages}

\centering{

\begin{tabular}{llll}
\toprule
group & female & male & Total\\
\midrule
T1 & 43.75\% & 56.25\% & 100.00\%\\
T2 & 73.33\% & 26.67\% & 100.00\%\\
T3 & 56.52\% & 43.48\% & 100.00\%\\
T4 & 64.29\% & 35.71\% & 100.00\%\\
Total & 58.82\% & 41.18\% & 100.00\%\\
\bottomrule
\end{tabular}

}

\end{minipage}%

\end{table}%

A contingency table may include only counts, as in
Table~\ref{tbl-aa-belc-contingency-tables-1}, or may include proportions
or percentages in an effort to normalize the counts and make them more
comparable, as in Table~\ref{tbl-aa-belc-contingency-tables-2}.

It is sometimes helpful to visualize a contingency table as a bar plot
when there are a larger number of levels in either or both of the
variables. Again, looking at the relationship between \texttt{sex} and
\texttt{group}, we see that we can plot the counts or the proportions.
In Figure~\ref{fig-aa-belc-bar-plots}, we see both.

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-bar-plots-1.pdf}

}

\subcaption{\label{fig-aa-belc-bar-plots-1}Counts}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-bar-plots-2.pdf}

}

\subcaption{\label{fig-aa-belc-bar-plots-2}Proportions}

\end{minipage}%

\caption{\label{fig-aa-belc-bar-plots}Bar plots for the relationship
between \texttt{sex} and \texttt{group} in the BELC dataset.}

\end{figure}%

To summarize and assess the relationship between a categorical or an
ordinal variable and a numeric variable, we cannot use a contingency
table. Instead, this type of relationship is best summarized in a table
using a summary statistic in a \textbf{pivot table}. A pivot table is a
table in which a class-type variable is used to group a numeric variable
by some summary statistic appropriate for numeric variables, \emph{e.g.}
mean, median, standard deviation, \emph{etc.}

In Table~\ref{tbl-aa-belc-pivot-table}, we see a pivot table for the
relationship between \texttt{group} and \texttt{tokens} in the BELC
dataset. Specifically, we see the mean number of tokens by group.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1944}}@{}}

\caption{\label{tbl-aa-belc-pivot-table}Pivot table for the relationship
between \texttt{group} and \texttt{tokens} in the BELC dataset.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
group
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
mean\_tokens
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
T1 & 35.4 \\
T2 & 62.5 \\
T3 & 85.0 \\
T4 & 118.9 \\

\end{longtable}

We see that the mean number of tokens increases from Group T1 to T4,
which is consistent with the idea that the students in the higher groups
are writing longer essays.

Although a pivot table may be appropriate for targeted numeric
summaries, a visualization is often more informative for assessing the
dispersion and distribution of a numeric variable by a categorical or
ordinal variable. There are two main types of visualizations for this
type of relationship: a boxplot and a \textbf{violin plot}. A violin
plot is a visualization that summarizes the distribution of a numeric
variable by a categorical or ordinal variable, adding the overall shape
of the distribution, much as a density plot does for histograms.

In Figure~\ref{fig-aa-belc-boxplot-violin-plot}, we see both a boxplot
and a violin plot for the relationship between \texttt{group} and
\texttt{tokens} in the BELC dataset.

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-boxplot-violin-plot-1.pdf}

}

\subcaption{\label{fig-aa-belc-boxplot-violin-plot-1}Boxplot}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-boxplot-violin-plot-2.pdf}

}

\subcaption{\label{fig-aa-belc-boxplot-violin-plot-2}Violin plot}

\end{minipage}%

\caption{\label{fig-aa-belc-boxplot-violin-plot}Boxplot and violin plot
for the relationship between \texttt{group} and \texttt{tokens} in the
BELC dataset.}

\end{figure}%

From the boxplot in Figure~\ref{fig-aa-belc-boxplot-violin-plot-1}, we
see that the general trend towards more tokens used by students in
higher groups. But we can also appreciate the dispersion of the data
within each group looking at the boxes and whiskers. On the surface it
appears that the data for groups T1 and T3 are closer to each other than
groups T2 and T4, in which there is more variability within these
groups. Furthermore, we can see outliers in groups T1 and T3, but not in
groups T2 and T4. From the violin plot in
Figure~\ref{fig-aa-belc-boxplot-violin-plot-2}, we can see the same
information, but we can also see the overall shape of the distribution
of tokens within each group. In this plot, it is very clear that group
T4 includes a wide range of token counts.

The last bivariate combination is numeric-numeric. To summarize this
type of relationship a scatterplot is used. A \textbf{scatterplot} is a
visualization that plots each data point as a point in a two-dimensional
space, with one numeric variable on the x-axis and the other numeric
variable on the y-axis. Depending on the type of relationship you are
trying to assess, you may want to add a trend line to the scatterplot. A
trend line is a line that summarizes the overall trend in the
relationship between the two numeric variables. To assess the extent to
which the relationship is linear, a straight line is drawn which
minimizes the distance between the line and the points.

In Figure~\ref{fig-aa-belc-scatter-plot}, we see a scatterplot and a
scatterplot with a trend line for the relationship between \texttt{ttr}
and \texttt{types} in the BELC dataset.

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-scatter-plot-1.pdf}

}

\subcaption{\label{fig-aa-belc-scatter-plot-1}Points}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-scatter-plot-2.pdf}

}

\subcaption{\label{fig-aa-belc-scatter-plot-2}Points with a linear trend
line}

\end{minipage}%

\caption{\label{fig-aa-belc-scatter-plot}Scatter plot for the
relationship between \texttt{ttr} and \texttt{types} in the BELC
dataset.}

\end{figure}%

We see that there is an apparent positive relationship between these two
variables, which is consistent with the idea that as the number of types
increases, the type-token ratio increases. In other words, as the number
of unique words increases, so does the lexical diversity of the text.
Since we are evaluating a linear relationship, we are assessing the
extent to which there is a \textbf{correlation} between \texttt{ttr} and
\texttt{types}. A correlation simply means that as the values of one
variable change, the values of the other variable change in a consistent
manner.

Before moving on to the next section, it is important to remember than
through the process of descriptive measures, we gain a thorough
understanding of our data's characteristics and quality, preparing us
for the analysis. The decisions we make at this stage, from estimating
central tendency to understanding the possible association between our
variables, can have significant implications on our approach and
interpretation in the subsequent analysis. So, this initial step of data
analysis deserves our careful attention and scrutiny.

\section{Analyze}\label{sec-aa-analyze}

The goal of analysis, generally, is to generate knowledge from
information. The type of knowledge generated and the process by which it
is generated, however, differ and can be broadly grouped into three
analysis types: exploratory, predictive, and inferential.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\caption{Overview of analysis
types.}\label{tbl-aa-analysis-types}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Aims
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Methods
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Evaluation
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Aims
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Methods
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Evaluation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Exploratory & Explore: gain insight & Inductive, data-driven, and
iterative & Descriptive, pattern detection with machine learning
(unsupervised) & Associative \\
Predictive & Examine: validate associations & Semi-deductive, data-/
theory-driven, and iterative & Predictive modeling with machine learning
(supervised) & Model performance, feature importance, and associative \\
Inferential & Explain: test hypotheses & Deductive, theory-driven, and
non-iterative & Hypothesis testing with statistical tests & Causal \\
\end{longtable}

In this section, I will elaborate briefly on the distinctions between
analysis types seen in Table~\ref{tbl-aa-analysis-types}. I will
structure the discussion moving from the least structured (inductive) to
most structured (deductive) approach to deriving knowledge from
information with the aim to provide enough information for you to
identify these research approaches in the literature and to make
appropriate decisions as to which approach your research should adopt.

\subsection{Explore}\label{sec-aa-explore}

In \textbf{Exploratory Data Analysis (EDA)}, we use a variety of methods
to identify patterns, trends, and relations within and between
variables. The goal of EDA is uncover insights in an inductive,
data-driven manner. That is to say, that we do not enter into EDA with a
fixed hypothesis in mind, but rather we explore intuition, probe
anecdote, and follow hunches to identify patterns and relationships and
to evaluate whether and why they are meaningful. We are admittedly
treading new or unfamiliar terrain letting the data guide our analysis.
This means that we can use and reuse the same data to explore different
angles and approaches adjusting our methods and measures as we go. In
this way, EDA is an iterative, meaning generating process.

In line with the investigative nature of EDA, the identification of
variables of interest is a discovery process. We most likely have a
intuition about the variables we would like to explore, but we are able
to adjust our variables as need be to suit our research aims. When the
identification and selection of variables is open, the process is known
as \textbf{feature engineering}. A process that is much an art as a
science, feature engineering leverages a mixture of relevant domain
knowledge, intuition, and trial and error to identify features that
serve to best represent the data and to best serve the research aims.
Furthermore, the roles of features in EDA are fluid --no variable has a
special status, as seen in Figure~\ref{fig-eda-variables}. We will see
that in other types of analysis, some or all the roles of the variables
are fixed.

\begin{figure}[H]

\centering{

\includegraphics[width=0.75\textwidth,height=\textheight]{figures/aa-eda-variables.drawio.png}

}

\caption{\label{fig-eda-variables}Roles of variables in EDA.}

\end{figure}%

Any given dataset could serve as a starting point to explore many
different types of research questions. In order to maintain research
coherence so our efforts to not careen into a free-for-all, we need to
tether our feature engineering to a unit of analysis that is relevant to
the research question. A \textbf{unit of analysis} is the entity that we
are interested in studying. Not to be confused with the unit of
observation, which is the entity that we are able to observe and measure
(\citeproc{ref-Sedgwick2015}{Sedgwick 2015}). Depending on the
perspective we are interested in investigating, the choice of how to
approach engineering features to gain insight will vary.

By the same token, approaches for interrogating the dataset can differ
significantly, between research projects and within the same project,
but for instructive purposes, let's draw a distinction between
descriptive methods and unsupervised learning methods, as seen in
Table~\ref{tbl-eda-methods}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4444}}@{}}

\caption{\label{tbl-eda-methods}Some common EDA methods}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Descriptive methods
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Unsupervised learning methods
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Frequency analysis & Cluster analysis \\
Co-occurence analysis & Principal component analysis \\
Keyness analysis & Topic Modeling \\
& Vector space models \\

\end{longtable}

The first group, \textbf{descriptive methods} can be seen as an
extenstion of the descriptive statistics covered earlier in this chapter
including statistic, tabular, and visual techniques. The second group,
\textbf{unsupervised learning}, is a subtype of machine learning in
which an algorithm is used to find patterns within and between variables
in the data without any guidance (supervision). In this way, the
algorithm, or machine learner, is left to make connections and
associations wherever they may appear in the input data.

Either through descriptive, unsupervised learning methods, or a
combination of both, EDA employs quantitative methods to summarize,
reduce, and sort complex datasets in order to provide the researcher
novel perspective to be qualitatively assessed. Exploratory methods
produce results that require associative thinking and pattern detection.
Speculative as they are, the results from exploratory methods can be
highly informative and lead to new insight and inspire further study in
directions that may not have been expected.

\subsection{Predict}\label{sec-aa-predict}

\textbf{Predictive Data Analysis (PDA)} employs a variety of techniques
to examine and evaluate the association strength between a variable or
set of variables, with a specific focus on predicting a target variable.
The aim of PDA is to construct models that can accurately forecast
future outcomes, using either data-driven or theory-driven approaches.
In this process, \textbf{supervised learning} methods, where the machine
learning algorithm is guided (supervised) by a target outcome variable,
are used. This means we don't begin PDA with a completely open-ended
exploration, but rather with an objective - accurate predictions.
However, the path to achieving this objective can be flexible, allowing
us freedom to adjust our models and methods. Unlike EDA, where the
entire dataset can be reused for different approaches, PDA requires a
portion of the data to be reserved for evaluation, enhancing the
validity of our predictive models. Thus, PDA is an iterative process
that combines the flexibility of exploratory analysis with the rigor of
confirmatory analysis.

There are two types of variables in PDA: the outcome variable and the
predictor variables, or features. The \textbf{outcome variable} is the
variable that the researcher is trying to predict. It is the only
variable that is necessarily fixed as part of the research question. The
features are the variables that are used to predict the outcome
variable. An overview of the roles of these variables in PDA is shown in
Figure~\ref{fig-pda-variables}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.75\textwidth,height=\textheight]{figures/aa-pda-variables.drawio.png}

}

\caption{\label{fig-pda-variables}Roles of variables in PDA.}

\end{figure}%

Feature selection can be either data-driven or theory-driven.
Data-driven features are those that are engineered to enhance predictive
power, while theory-driven features are those that are selected based on
theoretical relevance.

The approach to interrogating the dataset includes three main steps:
feature engineering, model selection, and model evaluation. We've
discussed feature engineering, so what is model selection and model
evaluation? And how do we go about performing these steps?

\textbf{Model selection} is the process of choosing a machine learning
algorithm and set of features that produces the best prediction accuracy
for the outcome variable. To refine our approach such that we arrive at
the best combination of algorithm and features, we need to train our
machine learner on a variety of combinations and evaluate the accuracy
of each. We don't want to train and evaluate on the same data, as this
would be cheating, and likely would not produce a model that generalizes
well to new data. Instead, we split our data into two sets: a training
set and a test set. The \textbf{training set} is used to train the
machine learner, while the \textbf{test set} is used to evaluate the
accuracy of the model\footnote{Depending on the application and the
  amount of available data, a third \emph{development set} is sometimes
  created as a pseudo test set to facilitate the testing of multiple
  approaches on data outside the training set before the final
  evaluation on the test set is performed.}. The larger portion of the
data, from 60\% to 80\%, is used for training, while the remaining
portion is used for testing.

The elephant in the room is, what type of machine learning algorithm do
I use? Well, there are many different types of machine learning
algorithms, each with their own strengths and weaknesses. The first
rough cut is to decide what type of outcome variable we are predicting:
categorical or numeric. If the outcome variable is categorical, we are
performing a \textbf{classification} task, and if the outcome variable
is numeric, we are performing a \textbf{regression} task. As we see in
Table~\ref{tbl-pda-algorithms}, there are various algorithms that can be
used for each task.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3300}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3300}}@{}}
\caption{Some common supervised learning algorithms used in
PDA.}\label{tbl-pda-algorithms}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Classification
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Regression
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Classification
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Regression
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Logistic Regression & Linear Regression \\
Random Forest Classifier & Random Forest Regressor \\
Support Vector Machine & Support Vector Regression \\
Neural Network Classifier & Neural Network Regressor \\
\end{longtable}

There are a number of algorithm-specific strengths and weaknesses to be
considered in the process of model selection. These hinge on
characteristics of the data, such as the size of the dataset, the number
of features, the type of features, and the expected type of
relationships between features or on computing resources, such as the
amount of time available to train the model or the amount of memory
available to store the model.

\textbf{Model evaluation} is the process of assessing the accuracy of
the model on the test set, which is a proxy for how well the model will
generalize to new data. Model evaluation is performed quantitatively by
calculating the accuracy of the model on the training, to develop the
model, and ultimately, the test set. It is important to note that
whether the accuracy metrics are good is to some degree qualitative
judgment.

In the end, PDA offers a versitle path to discover data-driven insights,
to probe theory-driven associations, or even simply to perform tasks
that are too complex or time-consuming for humans to perform.

\subsection{Infer}\label{sec-aa-infer}

The most commonly recognized of the three data analysis approaches,
\textbf{Inferential data analysis (IDA)} is the bread-and-butter of
science. IDA is a deductive, theory-driven approach in which all aspects
of analysis stem from a pre-determined premise, or hypothesis, about the
nature of a relationship in the world and then aims to test whether this
relationship is statistically supported given the evidence. Since the
goal is to infer conclusions about a certain relationship in the
population based on a statistical evaluation of a (corpus) sample, the
representativeness of the sample is of utmost importance. Furthermore,
the use of the data is limited to the scope of the hypothesis --that is,
the data cannot be used iteratively for exploratory purposes.

The selection of variables and the roles they play in the analysis are
determined by the hypothesis. In a nutshell, a \textbf{hypothesis} is a
formal statement about the state of the world. This statement is
theory-driven meaning that it is predicated on previous research. We are
not exploring or examining relationships, rather we are testing a
specific relationship. In practice, however, we are in fact proposing
two mutally exclusive hypotheses. The first is the \textbf{Alternative
Hypothesis}, or \(H_1\). This is the hypothesis I just described --the
statement grounded in the previous literature outlining a predicted
relationship. The second is the \textbf{Null Hypothesis}, or \(H_0\).
This is the flip-side of the hypothesis testing coin and states that
there is no difference or relationship. Together \(H_1\) and \(H_0\)
cover all logical outcomes.

Now, in standard IDA one variable is the response variable and one or
more variables are explanatory variables. The \textbf{response
variable}, sometimes referred to as the outcome or dependent variable,
is the variable which contains the information which is hypothesized to
depend on the information in the explanatory variable(s). It is the
variable whose variation a research study seeks to explain. An
\textbf{explanatory variable}, sometimes referred to as a independent or
predictor variable, is a variable whose variation is hypothesized to
explain the variation in the response variable.

Explanatory variables add to the complexity of a study because they are
part of our research focus, specifically our hypothesis. It is, however,
common to include other variables which are not of central focus, but
are commonly assumed to contribute to the explanation of the variation
of the response variable. These are known as \textbf{control variables}.
Control variables are included in the analysis to account for the
influence of other variables on the relationship between the response
and explanatory variables, but will not be included in the hypothesis
nor interpreted in our results.

We can now see in Figure~\ref{fig-aa-ida-variables} the variables roles
assigned to variables in a hypothesis-driven study.

\begin{figure}[H]

\centering{

\includegraphics[width=0.75\textwidth,height=\textheight]{figures/aa-ida-variables.drawio.png}

}

\caption{\label{fig-aa-ida-variables}Roles of variables in IDA.}

\end{figure}%

At this point let's look at the main characteristics that need to be
taken into account to statistically interrogate the variables we have
chosen to test our hypothesis. The type of statistical test that one
chooses is based on (1) the informational value of the dependent
variable and (2) the number of predictor variables included in the
analysis. Together these two characteristics go a long way in
determining the appropriate class of statistical test (see S. Th. Gries
(\citeproc{ref-Gries2013a}{2013}) and Paquot and Gries
(\citeproc{ref-Paquot2020a}{2020}) for a more exhaustive description).

IDA relies heavily on quantitative evaluation methods to draw
conclusions that can be generalized to the target population. It is key
to understand that our goal in hypothesis testing is not to find
evidence in support of \(H_1\), but rather to assess the likelihood that
we can reliably reject \(H_0\).

The metric used to determine if there is sufficient evidence is based on
the probability that given the nature of the relationship and the
characteristics of the data, the likelihood of there being no difference
or relationship is low. The threshold for likelihood has traditionally
been summarized in the \(p\)-value statistic. In the Social Sciences, a
\(p\)-value lower than .05 is considered \emph{statistically
significant} which when interpreted correctly means that there is more
than a 95\% chance that the observed relationship would not be predicted
by \(H_0\).

Note that we are working in the realm of probability, not in absolutes,
therefore an analysis that produces a significant result does not prove
\(H_1\) is correct or that \(H_0\) is incorrect, for that matter. A
margin of error is always present. For this reason, other metrics such
as effect size and confidence intervals are also used to interpret the
results of statistical tests.

\section{Communicate}\label{sec-aa-communicate}

Conducting research should be enjoyable and personally rewarding but the
effort you have invested and knowledge you have generated should be
shared with others. Whether part of a blog, presentation, journal
article, or for your own purposes it is important to document your
analysis results and process in a way that is informative and
interpretable. This enhances the value of your work, allowing others to
learn from your experience and build on your findings.

\subsection{Report}\label{sec-aa-report}

The most widely recognized form of communicating research is through a
report. A report is a narrative of your analysis, including the research
question, the data you used, the methods you applied, and the results
you obtained. We are both reporting our findings and documenting our
process to inform others of what we did and why we did it but also to
invite readers to evaluate our findings for themselves. The scientific
process is a collaborative one and evaluation by peers is a key
component of the process.

The audience for your report will determine the level of detail and the
type of information you will need to include in your report but there
are some common elements to reference in any report. First, the research
question and/ or hypothesis should be clearly stated and the motivation
for the question should be explained. This will help the reader
understand the context of the analysis and the importance of the
results. Second, diagnostic procedures to verifiy or describe the data
should be explained. This may include anomaly correction, missing data,
data transformation, etc. and/ or descriptive summaries of the data
including assessments of individual variables (central tendency,
dispersion, distribution) and/ or relationships between variables
(association strength). Third, a blueprint of the methods used will
describe the variable selection process, how the variables are
operationalized, what analysis methods are employed, and how the
variables are used in the statistical analysis. Fourth, the results from
the analysis are reported. Reporting details will depend on the type of
analysis and the particular method(s) employed. Finally, the results are
interpreted in light of the research question and/ or hypothesis. This
will include a discussion of the limitations of the analysis and a
discussion of the implications of the results for future research.

\subsection{Document}\label{sec-aa-document}

While a good report will include the most vital information to
understand the procedures, results, and findings of an analysis, there
is much more information generated in the course of an analysis which
does not traditionally appear in prose. If a research project is
conducted programmatically, however, data, code, and documentation can
be made available to others as part of the communication process.
Increasingly, researchers are sharing their data and code as part of the
publication process. This allows others to reproduce the analysis and
verify the results contributing to the collaborative nature of the
scientific process.

Together, data, code, and documentation form a \textbf{research
compendium}. As you can imagine the research process can quickly become
complex and unwieldy as the number of files and folders grows. If not
organized properly, it can be difficult to find the information you
need. Furthermore, if not documented, decisions made in the course of
the analysis can be difficult or impossible to trace. For this reason,
it is recommendable to follow a set of best practices for organizing and
documenting your research compendium. We will cover this in more detail
in subsequent chapters.

\section*{Activities}\label{activities-1}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

In the following activies, we will build on our understanding of how to
summarize data using statistics, tables, and plots. We will dive deeper
into the use of the \texttt{skimr} package
(\citeproc{ref-R-skimr}{Waring et al. 2022}) to summarize data and the
\texttt{ggplot2} package (\citeproc{ref-R-ggplot2}{Wickham, Chang, et
al. 2023}) to create plots. We also introduce producing Quarto tables
and figures with appropriate code block options. We will reinforce our
understanding of the \texttt{readr} package
(\citeproc{ref-R-readr}{Wickham, Hester, and Bryan 2024}) to read in
data and the \texttt{dplyr} package (\citeproc{ref-R-dplyr}{Wickham,
François, et al. 2023}) to manipulate data.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-3.html}{Descriptive
assessment of datasets}\\
\textbf{How}: Read Recipe 3, complete comprehension check, and prepare
for Lab 3.\\
\textbf{Why}: To explore appropriate methods for summarizing variables
in datasets given the number and informational values of the
variable(s).

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-03}{Trace the
datascape}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 3.\\
\textbf{Why}: To identify and apply the appropriate descriptive methods
for a vector's informational value and to assess both single variables
and multiple variables with the appropriate statistical, tabular, and/
or graphical summaries.

\end{tcolorbox}

\section*{Summary}\label{summary-2}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we have focused on description and analysis --the third
component of DIKI Hierarchy. This process is visually summarized in
Figure~\ref{fig-approaching-analysis-vis-sum}.

\begin{figure}[H]

\centering{

\includegraphics[width=0.75\textwidth,height=\textheight]{figures/aa-diki.drawio.png}

}

\caption{\label{fig-approaching-analysis-vis-sum}Approaching analysis:
visual summary}

\end{figure}%

The first key step in any analysis is to perform a diagnostic assessment
of the individual variables and relationships between variables. To
select the appropriate descriptive measures we covered the various
informational values that a variable can take. In addition to providing
key information for reporting purposes, descriptive measures are
important to explore so the researcher can get a better feel for the
dataset before conducting an analysis.

We outlined three data analysis types in this chapter: exploratory,
predictive, and inferential. Each of these embodies distinct approaches
to deriving knowledge from data. Ultimately the choice of analysis type
is highly dependent on the goals of the research. Inferential analysis
is centered around the goal of testing a hypothesis, and for this reason
it is the most highly structured approach to analysis. This structure is
aimed at providing the mechanisms to draw conclusions from the results
that can be generalized to the target population. Predictive analysis
has a less-ambitious but at times more relevant goal of examining the
extent to which a given relationship can be established from the data to
provide a model of language that can accurately predict an outcome using
new data. This methodology is highly effective for applying different
algorithmic approaches and examining relationships between an outcome
variable and various configurations of variables. The ability to explore
the data in multiple ways, is also a key strength of employing an
exploratory analysis. The least structured and most variable of the
analysis types, exploratory analyses are a powerful approach to
generating knowledge from data in an area where clear predictions cannot
be made.

I rounded out this chapter with a short description of the importance of
communicating the analysis process and results. Reporting, in its
traditional form, is documented in prose in an article. This reporting
aims to provide the key information that a reader will need to
understand what was done, how it was done, and why it was done. This
information also provides the necessary information for reader's with a
critical eye to understand the analysis in more detail. Yet even the
most detailed reporting in a write-up still leaves many practical, but
key, points of the analysis obscured. A programming approach provides
the procedural steps taken that when shared provide the exact methods
applied. Together with the write-up, a research compendium which
provides the scripts to run the analysis and documentation on how to run
the analysis forms an integral part of creating reproducible research.

\chapter{Research}\label{sec-framing-research}

\begin{quote}
Thus, the task is, not so much to see what no one has seen yet; but to
think what nobody has thought yet, about that what everybody sees.

--- Erwin Schrödinger
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Identify a research area and problem by listing key strategies and
  describing their contribution towards research identification.
\item
  Explain the significance of a well-framed research question in guiding
  the overall research project.
\item
  Comprehend how the conceptual and practical steps involved in
  developing a research blueprint aid not only the researcher but also
  the broader scientific community.
\end{itemize}

\end{tcolorbox}

In this chapter, we discuss how to frame research, that is how to
position your research project's findings to contribute insight to
understanding of the world. We will cover how to connect with the
literature, selecting a research area and identifying a research
problem, and how to design research best positioned to return relevant
findings that will connect with this literature, establishing a research
aim and research question. We will round out this chapter with a guide
on developing a research blueprint --a working plan to organize the
conceptual and practical steps to implement the research effectively and
in a way that supports communicating the research findings and the
process by which the findings were obtained.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{terminal} Lessons}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Project
Environment}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To highlight the importance of the computing environment
in R for project management and reproducibility.

\end{tcolorbox}

\section{Frame}\label{sec-fr-frame}

Together a research area, problem, aim and question and the research
blueprint that forms the conceptual and practical scaffolding of the
project ensure from the outset that the project is solidly grounded in
the main characteristics of good research. These characteristics,
summarized by Cross (\citeproc{ref-Cross2006}{2006}), are found in
Table~\ref{tbl-fr-cross-research-char-table}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8500}}@{}}

\caption{\label{tbl-fr-cross-research-char-table}Characteristics of
research (Cross, 2006).}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Characteristic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Purposive & Based on identification of an issue or problem worthy and
capable of investigation \\
Inquisitive & Seeking to acquire new knowledge \\
Informed & Conducted from an awareness of previous, related research \\
Methodical & Planned and carried out in a disciplined manner \\
Communicable & Generating and reporting results which are feasible and
accessible by others \\

\end{longtable}

With these characteristics in mind, let's get started with the first
component to address --connecting with the literature.

\section{Connect}\label{sec-fr-connect}

\subsection{Research area}\label{research-area}

The first decision to make in the research process is to identify a
research area. A \textbf{research area} is a general area of interest
where a researcher wants to derive insight and make a contribution to
understanding. For those with an established research trajectory in
language, the area of research to address through text analysis will
likely be an extension of their prior work. For others, which include
new researchers or researchers that want to explore new areas of
language research or approach an area through a language-based lens, the
choice of area may be less obvious. In either case, the choice of a
research area should be guided by a desire to contribute something
relevant to a theoretical, applied, and/ or practical matter of personal
interest. Personal relevance goes a long way to developing and carrying
out \emph{purposive} and \emph{inquisitive} research.

So how do we get started? Consider your interests in a language or set
of languages, a discipline, a methodology, or some applied area.
Language is at the heart of the human experience and therefore found in
some fashion anywhere one seeks to find it. But it is a big world and
more often than not the general question about what area to explore
language use is sometimes the most difficult. To get the ball rolling,
it is helpful to peruse disciplinary encyclopedias or handbooks of
linguistics and language-related an academic fields (\emph{e.g.}
Encyclopedia of Language and Linguistics (\citeproc{ref-Brown2005}{Brown
2005}), A Practical Guide to Electronic Resources in the Humanities
(\citeproc{ref-Dubnjakovic2010}{Dubnjakovic and Tomlin 2010}), Routledge
encyclopedia of translation technology (\citeproc{ref-Chan2014}{Chan
2014}))

A more personal, less academic, approach is to consult online forums,
blogs, \emph{etc}. that one already frequents or can be accessed via an
online search. Through social media you may find particular people that
maintain a blog worth browsing. Perusing these resources can help spark
ideas and highlight the kinds of questions that interest you.

Regardless of whether your inquiry stems from academic, professional, or
personal interest, try to connect these findings to academic areas of
research. Academic research is highly structured and well-documented and
making associations with this network will aid in subsequent steps in
developing a research project.

\subsection{Research problem}\label{sec-fr-problem}

Once you've made a rough-cut decision about the area of research, it is
now time to take a deeper dive into the subject area and jump into the
literature. This is where the rich structure of disciplinary research
will provide aid to traverse the vast world of academic knowledge and
identify a research problem. A \textbf{research problem} highlights a
particular topic of debate or uncertainty in existing knowledge which is
worthy of study.

Surveying the relevant literature is key to ensuring that your research
is \emph{informed}, that is, connected to previous work. Identifying
relevant research to consult can be a bit of a `chicken or the egg'
problem --some knowledge of the area is necessary to find relevant
topics, some knowledge of the topics is necessary to narrow the area of
research. Many times the only way forward is to jump into conducting
searches. These can be world-accessible resources (\emph{e.g.} Google
Scholar) or limited-access resources that are provided through an
academic institution (\emph{e.g.} Linguistics and Language Behavior
Abstracts, ERIC, PsycINFO, \emph{etc.}). Some organizations and academic
institutions provide research guides to help researcher's access the
primary literature. There are even a new breed of search engines that
are designed to help researchers aggregate and search academic
literature (\emph{e.g.} Scite, Elicit, \emph{etc.}).

Another avenue to explore are journals and conference proceedings
dedicated to linguistics and language-related research. Text analysis is
a rapidly expanding methodology which is being applied to a wide range
of research areas.

To explore research related to text analysis it is helpful to start with
the (sub)discipline name(s) you identified in when selecting your
research area, more specific terms that occur to you or key terms from
the literature, and terms such as `corpus study' or `corpus-based'. The
results from first searches may not turn out to be sources that end up
figuring explicitly in your research, but it is important to skim these
results and the publications themselves to mine information that can be
useful to formulate better and more targeted searches.

Relevant information for honing your searches can be found throughout an
academic publication. However, pay particular attention to the abstract,
in articles, and the table of contents, in books, and the cited
references. Abstracts and tables of contents often include
discipline-specific jargon that is commonly used in the field. In some
articles, there is even a short list of key terms listed below the
abstract which can be extremely useful to seed better and more precise
search results. The references section will contain relevant and
influential research. Scan these references for publications which
appear to narrowing in on topic of interest and treat it like a search
in its own right.

Once your searches begin to show promising results it is time to keep
track and organize these references. Whether you plan to collect
thousands of references over a lifetime of academic research or your aim
is centered around one project, software such as
\href{https://www.zotero.org/}{Zotero}\footnote{\href{https://guides.zsr.wfu.edu/zotero}{Zotero
  Guide}},
\href{https://www.mendeley.com/reference-management/reference-manager}{Mendeley},
or \href{https://bibdesk.sourceforge.io/}{BibDesk} provide powerful,
flexible, and easy-to-use tools to collect, organize, annotate, search,
and export references. Citation management software is indispensable for
modern research --and often free!

As your list of relevant references grows, you will want to start the
investigation process in earnest. Begin skimming (not reading) the
contents of each of these publications, starting with the most relevant
first\footnote{Or what appears to be most relevant. This may change as
  you start to take a closer look.}. Annotate these publications using
highlighting features of the citation management software to identify:
(1) the stated goal(s) of the research, (2) the data source(s) used, (3)
the information drawn from the data source(s), (4) the analysis approach
employed, and (5) the main finding(s) of the research as they pertain to
the stated goal(s).

Next, in your own words, summarize these five key areas in prose adding
your summary to the notes feature of the citation management software.
This process will allow you to efficiently gather and document
references with the relevant information to guide the identification of
a research problem, to guide the formation of your problem statement,
and ultimately, to support the literature review that will figure in
your project write-up.

From your preliminary annotated summaries you will undoubtedly start to
recognize overlapping and contrasting aspects in the research
literature. These aspects may be topical, theoretical, methodological,
or appear along other lines. Note these aspects and continue to conduct
more refine searches, annotate new references, and monitor for any
emerging patterns of uncertainty or debate (gaps) which align with your
research interest(s). When a promising pattern takes shape, it is time
to engage with a more detailed reading of those references which appear
most relevant highlighting the potential gap(s) in the literature.

At this point you can focus energy on more nuanced aspects of a
particular gap in the literature with the goal to formulate a problem
statement. A \textbf{problem statement} directly acknowledges a gap in
the literature and puts a finer point on the nature and relevance of
this gap for understanding. This statement reflects your first
deliberate attempt to establish a line of inquiry. It will be a
targeted, but still somewhat general, statement framing the gap in the
literature that will guide subsequent research design decisions.

\section{Define}\label{sec-fr-define}

\subsection{Research aim}\label{sec-fr-aim}

With a problem statement in hand, it is now time to consider the goal(s)
of the research. A \textbf{research aim} frames the type of inquiry to
be conducted. Will the research aim to explore, examine, or explain? As
you can appreciate, the research aim is directly related to the analysis
methods we touched upon in \hyperref[sec-approaching-analysis]{Chapter
3}.

To gauge how to frame your research aim, reflect on the literature that
led you to your problem statement and the nature of the problem
statement itself. If the gap at the center of the problem statement is a
lack of knowledge, your research aim may be exploratory. If the gap
concerns a conjecture about a relationship, then your research may take
a predictive approach. When the gap points to the validation of a
relationship, then your research will likely be inferential in nature.
Before selecting your research aim it is also helpful to consult the
research aims of the primary literature that led you to your research
statement.

Typically, a problem statement addressing a subtle, specific issue tends
to adopt research objectives similar to prior studies. In contrast, a
statement focusing on a broader, more distinct issue is likely to have
unique research goals. Yet, this is more of a guideline than a strict
rule.

It's crucial to understand both the existing literature and the nature
of various types of analyses. Being clear about your research goals is
important to ensure that your study is well-placed to produce results
that add value to the current understanding in an informed manner.

\subsection{Research question}\label{sec-fr-question}

The next step in research design is to craft the research question. A
\index{research question}\textbf{research question} is clearly defined
statement which identifies an aspect of uncertainty and the particular
relationships that this uncertainty concerns. The research question
extends and narrows the line of inquiry established in the research
statement and research aim. To craft a research question, we can use the
research statment for the content and the research aim for the form.

\subsubsection{Form}\label{sec-fr-question-form}

The form of a research question will vary based on the research aim,
which as I mentioned, is inimately connected to the analysis approach.
For inferential-based research, the research question will actually be a
statement, not a question. This statement makes a testable claim about
the nature of a particular relationship --\emph{i.e.} asserts a
hypothesis.

For illustration, let's posit a hypothesis (\(H_1\)), leaving aside the
implicit null hypothesis (\(H_0\)), seen in
Example~\ref{exm-fr-form-infer}.

\begin{example}[]\protect\hypertarget{exm-fr-form-infer}{}\label{exm-fr-form-infer}

Women use more questions than men in spontaneous conversations.

\end{example}

For predictive- and exploratory-based research, the research question is
in fact a question. A reframing of the example hypothesis for a
predictive-based research question might take the form seen in
Example~\ref{exm-fr-form-pred}.

\begin{example}[]\protect\hypertarget{exm-fr-form-pred}{}\label{exm-fr-form-pred}

Can the number of questions used in spontaneous conversations predict if
a speaker is male or female?

\end{example}

And a similar exploratory-based research question might take the form
seen in Example~\ref{exm-fr-form-exp}.

\begin{example}[]\protect\hypertarget{exm-fr-form-exp}{}\label{exm-fr-form-exp}

Do men and women differ in terms of the number of questions they use in
spontaneous conversations?

\end{example}

The central research interest behind these hypothetical research
questions is, admittedly, quite basic. But from these simplified
examples, we are able to appreciate the similarities and differences
between the forms of research statements that correspond to distinct
research aims.

\subsubsection{Content}\label{sec-fr-question-content}

In terms of content, the research question will make reference to two
key components. First, is the unit of analysis. The \textbf{unit of
analysis} is the entity which the research aims to investigate. For our
three example research aims, the unit of analysis is the same, namely
\emph{speakers}. Note, however, that the current unit of analysis is
somewhat vague in the example research questions. A more precise unit of
analysis would include more information about the population from which
the speakers are drawn (\emph{i.e.} English speakers, American English
speakers, American English speakers of the Southeast, \emph{etc}.).

The second key component is the unit of observation. The \textbf{unit of
observation} is the primary element on which the insight into the unit
of analysis is derived and in this way constitutes the essential
organizational unit of the dataset to be analyzed. In our examples, the
unit of observation, again, is unchanged and is \emph{spontaneous
conversations}. Note that while the unit of observation is key to
identify as it forms the organizational backbone of the research, it is
very common for the research to derive variables from this unit to
provide evidence to investigate the research question.

In examples \ref{exm-fr-form-infer}, \ref{exm-fr-form-pred}, and
\ref{exm-fr-form-exp}, we identified the number of conversations as part
of the research question. Later in the research process it will be key
to operationalize this variable. For example, will the number of
conversations be the total number of conversations in the dataset or
will it be the average number of conversations per speaker? These are
important questions to consider as they will influence variable
selection, statistical choices, and ultimately the interpretation of the
results.

\section{Blueprint}\label{sec-fr-blueprint}

The efforts to develop a research question will produce a clear and
focused line of inquiry with the necessary background literature and a
well-defined problem statement that forrms the basis of
\emph{purposeful}, \emph{inquisitive}, and \emph{informed} research
(returning to Cross's characteristics of research in
Table~\ref{tbl-fr-cross-research-char-table}).

Moving beyond the research question in the project means developing and
laying out the research design in a way such that the research is
\emph{methodical} and \emph{communicable}. In this textbook, the method
to achieve these goals is through the development of a research
blueprint. The blueprint includes two components: (1) the conceptual
plan and (2) the organizational scaffolding that will support the
implementation of the research.

As Ignatow and Mihalcea (\citeproc{ref-Ignatow2017}{2017}) point out:

\begin{quote}
Research design is essentially concerned with the basic architecture of
research projects, with designing projects as systems that allow theory,
data, and research methods to interface in such a way as to maximize a
project's ability to achieve its goals {[}\ldots{]}. Research design
involves a sequence of decisions that have to be taken in a project's
early stages, when one oversight or poor decision can lead to results
that are ultimately trivial or untrustworthy. Thus, it is critically
important to think carefully and systematically about research design
before committing time and resources to acquiring texts or mastering
software packages or programming languages for your text mining project.
\end{quote}

In what follows, I will cover the main aspects of developing a research
blueprint. I will start with the conceptual plan and then move on to the
organizational scaffolding.

\subsection{Plan}\label{sec-fr-plan}

Importance of establishing a feasible research design from the outset
and documenting the key aspects required to conduct the research cannot
be understated. On the one hand, this process links a conceptual plan to
a tangible implementation. In doing so, a researcher is
better-positioned to conduct research with a clear view of what will be
entailed. On the other hand, a promising research question may present
unexpected challenges once a researcher sets about to implement the
research. This is not uncommon to encounter issues that require
modification or reevaluation of the viability of the project. However, a
well-documented research plan will help a researcher to identify and
address many of these challenges at the conceptual level before
expending unnecessary effort during implementation.

Let's now consider the subsequent steps to develop a research plan,
outlined in Table~\ref{tbl-fr-plan-checklist}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7500}}@{}}
\caption{Research plan
checklist}\label{tbl-fr-plan-checklist}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Stage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Activity
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Stage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Activity
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Research Question or Hypothesis & Formulate a research question or
hypothesis based on a thorough review of existing literature including
references. This will guide every subsequent step from data selection to
interpretation of results. \\
2 & Data Source(s) & Identify viable data source(s) and vet the sample
data in light of the research question. Consider to what extent the goal
is to generalize findings to a target population, and ensure that the
corpus aligns as much as feasible with this target. \\
3 & Key Variables & Determine the key variables needed for the research,
define how they will be operationalized, and ensure they can be derived
from the corpus data. Additionally, identify any features that need to
be extracted, recoded, generated, or integrated from other data
sources. \\
4 & Analysis Method & Choose an appropriate method of analysis to
interrogate the dataset. This choice should be in line with your
research aim (\emph{e.g.}, exploratory, predictive, or inferential). Be
aware of what each method can offer and how it addresses your research
question. \\
5 & Interpretation \& Evaluation & Establish criteria to interpret and
evaluate the results. This will be a function of the relationship
between the research question and the analysis method. \\
\end{longtable}

First, identify a viable data source. Viability includes the
accessibility of the data, availability of the data, and the content of
the data. If a purported data source is not accessible and/ or it has
stringent restrictions on its use, then it is not a viable data source.
If a data source is accessible and available, but does not contain the
building blocks needed to address the research question, then it is not
a viable data source. A corpus resource should represent, to the extent
feasible, with the target population(s).

The second step is to identify the key variables needed to conduct the
research and then ensure that this information can be derived from the
corpus data. The research question will reference the unit of analysis
and the unit of observation, but it is important to pinpoint what the
key variables will be. We want to envision what needs to be done to
derive these variables. There may be features that need to be extracted,
recoded, generated, and/ or integrated from other sources to address the
research question, as discussed in Chapter~\ref{sec-understanding-data}.

The third step is to identify a method of analysis to interrogate the
dataset. The selection of the analysis approach that was part of the
research aim (\emph{i.e.} explore, predict, or infer) and then the
research question goes a long way to narrowing the methods that a
researcher must consider. But there are a number of factors which will
make some methods more appropriate than others.

Exploratory research is the least restricted of the three types of
analysis approaches. Although it may be the case that a research will
not be able to specify from the outset of a project what the exact
analysis methods will be, an attempt to consider what types of analysis
methods will be most promising to provide results to address the
research question goes a long way to steering a project in the right
direction and grounding the research. As with the other analysis
approaches, it is important to be aware of what the analysis methods
available and what type of information they produce in light of the
research question.

For predictive-based research, the informational value of the outcome
variable is key to deciding whether the prediction will be a
classification task or a regression task. This has downstream effects
when it comes time to evaluate and interpret the results. Although the
feature engineering process in predictive analyses means that the
features do not need to be specified from the outset and can be tweaked
and changed as needed during an analysis, it is a good idea to start
with a basic sense of what features most likely will be helpful in
developing a robust predictive model.

In inferential research, the number and information values of the
variables to be analyzed will be of key importance
(\citeproc{ref-Gries2013a}{S. Th. Gries 2013}). The informational value
of the response variable will again narrow the search for the
appropriate method. The number of explanatory variables also plays an
important role. These details need not be nailed down at this point, but
it is helpful to have them on your radar to ensure that when the time
comes to analyze the data, the appropriate steps are taken to apply the
correct test.

The last of the main components of the research plan concerns the
interpretation and evaluation of the results. This step brings the
research plan full circle connecting the research question to the
methods employed. It is important to establish from the outset what the
criteria will be to evaluate the results. This is in large part a
function of the relationship between the research question and the
analysis method. For example, in exploratory research, the results will
be evaluated qualitatively in terms of the associative patterns that
emerge. Predictive and inferential research leans more heavily on
quantitative metrics in particular the accuracy of the prediction or the
strength of the relationship between the response and explanatory
variable(s), respectively. However, these quantitative metrics require
qualitative interpretation to determine whether the results are
meaningful in light of the research question.

In addition to addressing the steps outlined in
Table~\ref{tbl-fr-plan-checklist}, it is also important to document the
strengths and shortcomings of the research plan including the data
source(s), the information to be extracted from the data, and the
analysis methods. If there are potential shortcomings, which there most
often are, sketch out contingency plans to address these shortcomings.
This will help buttress your research and ensure that your time and
effort is well-spent.

The research plan together with the information collected to develop the
research question is known as a prospectus. A \textbf{prospectus} is a
document that outlines the key aspects of the research plan and is used
to guide the research process. It is a living document that will be
updated as the research progresses and as new information is collected.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

You may consider pre-registering your prospectus to ensure that your
plans are well-documented and to provide a timestamp for your research.
Pre-registration can also be a helpful way to get feedback on your
research from colleagues and experts in the field. Popular
pre-registration platforms include \href{https://osf.io/}{Open Science
Framework} and \href{https://www.cos.io/initiatives/prereg}{Center for
Open Science}.

\end{tcolorbox}

\subsection{Scaffold}\label{sec-fr-scaffold}

The next step in developing a research blueprint is to consider how to
physically implement your project. This includes how to organize files
and directories in a fashion that both provides the researcher a logical
and predictable structure to work with but also ensures that the
research is \emph{Communicable}. On the one hand, communicable research
includes a strong write-up of the research, but, on the other hand, it
is also important that the research is reproducible.

Reproducibility strategies are a benefit to the researcher (in the
moment and in the future) as it leads to better work habits and to
better teamwork and it makes changes to the project easier.
Reproducibility is also of benefit to the scientific community as shared
reproducible research enhances replicability and encourages cumulative
knowledge development (\citeproc{ref-Gandrud2015}{Gandrud 2015}).

In Table~\ref{tbl-fr-repro-research}, I outline a set of guiding
principles that characterize reproducible research
(\citeproc{ref-Gentleman2007}{Gentleman and Temple Lang 2007};
\citeproc{ref-Marwick2018}{Marwick, Boettiger, and Mullen 2018}).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7000}}@{}}
\caption{Reproducible Research
Principles}\label{tbl-fr-repro-research}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
No.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Principle
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
No.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Principle
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Plain text & All files should be plain text which means they contain
no formatting information other than whitespace. \\
2 & Clear separation & There should be a clear separation between the
inputs, process steps, and outputs of research. This should be apparent
from the directory structure. \\
3 & Original data & A separation between original data and data created
as part of the research process should be made. Original data should be
treated as `read-only'. Any changes to the original data should be
justified, generated by the code, and documented (see point 7). \\
4 & Modular scripts & Each computing file (script) should represent a
particular, well-defined step in the research process. \\
5 & Modular files & Each script should be modular --that is, each file
should correspond to a specific goal in the analysis procedure with
input and output only corresponding to this step. \\
6 & Main script & The project should be tied together by a `main' script
that is used to coordinate the execution of all the project steps. \\
7 & Document everything & Everything should be documented. This includes
data collection, data preprocessing, processing steps, script code
comments, data description in data dictionaries, information about the
computing environment and packages used to conduct the analysis, and
detailed instructions on how to reproduce the research. \\
\end{longtable}

These seven principles in Table~\ref{tbl-fr-repro-research} can be
physically implemented in numerous ways. In recent years, there has been
a growing number of efforts to create R packages and templates to
quickly generate the scaffolding and tools to facilitate reproducible
research. Some notable R packages include
\href{https://jdblischak.github.io/workflowr/}{workflowr}
(\citeproc{ref-R-workflowr}{J. Blischak, Carbonetto, and Stephens
2023}), \href{http://projecttemplate.net/}{ProjectTemplate}
(\citeproc{ref-R-ProjectTemplate}{White 2023}), and
\href{https://github.com/ropensci/targets}{targets}
(\citeproc{ref-R-targets}{Landau 2024}), but there are many other
resources for R included on the
\href{https://cran.r-project.org/web/views/ReproducibleResearch.html}{CRAN
Task View for Reproducible Research}.

There are many advantages to working with pre-existing frameworks for
the savvy R programmer including the ability to quickly generate a
project scaffold, to efficiently manage changes to the project, and to
buy in to a common framework that is supported by a community of
developers.

On the other hand, these frameworks can be a bit daunting for the novice
R programmer. At the most basic level, a project can implement the seven
principles outlined above with a directory structure and a set of key
files seen in Example~\ref{exm-fr-basic-project}.

\begin{example}[]\protect\hypertarget{exm-fr-basic-project}{}\label{exm-fr-basic-project}

Minimal Project Framework

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{project/}
\ExtensionTok{├──}\NormalTok{ input/}
\ExtensionTok{│}\NormalTok{   └── ...}
\ExtensionTok{├──}\NormalTok{ code/}
\ExtensionTok{│}\NormalTok{   └── ...}
\ExtensionTok{├──}\NormalTok{ output/}
\ExtensionTok{│}\NormalTok{   └── ...}
\ExtensionTok{├──}\NormalTok{ DESCRIPTION}
\ExtensionTok{├──}\NormalTok{ Makefile}
\ExtensionTok{└──}\NormalTok{ README}
\end{Highlighting}
\end{Shaded}

\end{example}

The \emph{project/} directory is composed of three main sections:
\emph{input/}, \emph{code/}, and \emph{output/} making the destinction
between each transparent in the directory structure. The \emph{input/}
will house the data used and created in the project, ensuring that the
original data is kept separate from the data created in the research
process. The \emph{code/} section will house the scripts that will
conduct the project steps including acquiring, curating, transforming,
and analyzing the data. These scripts will read and write data and
generate output including figures, reports, results, and tables. Lastly,
the \emph{output/} section will house the resulting output from the
project steps.

At the root of the project directory are three files which describe,
document, and execute the project. The \emph{Makefile} is used to
automate the execution of the project steps. In effect, it is a script
that runs scripts. In addition to coordinating the execution of the
project steps, a Makefile will often include commands to set up the
computing environment and packages. The \emph{README} and
\emph{DESCRIPTION} files provide on overview of the project from both a
conceptual and technical perspective. The \emph{README} file includes a
description of the project rationale, aims, and findings and
instructions on how to reproduce the research. The \emph{DESCRIPTION}
file includes technical information about the computing environment and
packages used to conduct the analysis.

The project structure in Example~\ref{exm-fr-basic-project} meets the
minimal structural requirements for reproducible research and is a good
starting point for a project scaffold. However, aspects of this
structure can be adjusted in minimal or more sophisticated ways to meet
the needs of a particular project while still conforming to the
principles outlined in Table~\ref{tbl-fr-repro-research}, when we return
to this topic in Chapter~\ref{sec-contributing}.

\section*{Activities}\label{activities-2}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

The following activities will build on your experience with R and
cloning a GitHub repository, and recent experience with understanding
the computing environment. The goal will be to bring you up to speed
such that you can begin to work on your own research projects and
understand how to use the tools and resources available to you to manage
your project.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-4.html}{Understanding
the computing environment}\\
\textbf{How}: Read Recipe 4, complete comprehension check, and prepare
for Lab 4.\\
\textbf{Why}: To introduce components of the computing environment and
how to manage a reproducible research project structure.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-04}{Scaffolding
reproducible research}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 4.\\
\textbf{Why}: To establish a repository and project structure for
reproducible research and apply new Git and Github skills to fork,
clone, commit, and push changes.

\end{tcolorbox}

\section*{Summary}\label{summary-3}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

The aim of this chapter is to provide the key conceptual and practical
points to guide the development of a viable research project. Good
research is purposive, inquisitive, informed, methodological, and
communicable. It is not, however, always a linear process. Exploring
your area(s) of interest and connecting with existing work will help
couch and refine your research. But practical considerations, such as
the existence of viable data, technical skills, and/ or time constrains,
sometimes pose challenges and require a researcher to rethink and/ or
redirect the research in sometimes small and other times more
significant ways. The process of formulating a research question and
developing a viable research plan is key to supporting viable,
successful, and insightful research. To ensure that the effort to derive
insight from data is of most value to the researcher and the research
community, the research should strive to be methodological and
communicable adopting best practices for reproducible research.

\begin{figure}[H]

\centering{

\includegraphics[width=0.75\textwidth,height=\textheight]{figures/fr-diki.drawio.png}

}

\caption{\label{fig-fr-visual-summary}Framing research: visual summary}

\end{figure}%

This chapter concludes the Foundations part of this textbook. At this
stage our overview of fundamental characteristics of research are in
place to move a project towards implementation, as seen in
Figure~\ref{fig-fr-visual-summary}. From this point forward we will
integrate your conceptual knowledge and emerging R programming skills as
we cover common scenarios encountered when conducting reproducible
research with real-world data.

The next part, Preparation, aims to cover R coding strategies to
acquire, curate, and transform data in preparation for analysis. These
are the first steps in putting a research blueprint into action and by
no coincidence the first components in the Data to Insight Hierarchy.
Without further ado, let's get started!

\part{Preparation}

At this point we begin our journey to implement the research blueprint.
As such, the content will be more focused on the practical steps to
bring a plan to fruition integrating our conceptual understanding of the
research process from the previous chapters with our emerging
programming skills developed in lessons, recipes, and labs.

This part, Preparation, will address data acquistion, curation, and
transformation steps. The goal of data preparation is to create a
dataset which is ready for analysis. In each of these three upcoming
chapters, I will outline some of the main characteristics to consider in
each of these research steps and provide authentic examples of working
with R to implement these steps. In \hyperref[sec-acquire-data]{Chapter
5} this includes the most common strategies for acquiring data:
downloads and APIs. In \hyperref[sec-curate-data]{Chapter 6} we turn to
organize data into rectangular, or `tidy', format. Depending on the data
or dataset acquired for the research project, the steps necessary to
shape our data into a base dataset will vary, as we will see. In
\hyperref[sec-transform-data]{Chapter 7} we will work to manipulate
curated datasets to create datasets which are aligned with the research
aim and research question. This often includes normalizing values,
recoding variables, and generating new variables as well as and sourcing
and merging information from other datasets with the dataset to be
submitted for analysis.

Each of these chapters will cover the necessary documentation to trace
our steps and provide a record of the data preparation process.
Documentation serves to inform the analysis and interpretation of the
results and also forms the cornerstone of reproducible research.

\chapter{Acquire}\label{sec-acquire-data}

\begin{quote}
The scariest moment is always just before you start.

--- Stephen King
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Identify common strategies for acquiring corpus data.
\item
  Describe how to organize and document data acquisition to support
  reproducibility.
\item
  Recall R programming concepts and strategies relevant to acquiring
  data.
\end{itemize}

\end{tcolorbox}

As we start down the path to executing our research blueprint, our first
step is to acquire the primary data that will be employed in the
project. This chapter covers two commonly-used strategies for acquiring
corpus data: downloads and APIs. We will encounter various file formats
and folder structures in the process and we will address how to
effectively organize our data for subsequent processing. Crucial to our
efforts is the process of documenting our data. We will learn to provide
data origin information to ensure key characteristics of the data and
its source are documented.

Along the way, we will explore R coding concepts including control
statements and custom functions relevant to the task of acquiring data.
By the end of this chapter, you will not only be adept at acquiring data
from diverse sources but also capable of documenting it comprehensively,
enabling you to replicate the process in the future.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{terminal} Lessons}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Control
Statements, Custom Functions}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To recognize the logic behind code that can make dynamic
choices and to recall how functions serve to produce efficient,
reusable, and more legible code.

\end{tcolorbox}

\section{Downloads}\label{downloads}

The most common and straightforward method for acquiring corpus data is
through direct downloads. In a nutshell, this method involves navigating
to a website, locating the data, and downloading it to your computing
environment. In some cases access to the data requires manual
intervention and in others the process can be implemented
programmatically. The data may be contained in a single file or multiple
files. The files may be compressed or uncompressed. The data may be
hierarchically organized or not. Each resource will have its own unique
characteristics that will influence the process of acquiring the data.
In this section we will work through a few examples to demonstrate the
general process of acquiring data through downloads.

\subsection{Manual}\label{manual}

In contrast to the other data acquisition methods we will cover in this
chapter, \textbf{manual downloads} require human intervention. This
means that manual downloads are non-reproducible in a strict sense and
require that we keep track of and document our procedure. It is a very
common for research projects to acquire data through manual downloads as
many data resources require some legwork before they are accessible for
downloading. These can be resources that require institutional or
private licensing and fees, require authorization/ registration, and/ or
are only accessible via resource search interfaces.

The resource we will use for this demonstration is the
\href{http://cedel2.learnercorpora.com/}{Corpus Escrito del Español como
L2 (CEDEL2)} (\citeproc{ref-Lozano2009}{Lozano 2009}), a corpus of
Spanish learner writing. It includes L2 writing from students with a
variety of L1 backgrounds. For comparative puposes it also includes
native writing for Spanish, English, and several other languages.

The CEDEL2 corpus is a freely available resource, but to access the data
you must first use a search interface to select the relevant
characteristics of the data of interest. Following the search/ download
link you can find a search interface that allows the user to select the
subcorpus and filter the results by a set of attributes, seen in
Figure~\ref{fig-ad-cedel2-search}.

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=\textheight]{figures/ad-cedel2-search.png}

}

\caption{\label{fig-ad-cedel2-search}Search and download interface for
the CEDEL2 Corpus}

\end{figure}%

For this example let's assume that we want to acquire data to use in a
study comparing the use of the Spanish preterite and imperfect past
tense aspect in written texts by English L1 learners of Spanish to
native Spanish speakers. To acquire data for such a project, we will
first select the subcorpus ``Learners of L2 Spanish''. We will set the
results to provide full texts and filter the results to ``L1 English -
L2 Spanish''. Additionally, we will set the medium to ``Written''. This
will provide us with a set of texts for the L2 learners that we can use
for our study. The search parameters and results are shown in
Figure~\ref{fig-ad-cedel2-results}.

\begin{figure}[H]

\centering{

\includegraphics[width=1\textwidth,height=\textheight]{figures/ad-cedel2-results.png}

}

\caption{\label{fig-ad-cedel2-results}Search results for the CEDEL2
Corpus}

\end{figure}%

The `Download' link now appears for this search criteria. Following this
link will provide the user a form to fill out. This particular resource
allows for access to different formats to download (Texts only, Texts
with metadata, CSV (Excel), CSV (Others)). I will select the `CSV
(Others)' option so that the data is structured for easier processing
downstream in subsequent processing steps. Then I save the CSV in the
\emph{data/original/} directory of my project and create a sub-directory
named \emph{cedel2/}, as seen in
Example~\ref{exm-ad-cedel2-learners-download}.

\begin{example}[]\protect\hypertarget{exm-ad-cedel2-learners-download}{}\label{exm-ad-cedel2-learners-download}

Download CEDEL2 L2 Spanish Learners data

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ analysis/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{└──}\NormalTok{ cedel2/}
    \ExtensionTok{└──}\NormalTok{ cedel2{-}l1{-}english{-}learners.csv}
\end{Highlighting}
\end{Shaded}

\end{example}

Note that the file is named \emph{cedel2-l1-english-learners.csv} to
reflect the search criteria used to acquire the data. In combination
with other data documentation, this will help us to maintain
transparency.

Now, after downloading the L2 learner and the native speaker data into
the appropriate directory, we move on to the next processing step,
right? Not so fast! Imagine we are working on a project with a
collaborator. How will they know where the data came from? What if we
need to come back to this data in the future? How will we know what
characteristics we used to filter the data? The directory and filenames
may not be enough. To address these questions we need to document the
origin of the data, and in the case of data acquired through manual
downloads, we need to document the procedures we took to acquire the
data to the best of our ability.

As discussed in Section~\ref{sec-ud-data-origin}, all acquired data
should be accompanied by a data origin file. The majority of this
information can typically be identified on the resource's website and/
or the resource's documentation. In the case of the CEDEL2 corpus, the
corpus homepage provides most of the information we need.

Structurally, data documentation files should be stored close to the
data they describe. So for our data origin file this means adding it to
the \emph{data/original/} directory. Naming the file in a transparent
way is also important. I've named the file \emph{cedel2\_do.csv} to
reflect the name of the corpus, the meaning of the file as data origin
with a suffixed *\_do\emph{, and the file extension }.csv* to reflect
the file format. CSV files reflect tabular content. It is not required
that data origin files are tabular, but it makes it easier to read and
display them in literate programming documents.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{hand-point-up} Tip}

There are many ways to create and edit CSV files. You can use a
spreadsheet program like MS Excel or Google Sheets, a text editor like
Notepad or TextEdit, or a code editor like RStudio or VS Code. The
\texttt{qtalrkit} package provides a convenient function,
\texttt{create\_data\_origin()} to create a CSV file with the data
origin boilerplate structure. This CSV file then can be edited to add
the relevant information in any of the above mentioned programs.

Using a spreadsheet program is the easiest method for editing tabular
data. The key is to save the file as a CSV file, and not as an Excel
file, to maintain our adherence to the principle of using open formats
for reproducible research.

\end{tcolorbox}

In Table~\ref{tbl-ad-cedel2-do}, I've created a data origin file for the
CEDEL2 corpus.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7500}}@{}}

\caption{\label{tbl-ad-cedel2-do}Data origin file for the CEDEL2 corpus}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
attribute
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Resource name & CEDEL2: Corpus Escrito del Español como L2. \\
Data source & http://cedel2.learnercorpora.com/,
https://doi.org/10.1177/02676583211050522 \\
Data sampling frame & Corpus that contains samples of the language
produced from learners of Spanish as a second language. For comparative
purposes, it also contains a native control subcorpus of the language
produced by native speakers of Spanish from different varieties
(peninsular Spanish and all varieties of Latin American Spanish), so it
can be used as a native corpus in its own right. \\
Data collection date(s) & 2006-2020. \\
Data format & CSV file. Each row corresponds to a writing sample. Each
column is an attribute of the writing sample. \\
Data schema & A CSV file for L2 learners and a CSV file for native
speakers. \\
License & CC BY-NC-ND 3.0 ES \\
Attribution & Lozano, C. (2022). CEDEL2: Design, compilation and web
interface of an online corpus for L2 Spanish acquisition research.
Second Language Research, 38(4), 965-983.
https://doi.org/10.1177/02676583211050522. \\

\end{longtable}

Given this is a manual download we also need to document the procedure
used to retrieve the data in prose. The script in the \emph{code/}
directory that is typically used to acquire the data is not used to
programmatically retrieve data in this case. However, to keep things
predictable we will use this file to document the download procedure.
I've created a Quarto file named \emph{1\_acquire\_data.qmd} in the
\emph{code/} directory of my project.

A glimpse at the directory structure of the project at this point is
seen in Example~\ref{exm-ad-cedel2-structure}.

\begin{example}[]\protect\hypertarget{exm-ad-cedel2-structure}{}\label{exm-ad-cedel2-structure}

Project structure for the CEDEL2 corpus data acquisition

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{project/}
\ExtensionTok{├──}\NormalTok{ process/}
\ExtensionTok{│}\NormalTok{   ├── 1\_acquire\_data.qmd}
\ExtensionTok{│}\NormalTok{   └── ...}
\ExtensionTok{├──}\NormalTok{ data/}
\ExtensionTok{│}\NormalTok{   ├── analysis/}
\ExtensionTok{│}\NormalTok{   ├── derived/}
\ExtensionTok{│}\NormalTok{   └── original/}
\ExtensionTok{│}\NormalTok{       ├── cedel2\_do.csv}
\ExtensionTok{│}\NormalTok{       └── cedel2/}
\ExtensionTok{│}\NormalTok{           ├── cedel2{-}l1{-}english{-}learners.csv}
\ExtensionTok{│}\NormalTok{           └── cedel2{-}native{-}spanish{-}speakers.csv}
\ExtensionTok{├──}\NormalTok{ reports/}
\ExtensionTok{├──}\NormalTok{ DESCRIPTION}
\ExtensionTok{├──}\NormalTok{ Makefile}
\ExtensionTok{└──}\NormalTok{ README}
\end{Highlighting}
\end{Shaded}

\end{example}

Even though the \emph{1\_acquire\_data.qmd} file is not used to
programmatically retrieve the data, it is still a useful place to
document the download procedure. This includes the URL of the resource,
the search criteria used to filter the data, and the file format and
location of the data. It is also good to include and display your data
origin file in this file as a formatted table.

Manually downloading other resources will inevitably include unique
processes for obtaining the data, but in the end the data should be
archived in the project structure in the \emph{data/original/} directory
and documented in the appropriate places. Note that acquired data is
always treated as `read-only', meaning it is not modified in any way.
This gives us a fixed starting point for subsequent steps in the data
preparation process.

\subsection{Programmatic}\label{programmatic}

There are many resources that provide corpus data that is directly
accessible for which programmatic downloads can be applied. A
\textbf{programmatic download} is a download in which the process can be
automated through code. Thus, this is a reproducible process. The data
can be acquired by anyone with access to the necessary code.

In this case, and subsquent data acquisition procedures in this chapter,
we use the \emph{1\_acquire\_data.qmd} Quarto file to its full potential
intermingling prose, code, and code comments to execute and document the
download procedure.

To illustrate how this works to conduct a programmatic download, we will
work with the Switchboard Dialog Act Corpus (SWDA)
(\citeproc{ref-SWDA2008}{University of Colorado Boulder 2008}). The
version that we will use is found on the Linguistic Data Consortium
under the \href{https://catalog.ldc.upenn.edu/LDC97S62}{Switchboard-1
Release 2 Corpus}. The corpus and related documentation are linked on
the catalog page \url{https://catalog.ldc.upenn.edu/docs/LDC97S62/}.

From the documentation we learn that the corpus contains transcripts for
1155 5-minute two-way telephone conversations among English speakers for
all areas of the United States. The speakers were given a topic to
discuss and the conversations were recorded. The corpus metadata and
annotations for sociolinguistic and discourse features.

This corpus, as you can image, could support a wide range of interesting
reseach questions. Let's assume we are following research conducted by
Tottie (\citeproc{ref-Tottie2011}{2011}) to explore the use of filled
pauses such as ``um'' and ``uh'' and traditional sociolinguistic
variables such as sex, age, and education in spontaneous speech by
American English speakers.

With this goal in mind, let's get started writing the code to download
and organize the data in our project directory. First, we need to
identify the URL (Uniform Resource Locator) for the data that we want to
download. More often than not this file will be some type of compressed
archive file with an extension such as \emph{.zip} (Zipped file),
\emph{.tar} (Tarball file), or \emph{tar.gz} (Gzipped tarball file),
which is the case for the SWDA corpus. Compressed files make downloading
multiple files easy by grouping files and directories into one file.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

You may be wondering what the difference betwen \emph{.zip},
\emph{.tar}, and \emph{.tar.gz} files are. The \emph{.zip} file format
is the most common. It groups file and directories into one file (an
archive) and compresses it to reduce the size of the file in one step
when the file is created.

The \emph{.tar} file format is used archive files and folders, it does
not perform compression. Gzipping peforms the compression to the
\emph{.tar} file resulting in a file with the \emph{.tar.gz} extension.
Notably the \emph{.gz} compression is highly efficient for large files.
Take the \emph{swda.tar.gz} file for example. It has a compressed file
size of 4.6 MB, but when uncompressed it is 16.9 MB. This is a 73\%
reduction in file size.

\end{tcolorbox}

In R, we can use the \texttt{download.file()} function from base R. The
\texttt{download.file()} function minimally requires two arguments:
\texttt{url} and \texttt{destfile}. These correspond to the file to
download and the location where it is to be saved to disk. To break out
the process a bit, I will assign the URL and destination file path to
variables and then use the \texttt{download.file()} function to download
the file.

\begin{example}[]\protect\hypertarget{exm-ad-swda-download-file}{}\label{exm-ad-swda-download-file}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# URL to SWDA corpus compressed file}
\NormalTok{file\_url }\OtherTok{\textless{}{-}}
  \StringTok{"https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1\_dialogact\_annot.tar.gz"}

\CommentTok{\# Relative path to project/data/original directory}
\NormalTok{file\_path }\OtherTok{\textless{}{-}} \StringTok{"../data/original/swda.tar.gz"}

\CommentTok{\# Download SWDA corpus compressed file}
\FunctionTok{download.file}\NormalTok{(}\AttributeTok{url =}\NormalTok{ file\_url, }\AttributeTok{destfile =}\NormalTok{ file\_path)}
\end{Highlighting}
\end{Shaded}

\end{example}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{exclamation-triangle} Warning}

Note that the \texttt{file\_path} variable in
Example~\ref{exm-ad-swda-download-file} is a relative path to the
\emph{data/original/} directory. This is because the
\emph{1\_acquire\_data.qmd} file that we are using for this code is
located in the \emph{code/} directory and the \emph{data/} directory is
a sibling directory to the \emph{code/} directory.

It is also possible to use an absolute path to the \emph{data/original/}
directory. I will have more to say about the advantages and
disadvantages of relative and absolute paths in reproducible research in
Chapter~\ref{sec-contributing}.

\end{tcolorbox}

As we can see looking at the directory structure, in
Example~\ref{exm-ad-swda-download-location}, the \emph{swda.tar.zip}
file has been added to the \emph{data/original/} directory.

\begin{example}[]\protect\hypertarget{exm-ad-swda-download-location}{}\label{exm-ad-swda-download-location}

Downloaded SWDA corpus compressed file

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ analysis/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{└──}\NormalTok{ swda.tar.zip}
\end{Highlighting}
\end{Shaded}

\end{example}

Once an compressed file is downloaded, however, the file needs to be
`decompressed' to reveal the directory structure and files. To
decompress this \emph{.tar.gz} file we use the \texttt{untar()} function
with the arguments \texttt{tarfile} pointing to the \emph{.tar.gz} file
and \texttt{exdir} specifying the directory where we want the files to
be extracted to. Again, I will assign the arguments to variables. Then
we can decompress the file using the \texttt{untar()} function.

\begin{example}[]\protect\hypertarget{exm-ad-swda-decompress-file}{}\label{exm-ad-swda-decompress-file}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Relative path to the compressed file}
\NormalTok{tar\_file }\OtherTok{\textless{}{-}} \StringTok{"../data/original/swda.tar.gz"}

\CommentTok{\# Relative path to the directory to extract to}
\NormalTok{extract\_to\_dir }\OtherTok{\textless{}{-}} \StringTok{"../data/original/swda/"}

\CommentTok{\# Decompress .zip file and extract to our target directory}
\FunctionTok{untar}\NormalTok{(tar\_file, extract\_to\_dir)}
\end{Highlighting}
\end{Shaded}

\end{example}

The directory structure of \emph{data/} in
Example~\ref{exm-ad-swda-decompress-location} now shows the
\emph{swda.tar.gz} file and the \emph{swda} directory that contains the
decompressed directories and files.

\begin{example}[]\protect\hypertarget{exm-ad-swda-decompress-location}{}\label{exm-ad-swda-decompress-location}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ analysis/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{├──}\NormalTok{ swda/}
    \ExtensionTok{│}\NormalTok{   ├── README}
    \ExtensionTok{│}\NormalTok{   ├── doc/}
    \ExtensionTok{│}\NormalTok{   ├── sw00utt/}
    \ExtensionTok{│}\NormalTok{   ├── sw01utt/}
    \ExtensionTok{│}\NormalTok{   ├── sw02utt/}
    \ExtensionTok{│}\NormalTok{   ├── sw03utt/}
    \ExtensionTok{│}\NormalTok{   ├── sw04utt/}
    \ExtensionTok{│}\NormalTok{   ├── sw05utt/}
    \ExtensionTok{│}\NormalTok{   ├── sw06utt/}
    \ExtensionTok{│}\NormalTok{   ├── sw07utt/}
    \ExtensionTok{│}\NormalTok{   ├── sw08utt/}
    \ExtensionTok{│}\NormalTok{   ├── sw09utt/}
    \ExtensionTok{│}\NormalTok{   ├── sw10utt/}
    \ExtensionTok{│}\NormalTok{   ├── sw11utt/}
    \ExtensionTok{│}\NormalTok{   ├── sw12utt/}
    \ExtensionTok{│}\NormalTok{   └── sw13utt/}
    \ExtensionTok{└──}\NormalTok{ swda.tar.gz}
\end{Highlighting}
\end{Shaded}

\end{example}

At this point we have acquired the data programmatically and with this
code as part of our workflow anyone could run this code and reproduce
the same results.

The code as it is, however, is not ideally efficient. First, the
\emph{swda.tar.gz} file is not strictly needed after we decompress it
and it occupies disk space, if we keep it. And second, each time we run
this code the file will be downloaded from the remote server and
overwrite the existing data. This leads to unnecessary data transfer and
server traffic and will overwrite the data if it already exists in our
project directory which could be problematic if the data changes on the
remote server. Let's tackle each of these issues in turn.

To avoid writing the \emph{swda.tar.gz} file to disk (long-term) we can
use the \texttt{tempfile()} function to open a temporary holding space
for the file in the computing environment. This space can then be used
to store the file, decompress it, and then the temporary file will
automatically be deleted. We assign the temporary space to an R object
we will name \texttt{temp\_file} with the \texttt{tempfile()} function.
This object can now be used as the value of the argument
\texttt{destfile} in the \texttt{download.file()} function.

\begin{example}[]\protect\hypertarget{exm-ad-swda-temp-file}{}\label{exm-ad-swda-temp-file}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# URL to SWDA corpus compressed file}
\NormalTok{file\_url }\OtherTok{\textless{}{-}}
  \StringTok{"https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1\_dialogact\_annot.tar.gz"}

\CommentTok{\# Create a temporary file space for our .tar.gz file}
\NormalTok{temp\_file }\OtherTok{\textless{}{-}} \FunctionTok{tempfile}\NormalTok{()}

\CommentTok{\# Download SWDA corpus compressed file}
\FunctionTok{download.file}\NormalTok{(file\_url, temp\_file)}
\end{Highlighting}
\end{Shaded}

\end{example}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{hand-point-up} Tip}

In Example~\ref{exm-ad-swda-temp-file}, I've used the values stored in
the objects \texttt{file\_url} and \texttt{temp\_file} in the
\texttt{download.file()} function without specifying the argument names
--only providing the names of the objects. R will assume that values of
a function map to the ordering of the arguments. If your values do not
map to ordering of the arguments you are required to specify the
argument name and the value. To view the ordering of objects hit
\texttt{tab} after entering the function name or consult the function
documentation by prefixing the function name with \texttt{?} and hitting
\texttt{enter}.

\end{tcolorbox}

At this point our downloaded file is stored temporarily on disk and can
be accessed and decompressed to our target directory using
\emph{temp\_file} as the value for the argument \texttt{tarfile} from
the \texttt{untar()} function. I've assigned our target directory path
to \texttt{extract\_to\_dir} and used it as the value for the argument
\texttt{exdir}.

\begin{example}[]\protect\hypertarget{exm-ad-swda-untar-temp}{}\label{exm-ad-swda-untar-temp}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Assign our target directory to \textasciigrave{}extract\_to\_dir\textasciigrave{}}
\NormalTok{extract\_to\_dir }\OtherTok{\textless{}{-}} \StringTok{"../data/original/swda/"}

\CommentTok{\# Decompress .tar.gz file and extract to our target directory}
\FunctionTok{untar}\NormalTok{(}\AttributeTok{tarfile =}\NormalTok{ temp\_file, }\AttributeTok{exdir =}\NormalTok{ target\_dir)}
\end{Highlighting}
\end{Shaded}

\end{example}

Our directory structure in Example~\ref{exm-ad-swda-untar-temp} is the
same as in Example~\ref{exm-ad-swda-decompress-location}, minus the
\emph{swda.tar.gz} file.

The second issue I raised concerns the fact that running this code as
part of our project will repeat the download each time our script is
run. Since we would like to be good citizens and avoid unnecessary
traffic on the web and avoid potential issues in overwriting data, it
would be nice if our code checked to see if we already have the data on
disk and if it exists, then skip the download, if not then download it.

The desired functionality we've described can be implemented using the
\texttt{if()} function. The \texttt{if()} function is one of a class of
functions known as control statements. \textbf{Control statments} allow
us to control the flow of our code by evaluating logical statements and
processing subsequent code based on the logical value it is passed as an
argument.

So in this case we want to evaluate whether the data directory exists on
disk. If it does then skip the download, if not, proceed with the
download. In combination with \texttt{else} which provides the `if not'
part of the statement, we have the following logical flow in
Example~\ref{exm-ad-if-dir-exists}.

\begin{example}[]\protect\hypertarget{exm-ad-if-dir-exists}{}\label{exm-ad-if-dir-exists}

~

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (DIRECTORY\_EXISTS) \{}
  \CommentTok{\# Do nothing}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
  \CommentTok{\# Download data}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

We can simplify this statement by using the \texttt{!} operator which
negates the logical value of the statement it precedes. So if the
directory exists, \texttt{!DIRECTORY\_EXISTS} will return \texttt{FALSE}
and if the directory does not exist, \texttt{!DIRECTORY\_EXISTS} will
return \texttt{TRUE}. In other words, if the directory does not exist,
download the data. This is shown in
Example~\ref{exm-ad-if-dir-exists-simplified}.

\begin{example}[]\protect\hypertarget{exm-ad-if-dir-exists-simplified}{}\label{exm-ad-if-dir-exists-simplified}

~

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\NormalTok{DIRECTORY\_EXISTS) \{}
  \CommentTok{\# Download data}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

Now, to determine if a directory exists in our project directory we will
turn to the \texttt{fs} package (\citeproc{ref-R-fs}{Hester, Wickham,
and Csárdi 2023}). The \texttt{fs} package provides a set of functions
for interacting with the file system, including \texttt{dir\_exists()}.
\texttt{dir\_exists()} takes a path to a directory as an argument and
returns the logical value, \texttt{TRUE}, if that directory exists, and
\texttt{FALSE} if it does not.

We can use this function to evaluate whether the directory exists and
then use the \texttt{if()} function to process the subsequent code based
on the logical flow we set out in
Example~\ref{exm-ad-if-dir-exists-simplified}. Applied to our project,
the code will look like Example~\ref{exm-ad-swda-if-dir-exists}.

\begin{example}[]\protect\hypertarget{exm-ad-swda-if-dir-exists}{}\label{exm-ad-swda-if-dir-exists}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the \textasciigrave{}fs\textasciigrave{} package}
\FunctionTok{library}\NormalTok{(fs)}

\CommentTok{\# URL to SWDA corpus compressed file}
\NormalTok{file\_url }\OtherTok{\textless{}{-}}
  \StringTok{"https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1\_dialogact\_annot.tar.gz"}

\CommentTok{\# Create a temporary file space for our .tar.gz file}
\NormalTok{temp\_file }\OtherTok{\textless{}{-}} \FunctionTok{tempfile}\NormalTok{()}

\CommentTok{\# Assign our target directory to \textasciigrave{}extract\_to\_dir\textasciigrave{}}
\NormalTok{extract\_to\_dir }\OtherTok{\textless{}{-}} \StringTok{"../data/original/swda/"}

\CommentTok{\# Check if our target directory exists}
\CommentTok{\# If it does not exist, download the file and extract it}
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{dir\_exists}\NormalTok{(extract\_to\_dir)) \{}
  \CommentTok{\# Download SWDA corpus compressed file}
  \FunctionTok{download.file}\NormalTok{(file\_url, temp\_file)}

  \CommentTok{\# Decompress .tar.gz file and extract to our target directory}
  \FunctionTok{untar}\NormalTok{(}\AttributeTok{tarfile =}\NormalTok{ temp\_file, }\AttributeTok{exdir =}\NormalTok{ extract\_to\_dir)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

The code in Example~\ref{exm-ad-swda-if-dir-exists} is added to the
\emph{1\_acquire\_data.qmd} file. When this file is run, the SWDA corpus
data will be downloaded and extracted to our project directory. If the
data already exists, the download will be skipped, just as we wanted.

Now, before we move on, we need to make sure to document the process.
Now that our Quarto document includes code we can review, explain, and
comment this process. And, as always, create a data origin file as with
the relevant information. The data origin file will be stored in the
\emph{data/original/} directory and the Quarto file will be stored in
the \emph{code/} directory.

We've leveraged R to automate the download and extraction of the data,
depending on the existence of the data in our project directory. But you
may be asking yourself, ``Can't I just navigate to the corpus page and
download the data manually myself?'' The simple answer is, ``Yes, you
can.'' The more nuanced answer is, ``Yes, but consider the trade-offs.''

The following scenarios highlight the some advantages to automating the
process. If you are acquiring data from multiple files, it can become
tedious to document the manual process for each file such that it is
reproducible. It's possible, but it's error prone.

Now, if you are collaborating with others, you will want to share this
data with them. It is very common to find data that has limited
restrictions for use in academic projects, but the most common
limitation is redistribution. This means that you can use the data for
your own research, but you cannot share it with others. If you plan on
publishing your project to a code repository to share the data as part
of your reproducible project, you would be violating the terms of use
for the data. By including the programmatic download in your project,
you can ensure that your collaborators can easily and effectively
acquire the data themselves and that you are not violating the terms of
use.

\section{APIs}\label{sec-apis}

A convenient alternative method for acquiring data in R is through
package interfaces to web services. These interfaces are built using R
code to make connections with resources on the web through
\textbf{Application Programming Interfaces} (APIs). Websites such as
Project Gutenberg, Twitter, Reddit, and many others provide APIs to
allow access to their data under certain conditions, some more limiting
for data collection than others. Programmers (like you!) in the R
community take up the task of wrapping calls to an API with R code to
make accessing that data from R convenient, and of course reproducible.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

Many, many web services provide API access. These APIs span all kinds of
data, from text to images to video to audio. Visit the
\href{https://publicapis.io/}{Public APIs website} to explore the
diversity of APIs available.

ROpenSci maintains a curated list of R packages that provide access to
data from web services. Visit the
\href{https://ropensci.org/packages/data-access/}{ROpenSci website} to
explore the packages available.

\end{tcolorbox}

In addition to popular public APIs, there are also APIs that provide
access to repositories and databases which are of particular interest to
linguists. For example, \href{http://wordbank.stanford.edu/}{Wordbank}
provides access to a large collection of child language corpora through
the \texttt{wordbankr} package (\citeproc{ref-R-wordbankr}{Braginsky
2023}), and \href{https://glottolog.org/}{Glottolog},
\href{https://wals.info/}{World Atlas of Language Structures} (WALS),
and \href{https://phoible.org/}{PHOIBLE} provide access to large
collections of language metadata that can be accessed through the
\texttt{lingtypology} package (\citeproc{ref-R-lingtypology}{Moroz
2024}).

Let's work with an R package that provides access to the
\href{https://talkbank.org/}{TalkBank} database. The TalkBank project
(\citeproc{ref-Macwhinney2003}{Macwhinney 2003}) contains a large
collection of spoken language corpora from various contexts:
conversation, child language, multilinguals, \emph{etc}. Resource
information, web interfaces, and links to download data in various
formats can be found by perusing individual resources linked from the
main page. However, the \texttt{TBDBr} package
(\citeproc{ref-R-TBDBr}{Kowalski and Cavanaugh 2022}) provides
convenient access to corpora using R once a corpus resource is
identified.

The CABNC (\citeproc{ref-Albert2015}{Albert, de Ruiter, and de Ruiter
2015}) contains the
\href{http://www.natcorp.ox.ac.uk/docs/URG/BNCdes.html\#body.1_div.1_div.5_div.1}{demographically
sampled portion} of the spoken portion of the British National Corpus
(BNC) (\citeproc{ref-Leech1992}{Leech 1992}).

Useful for a study aiming to research spoken British English, either in
isoloation or in comparison to American English (SWDA).

First, we need to install and load the \texttt{TBDBr} package.
Example~\ref{exm-ad-load-pacman}.

\begin{example}[]\protect\hypertarget{exm-ad-load-pacman}{}\label{exm-ad-load-pacman}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the TBDBr package}
\FunctionTok{library}\NormalTok{(TBDBr)}
\end{Highlighting}
\end{Shaded}

\end{example}

The \texttt{TBDBr} package provides a set of common \texttt{get*()}
functions for acquiring data from the TalkBank corpus resources. These
include:

\begin{itemize}
\tightlist
\item
  \texttt{getParticipants()}
\item
  \texttt{getTranscripts()}
\item
  \texttt{getTokens()}
\item
  \texttt{getTokenTypes()}
\item
  \texttt{getUtterances()}
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{hand-point-up} Tip} List functions and arguments

For any package loaded in your R session, you can list all of its
functions and datasets using the \texttt{ls()} function. For example,
\texttt{ls("package:TBDBr")} will list all of the functions and datasets
in the \texttt{TBDBr} package.

To view all of the arguments for a function, use the \texttt{args()}
function. For example, \texttt{args(getUtterances)} will list all of the
arguments for the \texttt{getUtterances()} function.

\end{tcolorbox}

For each of these function the first argument is \texttt{corpusName},
which is the name of the corpus resource as it appears in the TalkBank
database. The second argument is \texttt{corpora}, which takes a
character vector describing the path to the data. For the CABNC, these
arguments are \texttt{"ca"} and \texttt{c("ca",\ "CABNC")} respectively.
To determine these values, TBDBr provides the \texttt{getLegalValues()}
interactive function which allows you to interactively select the
repository name, corpus name, and transcript name (if necessary).

Another important aspect of these function is that they return data
frame objects. Since we are accessing data that is in a structured
database, this makes sense. However, we should always check the
documentation for the object type that is returned by function to be
aware of how to work with the data.

Let's start by retrieving the utterance data for the CABNC and preview
the data frame it returns using \texttt{glimpse()}.

\begin{example}[]\protect\hypertarget{exm-ad-get-utterances}{}\label{exm-ad-get-utterances}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set corpus\_name and corpus\_path}
\NormalTok{corpus\_name }\OtherTok{\textless{}{-}} \StringTok{"ca"}
\NormalTok{corpus\_path }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ca"}\NormalTok{, }\StringTok{"CABNC"}\NormalTok{)}

\CommentTok{\# Get utterance data}
\NormalTok{utterances }\OtherTok{\textless{}{-}}
  \FunctionTok{getUtterances}\NormalTok{(}
    \AttributeTok{corpusName =}\NormalTok{ corpus\_name,}
    \AttributeTok{corpora =}\NormalTok{ corpus\_path}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] "Fetching data, please wait..."
> [1] "Success!"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview the data}
\FunctionTok{glimpse}\NormalTok{(utterances)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 235,901
> Columns: 10
> $ filename  <list> "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE000",~
> $ path      <list> "ca/CABNC/KB0/KB0RE000", "ca/CABNC/KB0/KB0RE000", "ca/CABNC~
> $ utt_num   <list> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1~
> $ who       <list> "PS002", "PS006", "PS002", "PS006", "PS002", "PS006", "PS00~
> $ role      <list> "Unidentified", "Unidentified", "Unidentified", "Unidentifi~
> $ postcodes <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NU~
> $ gems      <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NU~
> $ utterance <list> "You enjoyed yourself in America", "Eh", "did you", "Oh I c~
> $ startTime <list> "0.208", "2.656", "2.896", "3.328", "5.088", "6.208", "8.32~
> $ endTime   <list> "2.672", "2.896", "3.328", "5.264", "6.016", "8.496", "9.31~
\end{verbatim}

\end{example}

Inspecting the output from Example~\ref{exm-ad-get-utterances}, we see
that the data frame contains 235,901 observations and 10 variables.

The summary provided by \texttt{glimpse()} also provides other useful
information. First, we see the data type of each variable.
Interestingly, the data type for each variable in the data frame is a
list object. Being that a list is two-dimensional data type, like a data
frame, we have two-dimensional data inside two-dimensional data. This is
known as a \textbf{nested structure}. We will work with nested
structures in more depth later, but for now it will suffice to say that
we would like to `unnest' these lists and reveal the list-contained
vector types at the data frame level.

To do this we will pass the \texttt{utterances} data frame to the,
appropriately named, \texttt{unnest()} function from the \texttt{tidyr}
package (\citeproc{ref-R-tidyr}{Wickham, Vaughan, and Girlich 2024}).
\texttt{unnest()} takes a data frame and a vector of variable names to
unnest, \texttt{cols\ =\ c()}. To unnest all variables, we will use the
\texttt{everything()} function from \texttt{dplyr} to select all
variables at once. We will use the result to overwrite the
\texttt{utterances} object with the unnested data frame.

\begin{example}[]\protect\hypertarget{exm-ad-unnest}{}\label{exm-ad-unnest}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Unnest the data frame}
\NormalTok{utterances }\OtherTok{\textless{}{-}}
\NormalTok{  utterances }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{everything}\NormalTok{())}

\CommentTok{\# Preview the data}
\FunctionTok{glimpse}\NormalTok{(utterances)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 235,901
> Columns: 10
> $ filename  <chr> "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE000", ~
> $ path      <chr> "ca/CABNC/KB0/KB0RE000", "ca/CABNC/KB0/KB0RE000", "ca/CABNC/~
> $ utt_num   <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17~
> $ who       <chr> "PS002", "PS006", "PS002", "PS006", "PS002", "PS006", "PS002~
> $ role      <chr> "Unidentified", "Unidentified", "Unidentified", "Unidentifie~
> $ postcodes <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
> $ gems      <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
> $ utterance <chr> "You enjoyed yourself in America", "Eh", "did you", "Oh I co~
> $ startTime <chr> "0.208", "2.656", "2.896", "3.328", "5.088", "6.208", "8.32"~
> $ endTime   <chr> "2.672", "2.896", "3.328", "5.264", "6.016", "8.496", "9.312~
\end{verbatim}

\end{example}

The output from Example~\ref{exm-ad-unnest} shows that the variables are
now one-dimensional vector types.

Returning to the information about our data frame from
\texttt{glimpse()}, the second thing to notice is we get a short preview
of the values for each variable. There are a couple things we can gleen
from this. One is that we can confirm or clarify the meaning of the
variable names by looking at the values. The other thing to consider is
whether the values show any patterns that may be worthy of more
scrutiny. For example, various variables appear to contain the same
values for each observation. For a variable like \texttt{filename}, this
is expected as the first values likely correspond to the same file.
However, for the variables \texttt{postcodes} and \texttt{gems} the
values are `NA'. This suggests that these variables may not contain any
useful information and we may want to remove them later.

For now, however, we want to acquire and store the data in its original
form (or as closely as possible). So now, we have acquired the
utterances data and have it in our R session as a data frame. To store
this data in a file, we will first need to consider the file format.
Data frames are tabular, so that gives us a few options.

Since we are working in R, we could store this data as an R object, in
the form of an RDS file. An RDS file is a binary file that can be read
back into R as an R object. This is a good option if we want to store
the data for use in R, but not if we want to share the data with others
or use it in other software. Another option is to store the data as a
spreadsheet file, such as XSLX (MS Excel). This may make viewing and
editing the contents more convenient, but it depends on the software
available to you and others. A third, more viable option, is to store
the data as a CSV file. CSV files are plain text files that can be read
and written by most software. This makes CSV files one of the most
popular for sharing tablular data. For this reason, we will store the
data as a CSV file.

The \texttt{readr} package provides the \texttt{write\_csv()} function
for writing data frames to CSV files. The first argument is the data
frame to write, and the second argument is the path to the file to
write. Note, however, that the directories in the path we specify need
to exist. If they do not, we will get an error.

In this case, I would like to write the file \emph{utterances.csv} to
the \emph{../data/original/cabnc/} directory. The original project
structure does not contain a \emph{cabnc/} directory, so I need to
create one. To do this, I will use \texttt{dir\_create()} from the
\texttt{fs} package.

\begin{example}[]\protect\hypertarget{exm-ad-write-csv}{}\label{exm-ad-write-csv}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create the target directory}
\FunctionTok{dir\_create}\NormalTok{(}\StringTok{"../data/original/cabnc/"}\NormalTok{)}

\CommentTok{\# Write the data frame to a CSV file}
\FunctionTok{write\_csv}\NormalTok{(utterances, }\StringTok{"../data/original/cabnc/utterances.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

Chaining the steps covered in Examples \ref{exm-ad-get-utterances},
\ref{exm-ad-unnest}, and \ref{exm-ad-write-csv}, we have a succinct and
legible code to acquire, adjust, and write utterances from the CABNC in
Example~\ref{exm-ad-get-unnest-write-csv}.

\begin{example}[]\protect\hypertarget{exm-ad-get-unnest-write-csv}{}\label{exm-ad-get-unnest-write-csv}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set corpus\_name and corpus\_path}
\NormalTok{corpus\_name }\OtherTok{\textless{}{-}} \StringTok{"ca"}
\NormalTok{corpus\_path }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ca"}\NormalTok{, }\StringTok{"CABNC"}\NormalTok{)}

\CommentTok{\# Create the target directory}
\FunctionTok{dir\_create}\NormalTok{(}\StringTok{"../data/original/cabnc/"}\NormalTok{)}

\CommentTok{\# Get utterance data}
\FunctionTok{getUtterances}\NormalTok{(}
  \AttributeTok{corpusName =}\NormalTok{ corpus\_name,}
  \AttributeTok{corpora =}\NormalTok{ corpus\_path}
\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{write\_csv}\NormalTok{(}\StringTok{"../data/original/cabnc/utterances.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

If our goal is just to acquire utterances, then we are done acquiring
data and we move on to the next step. However, if we want to acquire
other datasets from the CABNC, say participants, tokens, \emph{etc.},
then we can either repeat the steps in
Example~\ref{exm-ad-get-unnest-write-csv} for each data type, or we can
write a function to do this for us!

A function serves us to make our code more legible and reusable for the
CABNC, and since the TalkBank data is structured similarly across
corpora, we can also use the function to acquire data from other
corpora, if need be.

To write a function, we need to consider the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the name of the function?
\item
  What arguments does the function take?
\item
  What functionality does the function provide?
\item
  Does the function have optional arguments?
\item
  How does the function return the results?
\end{enumerate}

Taking each in turn, the name of the function should be descriptive of
what the function does. In this case, we are acquiring and writing data
from Talkbank corpora. A possible name is
\texttt{get\_talkbank\_data()}. The required arguments of the the
\texttt{get*()} functions will definitely figure in our function. In
addition, we will need to specify the path to the directory to write the
data. With these considerations, we can write the function signature in
Example~\ref{exm-ad-get-talkbank-data-1}.

\begin{example}[]\protect\hypertarget{exm-ad-get-talkbank-data-1}{}\label{exm-ad-get-talkbank-data-1}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_talkbank\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(corpus\_name, corpus\_path, target\_dir) \{}
  \CommentTok{\# ...}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

The next thing to consider is what functionality the function provides.
In this case, we want to acquire and write data from Talkbank corpora.
We can start by leveraging the code steps in
Example~\ref{exm-ad-get-unnest-write-csv}, making some adjustments to
the code replacing the hard-coded values with the function arguments and
adding code to create the target file name based on the
\texttt{target\_dir} argument.

\begin{example}[]\protect\hypertarget{exm-ad-get-talkbank-data-2}{}\label{exm-ad-get-talkbank-data-2}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_talkbank\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(corpus\_name, corpus\_path, target\_dir) \{}

  \CommentTok{\# Create the target directory}
  \FunctionTok{dir\_create}\NormalTok{(target\_dir)}

  \CommentTok{\# Set up file path name}
\NormalTok{  utterances\_file  }\OtherTok{\textless{}{-}} \FunctionTok{path}\NormalTok{(target\_dir, }\StringTok{"utterances.csv"}\NormalTok{)}

  \CommentTok{\# Acquire data and write to file}
  \FunctionTok{getUtterances}\NormalTok{(}\AttributeTok{corpusName =}\NormalTok{ corpus\_name, }\AttributeTok{corpora =}\NormalTok{ corpus\_path) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{write\_csv}\NormalTok{(utterances\_file)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

Before we address the obvious feature missing, which is the fact that
this function in Example~\ref{exm-ad-get-talkbank-data-2} only acquires
and writes data for utterances, let's consider some functionality which
would make this function more user-friendly.

What if the data is already acquired? Do we want to overwrite it, or
should the function skip the process for files that already exist? By
skipping the process, we can save time and computing resources. If the
files are periodically updated, then we might want to overwrite existing
files.

To achieve this functionality we will use an \texttt{if()} statement to
check if the file exists. If it does, then we will skip the process. If
it does not, then we will acquire and write the data.

\begin{example}[]\protect\hypertarget{exm-ad-get-talkbank-data-3}{}\label{exm-ad-get-talkbank-data-3}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_talkbank\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(corpus\_name, corpus\_path, target\_dir) \{}

  \CommentTok{\# Create the target directory}
  \FunctionTok{dir\_create}\NormalTok{(target\_dir)}

  \CommentTok{\# Set up file path name}
\NormalTok{  utterances\_file  }\OtherTok{\textless{}{-}} \FunctionTok{path}\NormalTok{(target\_dir, }\StringTok{"utterances.csv"}\NormalTok{)}

  \CommentTok{\# If the file does not exist, then...}
  \CommentTok{\# Acquire data and write to file}
  \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file\_exists}\NormalTok{(utterances\_file)) \{}
    \FunctionTok{getUtterances}\NormalTok{(}\AttributeTok{corpusName =}\NormalTok{ corpus\_name, }\AttributeTok{corpora =}\NormalTok{ corpus\_path) }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{write\_csv}\NormalTok{(utterances\_file)}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

We can also add functionality to
Example~\ref{exm-ad-get-talkbank-data-3} to force overwrite existing
files, if need be. To do this, we will add an optional argument to the
function, \texttt{force}, which will be a logical value. We will set the
default to \texttt{force\ =\ FALSE} to preserve the existing
functionality. If \texttt{force\ =\ TRUE}, then we will overwrite
existing files. Then we add another condition to the \texttt{if()}
statement to check if \texttt{force\ =\ TRUE}. If it is, then we will
overwrite existing files.

\begin{example}[]\protect\hypertarget{exm-ad-get-talkbank-data-4}{}\label{exm-ad-get-talkbank-data-4}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_talkbank\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(corpus\_name, corpus\_path, target\_dir, }\AttributeTok{force =} \ConstantTok{FALSE}\NormalTok{) \{}

  \CommentTok{\# Create the target directory}
  \FunctionTok{dir\_create}\NormalTok{(target\_dir)}

  \CommentTok{\# Set up file path name}
\NormalTok{  utterances\_file  }\OtherTok{\textless{}{-}} \FunctionTok{path}\NormalTok{(target\_dir, }\StringTok{"utterances.csv"}\NormalTok{)}

  \CommentTok{\# If the file does not exist, then...}
  \CommentTok{\# Acquire data and write to file}
  \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file\_exists}\NormalTok{(utterances\_file) }\SpecialCharTok{|}\NormalTok{ force) \{}
    \FunctionTok{getUtterances}\NormalTok{(}\AttributeTok{corpusName =}\NormalTok{ corpus\_name, }\AttributeTok{corpora =}\NormalTok{ corpus\_path) }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{write\_csv}\NormalTok{(utterances\_file)}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

From this point, we add the functionality to acquire and write the other
data available from Talkbank corpora, such as participants, tokens,
\emph{etc.} This involves adding additional file path names and
\texttt{if()} statements to check if the files exist surrounding the
processing steps to Example~\ref{exm-ad-get-talkbank-data-4}. It may be
helpful to perform other input checks, print messages, \emph{etc.} for
functions that we plan to share with others. I will leave these
enhancements as an exercise for the reader.

Before we leave the topic of functions, let's consider where to put
functions after we write them. Here are a few options:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In the same script as the code that uses the function.
\item
  In a separate script, such as \emph{functions.R}.
\item
  In a package, which is loaded by the script that uses the function.
\end{enumerate}

The general heuristic for choosing where to put functions is to put them
in the same script as the code that uses them if the function is only
used in that script. If the function is used in multiple scripts or the
function or number of functions clutters the readability of the code,
then put it in a separate script. If the function is used in multiple
projects, then put it in an R package.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

If you are interested in learning more about writing functions, check
out the \href{https://r4ds.had.co.nz/functions.html}{Writing Functions
chapter} in the \href{https://r4ds.had.co.nz/}{R for Data Science} book.

If you find yourself writing functions that are useful for multiple
projects, you may want to consider creating an R package. R packages are
a great way to share your code with others. If you are interested in
learning more about creating R packages, check out the
\href{https://r-pkgs.org/}{R Packages book} by Hadley Wickham and Jenny
Bryan.

\end{tcolorbox}

In this case, we will put the function in a separate file,
\emph{functions.R}, in the same directory as the other code files as in
Example~\ref{exm-ad-functions-r}.

\begin{example}[]\protect\hypertarget{exm-ad-functions-r}{}\label{exm-ad-functions-r}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{code/}
\ExtensionTok{│}\NormalTok{   ├── 1\_acquire\_data.qmd}
\ExtensionTok{│}\NormalTok{   ├── ...}
\ExtensionTok{│}\NormalTok{   └── functions.R}
\end{Highlighting}
\end{Shaded}

\end{example}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{hand-point-up} Tip}

Note that that the \emph{functions.R} file is an R script, not a Quarto
document. Therefore code blocks that are used in \emph{.qmd} files are
not used, only the R code and code comments.

\end{tcolorbox}

To include this, or other functions in in the R session of the code file
that uses them, use the \texttt{source()} function, as seen in
Example~\ref{exm-ad-source-functions}.

\begin{example}[]\protect\hypertarget{exm-ad-source-functions}{}\label{exm-ad-source-functions}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Source functions}
\FunctionTok{source}\NormalTok{(}\StringTok{"functions.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

It is common to source functions at the top of the code file as part of
the package setup.

Given the utility of this function to my projects and potentially
others', I've included the \texttt{get\_talkbank\_data()} function in
the \texttt{qtalrkit} package. You can view the source code by calling
the function without parentheses \texttt{()}, or on the
\texttt{qtalrkit} GitHub repository.

After running the \texttt{get\_talkbank\_data()} function, we can see
that the data has been acquired and written to the
\emph{data/original/cabnc/} directory.

\begin{example}[]\protect\hypertarget{exm-ad-functions-r}{}\label{exm-ad-functions-r}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ analysis}
\ExtensionTok{├──}\NormalTok{ derived}
\ExtensionTok{└──}\NormalTok{ original}
    \ExtensionTok{└──}\NormalTok{ cabnc}
        \ExtensionTok{├──}\NormalTok{ participants.csv}
        \ExtensionTok{├──}\NormalTok{ token\_types.csv}
        \ExtensionTok{├──}\NormalTok{ tokens.csv}
        \ExtensionTok{├──}\NormalTok{ transcripts.csv}
        \ExtensionTok{└──}\NormalTok{ utterances.csv}
\end{Highlighting}
\end{Shaded}

\end{example}

Add comments to your code in \emph{1-acquire-data.qmd} and create and
complete the data origin documentation file for this resource, and the
acquisition is complete.

\section*{Activities}\label{activities-3}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

Building on the activities in the previous chapter, these activities
will focus on the implementation of the data acquisition process. Key
programming concepts include writing custom functions, control
statements, and applying functions iteratively will be covered in
addition to packages and functions which provide access to data from the
web.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-5.html}{Collecting
and documenting data}\\
\textbf{How}: Read Recipe 5, complete comprehension check, and prepare
for Lab 5.\\
\textbf{Why}: To refine programming strategies introduced in the lesson
for controlling program flow and making code more reusable in the
service of programmatically acquiring and documenting data.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-05}{Harvesting
research data}\\
\textbf{How}: Fork, clone, and complete the steps in Lab 5.\\
\textbf{Why}: To investigate data sources, plan data collection
strategies, and apply skills and knowledge to use R to collect and
document data.

\end{tcolorbox}

\section*{Summary}\label{summary-4}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter, we have covered a lot of ground. On the surface we have
discussed a few methods for acquiring corpus data for use in text
analysis. In the process we have delved into various aspects of the R
programming language. Some key concepts include writing control
statements and custom functions. We have also considered topics that are
more general in nature and concern interacting with data found on the
internet.

Each of these methods should be approached in a way that is transparent
to the researcher and to would-be collaborators and the general research
community. For this reason the documentation of the steps taken to
acquire data are key both in the code and in human-facing documentation.

At this point you have both a bird's eye view of the data available on
the web and strategies on how to access a great majority of it. It is
now time to turn to the next step in our data analysis project: data
curation. In the next chapter, I will cover how to wrangle your raw data
into a tidy dataset.

\chapter{Curate}\label{sec-curate-datasets}

\begin{quote}
The hardest bit of information to extract is the first piece.

--- Robert Ferrigno
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Describe the importance of data curation in text analysis
\item
  Recognize the different types of data formats
\item
  Associate the types data formats with the appropriate R programming
  techniques to curate the data
\end{itemize}

\end{tcolorbox}

In this chapter, we will now look at the next step in a text analysis
project: data curation. That is, the process of converting the original
data we acquire to a tidy dataset. Acquired data can come in a wide
variety of formats. These formats tend to signal the richness of the
metadata that is included in the file content. We will consider three
general types of content formats: (1) unstructured data, (2) structured
data, and (3) semi-structured data. Regardless of the file type and the
structure of the data, it will be necessary to consider how to curate a
dataset that such that the structure reflects the basic the unit of
analysis that we wish to investigate. The resulting dataset will form
the base from which we will work to further transform the dataset such
that it aligns with the unit(s) of observation required for the analysis
method that we will implement. Once the dataset is curated, we will
create a data dictionary that describes the dataset and the variables
that are included in the dataset for transparency and reproducibility.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{terminal} Lessons}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Pattern Matching,
Tidy Datasets}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To familiarize yourself with the basics of using the
pattern matching syntax Regular Expressions and the \texttt{dplyr}
package to manipulate data into Tidy datasets.

\end{tcolorbox}

\section{Unstructured}\label{unstructured}

The bulk of text ever created is of the unstructured variety.
Unstructured data is data that has not been organized to make the
information contained within machine-readable. Remember that text in
itself is not information. Only when given explicit context does text
become informative. The explicit contextual information that is included
with data is called metadata. Metadata can be linguistic or
non-linguistic in nature. So for unstructured data there is little to no
metadata directly associated with the data. This information needs to be
added or derived for the purposes of the research, either through manual
inspection or (semi-)automatic processes. For now, however, our job is
just to get the unstructured data into a structured format with as much
metadata as we can derive from the resource.

\subsection{Reading data}\label{reading-data}

Some of the common file formats which contain unstructured data include
TXT, PDF, and DOCX. Although these formats are unstructured, they are
not all the same. Reading these files into R requires different
techniques and tools.

Many times TXT files have the \emph{.txt} extension, but it is not
required. There are many ways to read TXT files into R and many packages
that can be used to do so. For example, using the \texttt{readr}
package, we can choose to read the entire file into a single vector of
character strings with \texttt{read\_file()} or read the file by lines
with \texttt{read\_lines()} in which each line is a character string in
a vector.

Less commonly used in prepared data resources, PDF and DOCX files are
more complex than TXT files as they contain formatting and embedded
metadata. However, these attributes are primarily for visual
presentation and not for machine-readability. Needless to say, we need
an alternate strategy to extract the text content from these files and
potentially some of the metadata. For example, using the
\texttt{readtext} package (\citeproc{ref-R-readtext}{Benoit and Obeng
2023}), we can read the text content from PDF and DOCX files into a
single vector of character strings with \texttt{readtext()}.

Whether in TXT, PDF, or DOCX format, the resulting data structure will
require further processing to convert the data into a tidy dataset. For
example, we may need to split the data into multiple columns or rows, or
extract metadata from the text content.

\subsection{Orientation}\label{orientation}

As an example of curating an unstructured source of corpus data, let's
take a look at the \href{https://www.statmt.org/europarl/}{Europarl
Parallel Corpus} (\citeproc{ref-Koehn2005}{Koehn 2005}). This corpus
contains parallel texts (source and translated documents) from the
European Parliamentary proceedings between 1996-2011 for some 21
European languages.

Let's assume we selected this corpus because we are interested in
researching Spanish to English translations. After consulting the corpus
website, downloading the compressed file, and inspecting the
decompressed structure, we have the the file structure seen in
Example~\ref{exm-cd-europarl-file-structure}.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-file-structure}{}\label{exm-cd-europarl-file-structure}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{project/}
\ExtensionTok{├──}\NormalTok{ process/}
\ExtensionTok{│}\NormalTok{   ├── 1{-}acquire{-}data.qmd}
\ExtensionTok{│}\NormalTok{   ├── 2{-}curate{-}data.qmd}
\ExtensionTok{│}\NormalTok{   └── ...}
\ExtensionTok{├──}\NormalTok{ data/}
\ExtensionTok{│}\NormalTok{   ├── analysis/}
\ExtensionTok{│}\NormalTok{   ├── derived/}
\ExtensionTok{│}\NormalTok{   └── original/}
\ExtensionTok{│}\NormalTok{       │── europarl\_do.csv}
\ExtensionTok{│}\NormalTok{       └── europarl/}
\ExtensionTok{│}\NormalTok{           ├── europarl{-}v7.es{-}en.en}
\ExtensionTok{│}\NormalTok{           └── europarl{-}v7.es{-}en.es}
\ExtensionTok{├──}\NormalTok{ reports/}
\ExtensionTok{├──}\NormalTok{ DESCRIPTION}
\ExtensionTok{├──}\NormalTok{ Makefile}
\ExtensionTok{└──}\NormalTok{ README}
\end{Highlighting}
\end{Shaded}

\end{example}

The \emph{europarl\_do.csv} file contains the data origin information
documented as part of the acquisition process. The contents are seen in
Table~\ref{tbl-cd-europarl-data-origin}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1688}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8312}}@{}}

\caption{\label{tbl-cd-europarl-data-origin}Data origin: Europarl
Corpus}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
attribute
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Resource name & Europarl Parallel Corpus \\
Data source & \url{https://www.statmt.org/europarl/} \\
Data sampling frame & Spanish transcripts from the European Parliament
proceedings \\
Data collection date(s) & 1996-2011 \\
Data format & TXT files with `.es' for source (Spanish) and `.en' for
target (English) files. \\
Data schema & Line-by-line unannotated parallel text \\
License & See: \url{https://www.europarl.europa.eu/legal-notice/en/} \\
Attribution & Please cite the paper: Koehn, P. 2005. `Europarl: A
Parallel Corpus for Statistical Machine Translation.' MT Summit X,
12-16. \\

\end{longtable}

Now let's get familiar with the corpus directory structure and the
files. In Example~\ref{exm-cd-europarl-file-structure}, we see that
there are two corpus files, \emph{europarl-v7.es-en.es} and
\emph{europarl-v7.es-en.en}, that contain the source and target language
texts, respectively. The file names indicate that the files contain
Spanish-English parallel texts. The \emph{.es} and \emph{.en} extensions
indicate the language of the text.

Looking at the beginning of the \emph{.es} and \emph{.en} files, in
File~\ref{lst-cd-europarl-es} and File~\ref{lst-cd-europarl-en}, we see
that the files contain a series of lines in either the source or target
language.

\begin{codelisting}

\caption{\label{lst-cd-europarl-es}Spanish source text}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Reanudación del período de sesiones}
\NormalTok{Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.}
\NormalTok{Como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.}
\NormalTok{Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.}
\NormalTok{A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\begin{codelisting}

\caption{\label{lst-cd-europarl-en}English target text}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Resumption of the session}
\NormalTok{I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.}
\NormalTok{Although, as you will have seen, the dreaded \textquotesingle{}millennium bug\textquotesingle{} failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.}
\NormalTok{You have requested a debate on this subject in the course of the next few days, during this part{-}session.}
\NormalTok{In the meantime, I should like to observe a minute\textquotesingle{} s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

We can clearly appreciate that the data is unstructured. That is, there
is no explicit metadata associated with the data. The data is just a
series of character strings separated by lines. The only information
that we can surmise from structure of the data is that the texts are
line-aligned and that the data in each file corresponds to source and
target languages.

Now, before embarking on a data curation process, it is recommendable to
define the structure of the data that we want to create. I call this the
``idealized structure'' of the data. For a curated dataset, we want to
maintain the original structure of the data as much as possible to
maintain the integrity of the data and create a transparent and
reproducible dataset.

Given what we know about the data, we can define the idealized structure
of the data as seen in Table~\ref{tbl-cd-europarl-structure-example}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1700}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1900}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5400}}@{}}

\caption{\label{tbl-cd-europarl-structure-example}Idealized structure
for the curated Europarl Corpus datasets.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable\_type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
type & Document type & character & Contains the type of document, either
`Source' or `Target' \\
line & Line & character & Contains the text of each line in the
document \\

\end{longtable}

Our task now is to develop code that will read the original data and
render the idealized structure as a curated dataset for each corpus
file. We will then write the datasets to the \emph{data/derived/}
directory. The code we develop will be added to the
\emph{2-curate-data.qmd} file. And finally, the datasets will be
documented with a data dictionary file.

\subsection{Tidy the data}\label{tidy-the-data}

To create the idealized dataset structure in
Table~\ref{tbl-cd-europarl-structure-example}, lets's start by reading
the files by lines into R. We will use the \texttt{read\_lines()}
function from the \texttt{readr} package.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-readr}{}\label{exm-cd-europarl-readr}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(readr)}

\CommentTok{\# Read Europarl files .es and .en}
\NormalTok{europarl\_es\_chr }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_lines}\NormalTok{(}\StringTok{"../data/original/europarl{-}v7.es{-}en.es"}\NormalTok{)}

\NormalTok{europarl\_en\_chr }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_lines}\NormalTok{(}\StringTok{"../data/original/europarl{-}v7.es{-}en.en"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

Using the \texttt{read\_lines()} function, we read each line of the
files into a character vector. Since the Europarl corpus is a parallel
corpus, the lines in the source and target files are aligned. This means
that the first line in the source file corresponds to the first line in
the target file, the second line in the source file corresponds to the
second line in the target file, and so on. This alignment is important
for the analysis of parallel corpora, as it allows us to compare the
source and target texts line by line.

Let's inspect our character vectors to ensure that they are of the
length and appear to be structured as we expect. We can use the
\texttt{length()} function to get the number of lines in each file and
the \texttt{head()} function to preview the first few lines of each
file.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-inspect-chr}{}\label{exm-cd-europarl-inspect-chr}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Inspect Spanish character vector}
\FunctionTok{length}\NormalTok{(europarl\_es\_chr)}
\FunctionTok{head}\NormalTok{(europarl\_es\_chr, }\DecValTok{5}\NormalTok{)}

\CommentTok{\# Inspect English character vector}
\FunctionTok{length}\NormalTok{(europarl\_en\_chr)}
\FunctionTok{head}\NormalTok{(europarl\_en\_chr, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] 1965734
> [1] "Reanudación del período de sesiones"                                                                                                                                                                                                 
> [2] "Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones."                                      
> [3] "Como todos han podido comprobar, el gran \"efecto del año 2000\" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles."                    
> [4] "Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones."                                                                                                                
> [5] "A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados."
> [1] 1965734
> [1] "Resumption of the session"                                                                                                                                                                                                               
> [2] "I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period."                         
> [3] "Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful."                                         
> [4] "You have requested a debate on this subject in the course of the next few days, during this part-session."                                                                                                                               
> [5] "In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union."
\end{verbatim}

\end{example}

The output of Example~\ref{exm-cd-europarl-inspect-chr} shows that the
number of lines in each file is the same. This is good. If the number of
lines in each file was different, we would need to figure out why and
fix it. We also see that the content of the files is aligned as
expected.

Let's now create a dataset for each of the character vectors. We will
use the \texttt{tibble()} function from the \texttt{tibble} package to
create a data frame object with the character vectors as the
\texttt{line} column and add a \texttt{type} column with the value
`Source' for the Spanish file and `Target' for the English file. We will
assign the output two new objects \texttt{europarl\_source\_df} and
\texttt{europarl\_target\_df}, respectively, as seen in
Example~\ref{exm-cd-europarl-df}.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-df}{}\label{exm-cd-europarl-df}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create source data frame}
\NormalTok{europarl\_source\_df }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{type =} \StringTok{"Source"}\NormalTok{,}
    \AttributeTok{lines =}\NormalTok{ europarl\_es\_chr}
\NormalTok{  )}
\CommentTok{\# Create target data frame}
\NormalTok{europarl\_target\_df }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{type =} \StringTok{"Target"}\NormalTok{,}
    \AttributeTok{lines =}\NormalTok{ europarl\_en\_chr}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\end{example}

Inspecting these data frames with \texttt{glimpse()} in
Example~\ref{exm-cd-europarl-glimpse}, we can see if the data frames
have the structure we expect.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-glimpse}{}\label{exm-cd-europarl-glimpse}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview source}
\FunctionTok{glimpse}\NormalTok{(europarl\_source\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 1,965,734
> Columns: 2
> $ type  <chr> "Source", "Source", "Source", "Source", "Source", "Source", "Sou~
> $ lines <chr> "Reanudación del período de sesiones", "Declaro reanudado el per~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview target}
\FunctionTok{glimpse}\NormalTok{(europarl\_target\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 1,965,734
> Columns: 2
> $ type  <chr> "Target", "Target", "Target", "Target", "Target", "Target", "Tar~
> $ lines <chr> "Resumption of the session", "I declare resumed the session of t~
\end{verbatim}

\end{example}

We now have our \texttt{type} and \texttt{lines} columns and the
associated observations for our idealized dataset, in
Table~\ref{tbl-cd-europarl-structure-example}. We can now write these
datasets to the \emph{data/derived/} directory using
\texttt{write\_csv()} and create corresponding data dictionary files.

\section{Structured}\label{structured}

Structured data already reflects the physical and semantic structure of
a tidy dataset. This means that the data is already in a tabular format
and the relationships between columns and rows are already well-defined.
Therefore the heavy lifting of curating the data is already done. There
are two remaining questions, however, that need to be taken into
account. One, logistical question, is what file format the dataset is in
and how to read it into R. And the second, more research-based, is
whether the data may benefit from some additional curation and
documentation to make it more amenable to analysis and more
understandable to others.

\subsection{Reading datasets}\label{reading-datasets}

Let's consider some common formats for structured data, \emph{i.e.}
datasets, and how to read them into R. First, we will consider R-native
formats, such as package datasets and RDS files. Then will consider
non-native formats, such as relational databases and datasets produced
by other software. Finally, we will consider software agnostic formats,
such as CSV.

R and some R packages provide structured datasets that are available for
use directly within R. For example, the \texttt{languageR} package
(\citeproc{ref-R-languageR}{R. H. Baayen and Shafaei-Bajestan 2019})
provides the \texttt{dative} dataset, which is a dataset containing the
realization of the dative as NP or PP in the Switchboard corpus and the
Treebank Wall Street Journal collection. The \texttt{janeaustenr}
package (\citeproc{ref-R-janeaustenr}{Silge 2022}) provides the
\texttt{austen\_books} dataset, which is a dataset of Jane Austen's
novels. Package datasets are loaded into an R session using either the
\texttt{data()} function, if the package is loaded, or the \texttt{::}
operator, if the package is not loaded. For example,
\texttt{data(dative)} or \texttt{languageR::dative}.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

To explore the available datasets in a package, you can use the
\texttt{data(package\ =\ "package\_name")} function. For example,
\texttt{data(package\ =\ "languageR")} will list the datasets available
in the \texttt{languageR} package. You can also explore all the datasets
available in the loaded packages with the \texttt{data()} function using
no arguments. For example, \texttt{data()}.

\end{tcolorbox}

R also provides a native file format for storing R objects, the RDS
file. Any R object, including data frames, can be written from an R
session to disk by using the \texttt{write\_rds()} function from
\texttt{readr}. The \emph{.rds} files will be written to disk in a
binary format that is not human-readable, which is not ideal for
transparent data sharing. However, the files and the R objects can be
read back into an R session using the \texttt{read\_rds()} function with
all the attributes intact, such as vector types, factor levels,
\emph{etc.}.

R provides a suite of tools for importing data from non-native
structured sources such as databases and datasets from software such as
SPSS, SAS, and Stata. For instance, if you are working with data stored
in a relational database such as MySQL, PostgreSQL, or SQLite, you can
use the \texttt{DBI} package (\citeproc{ref-R-DBI}{R Special Interest
Group on Databases (R-SIG-DB), Wickham, and Müller 2024}) to connect to
the database and the \texttt{dbplyr} package
(\citeproc{ref-R-dbplyr}{Wickham, Girlich, and Ruiz 2023}) to query the
database using the SQL language. Files from SPSS (\emph{.sav}), SAS
(\emph{.sas7bdat}), and Stata (\emph{.dta}) can be read into R using the
\texttt{haven} package (\citeproc{ref-R-haven}{Wickham, Miller, and
Smith 2023}).

Software agnostic file formats include delimited files, such as CSV,
TSV, \emph{etc.}. These file formats lack the robust structural
attributes of the other formats, but balance this shortcoming by storing
structured data in more accessible, human-readable format. Delimited
files are plain text files which use a delimiter, such as a comma
(\texttt{,}), tab (\texttt{\textbackslash{}t}), or pipe
(\texttt{\textbar{}}), to separate the columns and rows. For example, a
CSV file is a delimited file where the columns and rows are separated by
commas, as seen in Example~\ref{exm-cd-csv-example}.

\begin{example}[]\protect\hypertarget{exm-cd-csv-example}{}\label{exm-cd-csv-example}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{column\_1,column\_2,column\_3}
\NormalTok{row 1 value 1,row 1 value 2,row 1 value 3}
\NormalTok{row 2 value 1,row 2 value 2,row 2 value 3}
\end{Highlighting}
\end{Shaded}

\end{example}

Given the accessibility of delimited files, they are a common format for
sharing structured data in reproducible research. It is not surprising,
then, that this is the format which we have chosen for the derived
datasets in this book.

\subsection{Orientation}\label{orientation-1}

With an understanding of the various structured formats, we can now turn
to considerations about how the original dataset is structured and how
that structure is to be used for a given research project. As an
example, we will work with the CABNC datasets acquired in
Chapter~\ref{sec-acquire-data}. The structure of the original dataset is
shown in Example~\ref{exm-cd-cabnc-structure}.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-structure}{}\label{exm-cd-cabnc-structure}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ analysis/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{├──}\NormalTok{ cabnc\_do.csv}
    \ExtensionTok{└──}\NormalTok{ cabnc/}
        \ExtensionTok{├──}\NormalTok{ participants.csv}
        \ExtensionTok{├──}\NormalTok{ token\_types.csv}
        \ExtensionTok{├──}\NormalTok{ tokens.csv}
        \ExtensionTok{├──}\NormalTok{ transcripts.csv}
        \ExtensionTok{└──}\NormalTok{ utterances.csv}
\end{Highlighting}
\end{Shaded}

\end{example}

In addition to other important information, the data origin file
\emph{cabnc\_do.csv} shown in Table~\ref{tbl-cd-cabnc-do} informs us the
the datasets are related by a common variable.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1294}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8706}}@{}}

\caption{\label{tbl-cd-cabnc-do}Data origin: CABNC datasets}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
attribute
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Resource name & CABNC. \\
Data source & \url{https://ca.talkbank.org/access/CABNC.html},
\url{doi:10.21415/T55Q5R} \\
Data sampling frame & Over 400 British English Speakers from across the
UK stratified age, gender, social group, and region, and recording their
language output over a set period of time. \\
Data collection date(s) & \begin{minipage}[t]{\linewidth}\raggedright
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1991}
\tightlist
\item
\end{enumerate}
\end{minipage} \\
Data format & CSV Files \\
Data schema & The recordings are linked by \texttt{filename} and the
participants are linked by \texttt{who}. \\
License & CC BY NC SA 3.0 \\
Attribution & Saul Albert, Laura E. de Ruiter, and J.P. de Ruiter (2015)
CABNC: the Jeffersonian transcription of the Spoken British National
Corpus. https://saulalbert.github.io/CABNC/. \\

\end{longtable}

The CABNC datasets are structured in a relational format, which means
that the data is stored in multiple tables that are related to each
other. The tables are related by a common column or set of columns,
which are called a keys. A key is used to join the tables together to
create a single dataset. There are two keys in the CABNC datasets,
\texttt{filename} and \texttt{who}. Each variable corresponds to
recording- and/ or participant-oriented datasets.

Now, let's envision a scenario in which we are preparing our data for a
study that aims to investigate the relationship between speaker
demographics and utterances. In their original format, the CABNC
datasets separate information about utterances and speakers in separate
datasets, \texttt{cabnc\_utterances} and \texttt{cabnc\_participants},
respectively. Ideally, we would like to curate these datasets such that
the information about the utterances and the speakers are ready to be
joined as part of the dataset transformation process, while still
retaining the relevant original structure. This usually involves
removing redundant and/ or uninformative variables and/ or adjusting
variable names and writing these datasets and their documentation files
to disk.

\subsection{Tidy the dataset}\label{tidy-the-dataset}

With these goals in mind, let's start the process of curation by reading
the relevant datasets into an R session. Since we are working with CSV
files will will use the \texttt{read\_csv()} function, as seen in
Example~\ref{exm-cd-cabnc-read}.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-read}{}\label{exm-cd-cabnc-read}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read the relevant datasets}
\NormalTok{cabnc\_utterances }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/cabnc/original/utterances.csv"}\NormalTok{)}
\NormalTok{cabnc\_participants }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/cabnc/original/participants.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

The next step is to inspect the structure of the datasets. We can use
the \texttt{glimpse()} function for this task.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-glimpse}{}\label{exm-cd-cabnc-glimpse}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview the structure of the datasets}
\FunctionTok{glimpse}\NormalTok{(cabnc\_utterances)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 235,901
> Columns: 10
> $ filename  <chr> "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE000", ~
> $ path      <chr> "ca/CABNC/KB0/KB0RE000", "ca/CABNC/KB0/KB0RE000", "ca/CABNC/~
> $ utt_num   <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17~
> $ who       <chr> "PS002", "PS006", "PS002", "PS006", "PS002", "PS006", "PS002~
> $ role      <chr> "Unidentified", "Unidentified", "Unidentified", "Unidentifie~
> $ postcodes <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
> $ gems      <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
> $ utterance <chr> "You enjoyed yourself in America", "Eh", "did you", "Oh I co~
> $ startTime <dbl> 0.208, 2.656, 2.896, 3.328, 5.088, 6.208, 8.320, 8.480, 10.2~
> $ endTime   <dbl> 2.67, 2.90, 3.33, 5.26, 6.02, 8.50, 9.31, 11.23, 14.34, 15.9~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(cabnc\_participants)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 6,190
> Columns: 13
> $ filename  <chr> "KB0RE004", "KB0RE004", "KB0RE004", "KB0RE006", "KB0RE006", ~
> $ path      <chr> "ca/CABNC/0missing/KB0RE004", "ca/CABNC/0missing/KB0RE004", ~
> $ who       <chr> "PS008", "PS009", "KB0PSUN", "PS007", "PS008", "PS009", "KB0~
> $ name      <chr> "John", "Gethyn", "Unknown_speaker", "Alan", "John", "Gethyn~
> $ role      <chr> "Unidentified", "Unidentified", "Unidentified", "Unidentifie~
> $ language  <chr> "eng", "eng", "eng", "eng", "eng", "eng", "eng", "eng", "eng~
> $ monthage  <dbl> 481, 481, 13, 949, 481, 481, 13, 637, 565, 13, 637, 565, 13,~
> $ age       <chr> "40;01.01", "40;01.01", "1;01.01", "79;01.01", "40;01.01", "~
> $ sex       <chr> "male", "male", "male", "male", "male", "male", "male", "mal~
> $ numwords  <dbl> 28, 360, 156, 1610, 791, 184, 294, 93, 3, 0, 128, 24, 0, 150~
> $ numutts   <dbl> 1, 9, 27, 7, 5, 7, 6, 5, 1, 0, 11, 6, 0, 110, 74, 96, 12, 1,~
> $ avgutt    <dbl> 28.00, 40.00, 5.78, 230.00, 158.20, 26.29, 49.00, 18.60, 3.0~
> $ medianutt <dbl> 28, 39, 5, 84, 64, 9, 3, 15, 3, 0, 9, 3, 0, 7, 6, 4, 3, 12, ~
\end{verbatim}

\end{example}

From visual inspection of the output of
Example~\ref{exm-cd-cabnc-glimpse} we can see that there are common
variables in both datasets. In particular, we see the \texttt{filename}
and \texttt{who} variables mentioned in the data origin file
\emph{cabnc\_do.csv}.

The next step is to consider the variables that will be useful for
future analysis. Since we are creating a curated dataset, the goal will
be to retain as much information as possible from the original datasets.
There are cases, however, in which there may be variables that are not
informative and thus, will not prove useful for any analysis. These
removable variables tend to be of one of two types: variables which show
no variation across observations and variables where the information is
redundant.

As an example case, let's look at the \texttt{cabnc\_participants} data
frame. We can use the \texttt{skim()} function from the \texttt{skimr}
package to get a summary of the variables in the dataset. We can add the
\texttt{yank()} function to look at variable types one at a time. We
will start with the character variables, as seen in
Example~\ref{exm-cd-cabnc-skim-character}.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-skim-character}{}\label{exm-cd-cabnc-skim-character}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(skimr)}

\CommentTok{\# Summarize character variables}
\NormalTok{cabnc\_participants }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{skim}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{yank}\NormalTok{(}\StringTok{"character"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> 
> -- Variable type: character ----------------------------------------------------
>   skim_variable n_missing complete_rate min max empty n_unique whitespace
> 1 filename              0             1   8   8     0     2020          0
> 2 path                  0             1  21  26     0     2020          0
> 3 who                   0             1   4   7     0      581          0
> 4 name                  0             1   3  25     0      269          0
> 5 role                  0             1  12  12     0        1          0
> 6 language              0             1   3   3     0        1          0
> 7 age                   0             1   7   8     0       83          0
> 8 sex                   0             1   4   6     0        2          0
\end{verbatim}

\end{example}

We see from the output in Example~\ref{exm-cd-cabnc-skim-character},
that the variables \texttt{role} and \texttt{language} have a single
unique value. This means that these variables do not show any variation
across observations. We will remove these variables from the dataset.

Continuing on, let's look for redundant variables. We see that the
variables \texttt{filename} and \texttt{path} have the same number
unique values. And if we combine this with the visual summary in
Example~\ref{exm-cd-cabnc-glimpse}, we can see that the \texttt{path}
variable is redundant. We will remove this variable from the dataset.

Another potentially redundant set of variables are \texttt{who} and
\texttt{name} --both of which are speaker identifiers. The \texttt{who}
variable is a unique identifier, but there may be some redundancy with
the \texttt{name} variable, that is there may be two speakers with the
same name. We can check this by looking at the number of unique values
in the \texttt{who} and \texttt{name} variables from the \texttt{skim()}
output in Example~\ref{exm-cd-cabnc-skim-character}. \texttt{who} has
568 unique values and \texttt{name} has 269 unique values. This suggests
that there are multiple speakers with the same name.

Another way to explore this is to look at the number of unique values in
the \texttt{who} variable for each unique value in the \texttt{name}
variable. We can do this using the \texttt{group\_by()} and
\texttt{summarize()} functions from the \texttt{dplyr} package. For each
value of \texttt{name}, we will count the number of unique values in
\texttt{who} and then sort the results in descending order.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-who-name}{}\label{exm-cd-cabnc-who-name}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cabnc\_participants }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(name) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{n =} \FunctionTok{unique}\NormalTok{(who) }\SpecialCharTok{|\textgreater{}} \FunctionTok{length}\NormalTok{()) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 269 x 2
>    name                          n
>    <chr>                     <int>
>  1 None                         59
>  2 Unknown_speaker              59
>  3 Group_of_unknown_speakers    21
>  4 Chris                         9
>  5 David                         9
>  6 Margaret                      8
>  7 Ann                           7
>  8 John                          7
>  9 Alan                          6
> 10 Jackie                        5
> # i 259 more rows
\end{verbatim}

\end{example}

It is good that we performed the check in
Example~\ref{exm-cd-cabnc-who-name} beforehand. In addition to speakers
with the same name, such as `Chris' and `David', we also have multiple
speakers with generic codes, such as `None' and `Unknown\_speaker'. It
is clear that \texttt{name} is redundant and we can safely remove it
from the dataset.

With this in mind, we can then safely remove the following variables
from the dataset: \texttt{role}, \texttt{language}, \texttt{name}, and
\texttt{path}. To drop variables from a data frame we can use the
\texttt{select()} function in combination with the \texttt{-} operator.
The \texttt{-} operator tells the \texttt{select()} function to drop the
variables that follow it.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-drop-vars}{}\label{exm-cd-cabnc-drop-vars}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Drop variables}
\NormalTok{cabnc\_participants }\OtherTok{\textless{}{-}}
\NormalTok{  cabnc\_participants }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{role, }\SpecialCharTok{{-}}\NormalTok{language, }\SpecialCharTok{{-}}\NormalTok{name, }\SpecialCharTok{{-}}\NormalTok{path)}

\CommentTok{\# Preview the dataset}
\FunctionTok{glimpse}\NormalTok{(cabnc\_participants)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 6,190
> Columns: 9
> $ filename  <chr> "KB0RE004", "KB0RE004", "KB0RE004", "KB0RE006", "KB0RE006", ~
> $ who       <chr> "PS008", "PS009", "KB0PSUN", "PS007", "PS008", "PS009", "KB0~
> $ monthage  <dbl> 481, 481, 13, 949, 481, 481, 13, 637, 565, 13, 637, 565, 13,~
> $ age       <chr> "40;01.01", "40;01.01", "1;01.01", "79;01.01", "40;01.01", "~
> $ sex       <chr> "male", "male", "male", "male", "male", "male", "male", "mal~
> $ numwords  <dbl> 28, 360, 156, 1610, 791, 184, 294, 93, 3, 0, 128, 24, 0, 150~
> $ numutts   <dbl> 1, 9, 27, 7, 5, 7, 6, 5, 1, 0, 11, 6, 0, 110, 74, 96, 12, 1,~
> $ avgutt    <dbl> 28.00, 40.00, 5.78, 230.00, 158.20, 26.29, 49.00, 18.60, 3.0~
> $ medianutt <dbl> 28, 39, 5, 84, 64, 9, 3, 15, 3, 0, 9, 3, 0, 7, 6, 4, 3, 12, ~
\end{verbatim}

\end{example}

Now we have a frame with 9 more informative variables which describe the
participants. We would then repeat this process for the
\texttt{cabnc\_utterances} dataset to remove redundant and uninformative
variables.

Another, optional step, is to rename and/ or organize the order the
variables to make the dataset more understandable. Let's organize the
columns to read left to right from most general to most specific. Again,
we turn to the \texttt{select()} function, this time including the
variables in the order we want them to appear in the dataset. We will
take this opportunity to rename some of the variable names so that they
are more informative.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-rename-vars}{}\label{exm-cd-cabnc-rename-vars}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Rename variables}
\NormalTok{cabnc\_participants }\OtherTok{\textless{}{-}}
\NormalTok{  cabnc\_participants }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}
    \AttributeTok{doc\_id =}\NormalTok{ filename,}
    \AttributeTok{part\_id =}\NormalTok{ who,}
    \AttributeTok{part\_age =}\NormalTok{ monthage,}
    \AttributeTok{part\_sex =}\NormalTok{ sex,}
    \AttributeTok{num\_words =}\NormalTok{ numwords,}
    \AttributeTok{num\_utts =}\NormalTok{ numutts,}
    \AttributeTok{avg\_utt\_len =}\NormalTok{ avgutt,}
    \AttributeTok{median\_utt\_len =}\NormalTok{ medianutt}
\NormalTok{  )}

\CommentTok{\# Preview the dataset}
\FunctionTok{glimpse}\NormalTok{(cabnc\_participants)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 6,190
> Columns: 8
> $ doc_id         <chr> "KB0RE004", "KB0RE004", "KB0RE004", "KB0RE006", "KB0RE0~
> $ part_id        <chr> "PS008", "PS009", "KB0PSUN", "PS007", "PS008", "PS009",~
> $ part_age       <dbl> 481, 481, 13, 949, 481, 481, 13, 637, 565, 13, 637, 565~
> $ part_sex       <chr> "male", "male", "male", "male", "male", "male", "male",~
> $ num_words      <dbl> 28, 360, 156, 1610, 791, 184, 294, 93, 3, 0, 128, 24, 0~
> $ num_utts       <dbl> 1, 9, 27, 7, 5, 7, 6, 5, 1, 0, 11, 6, 0, 110, 74, 96, 1~
> $ avg_utt_len    <dbl> 28.00, 40.00, 5.78, 230.00, 158.20, 26.29, 49.00, 18.60~
> $ median_utt_len <dbl> 28, 39, 5, 84, 64, 9, 3, 15, 3, 0, 9, 3, 0, 7, 6, 4, 3,~
\end{verbatim}

\end{example}

The variable order is organized after running
Example~\ref{exm-cd-cabnc-rename-vars}. Now let's sort the rows by
\texttt{doc\_id} and \texttt{part\_id} so that the dataset is sensibly
organized. The \texttt{arrange()} function takes a data frame and a list
of variables to sort by, in the order they are listed.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-sort-rows}{}\label{exm-cd-cabnc-sort-rows}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sort rows}
\NormalTok{cabnc\_participants }\OtherTok{\textless{}{-}}
\NormalTok{  cabnc\_participants }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(doc\_id, part\_id)}

\CommentTok{\# Preview the dataset}
\NormalTok{cabnc\_participants }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 8
>    doc_id   part_id part_age part_sex num_words num_utts avg_utt_len
>    <chr>    <chr>      <dbl> <chr>        <dbl>    <dbl>       <dbl>
>  1 KB0RE000 KB0PSUN       13 male             2        2        1   
>  2 KB0RE000 PS002        721 female         759       74       10.3 
>  3 KB0RE000 PS006        601 male           399       64        6.23
>  4 KB0RE001 KB0PSUN       13 male             7        3        2.33
>  5 KB0RE001 PS005        481 female         257       32        8.03
>  6 KB0RE001 PS007        949 male           284       29        9.79
>  7 KB0RE002 KB0PSUN       13 male             0        0        0   
>  8 KB0RE002 PS003        601 female         379       30       12.6 
>  9 KB0RE002 PS007        949 male            98       29        3.38
> 10 KB0RE003 KB0PSUN       13 male             0        0        0   
> # i 1 more variable: median_utt_len <dbl>
\end{verbatim}

\end{example}

Applying the sorting in Example~\ref{exm-cd-cabnc-sort-rows}, we can see
that the utterances are now our desired order, a dataset that reads left
to right from document to participant-oriented attributes and top to
bottom by document and participant.

\section{Semi-structured}\label{semi-structured}

Between unstructured and structured data falls semi-structured data. And
as the name suggests, it is a hybrid data format. This means that there
will be important structured metadata included with unstructured
elements. The file formats and approaches to encoding the structured
aspects of the data vary widely from resource to resource and therefore
often requires more detailed attention to the structure of the data and
often includes more sophisticated programming strategies to curate the
data to produce a tidy dataset.

\subsection{Reading data}\label{reading-data-1}

The file formats associated with semi-structured data include a wide
range. These include file formats conducive to more structured-leaning
data, such as XML, HTML, and JSON, and file formats with more
unstructured-leaning data, such as annotated TXT files. Annotated TXT
files may in fact appear with the \emph{.txt} extension, but may also
appear with other, sometimes resource-specific, extensions, such as
\emph{.utt} for the Switchboard Dialogue Act Corpus or \emph{.cha} for
the CHILDES corpus annotation files, for example.

The more structured file formats use standard conventions and therefore
can be read into an R session with format-specific functions. Say, for
example, we are working with data in a JSON file format. We can read the
data into an R session with the \texttt{read\_json()} function from the
\texttt{jsonlite} package (\citeproc{ref-R-jsonlite}{Ooms 2023}). For
XML and HTML files, the \texttt{rvest} package
(\citeproc{ref-R-rvest}{Wickham 2024}) provides the \texttt{read\_xml()}
and \texttt{read\_html()} functions.

Semi-structured data in TXT files can be read either as a file or by
lines. The choice of which approach to take depends on the structure of
the data. If the data structure is line-based, then
\texttt{read\_lines()} often makes more sense than
\texttt{read\_file()}. However, in some cases, the data may be
structured in a way that requires the entire file to be read into an R
session and then subsequently parsed.

\subsection{Orientation}\label{sec-cd-semi-structured-orientation}

To provide an example of the curation process using semi-structured
data, we will work with the ENNTT corpus. The ENNTT corpus contains
native and translated English drawn from European Parliament
proceedings. Let's look at the directory structure for the ENNTT corpus
in Example~\ref{exm-cd-enntt-structure}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-structure}{}\label{exm-cd-enntt-structure}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{├──}\NormalTok{ analysis/}
\ExtensionTok{├──}\NormalTok{ derived/}
\ExtensionTok{└──}\NormalTok{ original/}
    \ExtensionTok{├──}\NormalTok{ enntt\_do.csv}
    \ExtensionTok{└──}\NormalTok{ enntt/}
        \ExtensionTok{├──}\NormalTok{ natives.dat}
        \ExtensionTok{├──}\NormalTok{ natives.tok}
        \ExtensionTok{├──}\NormalTok{ nonnatives.dat}
        \ExtensionTok{├──}\NormalTok{ nonnatives.tok}
        \ExtensionTok{├──}\NormalTok{ translations.dat}
        \ExtensionTok{└──}\NormalTok{ translations.tok}
\end{Highlighting}
\end{Shaded}

\end{example}

We now inspect the data origin file for the ENNTT corpus,
\emph{enntt\_do.csv}, in Table~\ref{tbl-cd-enntt-do}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7500}}@{}}

\caption{\label{tbl-cd-enntt-do}Data origin file for the ENNTT corpus.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
attribute
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Resource name & Europarl corpus of Native, Non-native and Translated
Texts - ENNTT \\
Data source & https://github.com/senisioi/enntt-release \\
Data sampling frame & English, European Parliament texts, transcribed
discourse, political genre \\
Data collection date(s) & Not specified in the repository \\
Data format & .tok, .dat \\
Data schema & \emph{.tok files contain the actual text; }.dat files
contain the annotations corresponding to each line in the *.tok
files. \\
License & Not specified. Contact the authors for more information. \\
Attribution & Nisioi, S., Rabinovich, E., Dinu, L. P., \& Wintner, S.
(2016). A corpus of native, non-native and translated texts. Proceedings
of the Tenth International Conference on Language Resources and
Evaluation (LREC 2016). \\

\end{longtable}

According to the data origin file, there are two important file types,
\emph{.dat} and \emph{.tok}. The \emph{.dat} files contain annotations
and the \emph{.tok} files contain the actual text. Let's inspect the
first couple of lines in the \emph{.dat} file for the native speakers,
\emph{nonnatives.dat}, in File~\ref{lst-cd-enntt-nonnatives-dat}.

\begin{codelisting}

\caption{\label{lst-cd-enntt-nonnatives-dat}Example \emph{.dat} file for
the non-native speakers.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}}\KeywordTok{LINE}\OtherTok{ STATE=}\StringTok{"Poland"}\OtherTok{ MEPID=}\StringTok{"96779"}\OtherTok{ LANGUAGE=}\StringTok{"EN"}\OtherTok{ NAME=}\StringTok{"Danuta Hübner,"}\OtherTok{ SEQ\_SPEAKER\_ID=}\StringTok{"184"}\OtherTok{ SESSION\_ID=}\StringTok{"ep{-}05{-}11{-}17"}\NormalTok{/\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{LINE}\OtherTok{ STATE=}\StringTok{"Poland"}\OtherTok{ MEPID=}\StringTok{"96779"}\OtherTok{ LANGUAGE=}\StringTok{"EN"}\OtherTok{ NAME=}\StringTok{"Danuta Hübner,"}\OtherTok{ SEQ\_SPEAKER\_ID=}\StringTok{"184"}\OtherTok{ SESSION\_ID=}\StringTok{"ep{-}05{-}11{-}17"}\NormalTok{/\textgreater{}}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

We see that the \emph{.dat} file contains annotations for various
session and speaker attributes. The format of the annotations is
XML-like. XML is a form of markup language, such as YAML, JSON,
\emph{etc.} \textbf{Markup languages} are used to annotate text with
additional information about the structure, meaning, and/ or
presentation of text. In XML, structure is built up by nesting of nodes.
The nodes are named with tags, which are enclosed in angle brackets,
\texttt{\textless{}} and \texttt{\textgreater{}}. Nodes are opened with
\texttt{\textless{}TAG\textgreater{}} and closed with
\texttt{\textless{}/TAG\textgreater{}}. In Example~\ref{exm-cd-xml} we
see an example of a simple XML file structure.

\begin{example}[]\protect\hypertarget{exm-cd-xml}{}\label{exm-cd-xml}

~

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{\textless{}?xml}\OtherTok{ version=}\StringTok{"1.0"}\OtherTok{ encoding=}\StringTok{"UTF{-}8"}\FunctionTok{?\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{book}\OtherTok{ category=}\StringTok{"fiction"}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{title}\OtherTok{ lang=}\StringTok{"en"}\NormalTok{\textgreater{}The Catcher in the Rye\textless{}/}\KeywordTok{title}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{author}\NormalTok{\textgreater{}J.D. Salinger\textless{}/}\KeywordTok{author}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{year}\NormalTok{\textgreater{}1951\textless{}/}\KeywordTok{year}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{book}\NormalTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

\end{example}

In Example~\ref{exm-cd-xml} there are four nodes, three of which are
nested inside of the \texttt{\textless{}book\textgreater{}} node. The
\texttt{\textless{}book\textgreater{}} node in this example is the root
node. XML files require a root node. Nodes can also have attributes,
such as the \texttt{category} attribute in the
\texttt{\textless{}book\textgreater{}} node, but they are not required.
Furthermore, XML files also require a declaration, which is the first
line in Example~\ref{exm-cd-xml}. The declaration specifies the version
of XML used and the encoding.

So the \emph{.dat} file is not strict XML, but is similar in that it
contains nodes and attributes. An XML variant you a likely familiar
with, HTML, has more relaxed rules than XML. HTML is a markup language
used to annotate text with information about the organization and
presentation of text on the web that does not require a root node or a
declaration --much like our \emph{.dat} file. So suffice it to say that
the \emph{.dat} file can safely be treated as HTML.

And the \emph{.tok} file for the native speakers, \emph{nonnatives.tok},
in File~\ref{lst-cd-enntt-nonnatives-tok}, shows the actual text for
each line in the corpus.

\begin{codelisting}

\caption{\label{lst-cd-enntt-nonnatives-tok}Example \emph{.tok} file for
the non-native speakers.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{The Commission is following with interest the planned construction of a nuclear power plant in Akkuyu , Turkey and recognises the importance of ensuring that the construction of the new plant follows the highest internationally accepted nuclear safety standards .}
\NormalTok{According to our information , the decision on the selection of a bidder has not been taken yet .}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

In a study in which we are interested in contrasting the language of
natives and non-natives, we will want to combine the \emph{.dat} and
\emph{.tok} files for these groups of speakers.

The question is what attributes we want to include in the curated
dataset. Given the research focus, we will not need the
\texttt{LANGUAGE} or \texttt{NAME} attributes. We may want to modify the
attribute names so they are a bit more descriptive.

An idealized version of the curated dataset based on this criteria is
shown in Table~\ref{tbl-cd-enntt-ideal}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1700}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1900}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5400}}@{}}

\caption{\label{tbl-cd-enntt-ideal}Idealized structure for the curated
ENNTT Corpus datasets.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable\_name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable\_type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
session\_id & Session ID & character & Unique identifier for each
session. \\
speaker\_id & Speaker ID & integer & Unique identifier for each
speaker. \\
state & State & character & The political state of the speaker. \\
type & Type & character & Indicates whether the text is native or
non-native \\
session\_seq & Session Sequence & integer & The sequence of the text in
the session. \\
text & Text & character & Contains the text of the line, and maintains
the structure of the original data. \\

\end{longtable}

\subsection{Tidy the data}\label{tidy-the-data-1}

Now that we have a better understanding of the corpus data and our
target curated dataset structure, let's work to extract and organize the
data from the native and non-native files.

The general approach we will take is, for native and then non-natives,
to read in the \emph{.dat} file as an HTML file and then extract the
line nodes and their attributes combining them into a data frame. Then
we'll read in the \emph{.tok} file as a text file and then combine the
two into a single data frame.

Starting with the natives, we read in the \emph{.dat} file as an XML
file with the \texttt{read\_html()} function and then extract the line
nodes with the \texttt{html\_elements()} function as in
Example~\ref{exm-cd-enntt-read-xml}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-read-xml}{}\label{exm-cd-enntt-read-xml}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load packages}
\FunctionTok{library}\NormalTok{(rvest)}

\CommentTok{\# Read in *.dat* file as HTML}
\NormalTok{ns\_dat\_lines }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_html}\NormalTok{(}\StringTok{"../data/original/enntt/natives.dat"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"line"}\NormalTok{)}

\CommentTok{\# Inspect}
\FunctionTok{class}\NormalTok{(ns\_dat\_lines)}
\FunctionTok{typeof}\NormalTok{(ns\_dat\_lines)}
\FunctionTok{length}\NormalTok{(ns\_dat\_lines)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] "xml_nodeset"
> [1] "list"
> [1] 116341
\end{verbatim}

\end{example}

When can see that the \texttt{ns\_dat\_lines} object is a special type
of list, \texttt{xml\_nodeset} which contains 116,341 line nodes. Let's
now jump out of sequence and read in the \emph{.tok} file as a text
file, in Example~\ref{exm-cd-enntt-read-lines}, again by lines using
\texttt{read\_lines()}, and compare the two to make sure that our
approach will work.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-read-lines}{}\label{exm-cd-enntt-read-lines}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read in *.tok* file by lines}
\NormalTok{ns\_tok\_lines }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_lines}\NormalTok{(}\StringTok{"../data/enntt/original/natives.tok"}\NormalTok{)}

\CommentTok{\# Inspect}
\FunctionTok{class}\NormalTok{(ns\_tok\_lines)}
\FunctionTok{typeof}\NormalTok{(ns\_tok\_lines)}
\FunctionTok{length}\NormalTok{(ns\_tok\_lines)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] "character"
> [1] "character"
> [1] 116341
\end{verbatim}

\end{example}

We do, in fact, have the same number of lines in the \emph{.dat} and
\emph{.tok} files. So we can proceed with extracting the attributes from
the line nodes and combining them with the text from the \emph{.tok}
file.

Let's start by listing the attributes of the first line node in the
\texttt{ns\_dat\_lines} object. To do this we will draw on the
\texttt{pluck()} function from the \texttt{purrr} package
(\citeproc{ref-R-purrr}{Wickham and Henry 2023}) to extract the first
line node. Then we use the \texttt{html\_attrs()} function to get the
attribute names and the values, as in
Example~\ref{exm-cd-enntt-list-attributes}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-list-attributes}{}\label{exm-cd-enntt-list-attributes}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(purrr)}

\CommentTok{\# List attributes line node 1}
\NormalTok{ns\_dat\_lines }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pluck}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{html\_attrs}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>             state             mepid          language              name 
>  "United Kingdom"            "2099"              "EN" "Evans, Robert J" 
>    seq_speaker_id        session_id 
>               "2"     "ep-00-01-17"
\end{verbatim}

\end{example}

No surprise here, these are the same attributes we saw in the
\emph{.dat} file preview in File~\ref{lst-cd-enntt-nonnatives-dat}. At
this point, it's good to make a plan on how to associate the attribute
names with the column names in our curated dataset.

\begin{itemize}
\tightlist
\item
  \texttt{session\_id} = \texttt{session\_id}
\item
  \texttt{speaker\_id} = \texttt{MEPID}
\item
  \texttt{state} = \texttt{state}
\item
  \texttt{session\_seq} = \texttt{seq\_speaker\_id}
\end{itemize}

We can do this one attribute at a time using the \texttt{html\_attr()}
function and then combine them into a data frame with the
\texttt{tibble()} function as in
Example~\ref{exm-cd-enntt-extract-attributes}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-extract-attributes}{}\label{exm-cd-enntt-extract-attributes}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract attributes from first line node}
\NormalTok{session\_id }\OtherTok{\textless{}{-}}\NormalTok{ ns\_dat\_lines }\SpecialCharTok{|\textgreater{}} \FunctionTok{pluck}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"session\_id"}\NormalTok{)}
\NormalTok{speaker\_id }\OtherTok{\textless{}{-}}\NormalTok{ ns\_dat\_lines }\SpecialCharTok{|\textgreater{}} \FunctionTok{pluck}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"mepid"}\NormalTok{)}
\NormalTok{state }\OtherTok{\textless{}{-}}\NormalTok{ ns\_dat\_lines }\SpecialCharTok{|\textgreater{}} \FunctionTok{pluck}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"state"}\NormalTok{)}
\NormalTok{session\_seq }\OtherTok{\textless{}{-}}\NormalTok{ ns\_dat\_lines }\SpecialCharTok{|\textgreater{}} \FunctionTok{pluck}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"seq\_speaker\_id"}\NormalTok{)}

\CommentTok{\# Combine into data frame}
\FunctionTok{tibble}\NormalTok{(session\_id, speaker\_id, state, session\_seq)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 4
>   session_id  speaker_id state          session_seq
>   <chr>       <chr>      <chr>          <chr>      
> 1 ep-00-01-17 2099       United Kingdom 2
\end{verbatim}

\end{example}

The results from Example~\ref{exm-cd-enntt-extract-attributes} show that
the attributes have been extracted and mapped to our idealized column
names, but this would be tedious to do for each line node. A function to
extract attributes and values from a line and add them to a data frame
would help simplify this process. The function in
Example~\ref{exm-cd-enntt-extract-attributes-function} does just that.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-extract-attributes-function}{}\label{exm-cd-enntt-extract-attributes-function}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Function to extract attributes from line node}
\NormalTok{extract\_dat\_attrs }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(line\_node) \{}
\NormalTok{  session\_id }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"session\_id"}\NormalTok{)}
\NormalTok{  speaker\_id }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"mepid"}\NormalTok{)}
\NormalTok{  state }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"state"}\NormalTok{)}
\NormalTok{  session\_seq }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"seq\_speaker\_id"}\NormalTok{)}

  \FunctionTok{tibble}\NormalTok{(session\_id, speaker\_id, state, session\_seq)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

It's a good idea to test out the function to verify that it works as
expected. We can do this by passing the various indices to the
\texttt{ns\_dat\_lines} object to the function as in
Example~\ref{exm-cd-enntt-test-extract-attributes-function}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-test-extract-attributes-function}{}\label{exm-cd-enntt-test-extract-attributes-function}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Test function}
\NormalTok{ns\_dat\_lines }\SpecialCharTok{|\textgreater{}} \FunctionTok{pluck}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{extract\_dat\_attrs}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 4
>   session_id  speaker_id state          session_seq
>   <chr>       <chr>      <chr>          <chr>      
> 1 ep-00-01-17 2099       United Kingdom 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ns\_dat\_lines }\SpecialCharTok{|\textgreater{}} \FunctionTok{pluck}\NormalTok{(}\DecValTok{20}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{extract\_dat\_attrs}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 4
>   session_id  speaker_id state          session_seq
>   <chr>       <chr>      <chr>          <chr>      
> 1 ep-00-01-17 1309       United Kingdom 40
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ns\_dat\_lines }\SpecialCharTok{|\textgreater{}} \FunctionTok{pluck}\NormalTok{(}\DecValTok{100}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{extract\_dat\_attrs}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 4
>   session_id  speaker_id state          session_seq
>   <chr>       <chr>      <chr>          <chr>      
> 1 ep-00-01-18 4549       United Kingdom 28
\end{verbatim}

\end{example}

Looks like the \texttt{extract\_dat\_attrs()} function is ready for
prime-time. Let's now apply it to all of the line nodes in the
\texttt{ns\_dat\_lines} object using the \texttt{map\_dfr()} function
from the \texttt{purrr} package as in
Example~\ref{exm-cd-enntt-extract-attributes-all}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-extract-attributes-all}{}\label{exm-cd-enntt-extract-attributes-all}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract attributes from all line nodes}
\NormalTok{ns\_dat\_attrs }\OtherTok{\textless{}{-}}
\NormalTok{  ns\_dat\_lines }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{map\_dfr}\NormalTok{(extract\_dat\_attrs)}

\CommentTok{\# Inspect}
\FunctionTok{glimpse}\NormalTok{(ns\_dat\_attrs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 116,341
> Columns: 4
> $ session_id  <chr> "ep-00-01-17", "ep-00-01-17", "ep-00-01-17", "ep-00-01-17"~
> $ speaker_id  <chr> "2099", "2099", "2099", "4548", "4548", "4541", "4541", "4~
> $ state       <chr> "United Kingdom", "United Kingdom", "United Kingdom", "Uni~
> $ session_seq <chr> "2", "2", "2", "4", "4", "12", "12", "12", "12", "12", "12~
\end{verbatim}

\end{example}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

The \texttt{map*()} functions from the \texttt{purrr} package are a
family of functions that apply a function to each element of a vector,
list, or data frame. The \texttt{map\_dfr()} function is a variant of
the \texttt{map()} function that returns a data frame that is the result
of row-binding the results, hence \texttt{\_dfr}.

\end{tcolorbox}

We can see that the \texttt{ns\_dat\_attrs} object is a data frame with
116,341 rows and 4 columns, just has we expected. We can now combine the
\texttt{ns\_dat\_attrs} data frame with the \texttt{ns\_tok\_lines}
vector to create a single data frame with the attributes and the text.
This is done with the \texttt{mutate()} function assigning the
\texttt{ns\_tok\_lines} vector to a new column named \texttt{text} as in
Example~\ref{exm-cd-enntt-combine-attributes-text}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-combine-attributes-text}{}\label{exm-cd-enntt-combine-attributes-text}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Combine attributes and text}
\NormalTok{ns\_dat }\OtherTok{\textless{}{-}}
\NormalTok{  ns\_dat\_attrs }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{text =}\NormalTok{ ns\_tok\_lines)}

\CommentTok{\# Inspect}
\FunctionTok{glimpse}\NormalTok{(ns\_dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 116,341
> Columns: 5
> $ session_id  <chr> "ep-00-01-17", "ep-00-01-17", "ep-00-01-17", "ep-00-01-17"~
> $ speaker_id  <chr> "2099", "2099", "2099", "4548", "4548", "4541", "4541", "4~
> $ state       <chr> "United Kingdom", "United Kingdom", "United Kingdom", "Uni~
> $ session_seq <chr> "2", "2", "2", "4", "4", "12", "12", "12", "12", "12", "12~
> $ text        <chr> "You will be aware from the press and television that ther~
\end{verbatim}

\end{example}

This is the data for the native speakers. We can now repeat this process
for the non-native speakers, \emph{or} we can create a function to do it
for us. Let's explore the later option.

I will name this function \texttt{combine\_dat\_tok()} and it will take
two arguments: \texttt{dat\_file} and \texttt{tok\_file}. The
\texttt{dat\_file} argument will be the path to the \emph{.dat} file and
the \texttt{tok\_file} argument will be the path to the \emph{.tok}
file. The function first reads both the files into R as lines. Then it
extracts the attributes from the \emph{.dat} file using the
\texttt{extract\_dat\_attrs()} function we created earlier. Finally, it
combines the attributes with the text from the \emph{.tok} file and
returns a data frame.

Note that there is at least one tweak we need to make to our existing
code base to generate our idealized curated structure. We will need to
add a column \texttt{type} to which we will indicate what type of data
it is (native or non-native).

There are multiple ways to do this. We could add a column to the data
frame in the function before we return it, or we could add a column to
the data frame after we return it. I will opt for the former and embed
it inside of our function \texttt{combine\_dat\_tok()}, as seen in
Example~\ref{exm-cd-enntt-combine-dat-tok-function}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-combine-dat-tok-function}{}\label{exm-cd-enntt-combine-dat-tok-function}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load packages}
\FunctionTok{library}\NormalTok{(dplyr) }\CommentTok{\# for data manipulation}
\FunctionTok{library}\NormalTok{(rvest) }\CommentTok{\# for reading/parsing HTML}
\FunctionTok{library}\NormalTok{(purrr) }\CommentTok{\# for mapping functions}
\FunctionTok{library}\NormalTok{(fs) }\CommentTok{\# for working with files}

\CommentTok{\# Function to combine *.dat* and *.tok* files}
\NormalTok{combine\_dat\_tok }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(dat\_file, tok\_file) \{}
  \CommentTok{\# Read in files by lines}
\NormalTok{  dat }\OtherTok{\textless{}{-}}
    \FunctionTok{read\_html}\NormalTok{(dat\_file) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"line"}\NormalTok{)}
\NormalTok{  tok }\OtherTok{\textless{}{-}} \FunctionTok{read\_lines}\NormalTok{(tok\_file)}

  \CommentTok{\# Create function to extract attributes}
\NormalTok{  extract\_dat\_attrs }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(line\_node) \{}
\NormalTok{    session\_id }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"session\_id"}\NormalTok{)}
\NormalTok{    speaker\_id }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"mepid"}\NormalTok{)}
\NormalTok{    state }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"state"}\NormalTok{)}
\NormalTok{    session\_seq }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"seq\_speaker\_id"}\NormalTok{)}

    \FunctionTok{tibble}\NormalTok{(session\_id, speaker\_id, state, session\_seq)}
\NormalTok{  \}}

  \CommentTok{\# Apply function to all line nodes}
\NormalTok{  dat\_attrs }\OtherTok{\textless{}{-}}
\NormalTok{    dat }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{map\_dfr}\NormalTok{(extract\_dat\_attrs)}

  \CommentTok{\# Combine attrs and text into data frame}
\NormalTok{  dataset }\OtherTok{\textless{}{-}}
\NormalTok{    dat\_attrs }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{text =}\NormalTok{ tok)}

  \CommentTok{\# Add type column with value of file name (w/o extension)}
\NormalTok{  type\_name }\OtherTok{\textless{}{-}} \FunctionTok{path\_file}\NormalTok{(dat\_file) }\SpecialCharTok{|\textgreater{}} \FunctionTok{path\_ext\_remove}\NormalTok{()}

\NormalTok{  dataset }\OtherTok{\textless{}{-}}
\NormalTok{    dataset }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =}\NormalTok{ type\_name)}

  \CommentTok{\# Return data frame}
  \FunctionTok{return}\NormalTok{(dataset)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

Apply the \texttt{combine\_dat\_tok()} function to read in the data for
the native speakers and the non-native speakers in a few lines of code,
as in Example~\ref{exm-cd-enntt-combine-dat-tok}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-combine-dat-tok}{}\label{exm-cd-enntt-combine-dat-tok}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Native speakers}
\NormalTok{enntt\_ns\_df }\OtherTok{\textless{}{-}}
  \FunctionTok{combine\_dat\_tok}\NormalTok{(}
    \AttributeTok{dat\_file =} \StringTok{"../data/original/enntt/natives.dat"}\NormalTok{,}
    \AttributeTok{tok\_file =} \StringTok{"../data/original/enntt/natives.tok"}
\NormalTok{  )}
\CommentTok{\# Preview}
\FunctionTok{glimpse}\NormalTok{(enntt\_ns\_df)}

\CommentTok{\# Non{-}native speakers}
\NormalTok{enntt\_nns\_df }\OtherTok{\textless{}{-}}
  \FunctionTok{combine\_dat\_tok}\NormalTok{(}
    \AttributeTok{dat\_file =} \StringTok{"../data/original/enntt/nonnatives.dat"}\NormalTok{,}
    \AttributeTok{tok\_file =} \StringTok{"../data/original/enntt/nonnatives.tok"}
\NormalTok{  )}
\CommentTok{\# Preview}
\FunctionTok{glimpse}\NormalTok{(enntt\_nns\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 116,341
> Columns: 6
> $ session_id  <chr> "ep-00-01-17", "ep-00-01-17", "ep-00-01-17", "ep-00-01-17"~
> $ speaker_id  <chr> "2099", "2099", "2099", "4548", "4548", "4541", "4541", "4~
> $ state       <chr> "United Kingdom", "United Kingdom", "United Kingdom", "Uni~
> $ session_seq <chr> "2", "2", "2", "4", "4", "12", "12", "12", "12", "12", "12~
> $ text        <chr> "You will be aware from the press and television that ther~
> $ type        <chr> "natives", "natives", "natives", "natives", "natives", "na~
\end{verbatim}

\begin{verbatim}
> Rows: 29,734
> Columns: 6
> $ session_id  <chr> "ep-00-01-18", "ep-00-01-18", "ep-00-01-18", "ep-00-01-18"~
> $ speaker_id  <chr> "653", "653", "653", "653", "653", "653", "653", "653", "6~
> $ state       <chr> "Belgium", "Belgium", "Belgium", "Belgium", "Belgium", "Be~
> $ session_seq <chr> "157", "157", "157", "157", "157", "157", "157", "157", "1~
> $ text        <chr> "The Commission is following with interest the planned con~
> $ type        <chr> "nonnatives", "nonnatives", "nonnatives", "nonnatives", "n~
\end{verbatim}

\end{example}

We now have met the idealized structure for the curated ENNTT Corpus
datasets, as shown in Table~\ref{tbl-cd-enntt-ideal}. The
\texttt{enntt\_ns\_df} and \texttt{enntt\_nns\_df} data frames are ready
to be written to disk and documented.

\section{Documentation}\label{documentation}

After applying the curation steps to our data, we will now want to write
the dataset to disk and to do our best to document the process and the
resulting dataset.

Since data frames are a tabular, we will have various options for the
file type to write. Many of these formats are software-specific, such as
\texttt{*.xlsx} for Microsoft Excel, \texttt{*.sav} for SPSS,
\texttt{*.dta} for Stata, and \texttt{*.rds} for R. We will use the
\texttt{*.csv} format since it is a common format that can be read by
many software packages. We will use the \texttt{write\_csv()} function
from the \texttt{readr} package to write the dataset to disk.

Now the question is where to save our CSV file. Since our dataset is
derived by our work, we will added it to the \texttt{derived/}
directory. If you are working with multiple data sources within the same
project, it is a good idea to create a subdirectory for each dataset.
This will help keep the project organized and make it easier to find and
access the datasets.

The final step, as always, is to document the dataset. For datasets the
documentation is a data dictionary, as discussed in
Section~\ref{sec-ud-data-dictionaries}. As with data origin files, you
can use spreadsheet software to create and/ or edit the data dictionary.

In the \texttt{qtalrkit} package we have a function,
\texttt{create\_data\_dictionary()} that will generate the scaffolding
for a data dictionary. The function takes two arguments, \texttt{data}
and \texttt{file\_path}. It reads the dataset columns and provides a
template for the data dictionary.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

The \texttt{create\_data\_dictionary()} function provides a rudimentary
data dictionary template by default. However, you can take advantage of
OpenAI's text generation models to generate a more detailed data
dictionary for you to edit. To do this create
\href{https://platform.openai.com/signup}{an OpenAI account} and
\href{https://platform.openai.com/account/api-keys}{an API key} and add
this key to your R environment
(\texttt{Sys.setenv(OPENAI\_API\_KEY\ =\ "sk..."}). Then you can specify
the model you would like to use in the function with the
\texttt{model\ =} argument. For example,
\texttt{model\ =\ "gpt-3.5-turbo"} will use the GPT-3.5 Turbo model.

\end{tcolorbox}

An example of a data dictionary, a data dictionary for the
\texttt{enntt\_ns\_df} dataset is shown in
Table~\ref{tbl-cd-unstructured-data-dictionary-example}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.6000}}@{}}

\caption{\label{tbl-cd-unstructured-data-dictionary-example}Data
dictionary for the \texttt{enntt\_ns\_df} dataset.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable\_type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
session\_id & Session ID & categorical & Unique identifier for each
session \\
speaker\_id & Speaker ID & categorical & Unique identifier for each
speaker \\
state & State & categorical & Name of the state or country the session
is linked to \\
session\_seq & Session Sequence & ordinal & Sequence number in the
session \\
text & Text & categorical & Text transcript of the session \\
type & Type & categorical & The type of the speaker, whether native or
nonnative \\

\end{longtable}

\section*{Activities}\label{activities-4}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

The following activities build on your skills and knowledge to use R to
read, inspect, and write data and datasets in R. In these activities you
will have an opportunity to learn and apply your skills and knowledge to
the task of curating datasets. This is a vital component of text
analysis research that uses unstructured and semi-structured data.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-6.html}{Organizing
and documenting datasets}\\
\textbf{How}: Read Recipe 6, complete comprehension check, and prepare
for Lab 6.\\
\textbf{Why}: To rehearse methods for deriving tidying datasets to use a
the base for further project-specific purposes. We will explore how
regular expressions are helpful in developing strategies for matching,
extracting, and/ or replacing patterns in character sequences and how to
organize datasets in rows and columns. We will also explore how to
document datasets in a data dictionary.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-06}{Taming data}\\
\textbf{How}: Fork, clone, and complete the steps in Lab 6.\\
\textbf{Why}: To gain experience working with coding strategies to
manipulate data using tidyverse functions and regular expressions, to
practice reading/ writing data from/ to disk, and to implement
organizational strategies for organizing and documenting a dataset in
reproducible fashion.

\end{tcolorbox}

\section*{Summary}\label{summary-5}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we looked at the process of structuring data into a
dataset. This included a discussion on three main types of data
--unstructured, structured, and semi-structured. The level of structure
of the original data(set) will vary from resource to resource and by the
same token so will the file format used to support the level of metadata
included. The results from data curation results in a dataset that is
saved separate from the original data to maintain modularity between
what the data(set) looked like before we intervene and afterwards. Since
there can be multiple analysis approaches applied the original data in a
research project, this curated dataset serves as the point of departure
for each of the subsequent datasets derived from the transformational
steps. In addition to the code we use to derive the curated dataset's
structure, we also include a data dictionary which documents the curated
dataset.

\chapter{Transform}\label{sec-transform-datasets}

\begin{quote}
Nothing is lost. Everything is transformed.

--- Michael Ende, The Neverending Story
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Understand the role of data transformation in a text analysis project.
\item
  Identify the main types of transformations used to prepare datasets
  for analysis.
\item
  Recognize the importance of planning and documenting the
  transformation process.
\end{itemize}

\end{tcolorbox}

In this chapter, we will focus on transforming curated datasets to
refine and possibly expand their relational characteristics to align
with our research. I will approach the transformation process by
breaking it down into two sub-processes: preparation and enrichment. The
preparation process involves normalizing and tokenizing text. The
enrichment process involves recoding, generating, and integrating
variables. These processes are not sequential but may occur in any order
based on the researcher's evaluation of the dataset characteristics and
the desired outcome.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{terminal} Lessons}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Reshape Datasets
by Rows, Reshape datasets by Columns}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: Explore data preprocessing skills to manipulate rows and
columns using powerful packages like \texttt{dplyr} and
\texttt{tidytext} to normalization, tokenize, and integrate datasets
equipping you with the essential techniques to structure datasets for
analysis.

\end{tcolorbox}

\section{Preparation}\label{sec-td-preparation}

In this section we will cover the processes of normalization and
tokenization. These processes are particularly relevant for text
analysis, as text conventions can introduce unwanted variability in the
data and the unit of observation may need to be adjusted to align with
the research question.

To illustrate these processes, we will use a curated version of the
Europarl Corpus (\citeproc{ref-Koehn2005}{Koehn 2005}). This dataset
contains transcribed source language (Spanish) and translated target
language (English) from the proceedings of the European Parliament. The
unit of observation is the \texttt{lines} variable whose values are
lines of dialog. We will use this dataset to explore the normalization
and tokenization processes.

The contents of the data dictionary for this dataset appears in
Table~\ref{tbl-td-europarl-dd}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0973}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1416}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1416}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.6018}}@{}}

\caption{\label{tbl-td-europarl-dd}Data dictionary for the curated
Europarl Corpus.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable\_type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
doc\_id & Document ID & numeric & Unique identification number for each
document \\
type & Document Type & categorical & Type of document; either `Source'
(Spanish) or `Target' (English) \\
line\_id & Line ID & numeric & Unique identification number for each
line in each document type \\
lines & Lines & categorical & Content of the lines in the document \\

\end{longtable}

Let's read in the dataset CSV file with \texttt{read\_csv()} and inspect
the first lines of the dataset with \texttt{slice\_head()} in
Example~\ref{exm-tb-europarl-preview}.

\begin{example}[]\protect\hypertarget{exm-tb-europarl-preview}{}\label{exm-tb-europarl-preview}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read in the dataset}
\NormalTok{europarl\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/derived/europarl\_curated.csv"}\NormalTok{)}

\CommentTok{\# Preview the first 10 lines}
\NormalTok{europarl\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>     doc_id type   line_id lines                                                 
>      <dbl> <chr>    <dbl> <chr>                                                 
>  1 1965735 Source       1 "Reanudación del período de sesiones"                 
>  2       1 Target       1 "Resumption of the session"                           
>  3 1965736 Source       2 "Declaro reanudado el período de sesiones del Parlame~
>  4       2 Target       2 "I declare resumed the session of the European Parlia~
>  5 1965737 Source       3 "Como todos han podido comprobar, el gran \"efecto de~
>  6       3 Target       3 "Although, as you will have seen, the dreaded 'millen~
>  7 1965738 Source       4 "Sus Señorías han solicitado un debate sobre el tema ~
>  8       4 Target       4 "You have requested a debate on this subject in the c~
>  9 1965739 Source       5 "A la espera de que se produzca, de acuerdo con mucho~
> 10       5 Target       5 "In the meantime, I should like to observe a minute' ~
\end{verbatim}

\end{example}

This dataset includes 3,931,468 observations and four variables. The key
variable for our purposes is the \texttt{lines} variable. This variable
contains the text we will be working with. The other variables are
metadata that may be of interest for our analyses.

\subsection{Normalization}\label{sec-td-normalization}

The process of normalizing datasets in essence is to sanitize the values
of variable or set of variables such that there are no artifacts that
will contaminate subsequent processing. It may be the case that
non-linguistic metadata may require normalization but more often than
not linguistic information is the most common target for normalization
as text often includes artifacts from the acquisition process which will
not be desired in the analysis.

Simply looking at the first 10 lines of the dataset from
Example~\ref{exm-tb-europarl-preview} gives us a clearer sense of the
dataset structure, but, in terms of normalization procedures we might
apply, it is likely not sufficient. We want to get a sense of any
potential inconsistencies in the dataset, in particular in the
\texttt{lines} variable. Since this is a large dataset with 3,931,468
observations, we will need to explore the dataset in more detail using
procedures for summarizing and filtering data.

After exploring variations in the \texttt{lines} variable, I identified
a number of artifacts in this dataset that we will want to consider
addressing. These are included in
Table~\ref{tbl-td-europarl-normalization}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3300}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3300}}@{}}
\caption{Characteristics of the Europarl Corpus dataset that may require
normalization.}\label{tbl-td-europarl-normalization}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Non-speech annotations & \texttt{(Abucheos)}, \texttt{(A4-0247/98)},
\texttt{(The\ sitting\ was\ opened\ at\ 09:00)} \\
Inconsistent whitespace & \texttt{5\ \%\ ,}, ~~~~~,
\texttt{Palacio\textquotesingle{}\ s} \\
Non-sentence punctuation & \texttt{-} \\
Abbreviations & \texttt{Mr.}, \texttt{Sr.}, \texttt{Mme.}, \texttt{Mr},
\texttt{Sr}, \texttt{Mme}, \texttt{Mister}, \texttt{Señor},
\texttt{Madam} \\
Text case & \texttt{The}, \texttt{the}, \texttt{White},
\texttt{white} \\
\end{longtable}

These artifacts either may not be of interest or may introduce unwanted
variability in the that could prove problematic for subsequent
processing (\emph{e.g} tokenization, calculating frequencies,
\emph{etc.}).

Identifying our normalization goals is an important first step. In this
case, let's just focus on removing parlimentary session descriptions for
demonstration purposes. The next step is to identify the procedures that
will accomplish this goal.

The majority of text normalization procedures incorporate the
\texttt{stringr} package (\citeproc{ref-R-stringr}{Wickham 2023a}). This
package provides a number of functions for manipulating text strings.
The workhorse functions we will use for our tasks are the
\texttt{str\_remove()} and \texttt{str\_replace()} functions. As the
these functions give us the ability to remove or replace text based on
literal strings and Regular Expressions.

Our first step, however, is to identify the patterns we want to remove
or replace. For this, we can call on either the \texttt{str\_view()}
function or the \texttt{str\_detect()} function inside a
\texttt{filter()} function to identify the lines that contain the
patterns we want to remove. We can also use the \texttt{str\_extract()}
and/ or \texttt{str\_extract\_all()} functions inside a
\texttt{mutate()} function to extract the text that matches the pattern.
In any of these cases, it can be helpful to use the
\texttt{slice\_sample()} function work with a small sample of the
dataset to get a sense of the patterns we are working with as we develop
a search pattern that does not over- or under-generalize the text we
want to remove or replace. If we are too general, we may end up removing
or replacing text that we want to keep. If we are too specific, we may
not remove or replace all the text we want to remove or replace.

Let's start by identifying non-parlimentary speech. In
Example~\ref{exm-td-europarl-search-non-speech}, we use
\texttt{str\_detect()} which detects a pattern in a character vector and
returns a logical vector, \texttt{TRUE} if the pattern is detected and
\texttt{FALSE} if it is not. In combination with \texttt{filter()} we
can identify a variable with rows that match a pattern.

From the examples above, we can see that these instances are wrapped
with parentheses \texttt{(} and \texttt{)}. The text within the
parentheses can vary, so we need a Regular Expression to do the heavy
lifting. To start out we can match any one or multiple characters with
\texttt{.+}. But it is important to recognize the \texttt{+} (and also
the \texttt{*}) operators are `greedy', meaning that if there are
multiple matches, the longest match will be returned. In this case, we
want to match the shortest match. To do this we can use the \texttt{?}
operator to make the \texttt{+} operator `lazy'. This will match the
shortest match.

\begin{example}[]\protect\hypertarget{exm-td-europarl-search-non-speech}{}\label{exm-td-europarl-search-non-speech}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(stringr)}

\CommentTok{\# Identify non{-}speech lines}
\NormalTok{europarl\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(lines, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{(.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>     doc_id type   line_id lines                                                 
>      <dbl> <chr>    <dbl> <chr>                                                 
>  1 3225772 Source 1260038 (PT) Señor Presidente, quisiera plantear dos pregunta~
>  2 3715842 Source 1750108 (El Parlamento decide la devolución a la Comisión)    
>  3 1961715 Target 1961715 (Parliament adopted the resolution)                   
>  4 1429470 Target 1429470 27, originally Greens/EFA amendment in FEMM); binding~
>  5   51632 Target   51632 Question No 8 by (H-0376/00):                         
>  6 2482671 Source  516937 La Comisión propone proporcionar a las Agencias nacio~
>  7 1059628 Target 1059628 (The President cut off the speaker)                   
>  8 1507254 Target 1507254 in writing. - (LT) I welcomed this document, because ~
>  9 2765325 Source  799591 (Aplausos)                                            
> 10 2668536 Source  702802    Las preguntas que, por falta de tiempo, no han rec~
\end{verbatim}

\end{example}

The results from Example~\ref{exm-td-europarl-search-non-speech} show
that we have identified the lines that contain at least one of the
parliamentary session description annotations. A more targeted search to
identify specific instances of the parliamentary session descriptions
can be accomplished adding the \texttt{str\_extract\_all()} function as
seen in Example~\ref{exm-td-europarl-search-non-speech-2}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-search-non-speech-2}{}\label{exm-td-europarl-search-non-speech-2}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract non{-}speech fragments}
\NormalTok{europarl\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(lines, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{(.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{non\_speech =} \FunctionTok{str\_extract\_all}\NormalTok{(lines, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{(.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 5
>     doc_id type   line_id lines                                       non_speech
>      <dbl> <chr>    <dbl> <chr>                                       <list>    
>  1 3225772 Source 1260038 (PT) Señor Presidente, quisiera plantear d~ <chr [1]> 
>  2 3715842 Source 1750108 (El Parlamento decide la devolución a la C~ <chr [1]> 
>  3 1961715 Target 1961715 (Parliament adopted the resolution)         <chr [1]> 
>  4 1429470 Target 1429470 27, originally Greens/EFA amendment in FEM~ <chr [1]> 
>  5   51632 Target   51632 Question No 8 by (H-0376/00):               <chr [1]> 
>  6 2482671 Source  516937 La Comisión propone proporcionar a las Age~ <chr [2]> 
>  7 1059628 Target 1059628 (The President cut off the speaker)         <chr [1]> 
>  8 1507254 Target 1507254 in writing. - (LT) I welcomed this documen~ <chr [1]> 
>  9 2765325 Source  799591 (Aplausos)                                  <chr [1]> 
> 10 2668536 Source  702802    Las preguntas que, por falta de tiempo,~ <chr [1]>
\end{verbatim}

\end{example}

OK, that might not be what you expected. The
\texttt{str\_extract\_all()} function returns a list of character
vectors. This is because for any given line in \texttt{lines} there may
be a different number of matches. To maintain the data frame as
rectangular, a list is returned for each value of \texttt{non\_speech}.
We could expand the list into a data frame with the \texttt{unnest()}
function from the \texttt{tidyr} package if our goal were to work with
these matches. But that is not our aim. Rather, we want to know if we
have multiple matches per line. Note that the information provided for
the \texttt{non\_speech} column by the tibble object tells use that we
have some lines with muliple matches, as we can see in line 6 of our
small sample. So good thing we checked!

Let's now remove these parliamentary session description annotations
from each line in the \texttt{lines} column. We turn to
\texttt{str\_remove\_all()}, a variant of \texttt{str\_remove()}, that,
as you expect, will remove multiple matches in a single line. We will
use the \texttt{mutate()} function to overwrite the \texttt{lines}
column with the modified text. The code is seen in
Example~\ref{exm-td-europarl-remove-non-speech}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-remove-non-speech}{}\label{exm-td-europarl-remove-non-speech}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Remove non{-}speech fragments}
\NormalTok{europarl\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  europarl\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lines =} \FunctionTok{str\_remove\_all}\NormalTok{(lines, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{(.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\end{example}

I recommend spot checking the results of this normalization step by
running the code in Example~\ref{exm-td-europarl-search-non-speech}
again, if nothing appears we've done our job.

When you are content with the results, drop the observations that have
no text in the \texttt{lines} column given the entire line was
non-speech. This can be done with the \texttt{is.na()} function and the
\texttt{filter()} function as seen in
Example~\ref{exm-td-europarl-drop-empty-lines}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-drop-empty-lines}{}\label{exm-td-europarl-drop-empty-lines}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Drop empty lines}
\NormalTok{europarl\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  europarl\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(lines))}
\end{Highlighting}
\end{Shaded}

\end{example}

Normalization goals will vary from dataset to dataset but the procedures
often follow a similar line of attack to those outlined in this section.
There are cases, however, in which normalization procedures are more
easily accomplished after subsequent transformation steps or need to be
post-poned to further the goals of other transformation steps. For
example, standardizing abbreviated forms may be more easily accomplished
after tokenization when each token is a word. Another example is the
case of case conversion. Even if we are not directly interested in the
case differences between words, certain generation procedures, Named
Entity Recognition (NER) for example, may use case information to
identify the names of people, locations, organizations, \emph{etc.}. In
these cases, it may be better to leave the case as is until after the
generation step.

\subsection{Tokenization}\label{sec-td-tokenization}

Another common transformation process that is particularly relevant for
preparing text for analysis is tokenization. Tokenization is the process
of segmenting units of language into components relevant for the
research question. This includes breaking text in curated datasets into
smaller units, such as words, \(n\)-grams, sentences, \emph{etc.} or
combining text into larger units relative to the original text.

The process of tokenization is fundamentally row-wise. By scaling the
text units up or down, we change the unit of observation. It is
important both for the research and the text processing to
operationalize our language units. For example, while it may appear
obvious to you what `word' or `sentence' means, a computer, and your
reproducible research, needs a definition. This can prove tricker than
it seems. For example, in English, we can segment text into words by
splitting on whitespace. This works fairly well but there are some cases
where this is not ideal. For example, in the case of contractions, such
as \texttt{don\textquotesingle{}t}, \texttt{won\textquotesingle{}t},
\texttt{can\textquotesingle{}t}, \emph{etc.} the apostrophe is not a
whitespace character. If we want to consider these contractions as
separate words, then we need to consider a different tokenization
strategy.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

Consider the following paragraph:

\begin{quote}
``As the sun dipped below the horizon, the sky was set ablaze with
shades of orange-red, illuminating the landscape. It's a sight
Mr.~Johnson, a long-time observer, never tired of. On the lakeside, he'd
watch with friends, enjoying the ever-changing hues---especially those
around 6:30 p.m.---and reflecting on nature's grand display. Even in the
half-light, the water's glimmer, coupled with the echo of distant
laughter, created a timeless scene. The so-called `magic hour' was
indeed magical, yet fleeting, like a well-crafted poem; it was the
essence of life itself.''
\end{quote}

What text conventions would pose issues for word tokenization based on a
whitespace critieron?

\end{tcolorbox}

Furthermore, tokenization strategies can vary between languages. For
German words are often compounded together, meaning many `words' will
not be captured by the whitespace convention. Whitespace may not even be
relevant for word tokenization in logographic writing systems such as
Chinese. The take home message is there is no one-size-fits-all
tokenization strategy.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

For processing Chinese text, including tokenization, see the
\texttt{jiebaR} package (\citeproc{ref-R-jiebaR}{Wenfeng and Yanyi
2019}) and the \texttt{gibasa} package (\citeproc{ref-R-gibasa}{Kato,
Ichinose, and Kudo 2023}).

\end{tcolorbox}

Let's say we continue to work with the Europarl Corpus dataset. We have
already removed non-speech annotations from the \texttt{lines} variable.
Now we want to tokenize the text. We will start by tokenizing the text
into words. If we envision what this should look like, we might imagine
something like Table~\ref{tbl-td-europarl-tokenization-words-example}.

\begin{longtable}[]{@{}llll@{}}
\caption{Example of tokenizing the \texttt{lines} variable into word
tokens.}\label{tbl-td-europarl-tokenization-words-example}\tabularnewline
\toprule\noalign{}
doc\_id & type & line\_id & token \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
doc\_id & type & line\_id & token \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Target & 2 & I \\
1 & Target & 2 & declare \\
1 & Target & 2 & resumed \\
1 & Target & 2 & the \\
1 & Target & 2 & session \\
\end{longtable}

Comparing Table~\ref{tbl-td-europarl-tokenization-words-example} to the
fourth row of the output of Example~\ref{exm-tb-europarl-preview}, we
can see that we want to segment the words in \texttt{lines} and then
have each segment appear as a separate observation, retaining the
relevant metadata variables.

This is a very common strategy in text analysis. So common, in fact,
that the \texttt{tidytext} package (\citeproc{ref-R-tidytext}{Robinson
and Silge 2023}) includes a function, \texttt{unnest\_tokens()} that
tokenizes text in just such a way. Various tokenization types can be
specified including `characters', `words', `ngrams', `sentences' among
others. We will use the `word' tokenization type to recreate the
structure we envisioned in
Table~\ref{tbl-td-europarl-tokenization-words-example}.

In Example~\ref{exm-td-europarl-tokenization-words-tidytext}, we see set
our output variable to \texttt{token} and our input variable to
\texttt{lines}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-tokenization-words-tidytext}{}\label{exm-td-europarl-tokenization-words-tidytext}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(tidytext)}

\CommentTok{\# Tokenize the lines into words}
\NormalTok{europarl\_unigrams\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  europarl\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest\_tokens}\NormalTok{(}
    \AttributeTok{output =}\NormalTok{ token,}
    \AttributeTok{input =}\NormalTok{ lines,}
    \AttributeTok{token =} \StringTok{"words"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\end{example}

Let's preview the very same lines we modeled in
Table~\ref{tbl-td-europarl-tokenization-words-example} to see the
results of our tokenization.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview}
\NormalTok{europarl\_unigrams\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(type }\SpecialCharTok{==} \StringTok{"Target"}\NormalTok{, line\_id }\SpecialCharTok{==} \DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>    doc_id type   line_id token     
>     <dbl> <chr>    <dbl> <chr>     
>  1      2 Target       2 i         
>  2      2 Target       2 declare   
>  3      2 Target       2 resumed   
>  4      2 Target       2 the       
>  5      2 Target       2 session   
>  6      2 Target       2 of        
>  7      2 Target       2 the       
>  8      2 Target       2 european  
>  9      2 Target       2 parliament
> 10      2 Target       2 adjourned
\end{verbatim}

The \texttt{token} column now contains our word tokens. One thing to
note, however, is that text is lowercased by default. If we want to
retain the original case, keep the original variable, or change the
tokenization strategy, we can update the \texttt{to\_lower},
\texttt{drop}, and \texttt{token} parameters, respectively.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

\texttt{unnest\_tokens()} approaches tokenization from a segmentation
point of view. That is, the context separating our tokens is targeted
and is removed. In some cases, it may be more feasible to turn the
approach around and instead target the tokens to extract. The
\texttt{str\_extract\_all()} can be used for this purpose. Note,
however, the former approach is often more effective and effecient. The
later requires a regular expression, or set of, which can be tricky to
develop for some tokenization tasks.

\end{tcolorbox}

As we derive datasets to explore, let's also create bigram tokens. We
can do this by changing the \texttt{token} parameter to
\texttt{"ngrams"} and specifying the value for \(n\) with the \texttt{n}
parameter. I will assign the result to \texttt{europarl\_bigrams\_tbl}
as we will have two-word tokens, as seen in
Example~\ref{exm-td-europarl-tokenization-bigrams-tidytext}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-tokenization-bigrams-tidytext}{}\label{exm-td-europarl-tokenization-bigrams-tidytext}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tokenize the lines into bigrams}
\NormalTok{europarl\_bigrams\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  europarl\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest\_tokens}\NormalTok{(}
    \AttributeTok{output =}\NormalTok{ token,}
    \AttributeTok{input =}\NormalTok{ lines,}
    \AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{,}
    \AttributeTok{n =} \DecValTok{2}
\NormalTok{  )}
\CommentTok{\# Preview}
\NormalTok{europarl\_bigrams\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(type }\SpecialCharTok{==} \StringTok{"Target"}\NormalTok{, line\_id }\SpecialCharTok{==} \DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>    doc_id type   line_id token               
>     <dbl> <chr>    <dbl> <chr>               
>  1      2 Target       2 i declare           
>  2      2 Target       2 declare resumed     
>  3      2 Target       2 resumed the         
>  4      2 Target       2 the session         
>  5      2 Target       2 session of          
>  6      2 Target       2 of the              
>  7      2 Target       2 the european        
>  8      2 Target       2 european parliament 
>  9      2 Target       2 parliament adjourned
> 10      2 Target       2 adjourned on
\end{verbatim}

\end{example}

The two-word token sequences for lines appear as observations in the
\texttt{europarl\_bigrams\_tbl} dataset.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

The \texttt{tidytext} package is one of a number of packages that
provide tokenization functions. Some other notable packages include
\texttt{tokenizers} (\citeproc{ref-R-tokenizers}{Mullen 2022}) and
\texttt{textrecipes} (\citeproc{ref-R-textrecipes}{Hvitfeldt 2023}). In
fact, the functions from the \texttt{tokenizers} package are used under
the hood in the \texttt{tidytext} package. The \texttt{textrecipes}
package is part of the \texttt{tidymodels} framework and is designed to
work with the \texttt{tidymodels} suite of packages. It is particularly
useful for integrating tokenization with other preprocessing steps and
machine learning models, as we will see in Chapter~\ref{sec-prediction}.

\end{tcolorbox}

The most common tokenization strategy is to segment text into smaller
units, often words. However, there are times when we may want text
segements to be larger than the existing token unit, effectively
collapsing over rows. For example, if we are working with a curated
dataset which is tokenized by words, we may want to group the words into
sentences. A couple considerations are in order, however. First, we need
to be clear about what we mean by `sentence' and how we will group the
words into sentences. In some cases key cues for sentence boundaries,
such as sentencial punctuation have been stripped from the text, making
a simple defnition of a sentence difficult or impossible to be performed
computationally. Second, we also need to be clear how we will handle the
metadata variables. In other words, when we collapse over rows, we need
to be aware and intentional about how we group these new units of
observation.

\section{Enrichment}\label{sec-td-enrichment}

Where preparation steps are focused on sanitizing and segmenting the
text, enrichment steps are aimed towards augmenting the dataset either
through recoding, generating, or integrating variables. These processes
can prove invaluable for aligning the dataset with the research question
and facilitating the analysis.

As a pratical example of these types of transformations, we'll posit
that we are conducting translation research. Specifically, we will set
up an investigation into the effect of translation on the syntactic
simplification of text. The basic notion is that when translators
translate text from one language to another, they subconsciously
simplify the text, relative to native texts (\citeproc{ref-Liu2021}{Liu
and Afzaal 2021}).

To address this research question, we will use the ENNTT corpus,
introduced in Section~\ref{sec-cd-semi-structured-orientation}. This
data contains European Parliament proceedings and the type of text
(native, non-native, or translation) from which the text was extracted.
There is one curated dataset for each of the text types.

The data dictionary for the curated native dataset appears in
Table~\ref{tbl-td-enntt-native-dd}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1414}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1919}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1616}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4848}}@{}}

\caption{\label{tbl-td-enntt-native-dd}Data dictionary for the curated
native ENNTT dataset.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable\_type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
session\_id & Session ID & categorical & Unique identifier for each
session \\
speaker\_id & Speaker ID & categorical & Unique identifier for each
speaker \\
state & State & categorical & The country or region the speaker is
from \\
session\_seq & Session Sequence & ordinal & The order in which the
session occurred \\
text & Text & categorical & The spoken text during the session \\
type & Type & categorical & The type of speaker. Natives in this
dataset. \\

\end{longtable}

All three curated datasets have the same variables. The unit of
observation for each dataset is the \texttt{text} variable.

Before we get started, let's consider what the transformed dataset might
look like and what its variables mean. First, we will need to
operationalize what we mean by syntactic simplification There are many
measures of syntactic complexity
(\citeproc{ref-Szmrecsanyi2004}{Szmrecsanyi 2004}). For our purposes, we
will focus on two measures of syntactic complexity: number of T-units
and sentence length (in words). A T-unit is a main clause and all of its
subordinate clauses. To calculate the number of T-units, we will need to
identify the main clauses and their subordinate clauses. The sentence
length is straightforward to calculate after word tokenization.

The next step is to operationalize what we mean by syntactic
simplification. For our purposes, let's focus on two: number of T-units
and sentence length (in words). Length is straightforward to calculate
after word tokenization but a T-unit is a bit more involved. A T-unit is
a main clause and all of its subordinate clauses. To calculate the
number of T-units, we will need to identify the main clauses and their
subordinate clauses.

An idealized transformed dataset dictionary for this investigation
should look something like Table~\ref{tbl-td-generation-idealized}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1700}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1900}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5400}}@{}}

\caption{\label{tbl-td-generation-idealized}Idealized transformed
dataset for the syntactic simplification investigation.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable\_name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable\_type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
doc\_id & Document ID & integer & Unique identifier for each
document. \\
type & Type & character & Type of text (native or translated). \\
t\_units & T-units & integer & Number of T-units in the text. \\
word\_len & Word Length & integer & Number of words in the text. \\

\end{longtable}

We will be using the the native and translated datasets for our purposes
so let's go ahead and read in these datasets.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read in curated natives}
\NormalTok{enntt\_natives\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/enntt\_natives\_curated.csv"}\NormalTok{)}

\CommentTok{\# Read in curated translations}
\NormalTok{enntt\_translations\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/enntt\_translations\_curated.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{Generation}\label{sec-td-generation}

The process of generation involves the addition of information to a
dataset. This differs from other transformation procedures in that
instead of manipulating, classifying, and/ or deriving information based
on characteristics explicit in a dataset, generation involves deriving
new information based on characteristics implicit in a dataset.

The most common type of operation involved in the generation process is
the addition of linguistic annotation. This process can be accomplished
manually by a researcher or research team or automatically through the
use of pre-trained linguistic resources and/ or software. Ideally the
annotation of linguistic information can be conducted automatically. It
is important to note, however, that the quality of the annotations is
contingent on the quality of the pre-trained resources, their alignment
with the language variety and register to be annotated, and the quality
of the text to be annotated. To be clear, no annotation method, whether
manual or automatic is guaranteed to be perfectly accurate, but we must
be aware of the limitations of the annotation method we choose.

To identify the main clauses and their subordinate clauses in our
datasets, we will need to derive syntactic annotation information from
the ENNTT \texttt{text} variable.

As fun as it would be to hand-annotate the ENNTT corpus, we will instead
turn to automatic linguistic annotation. Specifically, we will use the
\texttt{udpipe} package (\citeproc{ref-R-udpipe}{Wijffels 2023}) which
provides an interface for annotating text using pre-trained models from
the \href{https://universaldependencies.org/}{Universal Dependencies}
(UD) project (\citeproc{ref-Nivre2020}{Nivre et al. 2020}). The UD
project is an effort to develop cross-linguistically consistent treebank
annotation for a variety of languages.

Our first step, then, is to peruse the available pre-trained models for
the languages we are interested in and selected the most
register-aligned models. The models, model names, and licensing
information are documented in the \texttt{udpipe} package and can be
accessed by running \texttt{?udpipe::udpipe\_download\_model()} in the R
console. For illustrative purposes, the \texttt{english} treebank model
from the \emph{https://github.com/bnosac/udpipe.models.ud} repository
which is released under the
\href{https://creativecommons.org/licenses/by-sa/4.0/}{CC-BY-SA
license}. This model is trained on various sources including news,
Wikipedia, and web data of various genres.

Let's set the stage by providing an overview of the annotation process.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Load the \texttt{udpipe} package.
\item
  Select the pre-trained model to use and the directory where the model
  will be stored in your local environment.
\item
  Prepare the dataset to be annotated (if necessary). This includes
  ensuring that the dataset has a column of text to be annotated and a
  grouping column. By default, the names of these columns are expected
  to be \texttt{text} and \texttt{doc\_id}, respectively. The
  \texttt{text} column needs to be a character vector and the
  \texttt{doc\_id} column needs to be a unique index for each text to be
  annotated.
\item
  Annotate the dataset. The result returns a data frame.
\end{enumerate}

Steps 3 and 4 are repeated for the \texttt{enntt\_natives\_tbl} and the
\texttt{enntt\_translations\_tbl} datasets. For brevity, I will only
show the code for the dataset for the natives. Additionally, I will
subset the dataset to 10,000 randomly selected lines for both datasets,
as in Example~\ref{exm-td-generation-subset-natives} for the natives.
Syntactic annotation is a computationally expensive operation and the
natives and translations datasets contain 116,341 and 738,597
observations, respectively.

\begin{example}[]\protect\hypertarget{exm-td-generation-subset-natives}{}\label{exm-td-generation-subset-natives}

~

\end{example}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper} Computational Performance

In your own research computationally expensive cannot be avoided, but it
can be managed. One strategy is to work with a subset of the data until
your code is working as expected. Once you are confident that your code
is working as expected, then you can scale up to the full dataset.

If you are using Quarto, you can use the \texttt{cache:\ true} metadata
field in your code blocks to cache the results of computationally
expensive code blocks. This will allow you to run your code once and
then use the cached results for subsequent runs.

Parallel processing is another strategy for managing computationally
expensive code. Some packages, such as \texttt{udpipe}, have built-in
support for parallel processing. Other packages, such as
\texttt{tidytext}, do not. In these cases, you can use the
\texttt{future} package (\citeproc{ref-R-future}{Bengtsson 2023}) to
parallelize your code.

\end{tcolorbox}

With the subsetted \texttt{enntt\_natives\_tbl} object, let's execute
steps 1-4, as seen in Example~\ref{exm-td-generation-udpipe-natives}.

\begin{example}[]\protect\hypertarget{exm-td-generation-udpipe-natives}{}\label{exm-td-generation-udpipe-natives}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(udpipe)}

\CommentTok{\# Model and directory}
\NormalTok{model }\OtherTok{\textless{}{-}} \StringTok{"english"}
\NormalTok{model\_dir }\OtherTok{\textless{}{-}} \StringTok{"../data/"}

\CommentTok{\# Prepare the dataset to be annotated}
\NormalTok{enntt\_natives\_prepped\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  enntt\_natives\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{doc\_id =} \FunctionTok{row\_number}\NormalTok{()) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(doc\_id, text)}

\CommentTok{\# Annotate the dataset}
\NormalTok{enntt\_natives\_ann\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{udpipe}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ enntt\_natives\_prepped\_tbl,}
    \AttributeTok{object =}\NormalTok{ model,}
    \AttributeTok{model\_dir =}\NormalTok{ model\_dir}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tibble}\NormalTok{()}

\CommentTok{\# Preview}
\FunctionTok{glimpse}\NormalTok{(enntt\_natives\_ann\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 264,124
> Columns: 17
> $ doc_id        <chr> "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "~
> $ paragraph_id  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~
> $ sentence_id   <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~
> $ sentence      <chr> "It is extremely important that action is taken to ensur~
> $ start         <int> 1, 4, 7, 17, 27, 32, 39, 42, 48, 51, 58, 63, 66, 73, 77,~
> $ end           <int> 2, 5, 15, 25, 30, 37, 40, 46, 49, 56, 61, 64, 71, 75, 82~
> $ term_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1~
> $ token_id      <chr> "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11",~
> $ token         <chr> "It", "is", "extremely", "important", "that", "action", ~
> $ lemma         <chr> "it", "be", "extremely", "important", "that", "action", ~
> $ upos          <chr> "PRON", "AUX", "ADV", "ADJ", "SCONJ", "NOUN", "AUX", "VE~
> $ xpos          <chr> "PRP", "VBZ", "RB", "JJ", "IN", "NN", "VBZ", "VBN", "TO"~
> $ feats         <chr> "Case=Nom|Gender=Neut|Number=Sing|Person=3|PronType=Prs"~
> $ head_token_id <chr> "4", "4", "4", "0", "8", "8", "8", "4", "10", "8", "13",~
> $ dep_rel       <chr> "expl", "cop", "advmod", "root", "mark", "nsubj:pass", "~
> $ deps          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
> $ misc          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
\end{verbatim}

\end{example}

There is quite a bit of information which is returned from
\texttt{udpipe()}. Note that the input lines have been tokenized by
word. Each token includes the \texttt{token}, \texttt{lemma}, part of
speech (\texttt{upos} and \texttt{xpos}), morphological features
(\texttt{feats}), and syntactic relationships (\texttt{head\_token\_id}
and \texttt{dep\_rel}). The \texttt{token\_id} keeps track of the
token's position in the sentence and the \texttt{sentence\_id} keeps
track of the sentence's position in the original text. Finally, the
\texttt{doc\_id} column and its values correspond to the
\texttt{doc\_id} in the \texttt{enntt\_natives\_tbl} dataset.

The number of variables in the \texttt{udpipe()} annotation output is
quite overwhelming. However, these attributes come in handy for
manipulating, extracting, and plotting information based on lexical and
syntactic patterns. See the dependency tree in
Figure~\ref{fig-td-generation-udpipe-english-plot-tree} for an example
of the syntactic information that can be extracted from the
\texttt{udpipe()} annotation output.

\begin{figure}[H]

\centering{

\includegraphics{transform-datasets_files/figure-pdf/fig-td-generation-udpipe-english-plot-tree-1.pdf}

}

\caption{\label{fig-td-generation-udpipe-english-plot-tree}Plot of the
syntactic tree for a sentence in the ENNTT natives dataset.}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

The plot in Figure~\ref{fig-td-generation-udpipe-english-plot-tree} was
created using the \texttt{rsyntax} package
(\citeproc{ref-R-rsyntax}{Welbers and van Atteveldt 2022}). In addition
to creating dependency tree plots, the \texttt{rsyntax} package can be
used to extract syntactic patterns from the \texttt{udpipe()} annotation
output. \href{https://github.com/vanatteveldt/rsyntax}{See the
documentation for more information}.

\end{tcolorbox}

In Figure~\ref{fig-td-generation-udpipe-english-plot-tree} we see the
syntactic tree for a sentence in the ENNTT natives dataset. Each node is
labeled with the \texttt{token\_id} which provides the linear ordering
of the sentence. Above the nodes the \texttt{dep\_relation}, or
dependency relationship label is provided. These labels are based on the
UD project's
\href{https://universaldependencies.org/u/dep/index.html}{dependency
relations}. We can see that the `ROOT' relation is at the top of the
tree and corresponds to the verb `brought'. `ROOT' relations mark
predicates in the sentence. Not seen in the example tree, `cop' relation
is a copular, or non-verbal predicate and should be included. These are
the key syntactic pattern we will use to identify main clauses for
T-units.

\subsection{Recoding}\label{sec-td-recoding}

Recoding processes can be characterized by the creation of structural
changes which are derived from values in variables effectively recasting
values as new variables to enable more direct access in our analyses.

Specifically, we will need to identify and count the main clauses and
their subordinate clauses to create a variable \texttt{t\_units} from
our natives and translations annotations objects. In the UD project's
listings, the relations `ccomp' (clausal complement), `xcomp' (open
clausal complement), and `acl:relcl' (relative clause), as seen in
Figure~\ref{fig-td-generation-udpipe-english-plot-tree} are subordinate
clauses. Furthermore, we will also need to count the number of words in
each sentence to create a variable \texttt{word\_len}.

To calculate T-units and words per sentence we turn to the
\texttt{dplyr} package. We will use the \texttt{group\_by()} function to
group the dataset by \texttt{doc\_id} and \texttt{sentence\_id} and then
use the \texttt{summarize()} function to calculate the number of T-units
and words per sentence, where a T-unit is the combination of the sum of
main clauses and sum of subordinante clauses. The code is seen in
Example~\ref{exm-td-generation-udpipe-natives-tunits-words}.

\begin{example}[]\protect\hypertarget{exm-td-generation-udpipe-natives-tunits-words}{}\label{exm-td-generation-udpipe-natives-tunits-words}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the number of T{-}units and words per sentence}
\NormalTok{enntt\_natives\_syn\_comp\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  enntt\_natives\_ann\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(doc\_id, sentence\_id) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{main\_clauses =} \FunctionTok{sum}\NormalTok{(dep\_rel }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"ROOT"}\NormalTok{, }\StringTok{"cop"}\NormalTok{)),}
    \AttributeTok{subord\_clauses =} \FunctionTok{sum}\NormalTok{(dep\_rel }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"ccomp"}\NormalTok{, }\StringTok{"xcomp"}\NormalTok{, }\StringTok{"acl:relcl"}\NormalTok{)),}
    \AttributeTok{t\_units =}\NormalTok{ main\_clauses }\SpecialCharTok{+}\NormalTok{ subord\_clauses,}
    \AttributeTok{word\_len =} \FunctionTok{n}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{()}

\CommentTok{\# Preview}
\FunctionTok{glimpse}\NormalTok{(enntt\_natives\_syn\_comp\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 10,199
> Columns: 6
> $ doc_id         <chr> "1", "10", "100", "1000", "10000", "1001", "1002", "100~
> $ sentence_id    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~
> $ main_clauses   <int> 1, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 1, 1, 0, 2, 0, 1, 1, 1~
> $ subord_clauses <int> 3, 2, 1, 0, 1, 2, 1, 1, 0, 2, 1, 0, 3, 2, 0, 4, 2, 1, 1~
> $ t_units        <int> 4, 2, 1, 0, 3, 2, 1, 2, 1, 2, 2, 1, 4, 2, 2, 4, 3, 2, 2~
> $ word_len       <int> 21, 25, 27, 15, 40, 43, 29, 23, 13, 30, 33, 9, 68, 35, ~
\end{verbatim}

\end{example}

A quick spot check of some sentences calculations
\texttt{enntt\_natives\_syn\_comp\_tbl} dataset against the
\texttt{enntt\_natives\_ann\_tbl} is good to ensure that the calculation
is working as expected. In
Figure~\ref{fig-td-generation-udpipe-natives-tunits} we see a sentence
that has a word length of 13 and a T-unit value of 5.

\begin{figure}[H]

\centering{

\includegraphics{transform-datasets_files/figure-pdf/fig-td-generation-udpipe-natives-tunits-1.pdf}

}

\caption{\label{fig-td-generation-udpipe-natives-tunits}Sentence with a
word length of 13 and a T-unit value of 5.}

\end{figure}%

Now we can drop the intermediate columns we created to calculate our key
syntactic complexity measures using \texttt{select()} to indicate those
that we do want to keep, as seen in
Example~\ref{exm-td-generation-udpipe-natives-tunits-words-select}.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

As confident as we may be in our recoding process, it is always a good
idea to perform some data checks to ensure that the recoding process was
successful. In the process, we can also gain some insight into the data.
Considering the structure in the transformed ENNTT Corpus dataset, what
are some data checks we might want to perform? What are some insights we
might gain from these data checks?

\end{tcolorbox}

\begin{example}[]\protect\hypertarget{exm-td-generation-udpipe-natives-tunits-words-select}{}\label{exm-td-generation-udpipe-natives-tunits-words-select}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Select columns}
\NormalTok{enntt\_natives\_syn\_comp\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  enntt\_natives\_syn\_comp\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(doc\_id, sentence\_id, t\_units, word\_len)}
\end{Highlighting}
\end{Shaded}

\end{example}

Now we can repeat the process for the ENNTT translated dataset. I will
assign the result to \texttt{enntt\_translations\_syn\_comp\_tbl}. The
next step is to join the \texttt{sentences} from the annotated data
frames into our datasets so that we have the information we set out to
generate for both datasets. Then we will combine the native and
translations datasets into a single dataset. These steps are part of the
transformation process and will be covered in the next section.

\subsection{Integration}\label{sec-td-integration}

One final class of transformations that can be applied to curated
datasets to enhance their informativeness for a research project is the
process of integrating two or more datasets. There are two primary types
of integrations: joins and concatenation. \textbf{Joins} can be row- or
column-wise operations that combine datasets based on a common attribute
or set of attributes. \textbf{Concatenation} is exclusively a row-wise
operation that combines datasets that share the same attributes.

Of the two types, joins are the most powerful and sometimes more
difficult to understand. When two datasets are joined at least one
common variable must be shared between the two datasets. The common
variable(s) are referred to as \textbf{keys}. The keys are used to match
observations in one dataset with observations in another dataset by
serving as an index.

There are a number of join types. The most common are left, full, semi,
and anti. The type of join determines which observations are retained in
the resulting dataset. Let's see this in practice. First, let's create
two datasets to join with a common variable \texttt{key}, as seen in
Example~\ref{exm-td-merging-join-dfs}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-dfs}{}\label{exm-td-merging-join-dfs}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{key =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{),}
    \AttributeTok{a =}\NormalTok{ letters[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\NormalTok{  )}

\NormalTok{a\_tbl}
\NormalTok{b\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{key =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{),}
    \AttributeTok{b =}\NormalTok{ letters[}\DecValTok{6}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]}
\NormalTok{  )}

\NormalTok{b\_tbl}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}{0.50\linewidth}

\begin{verbatim}
> # A tibble: 5 x 2
>     key a    
>   <dbl> <chr>
> 1     1 a    
> 2     2 b    
> 3     3 c    
> 4     5 d    
> 5     8 e
\end{verbatim}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\begin{verbatim}
> # A tibble: 5 x 2
>     key b    
>   <dbl> <chr>
> 1     1 f    
> 2     2 g    
> 3     4 h    
> 4     6 i    
> 5     8 j
\end{verbatim}

\end{minipage}%

\end{figure}%

\end{example}

The \texttt{a\_tbl} and the \texttt{b\_tbl} datasets share the
\texttt{key} variable, but the values in the \texttt{key} variable are
not identical. The two datasets share values \texttt{1}, \texttt{2}, and
\texttt{8}. The \texttt{a\_tbl} dataset has values \texttt{3} and
\texttt{5} in the \texttt{key} variable and the \texttt{b\_tbl} dataset
has values \texttt{4} and \texttt{6} in the \texttt{key} variable.

If we apply a left join to the \texttt{a\_tbl} and \texttt{b\_tbl}
datasets, the result will be a dataset that retains all of the
observations in the \texttt{a\_tbl} dataset and only those observations
in the \texttt{b\_tbl} dataset that have a match in the \texttt{a\_tbl}
dataset. The result is seen in Example~\ref{exm-td-merging-join-left}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-left}{}\label{exm-td-merging-join-left}

~

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{left\_join}\NormalTok{(}\AttributeTok{x =}\NormalTok{ a\_tbl, }\AttributeTok{y =}\NormalTok{ b\_tbl, }\AttributeTok{by =} \StringTok{"key"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 5 x 3
>     key a     b    
>   <dbl> <chr> <chr>
> 1     1 a     f    
> 2     2 b     g    
> 3     3 c     <NA> 
> 4     5 d     <NA> 
> 5     8 e     j
\end{verbatim}

\end{example}

Now, if the key variable has the same name, R will recognize and assume
that this is the variable to join on and we don't need the
\texttt{by\ =} argument, but if there are multiple potential key
variables, we use \texttt{by\ =} to specify which one to use.

A full join retains all observations in both datasets, as seen in
Example~\ref{exm-td-merging-join-full}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-full}{}\label{exm-td-merging-join-full}

~

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{full\_join}\NormalTok{(}\AttributeTok{x =}\NormalTok{ a\_tbl, }\AttributeTok{y =}\NormalTok{ b\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 7 x 3
>     key a     b    
>   <dbl> <chr> <chr>
> 1     1 a     f    
> 2     2 b     g    
> 3     3 c     <NA> 
> 4     5 d     <NA> 
> 5     8 e     j    
> 6     4 <NA>  h    
> 7     6 <NA>  i
\end{verbatim}

\end{example}

Left and full joins maintain or increase the number of observations. On
the other hand, semi and anti joins aim to decrease the number of
observations. A semi join retains only those observations in the left
dataset that have a match in the right dataset, as seen in
Example~\ref{exm-td-merging-join-semi}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-semi}{}\label{exm-td-merging-join-semi}

~

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{semi\_join}\NormalTok{(}\AttributeTok{x =}\NormalTok{ a\_tbl, }\AttributeTok{y =}\NormalTok{ b\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 3 x 2
>     key a    
>   <dbl> <chr>
> 1     1 a    
> 2     2 b    
> 3     8 e
\end{verbatim}

\end{example}

And an anti join retains only those observations in the left dataset
that do not have a match in the right dataset, as seen in
Example~\ref{exm-td-merging-join-anti}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-anti}{}\label{exm-td-merging-join-anti}

~

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anti\_join}\NormalTok{(}\AttributeTok{x =}\NormalTok{ a\_tbl, }\AttributeTok{y =}\NormalTok{ b\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 2 x 2
>     key a    
>   <dbl> <chr>
> 1     3 c    
> 2     5 d
\end{verbatim}

\end{example}

Of these join types, the left join and the anti join are some of the
most common to encounter in research projects.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

In addition to datasets that are part of an acquired resource or derived
from a corpus resource, there are also a number of datasets that are
included in R packages that are particularly relevant for text analysis.
For example, the \texttt{tidytext} package includes \texttt{sentiments}
and \texttt{stop\_words} datasets. The \texttt{lexicon} package
(\citeproc{ref-R-lexicon}{Rinker 2019}) includes large number of
datasets that include sentiment lexicons, stopword lists, contractions,
and more.

\end{tcolorbox}

With this in mind, let's return to our syntactic simplification
investigation. Recall that we started with two curated ENNTT datasets:
the natives and translations. We manipulated these datasets subsetting
them to 10,000 randomly selected lines, prepped them for annotation by
adding a \texttt{doc\_id} column and dropping all columns except
\texttt{text}, and then annotated them using the \texttt{udpipe}
package. We then calculated the number of T-units and words per sentence
and created the variables \texttt{t\_units} and \texttt{word\_len} for
each.

These steps produced two datasets for both the natives and for the
translations. The first dataset for each is the annotated data frame.
The second is the data frame with the sytactic complexity measures we
calculated. The annotated data frames are named
\texttt{enntt\_natives\_ann\_tbl} and
\texttt{enntt\_translations\_ann\_tbl}. The data frames with the
syntactic complexity measures are named
\texttt{enntt\_natives\_syn\_comp\_tbl} and
\texttt{enntt\_translations\_syn\_comp\_tbl}.

In the end, we want a dataset that looks something like
Table~\ref{tbl-td-integration-idealized}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0698}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1279}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0814}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0930}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.6279}}@{}}
\caption{Idealized integrated dataset for the syntactic simplification
investigation.}\label{tbl-td-integration-idealized}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
doc\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
t\_units
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
word\_len
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
doc\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
t\_units
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
word\_len
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & natives & 1 & 5 & I am happy right now. \\
2 & translation & 3 & 11 & I think that John believes that Mary is a
good person. \\
\end{longtable}

To create this unified dataset, we will need to apply joins and
concatenation. First, we will join the prepped datasets with the
annotated datasets. Then, we will concatenate the two resulting
datasets.

Let's start by joining the annotated datasets
(\texttt{enntt\_natives\_ann\_tbl} and
\texttt{enntt\_translations\_ann\_tbl}) with the datasets with the
syntactic complexity calculations
(\texttt{enntt\_natives\_syn\_comp\_tbl} and
\texttt{enntt\_translations\_syn\_comp\_tbl}). In these joins, we can
see that the prepped and calculated datasets share a couple variables,
\texttt{doc\_id} and \texttt{sentence\_id}, in
Example~\ref{exm-td-merging-join-prepped-syn-comp}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-prepped-syn-comp}{}\label{exm-td-merging-join-prepped-syn-comp}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview datasets to join}
\NormalTok{enntt\_natives\_ann\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 3 x 17
>   doc_id paragraph_id sentence_id sentence    start   end term_id token_id token
>   <chr>         <int>       <int> <chr>       <int> <int>   <int> <chr>    <chr>
> 1 1                 1           1 It is extr~     1     2       1 1        It   
> 2 1                 1           1 It is extr~     4     5       2 2        is   
> 3 1                 1           1 It is extr~     7    15       3 3        extr~
> # i 8 more variables: lemma <chr>, upos <chr>, xpos <chr>, feats <chr>,
> #   head_token_id <chr>, dep_rel <chr>, deps <chr>, misc <chr>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{enntt\_natives\_syn\_comp\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 3 x 4
>   doc_id sentence_id t_units word_len
>   <chr>        <int>   <int>    <int>
> 1 1                1       4       21
> 2 10               1       2       25
> 3 100              1       1       27
\end{verbatim}

\end{example}

The \texttt{doc\_id} and \texttt{sentence\_id} variables are both keys
that we will use to join the datasets. The reason being that if we only
use one of the two we will not align the two datasets at the sentence
level. Only the combination of \texttt{doc\_id} and
\texttt{sentence\_id} isolates the sentences for which we have syntactic
complexity measures. Beyond a having common variable (or variables in
our case), we must also ensure that join key variables are of the same
vector type in both data frames and that we are aware of any differences
in the values. From the output in
Example~\ref{exm-td-merging-join-prepped-syn-comp}, we can see that the
\texttt{doc\_id} and \texttt{sentence\_id} variables aligned in terms of
vector type; \texttt{doc\_id} is character and \texttt{sentence\_id} is
integer in both data frames. If they happened not to be, there types
would need to be adjusted.

Now, we need to check for differences in the values. We can do this by
using the \texttt{setequal()} function. This function returns
\texttt{TRUE} if the two vectors are equal and \texttt{FALSE} if they
are not. If the two vectors are not equal, the function will return the
values that are in one vector but not the other. So if one has
\texttt{10001} and the other doesn't we will get \texttt{FALSE}. Let's
see this in practice, as seen in
Example~\ref{exm-td-merging-join-prepped-syn-comp-check}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-prepped-syn-comp-check}{}\label{exm-td-merging-join-prepped-syn-comp-check}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check for differences in the values}
\FunctionTok{setequal}\NormalTok{(}
\NormalTok{  enntt\_natives\_ann\_tbl}\SpecialCharTok{$}\NormalTok{doc\_id,}
\NormalTok{  enntt\_natives\_syn\_comp\_tbl}\SpecialCharTok{$}\NormalTok{doc\_id}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{setequal}\NormalTok{(}
\NormalTok{  enntt\_natives\_ann\_tbl}\SpecialCharTok{$}\NormalTok{sentence\_id,}
\NormalTok{  enntt\_natives\_syn\_comp\_tbl}\SpecialCharTok{$}\NormalTok{sentence\_id}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] TRUE
\end{verbatim}

\end{example}

So the values are the same. The final check is to see if the vectors are
of the same length. We know the values are the same, but we don't know
if the values are repeated. We do this by simply comparing the length of
the vectors, as seen in
Example~\ref{exm-td-merging-join-prepped-syn-comp-length}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-prepped-syn-comp-length}{}\label{exm-td-merging-join-prepped-syn-comp-length}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check for differences in the length}
\FunctionTok{length}\NormalTok{(enntt\_natives\_ann\_tbl}\SpecialCharTok{$}\NormalTok{doc\_id) }\SpecialCharTok{==}
  \FunctionTok{length}\NormalTok{(enntt\_natives\_syn\_comp\_tbl}\SpecialCharTok{$}\NormalTok{doc\_id)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(enntt\_natives\_ann\_tbl}\SpecialCharTok{$}\NormalTok{sentence\_id) }\SpecialCharTok{==}
  \FunctionTok{length}\NormalTok{(enntt\_natives\_syn\_comp\_tbl}\SpecialCharTok{$}\NormalTok{sentence\_id)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] FALSE
\end{verbatim}

\end{example}

So they are not the same length. Using the \texttt{nrow()} function, I
can see that the annotated dataset has 264,124 observations and the
calculated dataset has 10,199 observations. The annotation data frames
will have many more observations due to the fact that the unit of
observations is word tokens. The recoded syntactic complexity data
frames' unit of observation is the sentence.

To appreciate the difference in the number of observations, let's look
at the first 10 observations of the natives annotated frame for just the
columns of interest, as seen in
Example~\ref{exm-td-merging-join-prepped-syn-comp-ann}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-prepped-syn-comp-ann}{}\label{exm-td-merging-join-prepped-syn-comp-ann}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview the annotated dataset}
\NormalTok{enntt\_natives\_ann\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(doc\_id, sentence\_id, sentence, token) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>    doc_id sentence_id sentence                                             token
>    <chr>        <int> <chr>                                                <chr>
>  1 1                1 It is extremely important that action is taken to e~ It   
>  2 1                1 It is extremely important that action is taken to e~ is   
>  3 1                1 It is extremely important that action is taken to e~ extr~
>  4 1                1 It is extremely important that action is taken to e~ impo~
>  5 1                1 It is extremely important that action is taken to e~ that 
>  6 1                1 It is extremely important that action is taken to e~ acti~
>  7 1                1 It is extremely important that action is taken to e~ is   
>  8 1                1 It is extremely important that action is taken to e~ taken
>  9 1                1 It is extremely important that action is taken to e~ to   
> 10 1                1 It is extremely important that action is taken to e~ ensu~
\end{verbatim}

\end{example}

The annotated data frames have a lot of redundancy in for the join
variables and the \texttt{sentence} variable that we want to add to the
calculated data frames. We can reduce the redundancy by using the
\texttt{distinct()} function from the \texttt{dplyr} package. In this
case we want all observations where \texttt{doc\_id},
\texttt{sentence\_id} and \texttt{sentence} are distinct. We then select
these variables with \texttt{distinct()}, as seen in
Example~\ref{exm-td-merging-annotation-distinct}.

\begin{example}[]\protect\hypertarget{exm-td-merging-annotation-distinct}{}\label{exm-td-merging-annotation-distinct}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reduce annotated data frames to unique sentences}
\NormalTok{enntt\_natives\_ann\_distinct }\OtherTok{\textless{}{-}}
\NormalTok{  enntt\_natives\_ann\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{distinct}\NormalTok{(doc\_id, sentence\_id, sentence)}

\NormalTok{enntt\_translations\_ann\_distinct }\OtherTok{\textless{}{-}}
\NormalTok{  enntt\_translations\_ann\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{distinct}\NormalTok{(doc\_id, sentence\_id, sentence)}
\end{Highlighting}
\end{Shaded}

\end{example}

We now have two datasets that are ready to be joined with the recoded
datasets. The next step is to join the two. We will employ a left join
where the syntactic complexity data frames are on the left and the join
variables will be both the \texttt{doc\_id} and \texttt{sentence\_id}
variables. The code is seen in
Example~\ref{exm-td-merging-join-left-syn-comp}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-left-syn-comp}{}\label{exm-td-merging-join-left-syn-comp}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Join the native datasets}
\NormalTok{enntt\_natives\_transformed\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{left\_join}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ enntt\_natives\_syn\_comp\_tbl,}
    \AttributeTok{y =}\NormalTok{ enntt\_natives\_ann\_distinct,}
    \AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"doc\_id"}\NormalTok{, }\StringTok{"sentence\_id"}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Preview}
\FunctionTok{glimpse}\NormalTok{(enntt\_natives\_transformed\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 10,199
> Columns: 5
> $ doc_id      <chr> "1", "10", "100", "1000", "10000", "1001", "1002", "1003",~
> $ sentence_id <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~
> $ t_units     <int> 4, 2, 1, 0, 3, 2, 1, 2, 1, 2, 2, 1, 4, 2, 2, 4, 3, 2, 2, 1~
> $ word_len    <int> 21, 25, 27, 15, 40, 43, 29, 23, 13, 30, 33, 9, 68, 35, 16,~
> $ sentence    <chr> "It is extremely important that action is taken to ensure ~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Join the translations datasets}
\NormalTok{enntt\_translations\_transformed\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{left\_join}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ enntt\_translations\_syn\_comp\_tbl,}
    \AttributeTok{y =}\NormalTok{ enntt\_translations\_ann\_distinct,}
    \AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"doc\_id"}\NormalTok{, }\StringTok{"sentence\_id"}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Preview}
\FunctionTok{glimpse}\NormalTok{(enntt\_translations\_transformed\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 10,392
> Columns: 5
> $ doc_id      <chr> "1", "10", "100", "1000", "10000", "1001", "1002", "1003",~
> $ sentence_id <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~
> $ t_units     <int> 0, 2, 0, 1, 3, 0, 3, 2, 3, 3, 2, 0, 1, 0, 3, 1, 0, 1, 2, 2~
> $ word_len    <int> 24, 31, 5, 39, 44, 26, 67, 23, 46, 28, 24, 68, 19, 18, 36,~
> $ sentence    <chr> "To my great surprise , on leaving the sitting , I found t~
\end{verbatim}

\end{example}

The two data frames now have the same columns and we are closer to our
final dataset. The next step is to move toward concatenating the two
datasets. Before we do that, we need to do some preparation. First, and
most important, we need to add a \texttt{type} column to each dataset.
This column will indicate whether the sentence is a native or a
translation. The second is that our \texttt{doc\_id} does not serve as a
unique identifier for the sentences. Only in combination with
\texttt{sentence\_id} can we uniquely identify a sentence.

So our plan will be to add a \texttt{type} column to each dataset
specifying the values for all the observations in the respective
dataset. Then we will concatenate the two datasets. Note, if we combine
them before, distiguishing the type will be more difficult. After we
concatenate the two datasets, we will add a \texttt{doc\_id} column that
will serve as a unique identifier for the sentences and drop the
\texttt{sentence\_id} column. OK, that's the plan. Let's execute it in
Example~\ref{exm-td-merging-concatenation}.

\begin{example}[]\protect\hypertarget{exm-td-merging-concatenation}{}\label{exm-td-merging-concatenation}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add a type column}
\NormalTok{enntt\_natives\_transformed\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  enntt\_natives\_transformed\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"natives"}\NormalTok{)}

\NormalTok{enntt\_translations\_transformed\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  enntt\_translations\_transformed\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"translations"}\NormalTok{)}

\CommentTok{\# Concatenate the datasets}
\NormalTok{enntt\_transformed\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{bind\_rows}\NormalTok{(}
\NormalTok{    enntt\_natives\_transformed\_tbl,}
\NormalTok{    enntt\_translations\_transformed\_tbl}
\NormalTok{  )}

\CommentTok{\# Overwrite the doc\_id column with a unique identifier}
\NormalTok{enntt\_transformed\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  enntt\_transformed\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{doc\_id =} \FunctionTok{row\_number}\NormalTok{()) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(doc\_id, type, t\_units, word\_len, }\AttributeTok{text =}\NormalTok{ sentence)}

\CommentTok{\# Preview}
\FunctionTok{glimpse}\NormalTok{(enntt\_transformed\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 20,591
> Columns: 5
> $ doc_id   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18~
> $ type     <chr> "natives", "natives", "natives", "natives", "natives", "nativ~
> $ t_units  <int> 4, 2, 1, 0, 3, 2, 1, 2, 1, 2, 2, 1, 4, 2, 2, 4, 3, 2, 2, 1, 1~
> $ word_len <int> 21, 25, 27, 15, 40, 43, 29, 23, 13, 30, 33, 9, 68, 35, 16, 53~
> $ text     <chr> "It is extremely important that action is taken to ensure tha~
\end{verbatim}

\end{example}

The output of Example~\ref{exm-td-merging-concatenation} now looks like
Table~\ref{tbl-td-integration-idealized}. We have a dataset that has the
syntactic complexity measures for both the natives and the translations.
We can now write this dataset to disk and document it in the data
dictionary.

\section*{Activities}\label{activities-5}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

In the following activities, you will review the concept of transforming
data to prepare it for analysis and working to implement these steps
with R. This includes preparation and enrichment of curated datasets
using normalization, tokenization, recoding, generation, and/ or
integration strategies.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-7.html}{Transforming
and documenting datasets}\\
\textbf{How}: Read Recipe 7, complete comprehension check, and prepare
for Lab 7.\\
\textbf{Why}: To work with to primary types of transformations,
tokenization and joins. Tokenization is the process of recasting textual
units as smaller textual units. The process of joining datasets aims to
incorporate other datasets to augment or filter the dataset of interest.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-07}{Dataset alchemy}\\
\textbf{How}: Fork, clone, and complete the steps in Lab 7.\\
\textbf{Why}: To gain experience working with coding strategies for
transforming datasets using tidyverse functions and regular expressions,
practice reading/ writing data from/ to disk, and implement
organizational strategies for organizing and documenting a dataset in
reproducible fashion.

\end{tcolorbox}

\section*{Summary}\label{summary-6}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we covered the process of transforming datasets. The
goal is to manipulate the curated dataset to make it align better for
analysis. We covered various types of transformation procedures from
text normalization to data frame integrations. In any given research
project some or all of these steps will be employed --but not
necessarily in the order presented in this chapter. It is not uncommon
to mix procedures as well. The etiology of the transformation is as
unique as the data that you are working with.

Since you are applying techniques that have a significant factor on the
shape and contents of your dataset(s) it is important to perform data
checks to ensure that the transformations are working as expected. You
may not catch everything, and some things may not be caught until later
in the analysis process, but it is important to do as much as you can as
early as you can.

In line with the reproducible research principles, it is important to
write the transformed dataset to disk and to document it in the data
dictionary. This is especially important if you are working with
multiple datasets. Good naming conventions also come into play. Choosing
descriptive names is so easily overlooked by your present self but so
welcomed by your future self.

\part{Analysis}

In this section we turn to the analysis of datasets, the evaluation of
results, and the interpretation of the findings. We will outline the
three main types of statistical analyses: Exploratory Data Analysis
(EDA), Predictive Data Analysis (PDA), and Inferential Data Analysis
(IDA). Each of these analysis types have distinct, non-overlapping aims
and therefore should be determined from the outset of the research
project and included as part of the research blueprint. The aim of this
section is to establish a clearer picture of the goals, methods, and
value of each of these approaches.

\chapter{Explore}\label{sec-exploration}

\begin{quote}
The data speaks for itself, but only if we are willing to listen.

--- Nate Silver
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Identify when an exploratory data analysis approach is the best fit
  for a given research project.
\item
  Describe the fundamental methods of descriptive analysis and
  unsupervised learning, recognizing their strengths in revealing
  patterns and summarizing data.
\item
  Interpret the basic insights gained from data summarization and
  pattern recognition, considering how these insights could guide
  further questions or research.
\end{itemize}

\end{tcolorbox}

In this chapter, we examine a wide range of strategies for deriving
insight from data in cases where the researcher does not start with a
preconceived hypothesis or prediction, but rather the researcher aims to
uncover patterns and associations from data allowing the data to guide
the trajectory of the analysis. The chapter outlines two main branches
of exploratory data analysis: descriptive analysis which statistically
and/ or visually summarizes a dataset and unsupervised learning which is
a machine learning approach that does not assume any particular
relationship between variables in a dataset. Either through descriptive
or unsupervised learning methods, exploratory data analysis employs
quantitative methods to summarize, reduce, and sort complex datasets and
statistically and visually interrogate a dataset in order to provide the
researcher novel perspective to be qualitatively assessed.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{terminal} Lessons}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Advanced
objects}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To learn about advanced objects in R, including lists and
matrices, and create, inspect, access, and manipulate these objects.

\end{tcolorbox}

\section{Orientation}\label{sec-eda-orientation}

The aim of this section is to provide an overview of exploratory data
analysis (EDA). We will delve into various descriptive methods, such as
frequency analysis and co-occurrence analysis, which are fundamental
tools in linguistic research. However, our exploration won't stop there.
We will also integrate modern exploratory methods from unsupervised
learning approaches, including clustering, dimensionality reduction, and
vector space modeling. This may sound overwhelming, but I will strive to
keep explanations clear and concise, ensuring their practicality and
relevance to your linguistic inquiries is apparent. To this end, we will
provide real-world examples to exemplify the applicability of these
methodologies.

\subsection{Goals and approach}\label{sec-eda-goals-approach}

As discussed in Section~\ref{sec-aa-explore} and
Section~\ref{sec-fr-aim}, the goal of exploratory data analysis is to
discover, describe, and posit new hypotheses. This analysis approach is
best-suited for research where the literature on a research question is
limited, or where the researcher is interested in exploring a new
research question.

Since the researcher does not start with a preconceived hypothesis, the
researcher is not able to test a hypothesis and generalize to a
population, but rather the researcher is able to describe the data and
provide a new perspective to be qualitatively assessed. This is achieved
through an iterative and inductive process of data exploration, where
the researcher uses quantitative methods to summarize, reduce, and sort
complex datasets and statistically and visually interrogate a dataset
letting the data guide the analysis.

To reign in the analysis, however, it is important to have a research
question to guide the analysis. The research question will often be
broad and exploratory in nature, but it will provide a framework for the
analysis including the unit of analysis and sometimes the units of
observation. Yet the units of observation can be modified as needed to
address the research question. Furthermore, the methods applied to the
data can evolve as the research unfolds. The researcher may start with a
descriptive analysis and then move to an unsupervised learning approach,
or vice versa. The researcher may also pivot the approach to explore new
questions and new variables.

With a research question and relevant data in hand, we can look to
conduct the analysis. The general workflow for exploratory data analysis
is shown in Table~\ref{tbl-eda-workflow}.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.8500}}@{}}
\caption{Workflow for exploratory data
analysis}\label{tbl-eda-workflow}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Identify & Consider the research question and identify variables of
potential interest to provide insight into our question. \\
2 & Inspect & Check for missing data, outliers, \emph{etc}. and check
data distributions and transform if necessary. \\
3 & Interrogate & Submit the selected variables to descriptive
(frequency, keyword, co-occurrence analysis, \emph{etc.}) or
unsupervised learning (clustering, dimensionality reduction, vector
spacing modeling, \emph{etc.}) methods to provide quantitative measures
to evaluate. \\
4 & Interpret & Evaluate the results and determine if they are valid and
meaningful to respond to the research question. \\
5 & Iterate (Optional) & Repeat steps 1-4 as new questions emerge from
your interpretation. \\
\end{longtable}

Let's now demonstrate the workflow in practice.

\section{Analysis}\label{sec-eda-analysis}

To frame our discussion of exploratory data analysis, let's tackle a
task. The task will be to identify relevant materials for an English
Language Learner (ELL) textbook. This will involve multiple research
questions and allow us to illustrate some very fundamental concepts that
emerge across text analysis research in both descriptive and
unsupervised learning approaches.

Since our task is geared towards English language use, we will want a
data source that is representative of English language use. For this, we
will use the Manually Annotated Sub-Corpus (MASC) of the American
National Corpus (\citeproc{ref-Ide2008}{Ide et al. 2008}).

The data dictionary for the dataset we will use as our point of
departure is shown in Table~\ref{tbl-eda-masc-dd-show}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0973}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1504}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1416}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5929}}@{}}

\caption{\label{tbl-eda-masc-dd-show}Data dictionary for the MASC
dataset.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable\_type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
doc\_id & Document ID & numeric & Unique identifier for each document \\
modality & Modality & categorical & The form in which the document is
presented (written or spoken) \\
genre & Genre & categorical & The category or type of the document \\
term\_num & Term Number & numeric & Index number term per document \\
term & Term & categorical & Individual word forms in the document \\
lemma & Lemma & categorical & Base or dictionary form of the term \\
pos & Part of Speech & categorical & Grammatical category of the term
(modified PENN Treebank tagset) \\

\end{longtable}

First, I'll read in and preview the dataset in
Example~\ref{exm-eda-masc-read}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-read}{}\label{exm-eda-masc-read}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read the dataset}
\NormalTok{masc\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"../data/masc/masc\_transformed.csv"}\NormalTok{)}

\CommentTok{\# Preview the MASC dataset}
\NormalTok{masc\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 5 x 7
>   doc_id modality genre   term_num term         lemma        pos  
>    <dbl> <chr>    <chr>      <dbl> <chr>        <chr>        <chr>
> 1      1 Written  Letters        0 December     december     NNP  
> 2      1 Written  Letters        1 1998         1998         CD   
> 3      1 Written  Letters        2 Your         your         PRP$ 
> 4      1 Written  Letters        3 contribution contribution NN   
> 5      1 Written  Letters        4 to           to           TO
\end{verbatim}

\end{example}

From the output in Example~\ref{exm-eda-masc-read}, we get some sense of
the structure of the dataset. However, we also need to perform
diagnostic and descriptive procedures. This will include checking for
missing data and anomalies and assessing central tendency, dispersion,
and/ or distributions of the variables. This may include using
\texttt{skimr}, \texttt{dplyr}, \texttt{stringr}, \texttt{ggplot2},
\emph{etc.} to identify the most relevant variables for our task and to
identify any potential issues with the dataset.

After a descriptive and diagnostic assessment of the dataset, I
identified and addressed missing data and anomalies (including many
non-words). I also recoded the \texttt{doc\_id} variable to a character
variable. The dataset now has 486,368 observations, a reduction from the
original 591,036 observations. There are 392 documents, 2 modalities, 18
genres, almost 38k unique terms (which are words), almost 26k lemmas
(word base forms), and 34 distinct part of speech tags.

\subsection{Descriptive analysis}\label{sec-eda-descriptive}

Descriptive analysis includes common techniques such as frequency
analysis to determine the most frequent words or phrases, dispersion
analysis to see how terms are distributed throughout a document or
corpus, keyword analysis to identify distinctive terms, and/ or
co-occurrence analysis to see what terms tend to appear together.

Using the MASC dataset, we will entertain questions such as:

\begin{itemize}
\tightlist
\item
  What are the most common terms a beginning ELL should learn?
\item
  Are there term differences between spoken and written discourses that
  should be emphasized?
\item
  What are the most common verb particle constructions?
\end{itemize}

Along the way, we will introduce touch on frequency, dispersion, and
co-occurrence measures. In addition, we will apply various descriptive
analysis techniques and visualizations to explore the dataset and
identify new questions and new variables of interest.

\subsubsection{Frequency analysis}\label{sec-eda-frequency}

At its core, frequency analysis is a descriptive method that counts the
number of times a linguistic unit occurs in a dataset. The results of
frequency analysis can be used to describe the dataset and to identify
terms that are linguistically distinctive or distinctive to a particular
group or sub-group in the dataset.

\paragraph{Raw frequency}\label{sec-eda-frequency-raw}

Let's consider what the most common words in the MASC dataset are as a
starting point to making inroads on our task by identifying relevant
vocabulary for an ELL textbook.

In the \texttt{masc\_tbl} data frame we have the linguistic unit
\texttt{term} which corresponds to the word-level annotation of the
MASC. The \texttt{lemma} corresponds to the base form of each term, for
words with inflectional morphology, the lemma is the word sans the
inflection (\emph{e.g.} is - be, are - be). For other words, the
\texttt{term} and the \texttt{lemma} will be the same (\emph{e.g.} the -
the, in - in). These two variables pose a choice point for us: do we
consider words to be the actual forms or the base forms? There is an
argument to be made for both. In this case, I will operationalize our
linguistic unit as the \texttt{lemma} variable, as this will allow us to
group words with distinct inflectional morphology together.

To perform a basic word frequency analysis, we can apply
\texttt{summarize()} in combination with \texttt{n()} or the convenient
\texttt{count()} function from the \texttt{dplyr} package. Our sorted
lemma counts appear in Example~\ref{exm-eda-masc-count}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-count}{}\label{exm-eda-masc-count}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Lemma count, sorted in descending order}
\NormalTok{masc\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(lemma, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 25,923 x 2
>    lemma     n
>    <chr> <int>
>  1 the   26137
>  2 be    19466
>  3 to    13548
>  4 and   12528
>  5 of    12005
>  6 a     10461
>  7 in     8374
>  8 i      7783
>  9 that   7082
> 10 you    5276
> # i 25,913 more rows
\end{verbatim}

\end{example}

The output of this frequency tabulation in
Example~\ref{exm-eda-masc-count} is a data frame with two columns:
\texttt{lemma} and \texttt{n}.

At this point, it is important to define the vocabulary for concepts
that are fundamental to working with text. First, a \textbf{term} is a
defined linguistic unit extracted from a corpus. In our dataset, the
terms are words, such as `the', `houses', `are'. A lemma is an annotated
recoding of words which represent the uninflected base form of a word.
In either case, the term or lemma is an instance of a linguistic unit.
The individual instances of a linguistic unit are called
\textbf{tokens}. When we count the number of times a term or lemma
occurs in a dataset, we are counting the number of tokens (\texttt{n}),
such as in Example~\ref{exm-eda-masc-count}. Now, the list of unique
terms is a list of \textbf{types} (\texttt{lemma}). By definition, then,
there will always be at least as many tokens as types, but more often
than not (many) more tokens than types.

As we discussed in Section~\ref{sec-aa-distributions}, the frequency of
linguistic units in a corpus tends to be highly right-skewed
distribution, approximating the Zipf distribution. If we calculate the
cumulative frequency of the lemmas in the \texttt{masc\_tbl} data frame,
we can see that the top 10 types account for around 25\% of the lemmas
used in the entire corpus --by 100 types that increases to near 50\% and
1,000 around 75\%, as seen in
Example~\ref{exm-eda-masc-count-cumulative}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-count-cumulative}{}\label{exm-eda-masc-count-cumulative}

~

\begin{figure}[H]

\centering{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-count-cumulative-1.pdf}

}

\caption{\label{fig-eda-masc-count-cumulative}Cumulative frequency of
lemmas in the MASC dataset}

\end{figure}%

\end{example}

If we look at the types that appear within the first 50 most frequent,
you can likely also appreciate another thing about language use. Let's
list the top 50 types in Example~\ref{exm-eda-masc-count-top-50}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-count-top-50}{}\label{exm-eda-masc-count-top-50}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Top 50 types}
\NormalTok{lemma\_cumul\_freq }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(lemma) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{matrix}\NormalTok{(}\AttributeTok{ncol =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{col.names =} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llllllllll@{}}

\caption{\label{tbl-eda-masc-count-top-50}Top 50 lemma types in the MASC
dataset.}

\tabularnewline

\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
the & a & have & with & at & will & your & his & all & know \\
be & in & it & we & from & my & an & if & there & get \\
to & i & for & as & he & or & say & 's & me & make \\
and & that & on & this & but & n't & what & can & would & out \\
of & you & do & not & by & they & so & go & about & up \\

\end{longtable}

\end{example}

For the most part, the most frequent words are not content words, but
rather function words (\emph{e.g.} determiners, prepositions, pronouns,
auxiliary verbs). Function words include a closed class of relatively
few words that are used to express grammatical relationships between
content words. It then is no surprise that they are the comprise many of
the most frequent words in a corpus.

Another key observation is that among the most frequency content words
(\emph{e.g.} nouns, verbs, adjectives, adverbs) are words that are quite
semantically generic --that is, they are words that are used in a wide
range of contexts and take a wide range of meanings.

Take for example the adjective `good'. It can be used to describe a wide
range of nouns, such as `good food', `good people', `good times',
\emph{etc}. A sometimes near-synonym of `good', for example `good
student', is the word `studious'. Yet, `studious' is not as frequent as
`good' as it is used to describe a narrower range of nouns, such as
`studious student', `studious scholar', `studious researcher',
\emph{etc}. In this way, `studious' is more semantically specific than
`good'.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

Based on what you now know about the expected distribution of words in a
corpus, what if your were asked to predict what the most frequency
English word used is in each U.S. State? What would you predict? How
confident would you be in your prediction? What if you were asked to
predict what the most frequency word used is in the language of a given
country? What would you want to know before making your prediction?

\end{tcolorbox}

So common across corpus samples, in some analyses these usual suspects
of the most common words are considered irrelvant and are filtered out.
In our ELL materials task, however, we might exclude them for this
simple fact that it will be a given that we will teach these words given
their grammatical importance. If we want to focus on the most common
content words, we can filter out the function words.

One approach to filtering out these words is to use a pre-determined
list of \textbf{stopwords}. The \texttt{tidytext} package includes a
data frame \texttt{stop\_words} of stopword lexicons for English. We can
select a lexicon from \texttt{stop\_words} and use \texttt{anti\_join()}
to filter out the words that appear in the \texttt{word} variable from
the \texttt{lemma} variable in the \texttt{masc\_tbl} data frame.

Eliminating words in this fashion, however, may not always be the best
approach. Available lists of stopwords vary in their contents and are
determined by other researchers for other potential uses. We may instead
opt to create our own stopword list that is tailored to the task, or we
may opt to use a statistical approach based on their distribution in the
dataset using frequency and/ or dispersion measures.

For our case, however, we have another strategy to apply. Since our task
is to identify relevant vocabulary, beyond the fundamental function
words in English, we can use the part of speech tags to reduce our
dataset to just the content words, that is nouns, verbs, adjectives, and
adverbs. We need to consult the Penn Tagset again, to ensure we are
selecting the correct tags. I will assign this data frame to
\texttt{masc\_content\_tbl} to keep it separate from our main data frame
\texttt{masc\_tbl}, seen in Example~\ref{exm-eda-masc-filter-pos}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-filter-pos}{}\label{exm-eda-masc-filter-pos}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Penn Tagset for content words}
\CommentTok{\# Nouns: NN, NNS,}
\CommentTok{\# Verbs: VB, VBD, VBG, VBN, VBP, VBZ}
\CommentTok{\# Adjectives: JJ, JJR, JJS}
\CommentTok{\# Adverbs: RB, RBR, RBS}

\NormalTok{content\_pos }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"NN"}\NormalTok{, }\StringTok{"NNS"}\NormalTok{, }\StringTok{"VB"}\NormalTok{, }\StringTok{"VBD"}\NormalTok{, }\StringTok{"VBG"}\NormalTok{, }\StringTok{"VBN"}\NormalTok{, }\StringTok{"VBP"}\NormalTok{, }\StringTok{"VBZ"}\NormalTok{, }\StringTok{"JJ"}\NormalTok{, }\StringTok{"JJR"}\NormalTok{, }\StringTok{"JJS"}\NormalTok{, }\StringTok{"RB"}\NormalTok{, }\StringTok{"RBR"}\NormalTok{, }\StringTok{"RBS"}\NormalTok{)}

\CommentTok{\# Select content words}
\NormalTok{masc\_content\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(pos }\SpecialCharTok{\%in\%}\NormalTok{ content\_pos)}

\CommentTok{\# Preview top 50}
\NormalTok{masc\_content\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(lemma, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(lemma) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{matrix}\NormalTok{(}\AttributeTok{ncol =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{col.names =} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llllllllll@{}}

\caption{\label{tbl-eda-masc-filter-pos}Frequency of tokens in the MASC
dataset after filtering out lemmas with part of speech tags that are not
content words}

\tabularnewline

\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
be & say & think & other & work & look & also & new & t & man \\
have & go & more & see & year & then & need & find & first & ask \\
do & know & just & people & come & right & way & give & help & very \\
not & get & time & take & use & only & back & thing & day & much \\
n't & make & so & now & well & want & here & tell & many & even \\

\end{longtable}

\end{example}

The resulting list in Table~\ref{tbl-eda-masc-filter-pos} paints a
different picture of the most frequent words in the dataset. The most
frequent words are now content words, and included in most frequent
words are more semantically specific words. We now have reduced the
number of observations by 50\% focusing on the content words. We are
getting closer to identifying the vocabulary that we want to include in
our ELL materials, but we will need some more tools to help us identify
the most relevant vocabulary.

\paragraph{Dispersion}\label{sec-eda-frequency-dispersion}

\textbf{Dispersion} is a measure of how evenly distributed a linguistic
unit is across a dataset. This is a key concept in text analysis, as
important as frequency. It is important to recognize that frequency and
dispersion are measures of different characteristics. We can have two
words that occur with the same frequency, but one word may be more
evenly distributed across a dataset than the other. Depending on the
researcher's aims, this may be an important distinction to make. For our
task, it is likely the case that we want to capture words that are
well-dispersed across the dataset as words that have a high frequency
and a low dispersion tend to be connected to a particular context,
whether that be a particular genre, a particular speaker, a particular
topic, \emph{etc}. In other research, aim may be the reverse; to
identify words that are highly frequent and highly concentrated in a
particular context to identify words that are distinctive to that
context.

There are a variety of measures that can be used to estimate the
distribution of types across a corpus. Let's focus on three measures:
document frequency (\(df\)), inverse document frequency (\(idf\)), and
Gries' Deviation of Proportions (\(dp\)).

The most basic measure is \textbf{document frequency} (\(df\)). This is
the number of documents in which a type appears at least once. For
example, if a type appears in 10 documents, then the document frequency
is 10. This is a very basic measure, but it is a good starting point.

A nuanced version of document frequency is \textbf{inverse document
frequency} (\(idf\)). This measure takes the total number of documents
and divides it by the document frequency. This results in a measure that
is inversely proportional to the document frequency. That is, the higher
the document frequency, the lower the inverse document frequency. This
measure is often log-transformed to spread out the values.

One thing to consider about \(df\) and \(idf\) is that niether takes
into account the length of the documents in which the type appears nor
the spread of the type within each document. To take these factors into
account, we can use Gries' Deviation of Proportions (\(dp\)) measure
(\citeproc{ref-Gries2023}{S. T. Gries 2023, 87--88}). The \(dp\) measure
is calculated as the difference between the proportion of a tokens in a
document and tokens in the corpus. The metric can be subtracted from 1
to create a normalized measure of dispersion ranging between 0 and 1,
with lower values being more dispersed.

Let's consider how these measures differ with three scenarios:

Imagine a type with a token frequency of 100 appears in each of the 10
documents in a corpus.

A. Each of the documents is 100 words long. Te type appears 10 times in
each document.\\
B. Each of the documents is 100 words long. But now the type appears
once in 9 documents and 91 times in 1 document.\\
C. Nine of the documents constitute 99\% of the corpus. The type appears
once in each of the 9 documents and 91 times in the 10th document.

Scenario A is the most dispersed, scenario B is less dispersed, and
scenario C is the least dispersed. Yet, the type's \(df\) and \(idf\)
scores will be the same. But the \(dp\) score will reflect increasing
concentration of the type from A to B to C.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper} Dispersion measures

You may wonder why we would want to use \(df\) or \(idf\) at all. The
answer is some combination of the fact that they are computationally
less expensive to calculate, they are widely used (especially \(idf\)),
and/ or in many practical situations they often highly correlated with
\(dp\).

\end{tcolorbox}

So for our task we will use \(dp\) as our measure of dispersion. The
\texttt{qtalrkit} package includes the \texttt{calc\_type\_metrics()}
function which calculates, among other metrics, the dispersion metrics
\(df\), \(idf\), and/ or \(dp\). Let's select \texttt{dp} and assign the
result to \texttt{masc\_lemma\_disp}, as seen in
Example~\ref{exm-eda-masc-dp}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dp}{}\label{exm-eda-masc-dp}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(qtalrkit)}

\CommentTok{\# Calculate deviance of proportions (DP)}
\NormalTok{masc\_lemma\_disp }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_content\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calc\_type\_metrics}\NormalTok{(}
    \AttributeTok{type =}\NormalTok{ lemma,}
    \AttributeTok{documents =}\NormalTok{ doc\_id,}
    \AttributeTok{dispersion =} \StringTok{"dp"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(dp)}

\CommentTok{\# Preview}
\NormalTok{masc\_lemma\_disp }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 3
>    type      n    dp
>    <chr> <dbl> <dbl>
>  1 be    19231 0.123
>  2 have   5136 0.189
>  3 not    2279 0.240
>  4 make   1149 0.266
>  5 other   882 0.269
>  6 more   1005 0.276
>  7 take    769 0.286
>  8 only    627 0.286
>  9 time    931 0.314
> 10 see     865 0.327
\end{verbatim}

\end{example}

We would like to identify lemmas that are frequent and well-dispersed.
But an important question arises, what is the threshold for frequency
and dispersion that we should use to identify the lemmas that we want to
include in our ELL materials?

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

You may be wondering why the Inverse Document Frequency is, in fact, the
inverse of the document counts, instead of just a count of the documents
that each type appears in. The \(idf\) is a very common measure in
machine learning that is used in combination with (term) frequency to
calculate the \(tf\)-\(idf\) (term frequency-inverse document frequency)
measure. That is, the product of the frequency of a term and the inverse
document frequency of the term. This serves as a weighting measure that
lowers the \(tf\)-\(idf\) score for terms that are frequent across
documents and increases the \(tf\)-\(idf\) score for terms that are
infrequent across documents.

Consider what types will end up with a high or a low \(tf\)-\(idf\)
score. What use(s) could this measure have for distinguishing between
types in a corpus?

Hint: consider the earlier discussion of stopword lists.

\end{tcolorbox}

There are statistical approaches to identifying natural breakpoints,
including clustering, but a visual inspection is often good enough for
practical purposes. Let's create a density plot to see if there is a
natural break in the distribution of our dispersion measure, as seen in
Figure~\ref{fig-eda-masc-dp-density}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dp-density}{}\label{exm-eda-masc-dp-density}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Density plot of dp}
\NormalTok{masc\_lemma\_disp }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ dp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, .}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Deviation of Proportions"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-dp-density-1.pdf}

}

\caption{\label{fig-eda-masc-dp-density}Density plot of Deviation of
Proportions for lemmas in the MASC dataset}

\end{figure}%

\end{example}

What we are looking for is an elbow in the distribution of dispersion
measures. In Figure~\ref{fig-eda-masc-dp-density}, we can see that there
is distinctive bend in the distribution between .85 and .97. This bend
is called an elbow, and using this approach to make informed decisions
about thresholds is called the \textbf{elbow method}. We can split the
difference and use this as a threshold to filter out lemmas that are
less dispersed. In Example~\ref{exm-eda-masc-dp-filter}, I filter out
lemmas that have a dispersion measure less than .91. Then in
Table~\ref{tbl-eda-masc-dp-filter}, I preview the top and bottom 50
lemmas in the dataset.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dp-filter}{}\label{exm-eda-masc-dp-filter}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter for lemmas with dp \textless{}= .91}
\NormalTok{masc\_lemma\_disp\_thr }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_lemma\_disp }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(dp }\SpecialCharTok{\textless{}=}\NormalTok{ .}\DecValTok{91}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n))}

\CommentTok{\# Preview top}
\NormalTok{masc\_lemma\_disp\_thr }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(type) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{matrix}\NormalTok{(}\AttributeTok{ncol =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{col.names =} \ConstantTok{NULL}\NormalTok{)}
\CommentTok{\# Preview bottom}
\NormalTok{masc\_lemma\_disp\_thr }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_tail}\NormalTok{(}\AttributeTok{n =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(type) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{matrix}\NormalTok{(}\AttributeTok{ncol =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{col.names =} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tbl-eda-masc-dp-filter}Frequency of tokens in the MASC
dataset after filtering out lemmas with a Deviation of Proportions less
than .91}

\begin{minipage}{\linewidth}

\subcaption{\label{tbl-eda-masc-dp-filter-1}Top 50 lemmas}

\centering{

\begin{tabular}{llllllllll}
\toprule
be & say & think & other & work & look & also & new & t & man\\
have & go & more & see & year & then & way & find & first & ask\\
do & know & just & people & come & right & need & give & help & very\\
not & get & time & take & use & only & back & thing & day & much\\
n't & make & so & now & well & want & here & tell & many & even\\
\bottomrule
\end{tabular}

}

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\subcaption{\label{tbl-eda-masc-dp-filter-2}Bottom 50 lemmas}

\centering{

\begin{tabular}{llllllllll}
\toprule
dump & sting & tuition & unleash & blur & liberty & wound & nostalgic & accidentally & wax\\
instrument & mainstream & awaken & prosperous & litigation & going & devote & shy & protective & faith-based\\
triumph & wildly & hook & buzz & resistance & awkward & alright & proximity & preside & decidedly\\
harsh & dismiss & fetch & presume & absurd & afterwards & evolutionary & sandy & rethink & resolute\\
ignorance & liability & brave & summarize & qualify & eve & envy & interfere & strictly & evidently\\
\bottomrule
\end{tabular}

}

\end{minipage}%

\end{table}%

\end{example}

We now have a good candidate list of common vocabulary that is spread
well across the corpus.

\paragraph{Relative frequency}\label{sec-eda-frequency-relative}

Gauging frequency and dispersion across the entire corpus is a good
starting point for any frequency analysis, but it is often the case that
we want to compare the frequency and dispersion of linguistic units
across corpora or sub-corpora.

In the case of the MASC dataset, for example, we may want to compare
metrics across the two modalities or the various genres. Simply
comparing frequency counts across these sub-corpora is not a good
approach, and can be misleading, as the sub-corpora may vary in size.
For example, if one sub-corpus is twice as large as another sub-corpus,
then, all else being equal, the frequency counts will be twice as large
in the larger sub-corpus. This is why we use relative frequency
measures, which are normalized by the size of the sub-corpus.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

A variable in the MASC dataset that has yet to be used is the
\texttt{pos} part of speech variable. How could we use this variable to
refine our frequency and dispersion analysis of lemma types?

Hint: consider lemma forms that may be tagged with different
parts-of-speech.

\end{tcolorbox}

To normalize the frequency of linguistic units across sub-corpora, we
can use the \textbf{relative frequency} measure. This is the frequency
of a linguistic unit divided by the total number of linguistic units in
the sub-corpus. This bakes in the size of the sub-corpus into the
measure. The notion of relative frequency is key to all research working
with text, as it is the basis for the statistical approach to text
analysis where comparisons are made.

There are some field-specific terms that are used to refer to relative
frequency measures. For example, in information retrieval and Natural
Language Processing, the relative frequency measure is often referred to
as the \textbf{term frequency}. In corpus linguistics, the relative
frequency measure is often modified slightly to include a constant
(\emph{e.g.} \(rf * 100\)) which is known as the \textbf{observed
relative frequency}. Athough the observed relative frequency per number
of tokens is not strictly necessary, it is often used to make the values
more interpretable as we can now talk about an observed relative
frequency of 1.5 as a linguistic unit that occurs 1.5 times per 100
linguistic units.

Let's consider how we might compare the frequency and dispersion of
lemmas across the two modalities in the MASC dataset, spoken and
written. To make this a bit more interesting and more relevant, let's
add the \texttt{pos} variable to our analysis. The intent, then, will be
to identify lemmas tagged with particular parts of speech that are
particularly indicative of each of the modaliites.

We can do this by collapsing the \texttt{lemma} and \texttt{pos}
variables into a single variable, \texttt{lemma\_pos}, with the
\texttt{str\_c()} function, as seen in Example~\ref{exm-eda-masc-type}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-type}{}\label{exm-eda-masc-type}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Collapse lemma and pos into type}
\NormalTok{masc\_content\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_content\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lemma\_pos =} \FunctionTok{str\_c}\NormalTok{(lemma, pos, }\AttributeTok{sep =} \StringTok{"\_"}\NormalTok{))}

\CommentTok{\# Preview}
\NormalTok{masc\_content\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 8
>    doc_id modality genre   term_num term         lemma        pos   lemma_pos   
>    <chr>  <chr>    <chr>      <dbl> <chr>        <chr>        <chr> <chr>       
>  1 1      Written  Letters        3 contribution contribution NN    contributio~
>  2 1      Written  Letters        7 mean         mean         VB    mean_VB     
>  3 1      Written  Letters        8 more         more         JJR   more_JJR    
>  4 1      Written  Letters       12 know         know         VB    know_VB     
>  5 1      Written  Letters       15 help         help         VB    help_VB     
>  6 1      Written  Letters       17 see          see          VB    see_VB      
>  7 1      Written  Letters       19 much         much         JJ    much_JJ     
>  8 1      Written  Letters       21 contribution contribution NN    contributio~
>  9 1      Written  Letters       22 means        mean         VBZ   mean_VBZ    
> 10 1      Written  Letters       25 'm           be           VBP   be_VBP
\end{verbatim}

\end{example}

Now this will increase the number of lemma types in the dataset as we
are now considering lemmas where the same lemma form is tagged with
different parts-of-speech.

Getting back to calculating the frequency and dispersion of lemmas in
each modality, we can use the \texttt{calc\_type\_metrics()} function
with \texttt{lemma\_pos} as our type argument. We will, however, need to
apply this function to each sub-corpus independently and then
concatenate the two data frames. This function returns a (raw) frequency
measure by default, but we can specify the\texttt{frequency} argument to
\texttt{rf} to calculate the relative frequency of the linguistic units
as in Example~\ref{exm-eda-masc-metrics-modality}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-metrics-modality}{}\label{exm-eda-masc-metrics-modality}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate relative frequency}
\CommentTok{\# Spoken}
\NormalTok{masc\_spoken\_metrics }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_content\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(modality }\SpecialCharTok{==} \StringTok{"Spoken"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calc\_type\_metrics}\NormalTok{(}
    \AttributeTok{type =}\NormalTok{ lemma\_pos,}
    \AttributeTok{documents =}\NormalTok{ doc\_id,}
    \AttributeTok{frequency =} \StringTok{"rf"}\NormalTok{,}
    \AttributeTok{dispersion =} \StringTok{"dp"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{modality =} \StringTok{"Spoken"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n))}

\CommentTok{\# Written}
\NormalTok{masc\_written\_metrics }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_content\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(modality }\SpecialCharTok{==} \StringTok{"Written"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calc\_type\_metrics}\NormalTok{(}
    \AttributeTok{type =}\NormalTok{ lemma\_pos,}
    \AttributeTok{documents =}\NormalTok{ doc\_id,}
    \AttributeTok{frequency =} \StringTok{"rf"}\NormalTok{,}
    \AttributeTok{dispersion =} \StringTok{"dp"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{modality =} \StringTok{"Written"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n))}

\CommentTok{\# Concatenate spoken and written metrics}
\NormalTok{masc\_metrics }\OtherTok{\textless{}{-}}
  \FunctionTok{bind\_rows}\NormalTok{(masc\_spoken\_metrics, masc\_written\_metrics)}

\CommentTok{\# Preview}
\NormalTok{masc\_metrics }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 5 x 5
>   type         n     rf     dp modality
>   <chr>    <dbl>  <dbl>  <dbl> <chr>   
> 1 be_VBZ    2612 0.0489 0.0843 Spoken  
> 2 be_VBP    1282 0.0240 0.111  Spoken  
> 3 be_VBD    1020 0.0191 0.300  Spoken  
> 4 n't_RB     829 0.0155 0.139  Spoken  
> 5 have_VBP   766 0.0143 0.152  Spoken
\end{verbatim}

\end{example}

With the \texttt{rf} measure, we are now in a position to compare
`apples to apples', as you might say. We can now compare the relative
frequency of lemmas across the two modalities. Let's preview the top 5
lemmas in each modality, as seen in
Example~\ref{exm-eda-masc-relative-frequency-top}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-relative-frequency-top}{}\label{exm-eda-masc-relative-frequency-top}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview top 10 lemmas in each modality}
\NormalTok{masc\_metrics }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(modality) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_max}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{order\_by =}\NormalTok{ rf) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 20 x 5
>    type         n      rf     dp modality
>    <chr>    <dbl>   <dbl>  <dbl> <chr>   
>  1 be_VBZ    2612 0.0489  0.0843 Spoken  
>  2 be_VBP    1282 0.0240  0.111  Spoken  
>  3 be_VBD    1020 0.0191  0.300  Spoken  
>  4 n't_RB     829 0.0155  0.139  Spoken  
>  5 have_VBP   766 0.0143  0.152  Spoken  
>  6 do_VBP     728 0.0136  0.180  Spoken  
>  7 be_VB      655 0.0123  0.147  Spoken  
>  8 not_RB     638 0.0119  0.137  Spoken  
>  9 just_RB    404 0.00757 0.267  Spoken  
> 10 so_RB      387 0.00725 0.357  Spoken  
> 11 be_VBZ    4745 0.0249  0.230  Written 
> 12 be_VBD    3317 0.0174  0.366  Written 
> 13 be_VBP    2617 0.0137  0.237  Written 
> 14 be_VB     1863 0.00976 0.218  Written 
> 15 not_RB    1640 0.00859 0.259  Written 
> 16 have_VBP  1227 0.00643 0.291  Written 
> 17 n't_RB     905 0.00474 0.540  Written 
> 18 have_VBD   859 0.00450 0.446  Written 
> 19 have_VBZ   777 0.00407 0.335  Written 
> 20 say_VBD    710 0.00372 0.609  Written
\end{verbatim}

\end{example}

We can appreciate, now, that there are similarities and a few
differences between the most frequent lemmas for each modality. First,
there are similar lemmas in written and spoken modalities, such as `be',
`have', and `not'. Second, the top 10 include verbs and adverbs. Now we
are looking at the most frequent types, so it is not surprising that we
see more in common than not. However, looking close we can see that
contracted forms are more frequent in the spoken modality, such as
`isn't', `don't', and `can't' and that ordering of the verb tenses
differs to some degree. Whether these are important distinctions for our
task is something we will need to consider.

We can further cull our results by filtering out lemmas that are not
well-dispersed across the sub-corpora. Although it may be tempting to
use the threshold we used earlier, we should consider that the size of
the sub-corpora are different and the distribution of the dispersion
measure may be different. With this in mind, we need to visualize the
distribution of the dispersion measure for each modality and apply the
elbow method to identify a threshold for each modality.

After assessing the density plots for the dispersion of each modality
via the elbow method, we update our thresholds. We maintain the \(.91\)
threshold for the written subcorpus and use a \(.79\) threshold for the
spoken subcorpus. I apply these filters as seen in
Example~\ref{exm-eda-masc-subcorpora-filtered}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-subcorpora-filtered}{}\label{exm-eda-masc-subcorpora-filtered}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter for lemmas with}
\CommentTok{\# dp \textless{}= .91 for written and}
\CommentTok{\# dp \textless{}= .79 for spoken}
\NormalTok{masc\_metrics\_thr }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_metrics }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}
\NormalTok{    (modality }\SpecialCharTok{==} \StringTok{"Written"} \SpecialCharTok{\&}\NormalTok{ dp }\SpecialCharTok{\textless{}=}\NormalTok{ .}\DecValTok{91}\NormalTok{) }\SpecialCharTok{|}
\NormalTok{    (modality }\SpecialCharTok{==} \StringTok{"Spoken"} \SpecialCharTok{\&}\NormalTok{ dp }\SpecialCharTok{\textless{}=}\NormalTok{ .}\DecValTok{79}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(rf))}
\end{Highlighting}
\end{Shaded}

\end{example}

Filtering the less-dispersed types reduces the dataset from 33,428 to
4,865 observations. This will provide us with a more succinct list of
common and well-dispersed lemmas that are used in each modality.

As much as the frequency and dispersion measures can provide us with a
good starting point, it does not provide an understanding of what types
are more indicative of a particular sub-corpus, modality subcorpora in
our case. We can do this by calculating the log odds ratio of each lemma
in each modality.

The \textbf{log odds ratio} is a measure that quantifies the difference
between the frequencies of a type in two corpora or sub-corpora. In
spirit and in name, it compares the odds of a type occurring in one
corpus versus the other. The values range from negative to positive
infinity, with negative values indicating that the type is more frequent
in the first corpus and positive values indicating that the lemma is
more frequent in the second corpus. The magnitude of the value indicates
the strength of the association.

The \texttt{tidylo} package provides a convenient function
\texttt{bind\_log\_odds()} to calculate the log odds ratio, and a
weighed variant, for each type in each sub-corpus. The weighted log odds
ratio measure provides a more robust and interpretable measure for
comparing term frequencies across corpora, especially when term
frequencies are low or when corpora are of different sizes. The
weighting (or standardization) also makes it easier to identify terms
that are particularly distinctive or characteristic of one corpus over
another. Note that the weighted measure's interpretation is slightly
different that the log odds's. The larger positive values in each corpus
indicate that the type is more indicative of that (sub-)corpus, and the
larger negative values indicate that the type is less indicative.

Let's calculate the weighted log odds ratio for each lemma in each
modality and preview the top 10 lemmas in each modality, as seen in
Example~\ref{exm-eda-masc-log-odds-weighted}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-log-odds-weighted}{}\label{exm-eda-masc-log-odds-weighted}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(tidylo)}

\CommentTok{\# Calculate log odds ratio}
\NormalTok{masc\_metrics\_thr }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_metrics\_thr }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{bind\_log\_odds}\NormalTok{(}
    \AttributeTok{set =}\NormalTok{ modality,}
    \AttributeTok{feature =}\NormalTok{ type,}
    \AttributeTok{n =}\NormalTok{ n}
\NormalTok{  )}

\CommentTok{\# Preview top 10 lemmas in each modality}
\NormalTok{masc\_metrics\_thr }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(modality) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_max}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{order\_by =}\NormalTok{ log\_odds\_weighted) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 20 x 6
>    type                 n       rf     dp modality log_odds_weighted
>    <chr>            <dbl>    <dbl>  <dbl> <chr>                <dbl>
>  1 be_VBZ            2612 0.0489   0.0843 Spoken               14.0 
>  2 n't_RB             829 0.0155   0.139  Spoken               10.4 
>  3 do_VBP             728 0.0136   0.180  Spoken               10.3 
>  4 be_VBP            1282 0.0240   0.111  Spoken                8.67
>  5 think_VBP          350 0.00655  0.259  Spoken                8.32
>  6 have_VBP           766 0.0143   0.152  Spoken                8.01
>  7 know_VBP           282 0.00528  0.260  Spoken                7.04
>  8 well_RB            334 0.00626  0.283  Spoken                6.91
>  9 go_VBG             285 0.00534  0.207  Spoken                6.47
> 10 understanding_NN    45 0.000843 0.649  Spoken                6.41
> 11 t_NN               475 0.00249  0.778  Written              10.6 
> 12 figure_NN          140 0.000733 0.868  Written               5.75
> 13 financial_JJ       138 0.000723 0.880  Written               5.71
> 14 city_NN            137 0.000718 0.766  Written               5.69
> 15 email_NN           133 0.000697 0.866  Written               5.61
> 16 eye_NNS            129 0.000676 0.731  Written               5.52
> 17 style_NN           108 0.000566 0.829  Written               5.05
> 18 mail_NN            106 0.000555 0.876  Written               5.00
> 19 text_NN            103 0.000540 0.845  Written               4.93
> 20 gift_NN             98 0.000513 0.826  Written               4.81
\end{verbatim}

\end{example}

Let's imagine we would like to extract the most indicative verbs for
each modality using the weighted log odds as our measure. We can do this
with a little regular expression magic. Let's use the
\texttt{str\_subset()} function to filter for lemmas that contain
\texttt{\_V} and then use \texttt{slice\_max()} to extract the top 10
most indicative verb lemmas, as seen in
Example~\ref{exm-eda-masc-log-odds-weighted-verbs}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-log-odds-weighted-verbs}{}\label{exm-eda-masc-log-odds-weighted-verbs}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview (ordered by log\_odds\_weighted)}
\NormalTok{masc\_metrics\_thr }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(modality) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(type, }\StringTok{"\_V"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_max}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{order\_by =}\NormalTok{ log\_odds\_weighted) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{n) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 20 x 5
>    type                rf     dp modality log_odds_weighted
>    <chr>            <dbl>  <dbl> <chr>                <dbl>
>  1 be_VBZ        0.0489   0.0843 Spoken               14.0 
>  2 do_VBP        0.0136   0.180  Spoken               10.3 
>  3 be_VBP        0.0240   0.111  Spoken                8.67
>  4 think_VBP     0.00655  0.259  Spoken                8.32
>  5 have_VBP      0.0143   0.152  Spoken                8.01
>  6 know_VBP      0.00528  0.260  Spoken                7.04
>  7 go_VBG        0.00534  0.207  Spoken                6.47
>  8 do_VBD        0.00603  0.321  Spoken                5.97
>  9 mean_VBP      0.00247  0.543  Spoken                5.95
> 10 do_VB         0.00455  0.207  Spoken                5.61
> 11 don_VB        0.000361 0.839  Written               4.04
> 12 doe_VBZ       0.000351 0.870  Written               3.98
> 13 walk_VBD      0.000320 0.790  Written               3.79
> 14 associate_VBN 0.000304 0.777  Written               3.70
> 15 reply_VBD     0.000293 0.837  Written               3.64
> 16 develop_VBG   0.000288 0.812  Written               3.60
> 17 require_VBN   0.000272 0.793  Written               3.50
> 18 fall_VBD      0.000267 0.757  Written               3.47
> 19 meet_VB       0.000241 0.729  Written               3.30
> 20 regard_VBG    0.000225 0.823  Written               3.19
\end{verbatim}

\end{example}

Note that the log odds are larger for the spoken modality than the
written modality. This indicates that theses types are more strongly
indicative of the spoken modality than the types in the written modality
are indicative of the written modality. This is not surprising, as the
written modality is typically more diverse in terms of lexical usage
than the spoken modality, where the terms tend to be repeated more
often, including verbs.

\subsubsection{Co-occurrence analysis}\label{sec-eda-co-occurrence}

Moving forward on our task, we have a good idea of the general
vocabulary that we want to include in our ELL materials and can identify
lemma types that are particularly indicative of each modality. Another
useful approach to complement our analysis is to identify words that
co-occur with our target lemmas --in particular verbs. In English, it is
common for verbs to appear with a preposition or adverb, such as `give
up', `look after'. These `phrasal verbs' form a semantic unit that is
distinct from the verb alone.

In cases such as this, we are aiming to do a co-occurrence analysis.
Co-occurrence analysis is a set of methods that are used to identify
words that appear in close proximity to a target type.

An exploratory, primarily qualitative, approach is to display the
co-occurrence of words in a Keyword in Context (KWIC). This is a table
that displays the target word in the center of the table and the words
that appear before and after the target word. This is a useful approach
for spot identifying collocations of a target word or phrase.

The \texttt{quanteda} package includes a function \texttt{kwic()} that
can be used to create a KWIC table. It does require some transformation
to the data, however. We need to collapse the \texttt{lemma} column into
a single string for each document from the original transformed dataset,
\texttt{masc\_tbl}. Then we can apply the \texttt{corpus()} and then
\texttt{tokens} function to create a quanteda tokens object. Then we can
apply the \texttt{kwic()} function to create a KWIC table based on a
search pattern.

\begin{example}[]\protect\hypertarget{exm-eda-masc-kwic}{}\label{exm-eda-masc-kwic}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Collapse lemma, pos into a single string}
\NormalTok{masc\_text\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lemma\_pos =} \FunctionTok{str\_c}\NormalTok{(lemma, pos, }\AttributeTok{sep =} \StringTok{"\_"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(doc\_id, modality, genre) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{text =} \FunctionTok{str\_c}\NormalTok{(lemma\_pos, }\AttributeTok{collapse =} \StringTok{" "}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{()}

\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(quanteda)}

\NormalTok{masc\_corpus }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_text\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{corpus}\NormalTok{(}
    \AttributeTok{text\_field =} \StringTok{"text"}\NormalTok{,}
    \AttributeTok{docid\_field =} \StringTok{"doc\_id"}
\NormalTok{  )}

\NormalTok{masc\_corpus }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tokens}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kwic}\NormalTok{(}
    \AttributeTok{pattern =} \FunctionTok{phrase}\NormalTok{(}\StringTok{"*\_V* *\_IN*"}\NormalTok{),}
    \AttributeTok{window =} \DecValTok{3}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(docname, pre, keyword, post) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>    docname pre                           keyword               post             
>    <chr>   <chr>                         <chr>                 <chr>            
>  1 256     what_WDT when_WRB you_PRP     come_VBP in_IN        to_TO spank_NN m~
>  2 200     beer_NN rachel_NNP be_VBD     stand_VBG with_IN     a_DT plastic_NN ~
>  3 171     of_IN las_NNP vegas_NNP       skyrocket_VBD over_IN additional_JJ ho~
>  4 384     whose_WP $ stock_NN           peak_VBD at_IN        $ 65_NN a_DT     
>  5 98      know_VB that_IN he_PRP        be_VBZ that_IN        serious_JJ about~
>  6 216     western_JJ release_NNS be_VBP show_VBN in_IN        some_DT of_IN th~
>  7 155     can_MD best_JJS be_VB         do_VBN by_IN          discuss_VBG a_DT~
>  8 77      $ porch_NN light_NNS          be_VBP on_IN          they_PRP be_VBP ~
>  9 254     a_DT slut_NNP and_CC          put_VB on_IN          something_NN sex~
> 10 187     suffer_VBZ for_IN it_PRP      suggest_VBG that_IN   traditionally_RB~
\end{verbatim}

\end{example}

The output of a KWIC table, as in Example~\ref{exm-eda-masc-kwic}, is a
useful, qualitative, way to identify collocations of a target word or
phrase.

A straightforward quantitative way to explore co-occurrence is to set
the unit of observation to an \(n\)-gram of terms. Then, the frequency
and dispersion metrics can be calculated for each \(n\)-gram.

In general, deriving \(n\)-grams from a corpus is a straightforward
process. The \texttt{tidytext} package includes a function
\texttt{unnest\_tokens()} that can be used to create \(n\)-grams from a
corpus. The function can take a single column of untokenized text or a
tokenized column in combination with a variable to use as the grouping
variable. In the \texttt{masc\_tbl} dataset, we have tokenized text in
the \texttt{lemma} column and a grouping variable in the
\texttt{doc\_id} column. We can use the \texttt{unnest\_tokens()}
function to create a new data frame with a row for each \(n\)-gram in
each document by setting \texttt{collapse\ =\ "doc\_id"}, as seen in
Example~\ref{exm-eda-masc-bigrams-tidytext}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-bigrams-tidytext}{}\label{exm-eda-masc-bigrams-tidytext}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(tidytext)}

\CommentTok{\# Create bigrams}
\NormalTok{masc\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest\_tokens}\NormalTok{(}
    \AttributeTok{output =}\NormalTok{ bigrams,}
    \AttributeTok{input =}\NormalTok{ lemma,}
    \AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{,}
    \AttributeTok{n =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{collapse =} \StringTok{"doc\_id"}
\NormalTok{    ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 2
>    doc_id bigrams          
>    <chr>  <chr>            
>  1 1      december your    
>  2 1      your contribution
>  3 1      contribution to  
>  4 1      to goodwill      
>  5 1      goodwill will    
>  6 1      will mean        
>  7 1      mean more        
>  8 1      more than        
>  9 1      than you         
> 10 1      you may
\end{verbatim}

\end{example}

The result of this operation can be joined with the original dataset
using the \texttt{doc\_id} as the key variable. Then we can calculate
the frequency and dispersion metrics for each \(n\)-gram in each
modality. However, this approach is not ideal for our task, as we've
lost access to part of speech information.

Another, more informative approach is to create a new variable that
combines the lemma and part of speech for each observation before using
\texttt{unnest\_tokens()} to generate the bigrams. We can use the
\texttt{str\_c()} function from the \texttt{stringr} package to join the
\texttt{lemma} and \texttt{pos} columns into a single string, so that we
have a variable \texttt{lemma\_pos} with the lemma and part of speech
joined by an underscore.

One consideration that we need to take for our goal to identify verb
particle constructions, is how we ultimately want to group our
\texttt{lemma\_pos} values. This is particularly important given the
fact that our \texttt{pos} tags for verbs include information about the
verb's tense and person. This means that a verb in a verb particle
bigram, such as `look after', will be represented by multiple
\texttt{lemma\_pos} values, such as \texttt{look\_VB},
\texttt{look\_VBP}, \texttt{look\_VBD}, and \texttt{look\_VBG}. If we
want this level of detail, we just proceed as described above. However,
if we want to group the verb particle bigrams by a single verb value, we
need to recode the \texttt{pos} values for verbs. We can do this with
the \texttt{case\_when()} function from the \texttt{dplyr} package.

In Example~\ref{exm-eda-masc-lemma-pos}, I recode the \texttt{pos}
values for verbs to \texttt{V} and then join the \texttt{lemma} and
\texttt{pos} columns into a single string.

\begin{example}[]\protect\hypertarget{exm-eda-masc-lemma-pos}{}\label{exm-eda-masc-lemma-pos}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Collapse lemma into a single string}
\NormalTok{masc\_lemma\_pos\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{pos =} \FunctionTok{case\_when}\NormalTok{(}
    \FunctionTok{str\_detect}\NormalTok{(pos, }\StringTok{"\^{}V"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"V"}\NormalTok{,}
    \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}}\NormalTok{ pos}
\NormalTok{  )) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(doc\_id) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lemma\_pos =} \FunctionTok{str\_c}\NormalTok{(lemma, pos, }\AttributeTok{sep =} \StringTok{"\_"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{()}

\CommentTok{\# Preview}
\NormalTok{masc\_lemma\_pos\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 8
>    doc_id modality genre   term_num term         lemma        pos   lemma_pos   
>    <chr>  <chr>    <chr>      <dbl> <chr>        <chr>        <chr> <chr>       
>  1 1      Written  Letters        0 December     december     NNP   december_NNP
>  2 1      Written  Letters        2 Your         your         PRP$  your_PRP$   
>  3 1      Written  Letters        3 contribution contribution NN    contributio~
>  4 1      Written  Letters        4 to           to           TO    to_TO       
>  5 1      Written  Letters        5 Goodwill     goodwill     NNP   goodwill_NNP
>  6 1      Written  Letters        6 will         will         MD    will_MD     
>  7 1      Written  Letters        7 mean         mean         V     mean_V      
>  8 1      Written  Letters        8 more         more         JJR   more_JJR    
>  9 1      Written  Letters        9 than         than         IN    than_IN     
> 10 1      Written  Letters       10 you          you          PRP   you_PRP
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{masc\_lemma\_pos\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest\_tokens}\NormalTok{(}
    \AttributeTok{output =}\NormalTok{ bigrams,}
    \AttributeTok{input =}\NormalTok{ lemma\_pos,}
    \AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{,}
    \AttributeTok{n =} \DecValTok{2}\NormalTok{,}
    \AttributeTok{to\_lower =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{collapse =} \StringTok{"doc\_id"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(bigrams, }\StringTok{"\_V.*\_IN"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calc\_type\_metrics}\NormalTok{(}
    \AttributeTok{type =}\NormalTok{ bigrams,}
    \AttributeTok{documents =}\NormalTok{ doc\_id,}
    \AttributeTok{frequency =} \StringTok{"rf"}\NormalTok{,}
    \AttributeTok{dispersion =} \StringTok{"dp"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(rf))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 4,513 x 4
>    type                n      rf    dp
>    <chr>           <dbl>   <dbl> <dbl>
>  1 be_V in_IN        359 0.0259  0.366
>  2 be_V that_IN      210 0.0151  0.504
>  3 look_V at_IN      157 0.0113  0.528
>  4 say_V that_IN     134 0.00965 0.580
>  5 be_V on_IN        120 0.00865 0.523
>  6 talk_V about_IN   114 0.00821 0.622
>  7 be_V of_IN        111 0.00800 0.504
>  8 know_V that_IN     98 0.00706 0.605
>  9 think_V that_IN    90 0.00648 0.643
> 10 be_V about_IN      76 0.00548 0.608
> # i 4,503 more rows
\end{verbatim}

\end{example}

We have identified and derived frequency and dispersion metrics for
\(n\)-grams that include verb particle construction candidates. Yet,
there is a issue with this approach. The frequency and dispersion of
\(n\)-grams does not necessarily relate to whether the two words form a
semantic unit.

To address this issue, we can use a statistical measures to estimate
collocational strength between two words. A \textbf{collocation} is a
sequence of words that co-occur more often than would be expected by
chance. A common measure of collocation is the \textbf{pointwise mutual
information} (PMI) measure. The PMI measure is calculated as the log
ratio of the observed frequency of two words co-occurring to the
expected frequency of the two words co-occurring. The expected frequency
is calculated as the product of the frequency of each word. The PMI
measure is a log ratio, the direction indicates relatedness and the
magnitude of the value indicates the strength of the association.

Let's calculate the PMI for all the bigrams in the MASC dataset. We can
use the \texttt{calc\_assoc\_metrics()} function from \texttt{qtalrkit}.
We need to specify the \texttt{association} argument to \texttt{pmi} and
the \texttt{type} argument to \texttt{bigrams}, as seen in
Example~\ref{exm-eda-masc-bigrams-pmi}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-bigrams-pmi}{}\label{exm-eda-masc-bigrams-pmi}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{masc\_lemma\_pos\_assoc }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_lemma\_pos\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calc\_assoc\_metrics}\NormalTok{(}
    \AttributeTok{doc\_index =}\NormalTok{ doc\_id,}
    \AttributeTok{token\_index =}\NormalTok{ term\_num,}
    \AttributeTok{type =}\NormalTok{ lemma\_pos,}
    \AttributeTok{association =} \StringTok{"pmi"}
\NormalTok{  )}

\CommentTok{\# Preview}
\NormalTok{masc\_lemma\_pos\_assoc }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(pmi)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>    x               y                   n   pmi
>    <chr>           <chr>           <dbl> <dbl>
>  1 #Christian_NN   bigot_NN            1  12.4
>  2 #FAIL_NN        phenomenally_RB     1  12.4
>  3 #NASCAR_NN      #indycar_NN         1  12.4
>  4 #PALM_NN        merchan_NN          1  12.4
>  5 #Twitter_NN     #growth_NN          1  12.4
>  6 #college_NN     #jobs_NN            1  12.4
>  7 #education_NN   #teaching_NN        1  12.4
>  8 #faculty_NN     #cites_NN           1  12.4
>  9 #fb_NN          siebel_NNP          1  12.4
> 10 #glitchmyass_NN reps_NNP            1  12.4
\end{verbatim}

\end{example}

One caveat to using the PMI measure is that it is sensitive to the
frequency of the words. If the words in a bigram pair are infrequent,
and especially if they only occur once, then the PMI measure will be
inflated. To mitigate this issue, we can apply a frequency threshold to
the bigrams before calculating the PMI measure. Let's filter out bigrams
that occur less than 10 times, as seen in
Example~\ref{exm-eda-masc-bigrams-pmi-filtered}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-bigrams-pmi-filtered}{}\label{exm-eda-masc-bigrams-pmi-filtered}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter for bigrams that occur \textgreater{}= 10 times}
\NormalTok{masc\_lemma\_pos\_assoc\_thr }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_lemma\_pos\_assoc }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}=} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(pmi))}

\CommentTok{\# Preview}
\NormalTok{masc\_lemma\_pos\_assoc\_thr }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>    x             y                n   pmi
>    <chr>         <chr>        <dbl> <dbl>
>  1 pianista_NN   irlandesa_NN    10 10.0 
>  2 costa_NNP     rica_NNP        10  9.95
>  3 nanowrimo_NNP novel_NNP       12  9.87
>  4 bin_NN        laden_NNP       11  9.79
>  5 osama_NNP     bin_NN          11  9.79
>  6 bin_NNP       ladin_NNP       11  9.70
>  7 los_NNP       angeles_NNP     11  9.64
>  8 chilean_JJ    seabass_NNS     13  9.64
>  9 novel_NNP     ch_NNP          12  9.58
> 10 st_NNP        zip_NNP         10  9.52
\end{verbatim}

\end{example}

Now we are in a position to identify verb particle constructions. We can
filter for bigrams that include a verb and a particle and that have a
PMI measure greater than 0, as seen in
Example~\ref{exm-eda-masc-bigrams-pmi-filtered-vpc}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-bigrams-pmi-filtered-vpc}{}\label{exm-eda-masc-bigrams-pmi-filtered-vpc}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter for bigrams that include a verb and a particle}
\CommentTok{\# and that have a PMI measure greater than 0}
\NormalTok{masc\_verb\_part\_assoc }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_lemma\_pos\_assoc\_thr }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(x, }\StringTok{"\_V"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(y, }\StringTok{"\_IN"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(pmi }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(x)}

\CommentTok{\# Preview}
\NormalTok{masc\_verb\_part\_assoc }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>    x             y           n   pmi
>    <chr>         <chr>   <dbl> <dbl>
>  1 account_V     for_IN     17  3.74
>  2 acknowledge_V that_IN    13  3.62
>  3 act_V         as_IN      14  2.96
>  4 agree_V       with_IN    45  3.21
>  5 agree_V       that_IN    14  1.94
>  6 appear_V      on_IN      12  1.97
>  7 appear_V      in_IN      24  1.76
>  8 argue_V       that_IN    20  3.26
>  9 arrive_V      at_IN      18  3.39
> 10 arrive_V      in_IN      10  1.48
\end{verbatim}

\end{example}

We can clean up the results a bit by removing the part of speech tags
from the \texttt{x} and \texttt{y} variables, up our minimum PMI value,
and create a network plot to visualize the results, as seen in
Figure~\ref{fig-eda-masc-verb-part-network}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Clean up results}
\NormalTok{masc\_verb\_part\_assoc\_plot }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_verb\_part\_assoc }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(pmi }\SpecialCharTok{\textgreater{}=} \DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{x =} \FunctionTok{str\_remove}\NormalTok{(x, }\StringTok{"\_V.*"}\NormalTok{),}
    \AttributeTok{y =} \FunctionTok{str\_remove}\NormalTok{(y, }\StringTok{"\_IN"}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Create an association network plot}
\CommentTok{\# \textasciigrave{}x\textasciigrave{} and \textasciigrave{}y\textasciigrave{} are the nodes}
\CommentTok{\# \textasciigrave{}pmi\textasciigrave{} is the edge weight}

\FunctionTok{library}\NormalTok{(igraph)}
\FunctionTok{library}\NormalTok{(ggraph)}

\NormalTok{masc\_verb\_part\_assoc\_plot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{graph\_from\_data\_frame}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggraph}\NormalTok{(}\AttributeTok{layout =} \StringTok{"nicely"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_edge\_link}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ pmi),}
    \AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{,}
    \AttributeTok{edge\_width =} \FloatTok{0.8}\NormalTok{,}
    \AttributeTok{arrow =}\NormalTok{ grid}\SpecialCharTok{::}\FunctionTok{arrow}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_node\_point}\NormalTok{(}\AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_node\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ name), }\AttributeTok{repel =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_edge\_color\_gradient}\NormalTok{(}\AttributeTok{low =} \StringTok{"grey90"}\NormalTok{, }\AttributeTok{high =} \StringTok{"grey20"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-verb-part-network-1.pdf}

}

\caption{\label{fig-eda-masc-verb-part-network}Network plot of verb
particle constructions in the MASC dataset}

\end{figure}%

From this plot, and from the underlying data, we can explore verb
particle constructions. We could go further and apply our co-occurrence
methods to each modality separately, if we wanted to identify verb
particle constructions that are distinctive to each modality. We could
also apply our co-occurrence methods to other parts-of-speech, such as
adjectives and nouns, to identify collocations of these parts-of-speech.
There is much more to explore with co-occurrence analysis, but this
should give you a good idea of the types of questions that can be
addressed.

\subsection{Unsupervised learning}\label{sec-eda-unsupervised}

Aligned in purpose with descriptive approaches, unsupervised learning
approaches to exploratory data analysis are used to identify patterns in
the data from an algorithmic perspective. Common methods in text
analysis include principle component analysis, clustering, and vector
space modeling.

We will continue to use the MASC dataset as we develop materials for our
ELL textbook to illustrate unsupervised learning methods. In the
process, we will explore the following questions:

\begin{itemize}
\tightlist
\item
  Can we identify and group documents based on linguistic features or
  co-occurrence patterns of the data itself?
\item
  Do the groups of documents relate to categories in the dataset?
\item
  Can we estimate the semantics of words based on their co-occurrence
  patterns?
\end{itemize}

Through these questions we will build on our knowledge of frequency,
dispersion, and co-occurrence analysis and introduce concepts and
methods associated with machine learning.

\subsubsection{Clustering}\label{sec-eda-clustering}

\textbf{Clustering} is a unsupervised learning technique that can be
used to group similar items in the text data, helping to organize the
data into distinct categories and discover relationships between
different elements in the text. The main steps in the procedure includes
identifying the relevant linguistic features to use for clustering,
representing the features in a way that can be used for clustering, and
applying a clustering algorithm to the data. However, it is important to
consider the strengths and weaknesses of the clustering algorithm for a
particular task and how the results will be evaluated.

In our ELL textbook task, we may very well want to explore the
similiarities and/ or differences between the documents based on the
distribution of linguistic features. This provides us a view to evaluate
to what extent the labels in the dataset (modality and genre) map to the
distribution of linguistic features. Based on this evaluation, we may
want to consider re-labeling the documents, collapsing labels, or even
adding new labels.

Enter clustering. Instead of relying entirely on the labels in the MASC
dataset, we can let the data itself say something about how related the
documents are. Yet, a pivotal question is what features should we use,
otherwise known as \textbf{feature selection}. We could use terms or
lemmas, but we may want to consider other features, such as
parts-of-speech or some co-occurrence patterns. We are not locked into
using one criterion, and we can perform clustering multiple times with
different features, but we should consider the implications of our
feature selection for our interpretation of the results.

Another key question is what clustering algorithm to use. Again, we are
not married to one algorithm, and we can perform clustering multiple
times with different algorithms, but not all algorithms are created
equal. Some algorithms are better suited for certain types of data and
certain types of tasks. For example, \textbf{Hierarchical clustering} is
a good choice when we are not sure how many clusters we want to
identify, as it does not require us to specify the number of clusters
from the outset. However, it is not a good choice when we have a large
dataset, as it can be computationally expensive compared to some other
algorithms. \textbf{K-means clustering}, on the other hand, is a good
choice when we want to identify a pre-defined number of clusters, and
the aim is to gauge how well the data fit the clusters. These two
clustering techniques, therefore, complement each other with
Hierarchical clustering being a good choice for initial exploration and
K-means clustering being a good choice for targeted evaluation.

With these considerations in mind, let's start by identifying the
linguistic features that we want to use for clustering. Imagine that
among the various features that we are interested in associating
documents, we consider lemma use and part of speech use. However, we
need to operationalize what we mean by `use'. In machine learning, this
process is known as \textbf{feature engineering}. Since we aim to
compare documents it is logical for us to use the document-normalized
features. So in both lemma and part of speech tags, we will use the
relative frequency. An additional operation that we can apply to the
lemma feature is to weight the relative frequency by the dispersion of
the lemma. This will give us a measure of the distinctiveness of the
lemma in the document. A common implementation of this approach is to
use the \(tf\)-\(idf\) measure, which is the product of the relative
frequency and the inverse document frequency.

Each of these engineered feature sets represents a different aspect of
the lexical nature of the documents. The relative frequency of lemmas
represents the lexical diversity of the documents, the
dispersion-weighted \(tf\)-\(idf\) of lemmas represents the
distinctiveness of the lemmas in the documents, and the relative
frequency of part of speech tags represents the grammatical diversity of
the documents (\citeproc{ref-Petrenz2011}{Petrenz and Webber 2011}).

The next question to address in any analysis is how to represent the
features. In our case, we want to represent the features in each
document. In machine learning, the most common way to represent features
is in a matrix. In our case, we want to create a matrix with the
documents in the rows and the features in the columns. The values in the
matrix will be the operationalization of lexical use in each document
for each of our three candidate measures. This configuration is known as
a \textbf{document-term matrix} (DTM).

To recast a data frame into a DTM, we can use the \texttt{cast\_dtm()}
function from the \texttt{tidytext} package. This function takes a data
frame with a document identifier, a feature identifier, and a value for
each observation and casts it into a matrix. Operations such as
normalization are easily and efficiently performed in R on matrices, so
initially we can cast a frequency table of lemmas and part of speech
tags into a matrix and then normalize the matrix by documents. For the
\(tf\)-\(idf\) measure we use the \texttt{bind\_tf\_idf()} function from
the \texttt{tidytext} package. This function takes a data frame with a
document, feature, and frequency column (\texttt{n}) and calculates the
\(tf\)-\(idf\) measure for each feature in each document. This is a
normalized measure, so we do not need to normalize the matrix by
documents. Let's see how this works with the MASC dataset in
Example~\ref{exm-eda-masc-dtms}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dtms}{}\label{exm-eda-masc-dtms}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a document{-}term matrix of lemmas}
\NormalTok{masc\_lemma\_dtm }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(doc\_id, lemma) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{cast\_dtm}\NormalTok{(doc\_id, lemma, n) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{as.matrix}\NormalTok{()}

\CommentTok{\# Create a document{-}term matrix of part of speech tags}
\NormalTok{masc\_pos\_dtm }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(doc\_id, pos) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{cast\_dtm}\NormalTok{(doc\_id, pos, n) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{as.matrix}\NormalTok{()}

\CommentTok{\# Create a document{-}term matrix of tf{-}idf weighted lemmas}
\NormalTok{masc\_lemma\_tfidf\_dtm }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(doc\_id, lemma) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{bind\_tf\_idf}\NormalTok{(doc\_id, lemma, n) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{cast\_dtm}\NormalTok{(doc\_id, lemma, tf\_idf) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{as.matrix}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\end{example}

Note preview the a subset of the contents of a matrix, such as in
Example~\ref{exm-eda-masc-dtms}, we use bracket syntax \texttt{{[}{]}}
instead of the \texttt{head()} function. Let's take a look at the first
5 rows and 5 columns of the matrices, as seen in
Example~\ref{exm-eda-masc-dtms-preview}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dtms-preview}{}\label{exm-eda-masc-dtms-preview}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview}
\NormalTok{masc\_lemma\_dtm[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>      Terms
> Docs  'd 's M.  a account
>   1    1  1  1 15       1
>   10   0  0  0  7       0
>   100  0  0  0  0       0
>   101  0  0  0  2       0
>   102  0  0  0  1       0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{masc\_pos\_dtm[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>      Terms
> Docs  CC DT EX IN JJ
>   1   14 35  1 44 27
>   10  11 38  0 39 18
>   100  0  2  0  2  3
>   101  3 16  0 23  7
>   102 20 29  0 34 20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{masc\_lemma\_tfidf\_dtm[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>      Terms
> Docs      'd      's    M.        a account
>   1   0.0555 0.00324 0.227 0.006533  0.0413
>   10  0.0000 0.00000 0.000 0.003312  0.0000
>   100 0.0000 0.00000 0.000 0.000000  0.0000
>   101 0.0000 0.00000 0.000 0.001064  0.0000
>   102 0.0000 0.00000 0.000 0.000484  0.0000
\end{verbatim}

\end{example}

Now we can normalize the lemma and pos matrices by documents. We can do
this by dividing each feature count by the total count in each document.
This is a row-wise transformation, so we can use the \texttt{rowSums()}
function from base R to calculate the total count in each document. Then
each count divided by its row's total count, as seen in
Example~\ref{exm-eda-masc-dtms-normalized}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dtms-normalized}{}\label{exm-eda-masc-dtms-normalized}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Normalize lemma and pos matrices by documents}
\NormalTok{masc\_lemma\_dtm }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_lemma\_dtm }\SpecialCharTok{/} \FunctionTok{rowSums}\NormalTok{(masc\_lemma\_dtm)}

\NormalTok{masc\_pos\_dtm }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_pos\_dtm }\SpecialCharTok{/} \FunctionTok{rowSums}\NormalTok{(masc\_pos\_dtm)}
\end{Highlighting}
\end{Shaded}

\end{example}

There are two concerns to address before we can proceed with clustering.
First, clustering algorithm performance tends to degrade with the number
of features. If we consider either the relative frequency of lemmas or
the dispersion-weighted \(tf\)-\(idf\) of lemmas, we are looking at over
25k features! Second, clustering algorithms perform better with more
informative features. That is to say, features that are more distinct
across the documents provide better information for deriving useful
clusters.

We can address both of these concerns by reducing the number of features
and increasing the informativeness of the features. To accomplish this
is to use \textbf{dimensionality reduction}. Dimensionality reduction is
a set of methods that are used to reduce the number of features in a
dataset while retaining as much information as possible. The most common
method for dimensionality reduction is \textbf{principle component
analysis} (PCA). PCA is a method that transforms a set of correlated
variables into a set of uncorrelated variables, known as principle
components. The principle components are ordered by the amount of
variance that they explain in the data. The first principle component
explains the most variance, the second principle component explains the
second most variance, and so on.

We can apply PCA to each of these features and assess how well the
features account for the variation in the data. We can then use the
features that account for the most variation in the data for clustering.
The \texttt{prcomp()} function from base R can be used to perform PCA.
Let's apply PCA to each of our candidate feature matrices, as seen in
Example~\ref{exm-eda-masc-dtms-pca}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dtms-pca}{}\label{exm-eda-masc-dtms-pca}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Apply PCA to each feature matrix}
\NormalTok{masc\_lemma\_pca }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_lemma\_dtm }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{prcomp}\NormalTok{()}

\NormalTok{masc\_pos\_pca }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_pos\_dtm }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{prcomp}\NormalTok{()}

\NormalTok{masc\_lemma\_tfidf\_pca }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_lemma\_tfidf\_dtm }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{prcomp}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\end{example}

We can visualize the amount of variance explained by each principle
component with a scree plot. The \texttt{fviz\_eig()} function from the
\texttt{factoextra} package can be used to create a scree plot. The
\texttt{fviz\_eig()} function takes the output of the \texttt{prcomp()}
function as its argument and an argument \texttt{ncp\ =\ 10} to specify
the number of principle components to include in the plot. Let's create
a scree plot for each of our candidate feature matrices, as seen in
Figure~\ref{fig-eda-masc-dtms-pca-scree}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(factoextra)}

\CommentTok{\# Scree plot: lemma relative frequency}
\FunctionTok{fviz\_eig}\NormalTok{(masc\_lemma\_pca, }\AttributeTok{ncp =} \DecValTok{10}\NormalTok{)}

\CommentTok{\# Scree plot: lemma dispersion{-}weighted tf{-}idf}
\FunctionTok{fviz\_eig}\NormalTok{(masc\_lemma\_tfidf\_pca, }\AttributeTok{ncp =} \DecValTok{10}\NormalTok{)}

\CommentTok{\# Scree plot: part of speech relative frequency}
\FunctionTok{fviz\_eig}\NormalTok{(masc\_pos\_pca, }\AttributeTok{ncp =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\begin{minipage}{0.33\linewidth}

\centering{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-dtms-pca-scree-1.pdf}

}

\subcaption{\label{fig-eda-masc-dtms-pca-scree-1}Lemma relative
frequency}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-dtms-pca-scree-2.pdf}

}

\subcaption{\label{fig-eda-masc-dtms-pca-scree-2}Lemma
dispersion-weighted tf-idf}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-dtms-pca-scree-3.pdf}

}

\subcaption{\label{fig-eda-masc-dtms-pca-scree-3}part of speech relative
frequency}

\end{minipage}%

\caption{\label{fig-eda-masc-dtms-pca-scree}Scree plot of the principle
components of the MASC dataset}

\end{figure}%

From Figure~\ref{fig-eda-masc-dtms-pca-scree}, we can see the plots are
different. All trend toward less variance explained as the number of
dimensions increase. But for our purposes, we are interested in the
largest variance explained with the fewest dimensions. To that end, the
Figure~\ref{fig-eda-masc-dtms-pca-scree-3} plot is the most reduced and
most informative. The amount of variance explained is over 30\% for the
first dimension alone, however, the variance explained decreases between
4 and 5 dimensions. This is a good indication that we should use 4
dimensions for our clustering algorithm.

To calculate the amount of variance explained by each principle
component we can square the standard deviations of the principle
components and divide by the sum of the squared standard deviations.
Let's calculate the amount of variance explained by each principle
component for each of our candidate feature matrices, as seen in
Example~\ref{exm-eda-masc-dtms-pca-variance}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dtms-pca-variance}{}\label{exm-eda-masc-dtms-pca-variance}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate variance explained for first 3 principle components}
\CommentTok{\# lemma}
\NormalTok{masc\_lemma\_pca\_var }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_lemma\_pca}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(masc\_lemma\_pca}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \DecValTok{100}

\FunctionTok{sum}\NormalTok{(masc\_lemma\_pca\_var[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] 24.3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# pos}
\NormalTok{masc\_pos\_pca\_var }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_pos\_pca}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(masc\_pos\_pca}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \DecValTok{100}

\FunctionTok{sum}\NormalTok{(masc\_pos\_pca\_var[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] 67
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# lemma tf{-}idf}
\NormalTok{masc\_lemma\_tfidf\_pca\_var }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_lemma\_tfidf\_pca}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(masc\_lemma\_tfidf\_pca}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \DecValTok{100}

\FunctionTok{sum}\NormalTok{(masc\_lemma\_tfidf\_pca\_var[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] 7.94
\end{verbatim}

\end{example}

Combining the findings in the Scree plots and the variance explained
calculations, we can see that the first four principle components of the
part of speech features account for a good proportion of the variance.
Therefore, all else being equal, we should use the part of speech
features. As with all things exploratory, however, it is important to
consider the implications of our feature selection for our
interpretation of the results. In this case, the part of speech features
approximate grammatical diversity of the documents, more so than lexical
diversity. This means that the clusters that we identify will be based
on a particular measure of grammatical diversity of the documents. If,
for example, we want to identify clusters based on the lexical diversity
of the documents, we may opt to use the lemma features, or some other
operationalized measure of lexical diversity.

Before we leave PCA, let's also take a look at the principle components
themselves. The \texttt{get\_pca\_var()} function from the
\texttt{factoextra} package can be used to extract the principle
components from the output of the \texttt{prcomp()} function. The
\texttt{get\_pca\_var()} function takes the output of the
\texttt{prcomp()} function as its argument and an argument
\texttt{ncp\ =\ 10} to specify the number of principle components to
include in the plot. Let's create a plot of the first five principle
components for the part of speech data, as seen in
Figure~\ref{fig-eda-masc-dtms-pca-pc}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(factoextra)}

\CommentTok{\# Plot principle components: pos}
\NormalTok{masc\_pos\_pca }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fviz\_contrib}\NormalTok{(}
    \AttributeTok{choice =} \StringTok{"var"}\NormalTok{,}
    \AttributeTok{axes =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}
    \AttributeTok{top =} \DecValTok{20}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-dtms-pca-pc-1.pdf}

}

\caption{\label{fig-eda-masc-dtms-pca-pc}Feature contributions to the
PCA of the MASC dataset}

\end{figure}%

From Figure~\ref{fig-eda-masc-dtms-pca-pc}, we can see that the four
principle components are dominated by the relative frequency of nouns,
personal pronouns, prepositions, and determiners. This information can
help us better understand the results of the clustering algorithm.

Now that we have identified the features that we want to use for
clustering and we have represented the features in a way that can be
used for clustering, we can apply a clustering algorithm to the data.
For Hiearchical clustering, we can use the \texttt{hclust()} function
from base R. The \texttt{hclust()} function takes a distance matrix as
its argument and an argument \texttt{method\ =\ "average"} to specify
the average linkage method. The average linkage method takes the average
of the dissimilarities between all pairs in two clusters. It is less
sensitive to outliers compared to other methods. Let's apply the
clustering algorithm to the part of speech features, as seen in
Example~\ref{exm-eda-masc-pos-hclust}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-pos-hclust}{}\label{exm-eda-masc-pos-hclust}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract first 4 principle components}
\NormalTok{masc\_pos\_pca\_pc }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_pos\_pca}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}

\CommentTok{\# Create distance matrix}
\NormalTok{masc\_pos\_dist }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_pos\_pca\_pc }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{dist}\NormalTok{(}\AttributeTok{method =} \StringTok{"manhattan"}\NormalTok{)}

\CommentTok{\# Apply the clustering algorithm}
\NormalTok{masc\_pos\_hc }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_pos\_dist }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{hclust}\NormalTok{(}\AttributeTok{method =} \StringTok{"average"}\NormalTok{)}

\CommentTok{\# Visualize}
\NormalTok{masc\_pos\_hc }\SpecialCharTok{|\textgreater{}} \FunctionTok{fviz\_dend}\NormalTok{(}\AttributeTok{show\_labels =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-pos-hclust-1.pdf}

}

\caption{\label{fig-eda-masc-pos-hclust}Hierarchical clustering of the
MASC dataset}

\end{figure}%

\end{example}

Since we are exploring the usefulness of the 18 genre labels used in the
MASC dataset we have a good idea of how many clusters we want to start
with. This is a good case to employ the K-means clustering algorithm. In
K-means clustering, we specify the number of clusters that we want to
identify. For each cluster number, a random center is generated. Then
each observation is assigned to the cluster with the nearest center. The
center of each cluster is then recalculated based on the distribution of
the observations in the cluster. This process is iterates either a
pre-defined number of times, or until the centers converge (\emph{i.e}
observations stop switching clusters).

We can use the \texttt{kmeans()} function from base R to apply the
K-means clustering algorithm. The \texttt{kmeans()} function takes the
matrix of features as its first argument and the number of clusters as
its second argument. We can specify the number of clusters with the
\texttt{centers} argument. The \texttt{kmeans()} function also takes an
argument \texttt{nstart} to specify the number of random starts. The
K-means algorithm is sensitive to the initial starting points, so it is
a good idea to run the algorithm multiple times with different starting
points. The \texttt{nstart} argument specifies the number of random
starts. The default value is 1, but we can increase this to 10 or 20 to
increase the likelihood of finding a good solution.

Our goal, then, will be to assess how well this number of clusters fits
the data. If it does not fit the data well, we can try a different
number of clusters. We can then compare the results of the clustering
with the genre labels to see how well the clusters map to the labels and
make ajustments to the way we group the labels as necessary.

Let's start with 18 clusters, assuming the target of the number of
genres in the MASC dataset. We can apply the K-means clustering
algorithm to the part of speech features, as seen in
Example~\ref{exm-eda-masc-pos-kmeans}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-pos-kmeans}{}\label{exm-eda-masc-pos-kmeans}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract first 4 principle components}
\NormalTok{masc\_pos\_pca\_pc }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_pos\_pca}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}

\CommentTok{\# K{-}means clustering}
\NormalTok{masc\_pos\_kmeans }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_pos\_pca\_pc }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kmeans}\NormalTok{(}
    \AttributeTok{centers =} \DecValTok{18}\NormalTok{,}
    \AttributeTok{nstart =} \DecValTok{25}\NormalTok{,}
    \AttributeTok{iter.max =} \DecValTok{20}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\end{example}

The \texttt{factoextra} package provides \texttt{fviz\_cluster()}
function for visualizing the results of clustering algorithms. The
\texttt{fviz\_cluster()} function takes the output of the
\texttt{kmeans()} function as its first argument and the matrix of
features as its second argument. The \texttt{fviz\_cluster()} function
can be used to visualize the clusters in the data, as seen in
Figure~\ref{fig-eda-masc-pos-kmeans}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize}
\NormalTok{masc\_pos\_kmeans }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# output of kmeans()}
  \FunctionTok{fviz\_cluster}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ masc\_pos\_pca\_pc, }\CommentTok{\# matrix of features}
    \AttributeTok{ellipse.type =} \StringTok{"norm"}\NormalTok{,}
    \AttributeTok{ellipse.level =} \FloatTok{0.95}\NormalTok{,}
    \AttributeTok{geom =} \StringTok{"point"}\NormalTok{,}
    \AttributeTok{pointsize =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{palette =} \StringTok{"grey"}\NormalTok{,}
    \AttributeTok{ggtheme =} \FunctionTok{theme\_qtalr}\NormalTok{()}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-pos-kmeans-1.pdf}

}

\caption{\label{fig-eda-masc-pos-kmeans}K-means clustering of the MASC
dataset}

\end{figure}%

The ellipses in a k-means plot represent the 95\% confidence interval
for each cluster. The ellipses are based on the multivariate normal
distribution of the data in each cluster. The size and shape of the
ellipses tell us about the variance of the data in each cluster. The
larger the ellipse the greater the dispersion of values. A wide ellipse
suggests high within cluster variability. The distance between the
clusters also tells us about the similarity between the clusters. The
closer the clusters are to each other, the more similar they are. The
further the clusters are from each other, the more dissimilar they are.

So in Figure~\ref{fig-eda-masc-pos-kmeans}, we see a mix of shapes and
sizes. This suggests that some clusters are more homogeneous than
others. We also see separation between the large wide clusters towards
the top of the plot and the smaller, more circular clusters towards the
center. With 18 clusters, we have a lot of clusters, so it is difficult
to interpret the results as there is a large amount of overlap. In sum,
18 clusters is likely not an optimal number for this clustering
approach.

We could run the code in Example~\ref{exm-eda-masc-pos-kmeans} for
different values for \(k\) and plot each in turn. But a more effective
way to determine the optimal number of clusters is to plot the
within-cluster sum of squares (WSS) for a range of values for \(k\). The
WSS is the sum of the squared distance between each observation and its
cluster center. With a plot of the WSS for a range of values for \(k\),
we can identify the value for \(k\) where the WSS begins to level off,
using the elbow method. It is not always clear where the elbow is, yet
it is a good starting point for identifying the optimal number of
clusters.

Again, the \texttt{factoextra} package has us covered. The
\texttt{fviz\_nbclust()} function can be used to plot the WSS for a
range of values for \(k\). The \texttt{fviz\_nbclust()} function takes
the \texttt{kmeans()} function as its first argument and the matrix of
features as its second argument. The \texttt{fviz\_nbclust()} function
also takes arguments \texttt{method\ =\ "wss"} to specify the WSS method
and \texttt{k.max\ =\ 20} to specify the maximum number of clusters to
plot. Let's plot the WSS for a range of values for \(k\), as seen in
Figure~\ref{fig-eda-masc-pos-kmeans-elbow}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Determine the optimal number of clusters}
\NormalTok{masc\_pos\_pca\_pc }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fviz\_nbclust}\NormalTok{(}
    \AttributeTok{FUNcluster =}\NormalTok{ kmeans,}
    \AttributeTok{method =} \StringTok{"wss"}\NormalTok{, }\CommentTok{\# method}
    \AttributeTok{k.max =} \DecValTok{20}\NormalTok{,}
    \AttributeTok{nstart =} \DecValTok{25}\NormalTok{,}
    \AttributeTok{iter.max =} \DecValTok{20}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-pos-kmeans-elbow-1.pdf}

}

\caption{\label{fig-eda-masc-pos-kmeans-elbow}Elbow method for k-means
clustering of the MASC dataset}

\end{figure}%

It is clear that there is significant gains in cluster fit from 1 to 4
clusters, but the gains begin to level off after 5-7 clusters. Now we
can skip ahead and try 4 clusters, as seen in
Example~\ref{exm-eda-masc-pos-kmeans-fit}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-pos-kmeans-fit}{}\label{exm-eda-masc-pos-kmeans-fit}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# K{-}means: for 5 clusters}
\NormalTok{masc\_pos\_kmeans\_fit }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_pos\_pca\_pc }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kmeans}\NormalTok{(}
    \AttributeTok{centers =} \DecValTok{4}\NormalTok{,}
    \AttributeTok{nstart =} \DecValTok{25}\NormalTok{,}
    \AttributeTok{iter.max =} \DecValTok{20}
\NormalTok{  )}

\CommentTok{\# Visualize}
\NormalTok{masc\_pos\_kmeans\_fit }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fviz\_cluster}\NormalTok{(}
  \AttributeTok{data =}\NormalTok{ masc\_pos\_pca\_pc,}
  \AttributeTok{ellipse.type =} \StringTok{"norm"}\NormalTok{,}
  \AttributeTok{ellipse.level =} \FloatTok{0.95}\NormalTok{,}
  \AttributeTok{geom =} \StringTok{"point"}\NormalTok{,}
  \AttributeTok{pointsize =} \DecValTok{1}\NormalTok{,}
  \AttributeTok{palette =} \StringTok{"grey"}\NormalTok{,}
  \AttributeTok{ggtheme =} \FunctionTok{theme\_qtalr}\NormalTok{()}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-pos-kmeans-fit-1.pdf}

}

\caption{\label{fig-eda-masc-pos-kmeans-fit}K-means clustering of the
MASC dataset with 4 clusters}

\end{figure}%

\end{example}

The results are much more interpretable with 4 clusters. We can see that
the clusters are more homogeneous and more distinct from each other, in
particular clusters 1 and 2, and are generally similar in shape and
size. However, there is still some overlap between the clusters, in
particular for clusters 3 and 4. We expect there to be noise as we have
paired down the number of features from over 25k to 4, so this is a good
working solution.

From this point we can join document-cluster pairings produced by the
k-means algorithm with the original dataset. We can then explore the
clusters in terms of the original features. We can also explore the
clusters in terms of the original labels. Let's join the cluster
assignments to the original dataset, as seen in
Example~\ref{exm-eda-masc-pos-kmeans-join}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-pos-kmeans-join}{}\label{exm-eda-masc-pos-kmeans-join}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Organize k{-}means clusters into a tibble}
\NormalTok{masc\_pos\_cluster\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{doc\_id =} \FunctionTok{names}\NormalTok{(masc\_pos\_kmeans\_fit}\SpecialCharTok{$}\NormalTok{cluster),}
    \AttributeTok{cluster =}\NormalTok{ masc\_pos\_kmeans\_fit}\SpecialCharTok{$}\NormalTok{cluster}
\NormalTok{  )}

\CommentTok{\# Join cluster assignments to original dataset}
\NormalTok{masc\_cluster\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_tbl}\SpecialCharTok{|\textgreater{}}
  \FunctionTok{left\_join}\NormalTok{(}
\NormalTok{    masc\_pos\_cluster\_tbl,}
    \AttributeTok{by =} \StringTok{"doc\_id"}
\NormalTok{  )}

\CommentTok{\# Preview}
\NormalTok{masc\_cluster\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 5 x 8
>   doc_id modality genre   term_num term         lemma        pos   cluster
>   <chr>  <chr>    <chr>      <dbl> <chr>        <chr>        <chr>   <int>
> 1 1      Written  Letters        2 Your         your         PRP$        1
> 2 1      Written  Letters        3 contribution contribution NN          1
> 3 1      Written  Letters        4 to           to           TO          1
> 4 1      Written  Letters        6 will         will         MD          1
> 5 1      Written  Letters        7 mean         mean         VB          1
\end{verbatim}

\end{example}

We now see that the cluster assignments from the k-means algorithm have
been joined to the original dataset. We can now explore the clusters in
terms of the original features. For example, let's look at the
distribution of the clusters across modality first, as seen in
Example~\ref{exm-eda-masc-pos-kmeans-modality}. To do this, we first
need to reduce our dataset to the distinct combinations of modality,
genre, and cluster. Then, we can use the \texttt{janitor} package's
\texttt{tabyl()} function to provided formatted percentages.

\begin{example}[]\protect\hypertarget{exm-eda-masc-pos-kmeans-modality}{}\label{exm-eda-masc-pos-kmeans-modality}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(janitor)}

\CommentTok{\# Reduce to distinct combinations of modality, genre, and cluster}
\NormalTok{masc\_meta\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_cluster\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{distinct}\NormalTok{(modality, genre, cluster)}

\CommentTok{\# Tabulate: cluster by modality}
\NormalTok{masc\_meta\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tabyl}\NormalTok{(cluster, modality) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{adorn\_percentages}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{(}\AttributeTok{digits =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>  cluster Spoken Written
>        1  30.8%   69.2%
>        2   0.0%  100.0%
>        3   8.3%   91.7%
>        4   0.0%  100.0%
\end{verbatim}

\end{example}

From Example~\ref{exm-eda-masc-pos-kmeans-modality}, we can see that the
clusters are not evenly distributed across the modalities. In
particular, cluster 1 is where the great majority of the spoken modality
appears. We can also appreciate that there may be some written genres
that are more similar to spoken genres than other written genres. This
may be something we could like to explore further.

Let's dive into genres and limit our analysis to the written modality,
as seen in Example~\ref{exm-eda-masc-pos-kmeans-genre}. To do this, we
first need to filter the dataset to the written modality. In addition to
the row-wise percentages which capture the proportion of the genre that
contributes to each cluster, let's also consider the column-wise
percentages to consider how each genre is distrributed across the
clusters.

\begin{example}[]\protect\hypertarget{exm-eda-masc-pos-kmeans-genre}{}\label{exm-eda-masc-pos-kmeans-genre}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tabulate: cluster by genre for written modality}
\NormalTok{masc\_meta\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(modality }\SpecialCharTok{==} \StringTok{"Written"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tabyl}\NormalTok{(cluster, genre) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{adorn\_percentages}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{(}\AttributeTok{digits =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>  cluster  Blog Email Essay Fiction Fictlets Government Jokes Journal Letters
>        1 11.1% 11.1% 11.1%   11.1%    11.1%       0.0% 11.1%   11.1%   11.1%
>        2 20.0% 20.0%  0.0%    0.0%     0.0%       0.0%  0.0%    0.0%   20.0%
>        3  9.1%  9.1%  9.1%    9.1%     0.0%       9.1%  0.0%    9.1%    9.1%
>        4  0.0% 20.0% 20.0%    0.0%     0.0%       0.0%  0.0%    0.0%    0.0%
>  Newspaper Non-fiction Technical Travel Guide Twitter
>      11.1%        0.0%      0.0%         0.0%    0.0%
>      20.0%        0.0%      0.0%         0.0%   20.0%
>       9.1%        9.1%      9.1%         9.1%    0.0%
>      20.0%       20.0%     20.0%         0.0%    0.0%
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tabulate: cluster by genre for written modality}
\NormalTok{masc\_meta\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(modality }\SpecialCharTok{==} \StringTok{"Written"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tabyl}\NormalTok{(cluster, genre) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{adorn\_percentages}\NormalTok{(}\StringTok{"col"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{(}\AttributeTok{digits =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>  cluster  Blog Email Essay Fiction Fictlets Government  Jokes Journal Letters
>        1 33.3% 25.0% 33.3%   50.0%   100.0%       0.0% 100.0%   50.0%   33.3%
>        2 33.3% 25.0%  0.0%    0.0%     0.0%       0.0%   0.0%    0.0%   33.3%
>        3 33.3% 25.0% 33.3%   50.0%     0.0%     100.0%   0.0%   50.0%   33.3%
>        4  0.0% 25.0% 33.3%    0.0%     0.0%       0.0%   0.0%    0.0%    0.0%
>  Newspaper Non-fiction Technical Travel Guide Twitter
>      25.0%        0.0%      0.0%         0.0%    0.0%
>      25.0%        0.0%      0.0%         0.0%  100.0%
>      25.0%       50.0%     50.0%       100.0%    0.0%
>      25.0%       50.0%     50.0%         0.0%    0.0%
\end{verbatim}

\end{example}

On the other hand, looking at the written genres, we see that there are
some genres which are grouped entirely in cluster 1. In other words,
these genres, such as `Fictlets' and `Jokes', align with spoken genres
in terms of their grammatical diversity. This is an interesting finding
that we may want to explore further. Furthermore, it may be of interest
to explore individual documents in genres which have a signifcant
proportion of documents in cluster 1. There are too many possibilities
to explore here but this is a good example of how exploratory data
analysis can be used to identify new questions and new variables of
interest.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

Given the cluster assignments derived using the distribution of part of
speech tags, what other relationships between the clusters and the
original features could one explore? What are the limitations of this
approach? What are the implications of this approach for the
interpretation of the results?

\end{tcolorbox}

\subsubsection{Vector space models}\label{sec-eda-vector-space-models}

In our discussion of clustering, we targeted associations between
documents based on the distribution of linguistic features. We now turn
to targeting associations between linguistic features based on their
distribution across documents. The technique we will introduce is known
as \textbf{vector space modeling}. Vector space modeling aims to
represent linguistic features as numerical vectors which reflect the
various linguistic contexts in which the features appear. Together these
vectors form a feature-context space in which features with similar
contextual distributions are closer together.

An interesting property of vector space models is that are able to
capture semantic and/ or syntactic relationships between features based
on their distribution. In this way, vector space modeling can be seen as
an implementation of the \textbf{distributional hypothesis} --that is,
words that appear in similar linguistic contexts tend to have similar
meanings (\citeproc{ref-Harris1954}{Harris 1954}). As Firth
(\citeproc{ref-Firth1957}{1957}) states ``you shall know a word by the
company it keeps''.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-alt} Case study}

Garg et al. (\citeproc{ref-Garg2018}{2018}) quantify and compare gender
and ethnic stereotypes over time using word embeddings. The authors
explore the temporal dynamics of stereotypes using word embeddings as a
quantitative measure of bias. The data used includes word embeddings
from the Google News dataset for contemporary analysis, as well as
embeddings from the COHA and Google Books datasets for historical
analysis. Additional validation is done using embeddings from the New
York Times Annotated Corpus. Several word lists representing gender,
ethnicity, and neutral words are collated for analysis. The main finding
is that language reflects and perpetuates cultural stereotypes, and the
analysis shows consistency in the relationships between embedding bias
and external metrics across datasets over time. The results also
highlight the impact of historical events, such as the women's movement
of the 1960s, on the encoding of stereotypes.

\end{tcolorbox}

Let's assume in our textbook project we are interested in gathering
information about English's expression of the semantic concepts of
manner and motion. For learners of English, this can be an area of
difficulty as languages differ in how these semantic properties are
expressed. English is a good example of a ``satellite-framed'' language,
that is that manner and motion are often encoded in the same verb with a
particle encoding the motion path (``rush out'', ``climb up''). Other
languages such as Spanish, Turkish, and Japanese are ``verb-framed''
languages, that is that motion but not manner is encoded in the verb
(``salir corriendo'', ``koşarak çıkmak'', ``走り出す'').

We can use vector space modeling to represent the distribution of verbs
in the MASC dataset and then target the concepts of manner and motion to
then explore how English encodes these concepts. The question will be
what will our features be. They could be terms, lemmas, pos tags,
\emph{etc}. Or they could be some combination. Considering the task at
hand which we will ultimately want to know something about verbs, it
makes sense to include the part of speech information in combination
with either the term or the lemma.

If we include term and pos then we have a feature for every
morphological variant of the term (\emph{e.g.} house\_VB, housed\_VBD,
housing\_VBG). This can make the model more sizeable than it needs to
be. If we include lemma and pos then we have a feature for every lemma
with a distinct grammatical category (\emph{e.g.} house\_NN, house\_VB).
Note that as the pos tags are from the Penn tagset, many morphological
variants appear in the tag itself (\emph{e.g.} house\_VB, houses\_VBZ,
housing\_VBG). This is a good example of how the choice of features can
impact the size of the model. In our case, it is not clear that we need
to include the morphological variants of the verbs, so we will use lemma
and a simplified pos as our features.

To engineer these features we will need to simplify the tags. We will
conflate Penn tagset distinctions between nouns, verbs, adjectives, and
adverbs. This will give us a feature for every lemma with a distinct
grammatical category (\emph{e.g.} house\_NOUN, house\_VERB), and no more
than that. To do this we can apply the \texttt{case\_when()} function
from the \texttt{dplyr} package. The \texttt{case\_when()} function
takes a series of logical statements and returns a value based on the
first logical statement that is \texttt{TRUE}. Let's conflate the Penn
tagset distinctions between nouns, verbs, adjectives, and adverbs, as
seen in Example~\ref{exm-eda-masc-vsm-lemma-pos}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-vsm-lemma-pos}{}\label{exm-eda-masc-vsm-lemma-pos}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Conflate Penn tagset into Universal{-}like tagset}
\NormalTok{masc\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{xpos =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    pos }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"NN"}\NormalTok{, }\StringTok{"NNS"}\NormalTok{, }\StringTok{"NNP"}\NormalTok{, }\StringTok{"NNPS"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"NOUN"}\NormalTok{,}
\NormalTok{    pos }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"VB"}\NormalTok{, }\StringTok{"VBD"}\NormalTok{, }\StringTok{"VBG"}\NormalTok{, }\StringTok{"VBN"}\NormalTok{, }\StringTok{"VBP"}\NormalTok{, }\StringTok{"VBZ"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"VERB"}\NormalTok{,}
\NormalTok{    pos }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"JJ"}\NormalTok{, }\StringTok{"JJR"}\NormalTok{, }\StringTok{"JJS"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"ADJ"}\NormalTok{,}
\NormalTok{    pos }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"RB"}\NormalTok{, }\StringTok{"RBR"}\NormalTok{, }\StringTok{"RBS"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"ADV"}\NormalTok{,}
    \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}}\NormalTok{ pos }\CommentTok{\# keep other tags}
\NormalTok{  ))}

\CommentTok{\# Lemma + pos}
\NormalTok{masc\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lemma\_pos =} \FunctionTok{str\_c}\NormalTok{(lemma, xpos, }\AttributeTok{sep =} \StringTok{"\_"}\NormalTok{))}

\CommentTok{\# Preview}
\NormalTok{masc\_tbl }\SpecialCharTok{|\textgreater{}} \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 439,215
> Columns: 9
> $ doc_id    <chr> "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", ~
> $ modality  <chr> "Written", "Written", "Written", "Written", "Written", "Writ~
> $ genre     <chr> "Letters", "Letters", "Letters", "Letters", "Letters", "Lett~
> $ term_num  <dbl> 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20,~
> $ term      <chr> "Your", "contribution", "to", "will", "mean", "more", "than"~
> $ lemma     <chr> "your", "contribution", "to", "will", "mean", "more", "than"~
> $ pos       <chr> "PRP$", "NN", "TO", "MD", "VB", "JJR", "IN", "PRP", "MD", "V~
> $ xpos      <chr> "PRP$", "NOUN", "TO", "MD", "VERB", "ADJ", "IN", "PRP", "MD"~
> $ lemma_pos <chr> "your_PRP$", "contribution_NOUN", "to_TO", "will_MD", "mean_~
\end{verbatim}

\end{example}

When VSM is applied to words, it is known as \textbf{word embedding}. To
calculate word embeddings there are various algorithms that can be used
(BERT, word2vec, GloVe, \emph{etc.}) The most common algorithm is
\textbf{word2vec} (\citeproc{ref-Mikolov2013b}{Mikolov et al. 2013}).
Word2vec is a neural network-based algorithm that learns word embeddings
from a large corpus of text. In the word2vec algorithm the researcher
can choose to learn embeddings from a Continuous Bag of Words (CBOW) or
a Skip-gram model. The CBOW model predicts a target word based on the
context words. The Skip-gram model predicts the context words based on
the target word. The CBOW model is faster to train and is better for
frequent words. The Skip-gram model is slower to train and is better for
infrequent words.

Another consideration to take into account is the size of the corpus
used to train the model. VSM provide more reliable results when trained
on larger corpora. The MASC dataset is relatively small. We've
simplified our features in order to have a smaller vocabulary in hopes
to offset this limitation to a degree. But the choice of either CBOW or
Skip-gram can also help to offset this limitation. CBOW can be better
for smaller corpora as it aggregates context infomation.

To implement the word2vec algorithm on our lemma + pos features, we will
use the \texttt{word2vec} package. The \texttt{word2vec()} function
takes a text file and uses it to train the vector representations. To
prepare the MASC dataset for training, we will need to write the lemma +
pos features to a text file as a single character string. We can do this
by first collapsing the \texttt{lemma\_pos} variable into a single
string for each document using the \texttt{str\_c()} function. Then we
can use the \texttt{write\_lines()} function to write the string to a
text file. Let's prepare the MASC dataset for training, as seen in
Example~\ref{exm-eda-masc-vsm-word2vec-text}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-vsm-word2vec-text}{}\label{exm-eda-masc-vsm-word2vec-text}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Prepare data for word2vec training}
\NormalTok{masc\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{text =} \FunctionTok{str\_c}\NormalTok{(lemma\_pos, }\AttributeTok{collapse =} \StringTok{" "}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(text) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{write\_lines}\NormalTok{(}
    \AttributeTok{file =} \StringTok{"../data/analysis/masc\_word2vec.txt"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\end{example}

With our \emph{masc\_word2vect.txt} file, we read in. apply the word2vec
algorithm using the \texttt{word2vec} package, and write the model to
disk. By default, te \texttt{word2vec()} function applies the CBOW
model, with 50 dimensions, a window size of 5, and a minimum word count
of 5. We can change these parameters as needed, but let's apply the
default algorithm to the text file spliting features by sentence
punctuation, as seen in Example~\ref{exm-eda-masc-vsm-word2vec-train}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-vsm-word2vec-train}{}\label{exm-eda-masc-vsm-word2vec-train}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(word2vec)}

\CommentTok{\# Traing word2vec model}
\NormalTok{masc\_model }\OtherTok{\textless{}{-}}
  \FunctionTok{word2vec}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"../data/analysis/masc\_word2vec.txt"}\NormalTok{,}
    \AttributeTok{split =} \FunctionTok{c}\NormalTok{(}\StringTok{" "}\NormalTok{, }\StringTok{".?!"}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Write model to disk}
\FunctionTok{write.word2vec}\NormalTok{(}
\NormalTok{  masc\_model,}
  \AttributeTok{file =} \StringTok{"../data/analysis/masc\_word2vec.bin"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

Writing the model to disk is important as it allows us to read the model
in without having to retrain it. In cases where the corpus is large,
this can save a lot of computational time.

Now that we have a trained model, we can read it in with the
\texttt{read.vectors()} function from the \texttt{wordVectors} package.

\begin{example}[]\protect\hypertarget{exm-eda-masc-vsm-word2vec-read}{}\label{exm-eda-masc-vsm-word2vec-read}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(wordVectors)}

\CommentTok{\# Read word2vec model}
\NormalTok{masc\_model }\OtherTok{\textless{}{-}}
  \FunctionTok{read.vectors}\NormalTok{(}
    \AttributeTok{filename =} \StringTok{"../data/analysis/masc\_word2vec.bin"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\end{example}

The \texttt{read.vectors()} function returns a matrix where each row is
a term in the model and each column is a dimension in the vector space,
as seen in Example~\ref{exm-eda-masc-vsm-word2vec-vector-object}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-vsm-word2vec-vector-object}{}\label{exm-eda-masc-vsm-word2vec-vector-object}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Inspect}
\FunctionTok{dim}\NormalTok{(masc\_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] 5892   50
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview}
\NormalTok{masc\_model[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> A VectorSpaceModel object of  5  words and  5  vectors
>                     [,1]   [,2]   [,3]    [,4]    [,5]
> map_VERB          -1.487  1.510 -0.662 -1.2129 -0.6458
> KGCUBS10_NOUN     -0.387  1.751 -0.603  0.6270 -0.1965
> MICKEY3_NOUN       0.675  1.035 -0.516  1.1289 -0.0784
> amenity_NOUN      -0.311  0.682 -1.225  0.0541 -0.8392
> transmission_NOUN -0.837 -0.354 -1.193 -0.5576 -0.7866
> attr(,".cache")
> <environment: 0x7f9f5bf5dd98>
\end{verbatim}

\end{example}

The row-wise vector in the model is the vector representation of each
feature. The notion is that these values can now be compared with other
terms to explore distributional relatedness. We can extract specific
features from the matrix using the \texttt{{[}{]}} operator.

As an example, let's compare the vectors for noun-verb pairs for the
lemmas `run' and `walk'. To do this we extract these features from the
model. To appreciate the relatedness of these features it is best to
visualize them. We can do this by first reducing the dimensionality of
the vectors using principal components analysis (PCA). We can then plot
the first two principle components, as seen in
Figure~\ref{fig-eda-masc-vsm-word2vec-similarity}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-vsm-word2vec-similarity}{}\label{exm-eda-masc-vsm-word2vec-similarity}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract vectors}
\NormalTok{word\_vectors }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_model[}\FunctionTok{c}\NormalTok{(}\StringTok{"run\_VERB"}\NormalTok{, }\StringTok{"walk\_VERB"}\NormalTok{, }\StringTok{"run\_NOUN"}\NormalTok{, }\StringTok{"walk\_NOUN"}\NormalTok{), ] }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{as.matrix}\NormalTok{()}

\NormalTok{pca }\OtherTok{\textless{}{-}}
\NormalTok{  word\_vectors }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{scale}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{prcomp}\NormalTok{()}

\NormalTok{pca\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{as\_tibble}\NormalTok{(pca}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{word =} \FunctionTok{rownames}\NormalTok{(word\_vectors))}

\NormalTok{pca\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ PC1, }\AttributeTok{y =}\NormalTok{ PC2, }\AttributeTok{label =}\NormalTok{ word)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
\NormalTok{  ggrepel}\SpecialCharTok{::}\FunctionTok{geom\_text\_repel}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-vsm-word2vec-similarity-1.pdf}

}

\caption{\label{fig-eda-masc-vsm-word2vec-similarity}Similarity between
`run' and `walk' in the MASC dataset}

\end{figure}%

\end{example}

From Figure~\ref{fig-eda-masc-vsm-word2vec-similarity}, we can see that
each of these features occupies a distinct position in the reduced
vector space. But on closer inspection, we can see that there is a
relationship between the lemma pairs. Remember that PCA reduces the
dimensionality of the data by identifying the dimensions that capture
the greatest amount of variance in the data. This means that of the 50
dimensions in the model, the PC1 and PC2 correspond to orthogonal
dimensions that capture the greatest amount of variance in the data. If
we look along PC2, we can see that there is a distinction between part
of speech. Looking along PC1, we see some pariety between lemma
meanings. Given these features, we can see that meaning and grammatical
category can be approximated in the vector space.

An interesting property of vector space models is that we can build up a
dimension of meaning by adding vectors that we expect to approximate
that meaning. For example, we can add the vectors for typical motion
verbs to create a vector for motion-similarity and one for
manner-similarity. We can then compare the feature vectors for all verbs
and assess their motion-similarity and manner-similarity.

To do this let's first subset the model to only include verbs, as in
Example~\ref{exm-eda-masc-vsm-word2vec-verbs}. We will also remove the
part of speech tags from the rownames of the matrix as they are no
longer needed.

\begin{example}[]\protect\hypertarget{exm-eda-masc-vsm-word2vec-verbs}{}\label{exm-eda-masc-vsm-word2vec-verbs}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter to verbs}
\NormalTok{verbs }\OtherTok{\textless{}{-}} \FunctionTok{str\_subset}\NormalTok{(}\FunctionTok{rownames}\NormalTok{(masc\_model), }\StringTok{".*\_VERB"}\NormalTok{)}
\NormalTok{verb\_vectors }\OtherTok{\textless{}{-}}\NormalTok{ masc\_model[verbs, ]}

\CommentTok{\# Remove part of speech tags}
\FunctionTok{rownames}\NormalTok{(verb\_vectors) }\OtherTok{\textless{}{-}}
\NormalTok{  verb\_vectors }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{rownames}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{str\_replace\_all}\NormalTok{(}\StringTok{"\_VERB"}\NormalTok{, }\StringTok{""}\NormalTok{)}

\CommentTok{\# Inspect}
\FunctionTok{dim}\NormalTok{(verb\_vectors)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] 1115   50
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview}
\NormalTok{verb\_vectors[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> A VectorSpaceModel object of  5  words and  5  vectors
>             [,1]  [,2]   [,3]    [,4]   [,5]
> map      -1.4875 1.510 -0.662 -1.2129 -0.646
> whip      0.0496 0.919 -1.020  0.3087  0.350
> enroll   -0.6251 1.620 -1.878  0.0861 -0.428
> tuck      0.5610 1.054 -1.331 -0.5298  1.598
> suppress  0.3824 1.928 -1.750 -0.3229 -0.643
> attr(,".cache")
> <environment: 0x7f9f3858aea0>
\end{verbatim}

\end{example}

We now have \texttt{verb\_vectors} which includes the vector
representations for all verbs 1,115 in the MASC dataset. Next, let's
seed the vectors for motion-similarity and manner-similarity and
calculate the vector `closeness' to the motion and manner seed vectors
with the \texttt{closest\_to()} function from the \texttt{wordVectors()}
package.

\begin{example}[]\protect\hypertarget{exm-eda-masc-vsm-word2vec-manner-motion}{}\label{exm-eda-masc-vsm-word2vec-manner-motion}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add vectors for motion{-}similarity and manner{-}similarity}
\NormalTok{motion }\OtherTok{\textless{}{-}}
  \FunctionTok{c}\NormalTok{(}\StringTok{"go"}\NormalTok{, }\StringTok{"come"}\NormalTok{, }\StringTok{"leave"}\NormalTok{, }\StringTok{"arrive"}\NormalTok{, }\StringTok{"enter"}\NormalTok{, }\StringTok{"exit"}\NormalTok{, }\StringTok{"depart"}\NormalTok{, }\StringTok{"return"}\NormalTok{)}

\NormalTok{motion\_similarity }\OtherTok{\textless{}{-}}
\NormalTok{  verb\_vectors }\SpecialCharTok{|\textgreater{}} \FunctionTok{closest\_to}\NormalTok{(motion, }\AttributeTok{n =} \ConstantTok{Inf}\NormalTok{)}

\CommentTok{\# Preview}
\NormalTok{motion\_similarity }\SpecialCharTok{|\textgreater{}} \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 1,115
> Columns: 2
> $ word                   <chr> "tie", "approach", "enter", "step", "race", "ru~
> $ `similarity to motion` <dbl> 0.803, 0.803, 0.786, 0.784, 0.781, 0.770, 0.769~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{manner }\OtherTok{\textless{}{-}}
  \FunctionTok{c}\NormalTok{(}\StringTok{"run"}\NormalTok{, }\StringTok{"walk"}\NormalTok{, }\StringTok{"jump"}\NormalTok{, }\StringTok{"crawl"}\NormalTok{, }\StringTok{"swim"}\NormalTok{, }\StringTok{"fly"}\NormalTok{, }\StringTok{"drive"}\NormalTok{, }\StringTok{"ride"}\NormalTok{)}

\NormalTok{manner\_similarity }\OtherTok{\textless{}{-}}
\NormalTok{  verb\_vectors }\SpecialCharTok{|\textgreater{}} \FunctionTok{closest\_to}\NormalTok{(manner, }\AttributeTok{n =} \ConstantTok{Inf}\NormalTok{)}

\CommentTok{\# Preview}
\NormalTok{manner\_similarity }\SpecialCharTok{|\textgreater{}} \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 1,115
> Columns: 2
> $ word                   <chr> "walk", "drop", "pull", "climb", "step", "throw~
> $ `similarity to manner` <dbl> 0.885, 0.877, 0.864, 0.860, 0.859, 0.842, 0.840~
\end{verbatim}

\end{example}

The \texttt{motion\_similarity} and \texttt{motion\_similarity} data
frames each contain all the verbs with a corresponding closeness
measure. We can join these two data frames by feature to create a single
data frame with the motion-similarity and manner-similarity measures, as
seen in Example~\ref{exm-eda-masc-vsm-word2vec-manner-motion}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-vsm-word2vec-manner-motion}{}\label{exm-eda-masc-vsm-word2vec-manner-motion}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Join motion{-}similarity and manner{-}similarity}
\NormalTok{manner\_motion\_similarity }\OtherTok{\textless{}{-}}
\NormalTok{  manner\_similarity }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{inner\_join}\NormalTok{(motion\_similarity)}

\CommentTok{\# Preview}
\NormalTok{manner\_motion\_similarity }\SpecialCharTok{|\textgreater{}} \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 1,115
> Columns: 3
> $ word                   <chr> "walk", "drop", "pull", "climb", "step", "throw~
> $ `similarity to manner` <dbl> 0.885, 0.877, 0.864, 0.860, 0.859, 0.842, 0.840~
> $ `similarity to motion` <dbl> 0.743, 0.751, 0.623, 0.753, 0.784, 0.582, 0.709~
\end{verbatim}

\end{example}

The result of Example~\ref{exm-eda-masc-vsm-word2vec-manner-motion} is a
data frame with the motion-similarity and manner-similarity measures for
all verbs in the MASC dataset. We can now visualize the distribution of
motion-similarity and manner-similarity measures, as seen in
Figure~\ref{fig-eda-masc-vsm-word2vec-manner-motion-compare}.

\begin{figure}[H]

\centering{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-vsm-word2vec-manner-motion-compare-1.pdf}

}

\caption{\label{fig-eda-masc-vsm-word2vec-manner-motion-compare}Motion-similarity
and manner-similarity of verbs in the MASC dataset}

\end{figure}%

From Figure~\ref{fig-eda-masc-vsm-word2vec-manner-motion-compare}, we
can see that the manner-similarity is plotted on the x-axis and the
motion-similarity on the y-axis. I've added horizontal and vertical
lines to break the scatterplot into quadrants --the top-right
corresponding to high manner- and motion-similiarity and the bottom-left
corresponding to low manner- and motion-similarity. This captures the
majority of the verbs in the dataset. The verbs in the top-left quadrant
have high motion-similarity but lower manner similarity, and verbs in
the bottom-right quadrant have high manner-similarity but lower
motion-similarity.

I've randomly sampled 50 verbs from the dataset and plotted them as text
labels. I've also plotted the motion and manner seed vectors as triangle
and box points, respectively. We can see that motion- and
manner-similiarity seed verbs are found in the top-left quandrant
together, showing that they are semantically related. Verbs in the other
quadrants are either lower in motion- or manner-similarity, or both.
From a qualitative point of view it appears that many of the verbs
coincide with inuition. Some, however, less so. This is to be expected
to some degree as the model is trained on a relatively small corpus. All
in all, this serves as an example of how vector space modeling can be
used to explore semantic relationships between linguistic features.

\section*{Activities}\label{activities-6}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-8.html}{Exploratory
analysis methods}\\
\textbf{How}: Read Recipe 8 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To illustrate how to prepare a dataset for descriptive and
unsupervised machine learning methods and evaluate the results for
exploratory data analysis.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-08}{Pattern
discovery}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 08.\\
\textbf{Why}: To gain experience working with coding strategies to
prepare, feature engineer, explore, and evaluate results from
exploratory data analyses, practice transforming datasets into new
object formats and visualizing relationships, and implement
organizational strategies for organizing and reporting results in a
reproducible fashion.

\end{tcolorbox}

\section*{Summary}\label{summary-7}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we surveyed a range of methods for uncovering insights
from data, particularly when we do not have a predetermined hypothesis.
We broke the chapter discussion along the two central branches of
exploratory data analysis: descriptive analysis and unsupervised
learning. Descriptive analysis offers statistical or visual summaries of
datasets through frequency, dispersion, and co-occurrence measures,
while unsupervised learning utilizes machine learning techniques to
uncover patterns without predefining variable relationships. Here we
covered a few unsupervised learning methods including clustering,
diminensionality reduction, and vector space modeling. Through either
descriptive or unsupervised learning methodologies, we probe questions
in a data-driven fashion and apply methods to summarize, reduce, and
sort complex datasets. This in turn facilitates novel, quantitative
perspectives that can subsequently be evaluated qualitatively, offering
us a robust approach to exploring and generating research questions.

\chapter{Predict}\label{sec-prediction}

\begin{quote}
All models are wrong, but some are useful.

--- George E.P. Box
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Identify the research goals of predictive data analysis
\item
  Describe the workflow for predictive data analysis
\item
  Recognize quantitative and qualitative methods for evaluating
  predictive models
\end{itemize}

\end{tcolorbox}

In this chapter, I introduce supervised learning as an approach to data
analysis, specifically focusing on its applications in text analysis.
Supervised learning aims to establish a relationship between a target
(or outcome) variable and a set of feature variables derived from text
data. By leveraging this relationship, statistical generalizations
(models) can be created to accurately predict values of the target
variable based on the values of the feature variables. Throughout the
chapter, we explore practical tasks and theoretical applications of
statistical learning in text analysis. We also cover the standard
workflow for building predictive models, testing and evaluating model
performance, improving model accuracy, and interpreting and reporting
findings.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{terminal} Lessons}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Advanced
Visualization}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: \ldots{} \faIcon{wrench}

\end{tcolorbox}

\section{Orientation}\label{sec-pda-orientation}

In this section, I introduce the concept of supervised learning and
provide an overview of the workflow for building and evaluating
predictive models for text analysis. First we will discuss the research
goals that are typically addressed using supervised learning,
contrasting them with the goals of exploratory and inferential analysis.
Next, we will discuss the approaches that are typically used to address
these goals, including the types of data structures and algorithms that
are used. Finally, we will discuss the workflow for building predictive
models, including the steps for preparing data, training and testing
models, and evaluating and reporting results.

\subsection{Research goal}\label{sec-pda-research-goal}

Predictive data analysis (PDA) is a powerful analysis method for
linguists and other researchers interested in making predictions about
new or future data based on patterns in existing data. As discussed in
Section~\ref{sec-aa-predict} and Section~\ref{sec-fr-plan}, PDA is a
type of supervised learning, which means that it involves training a
model on a labeled dataset where the input data and desired output are
both provided. The model is able to make predictions or classifications
based on the input data by learning the relationships between the input
and output data. Supervised machine learning is an important tool for
linguists studying language and communication, as it allows us to
analyze language data to identify patterns or trends in language use,
assess hypotheses, and prescribe actions.

In contrast to EDA, PDA does require that we have a particular goal in
mind from the outset. This goal is to predict a fixed outcome variable
based on a set of predictor variables. However, PDA can be applied with
distinct aims in mind: exploratory-oriented or hypothesis-driven
prediction. In exploratory-oriented prediction, the goal is to examine
potential relationships between an outcome and a mutable set of
predictor variables. The results of this type of analysis can be used to
generate new insight and questions. In hypothesis-driven prediction, in
contrast, the goal is to use predictive data analysis to test a
hypothesis about the relationship between the outcome and a pre-defined
set of predictor variables. In this case, the results of the analysis is
used to explain or infer a relationship between the outcome and
predictor variables.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

\begin{itemize}
\item[$\square$]
  expand description of hypothesis-driven prediction
\item
  MuPDAR approach (Multifactorial Prediction and Deviation Analysis with
  Regressions) (\citeproc{ref-Deshors2016}{Deshors and Gries 2016};
  \citeproc{ref-Gries2014}{S. Th. Gries and Deshors 2014}) and

  \begin{itemize}
  \tightlist
  \item
    Training on group
  \item
    Testing on another group (or groups)
  \end{itemize}
\item
  Discriminant analysis (\citeproc{ref-Baayen2011}{R. Harald Baayen
  2011})
\end{itemize}

\end{tcolorbox}

It is important to establish which aim is being taken, as this will
influence the approach that is taken. In this chapter we will focus on
the exploratory-driven approach to predictive data analysis.

\subsection{Approach}\label{sec-pda-approach}

The approach to conducting predictive analysis shares some commonalities
with exploratory data analysis (\textbf{?@sec-eda-approach}) (as well as
inferential analysis Chapter~\ref{sec-inference}), but there are also
some key differences. Let's look at the workflow in
Table~\ref{tbl-pda-workflow} and then discuss these commonalities and
differences.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7000}}@{}}
\caption{Workflow for predictive data
analysis}\label{tbl-pda-workflow}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Identify & Consider the research question and aim and identify
relevant variables \\
2 & & Split the data into representative training and testing sets \\
3 & & Apply variable selection and engineering procedures \\
4 & Inspect & Inspect the data to ensure that it is in the correct
format and that the training and testing sets are representative of the
data \\
5 & Interrogate & Train and evaluate the model on the training set,
adjusting models or hyperparameters as needed, to produce a final
model \\
6 & (Optional) Iterate & Repeat steps 3-5 to selecting new variables,
models, hyperparameters \\
7 & Interpret & Interpret the results of the final model in light of the
research question or hypothesis \\
\end{longtable}

Focusing on the overlap with other analysis methods, we can see some
fundamentals steps such as identifying relevant variables, inspecting
the data, interrogating the data, and interpreting the results. And if
our research aim is exploratory in nature, iteration may also be a part
of the workflow. These steps highlight the importance conducting
methodologic and communicable research, as discussed in
Section~\ref{sec-fr-frame}.

There are two main differences, however, between the PDA and the EDA
workflow we discussed in Chapter~\ref{sec-exploration}. The first,
reflected the majority of the steps in the workflow, is that PDA
requires partitioning the data into training and testing sets. As
discussed in Section~\ref{sec-aa-predict}, the training set is used to
develop the model, and the testing set is used to evaluate the model's
performance. This strategy is used to ensure that the model is robust
and generalizes well to new data. It is well known, and makes intuitive
sense, that using the same data to develop and evaluate a model likely
will not produce a model that generalizes well to new data. This is
because the model will have potentially conflated the nuances of the
data (`the noise') with any real trends (`the signal') and therefore
will not be able to generalize well to new data. This is called
overfitting and by holding out a portion of the data for testing, we can
evaluate the model's performance on data that it has not seen before and
therefore get a more accurate estimate of the generalizable trends in
the data.

Another procedure to avoid the perils of overfitting, is to use
resampling methods as part of the model evaluation on the training set.
Resampling is the process of repeatedly drawing samples from the
training set and evaluating the model on each sample. The two most
common resampling methods are \textbf{bootstrapping} (resampling with
replacement) and \textbf{cross-validation} (resampling without
replacement). The performance of these multiple models are summarized
and the error between them is assessed. The goal is to minimize the
performance differences between the models while maximizing the overall
performance. These measures go a long way to avoiding overfitting and
therefore maximizing the chance that the training phase will produce a
model which is robust at the testing phase.

The second difference, not reflected in the workflow but inherent in
predictive analysis, is that PDA requires a fixed outcome variable. This
means that the outcome variable must be defined from the outset and
cannot be changed during the analysis. Furthermore, the informational
nature of the outcome variable will dictate the what type of algorithm
we choose to interrogate the data and how we will evaluate the model's
performance. If the outcome is categorical in nature, we will use a
\textbf{classification algorithm} (\emph{e.g.} logistic regression,
naive bayes, \emph{etc.}). Classification evaluation metrics include
accuracy, precision, recall, and F1 score which can be derived from and
visualized in a cross-tabulation of the predicted and actual outcome
values.

To understand these measures it is helpful to consider a confusion
matrix, which is a table that describes the performance of a
classification model on data for which the true values are known. The
confusion matrix is a two-by-two matrix that shows the number of true
positives (TP), false positives (FP), true negatives (TN), and false
negatives (FN), as seen in Table~\ref{tbl-pda-confusion-matrix-metrics}.

\begin{longtable}[t]{lll}

\caption{\label{tbl-pda-confusion-matrix-metrics}A labeled confusion
matrix}

\tabularnewline

\toprule
 & Actual positive & Actual negative\\
\midrule
Predicted positive & TP & FP\\
Predicted negative & FN & TN\\
\bottomrule

\end{longtable}

If the outcome is numeric in nature, we will use a \textbf{regression
algorithm} (\emph{e.g.} linear regression, support vector regression,
\emph{etc.}). Since the difference between prediction and actual values
is numeric, metrics that quantify numerical differences, such as root
mean square error (RMSE) or \(R^2\), are used to evaluate the model's
performance.

\begin{figure}[H]

\centering{

\includegraphics{prediction_files/figure-pdf/fig-pda-regression-metrics-1.pdf}

}

\caption{\label{fig-pda-regression-metrics}A plot of the actual and
predicted values for a regression model}

\end{figure}%

The evaluation of the model is quantitative on the one hand, but it is
also qualitative in that we need to consider the implications of the
model's performance in light of the research question or hypothesis.
Furthermore, depending on our research question we may be interested in
exploring the features that are most important to the model's
performance. This is called \textbf{feature importance} and can be
derived from the model's coefficients or weights. Notably, however, some
of the most powerful models in use today, such as deep neural networks,
are not easily interpretable and therefore feature importance is not
easily derived. This is something to keep in mind when considering the
research question and the type of model that will be used to address it.

\section{Analysis}\label{sec-pda-analysis}

In this section we now turn to the practical application of predictive
data analysis. The dicussion will be separated into classification and
regression tasks, as model selection and evaluation procedures differ
between the two. For each task, we will frame a research goal and work
through the process of building a predictive model to address that goal.
Along the way we will cover concepts and methods that are common to both
classification and regression tasks and specific to each.

To frame our analyses, we will posit research aimed at identifying
language usage patterns in second language use, one for a classification
task and one for a regression task. Our first research question will be
to identify potential salient differences in Spanish language use
between natives and L1 English learners (categorical). Our second
research question will be to gauge the extent to which the the L1
English learners' Spanish language placement test scores (numeric) can
be predicted based on their language use.

We will use data from the CEDEL2 corpus\footnote{See
  Appendix~\ref{sec-data-cabnc} for more information on the CEDEL2
  corpus.}. We will include a subset of the variables from this data
that are relevant to our research questions. The data dictionary for
this dataset is seen in Table~\ref{tbl-pda-cedel2-data-dictionary}.

\begin{longtable}[t]{llll}

\caption{\label{tbl-pda-cedel2-data-dictionary}Data dictionary for the
CEDEL2 corpus}

\tabularnewline

\toprule
variable & name & variable\_type & description\\
\midrule
doc\_id & Document ID & numeric & Unique identifier for each document\\
subcorpus & Subcorpus & categorical & The subcorpus to which the document belongs ('Learner' or 'Native')\\
placement\_score & Placement Score & numeric & The score obtained by the document author in a placement test. Null values indicate missing data (i.e. the document author did not take the placement test)\\
proficiency & Proficiency & ordinal & The level of language proficiency of the document author ('Upper intermediate', 'Lower advanced', 'Upper beginner', or 'Native')\\
text & Text & character & The written text provided by the document author\\
\bottomrule

\end{longtable}

We will be using the \texttt{tidymodels} framework in R to perform this
analysis. \texttt{tidymodels} is a metapackage, much like
\texttt{tidyverse}, that provides a consistent interface for modeling
and machine learning. Some key packages unique to \texttt{tidymodels}
are \texttt{recipes}, \texttt{parsnip}, \texttt{workflows}, and
\texttt{tune}. \texttt{recipes} includes functions for preprocessing and
engineering features. \texttt{parsnip} provides a consistent interface
for specifying modeling algorithms. \texttt{worflows} allows us to
combine recipes and models into a single pipeline. Finally,
\texttt{tune} give us the ability to evaluate and tune hyperparameters
of models.

Since we are using text data, we will also be using the
\texttt{textrecipes} package which makes various functions available for
preprocessing text including extracting and engineering features.

Let's go ahead and do the setup, loading the necessary packages and
data. This is seen in Example~\ref{exm-pda-packages-data}.

\begin{example}[]\protect\hypertarget{exm-pda-packages-data}{}\label{exm-pda-packages-data}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load packages}
\FunctionTok{library}\NormalTok{(tidymodels)   }\CommentTok{\# modeling metapackage}
\FunctionTok{library}\NormalTok{(textrecipes)  }\CommentTok{\# text preprocessing}
\FunctionTok{library}\NormalTok{(janitor)      }\CommentTok{\# data inspection}

\CommentTok{\# Set global options}
\FunctionTok{tidymodels\_prefer}\NormalTok{()   }\CommentTok{\# prefer tidymodels functions over other functions with the same name}

\CommentTok{\# Read in the dataset}
\NormalTok{df }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"../data/cedel2/cedel2\_df.csv"}\NormalTok{)}

\CommentTok{\# Preview}
\NormalTok{df }\SpecialCharTok{|\textgreater{}} \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 2,957
> Columns: 5
> $ doc_id          <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,~
> $ subcorpus       <chr> "Learner", "Learner", "Learner", "Learner", "Learner",~
> $ placement_score <dbl> 14.0, 16.3, 16.3, 18.6, 18.6, 18.6, 20.9, 20.9, 20.9, ~
> $ proficiency     <chr> "Lower beginner", "Lower beginner", "Lower beginner", ~
> $ text            <chr> "Yo vivo es Alanta, Georgia. Atlanta es muy grande ciu~
\end{verbatim}

\end{example}

The results from Example~\ref{exm-pda-packages-data} show that we have
five variables and 2957 observations. The primary variables for the
classification task are the \texttt{subcorpus} variable and the
\texttt{text} variable. The \texttt{placement\_score} variable is the
outcome variable for the regression task.

\subsection{Text classification}\label{sec-pda-text-classification}

The goal of this analysis is to classify texts as either native or
learner based on the writing samples. This is a binary classification
problem. We will approach this problem from an exploratory perspective,
and therefore our aim is to identify features from the text that best
distinguish between the two classes.

Let's modify the data frame to include only the variables we need for
this analysis. In the process, we will rename the \texttt{subcorpus}
variable to \texttt{outcome} to reflect that it is the outcome variable
and convert it to a factor vector to meet requirements of the modeling
functions we will use in our analysis. This is seen in
Example~\ref{exm-pda-class-data}.

\begin{example}[]\protect\hypertarget{exm-pda-class-data}{}\label{exm-pda-class-data}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Select variables and factor outcome}
\NormalTok{cls\_df }\OtherTok{\textless{}{-}}
\NormalTok{  df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\AttributeTok{outcome =}\NormalTok{ subcorpus, proficiency, text) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{outcome =} \FunctionTok{factor}\NormalTok{(outcome))}

\CommentTok{\# Preview}
\NormalTok{cls\_df }\SpecialCharTok{|\textgreater{}} \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 2,957
> Columns: 3
> $ outcome     <fct> Learner, Learner, Learner, Learner, Learner, Learner, Lear~
> $ proficiency <chr> "Lower beginner", "Lower beginner", "Lower beginner", "Low~
> $ text        <chr> "Yo vivo es Alanta, Georgia. Atlanta es muy grande ciudad.~
\end{verbatim}

\end{example}

To view the distribution of the outcome variable between the two levels
we can use the \texttt{tabyl()} function from the \texttt{janitor}
package, as seen in Example~\ref{exm-pda-class-data-tabyl}.

\begin{example}[]\protect\hypertarget{exm-pda-class-data-tabyl}{}\label{exm-pda-class-data-tabyl}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# View the distribution of the outcome variable}
\NormalTok{cls\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tabyl}\NormalTok{(outcome) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{(}\AttributeTok{digits =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>  outcome    n percent
>  Learner 1906   64.5%
>   Native 1051   35.5%
\end{verbatim}

\end{example}

So a little less than two-thirds of the texts are from learners. It is
important to gauge the distribution of the outcome variable to see if it
is balanced or imbalanced. The classes need not be perfectly balanced,
but if they are wildly imbalanced it can cause problems for the model.

So in step 1 of our workflow (from Table~\ref{tbl-pda-workflow}), we
need to identify the features that we will use to classify the texts.
There may be many features that we could use. These could be features
derived from raw text (\emph{e.g.} characters, words, n-grams,
\emph{etc.}), feature vectors (\emph{e.g.} word embeddings), or
meta-linguistic features (\emph{e.g.} part-of-speech tags, syntactic
parses, or semantic features) that have been derived from these through
manual or automatic annotation.

Let's start simple and use words as the predictor features. This is a
simple approach that is often used as a baseline for more complex
models. If it doesn't work well, we can try something else.

This provides us the linguistic unit we will use but we still need to
decide how to represent these words. Do we use raw token counts? Do we
use normalized frequencies? Do we use some type of weighting scheme?
These are questions that we need to consider as we embark on this
analysis. Since we are exploring we can use trial-and-error or consider
the implications of each approach and choose the one that best fits our
research question --or both.

As a first pass, let's use raw token counts with our word features.

With our features identified, we can move on to step 2 of our workflow
and split the data into training and testing sets. We make the splits to
our data at this point to draw a line in the sand between the data we
will use to train the model and the data we will use to test the model.
A typical approach in supervised machine learning is to allocate around
75-80\% of the data to the training set and the remaining 20-25\% to the
testing set, depending on the number of observations. We have 2957
observations in our data set, so we can allocate 80\% of the data to the
training set and 20\% of the data to the testing set.

In Example~\ref{exm-pda-class-split}, we will use the
\texttt{initial\_split()} function from the \texttt{rsample} package to
split the data into training and testing sets. The
\texttt{initial\_split()} function takes a data frame and a proportion
and returns a \texttt{split} object which contains the training and
testing sets. We will use the \texttt{strata} argument to stratify the
data by the \texttt{outcome} variable. This will ensure that the
training and testing sets have the same proportion of native and learner
texts.

\begin{example}[]\protect\hypertarget{exm-pda-class-split}{}\label{exm-pda-class-split}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Split the data into training and testing sets}
\NormalTok{cls\_split }\OtherTok{\textless{}{-}}
  \FunctionTok{initial\_split}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ cls\_df,}
    \AttributeTok{prop =} \FloatTok{0.8}\NormalTok{,}
    \AttributeTok{strata =}\NormalTok{ outcome}
\NormalTok{  )}

\CommentTok{\# Create training set}
\NormalTok{cls\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(cls\_split)  }\CommentTok{\# 80\% of data}

\CommentTok{\# Create testing set}
\NormalTok{cls\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(cls\_split)    }\CommentTok{\# 20\% of data}
\end{Highlighting}
\end{Shaded}

\end{example}

A confirmation of the distribution of the data across the training and
testing sets as well as a break down of the outcome variable can be seen
in Example~\ref{exm-pda-class-split-tabyl}.

\begin{example}[]\protect\hypertarget{exm-pda-class-split-tabyl}{}\label{exm-pda-class-split-tabyl}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# View the distribution of the outcome variables}
\NormalTok{cls\_train }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tabyl}\NormalTok{(outcome) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{adorn\_totals}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{(}\AttributeTok{digits =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>  outcome    n percent
>  Learner 1524   64.5%
>   Native  840   35.5%
>    Total 2364  100.0%
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cls\_test }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tabyl}\NormalTok{(outcome) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{adorn\_totals}\NormalTok{(}\StringTok{"row"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{(}\AttributeTok{digits =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>  outcome   n percent
>  Learner 382   64.4%
>   Native 211   35.6%
>    Total 593  100.0%
\end{verbatim}

\end{example}

We can see that the split was successful. The training and testing sets
have very similiar proportion of native and learner texts.

We are now ready to create a `recipe', step 3 in our analysis. A recipe
is a set of instructions or blueprint which specify the outcome variable
and the predictor variable and determines how to preprocess and engineer
the feature variables.

We will use the \texttt{recipe()} function from the \texttt{recipes}
package to create the recipe. The \texttt{recipe()} function minimally
takes a formula and a data frame and returns a \texttt{recipe} object.
The formula specifies the outcome variable (\(y\)) and the predictor
variable(s) (\(x_1 .. x_n\)). For example
\texttt{y\ \textasciitilde{}\ x} can be read as ``y as a function of
x''. In our particular case, we will use the formula
\texttt{outcome\ \textasciitilde{}\ text} to specify that the outcome
variable is the \texttt{outcome} variable and the predictor variable is
the \texttt{text} variable. The code is seen in
Example~\ref{exm-pda-class-recipe}.

\begin{example}[]\protect\hypertarget{exm-pda-class-recipe}{}\label{exm-pda-class-recipe}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a recipe}
\NormalTok{base\_rec }\OtherTok{\textless{}{-}}
  \FunctionTok{recipe}\NormalTok{(}
    \AttributeTok{formula =}\NormalTok{ outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ text,}
    \AttributeTok{data =}\NormalTok{ cls\_train}
\NormalTok{    )}

\CommentTok{\# Preview}
\NormalTok{base\_rec}
\end{Highlighting}
\end{Shaded}

\end{example}

The recipe object at this moment contains just one instruction, what the
variables are and what their relationship is.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{hand-point-up} Tip}

R formulas are a powerful way to specify relationships between variables
and are used extensively in data modeling including exploratory,
predictive, and inferential analysis. The basic formula syntax is
\texttt{y\ \textasciitilde{}\ x} where \texttt{y} is the outcome
variable and \texttt{x} is the feature variable. The formula syntax can
be extended to include multiple feature variables, interactions, and
transformations. For more information on R formulas, see
\href{https://r4ds.github.io/bookclub-tmwr/r-formula-syntax.html}{R for
Data Science}.

\end{tcolorbox}

The \texttt{recipes} package provides a wide range of \texttt{step\_*()}
functions which can be applied to the recipe to specify how to engineer
the variables in our recipe call. These include functions to scale
(\emph{e.g} \texttt{step\_center()}, \texttt{step\_scale()},
\emph{etc.}) and transform (\emph{e.g.} \texttt{step\_log()},
\texttt{step\_pca()}, \emph{etc.}) numeric variables, and functions to
encode (\emph{e.g.} \texttt{step\_dummy()},
\texttt{step\_labelencode()}, \emph{etc.}) categorical variables.

These step functions are great when we have selected the variables we
want to use in our model and we want to engineer them in a particular
way. In our case, however, we need to derive features from the text in
the \texttt{text} column of datasets before we engineer them. To ease
this process, the \texttt{textrecipes} package provides a number of step
functions for preprocessing text data. These include functions to
tokenize (\emph{e.g.} \texttt{step\_tokenize()}), remove stop words
(\emph{e.g.} \texttt{step\_stopwords()}), and to derive meta-features
(\emph{e.g.} \texttt{step\_lemma()}, \texttt{step\_stem()}, \emph{etc.})
\footnote{Note that functions for meta-features require more
  sophisticated text analysis software to be installed on the computing
  environment (e.g.~\texttt{spacyr} for \texttt{step\_lemma()},
  \texttt{step\_pos()}, \emph{etc.}). See the \texttt{textrecipes}
  package documentation for more information.}. Furthermore, there are
functions to engineer features in ways that are particularly relevant to
text data, such as feature frequencies and weights (\emph{e.g.}
\texttt{step\_tf()}, \texttt{step\_tfidf()}, \emph{etc.}) and token
filtering (\emph{e.g.} \texttt{step\_tokenfilter()}).

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

For text processing steps for Japanese, see the \texttt{washoku} package
(\citeproc{ref-R-washoku}{Uryu 2024}).

\end{tcolorbox}

So let's build on our basic recipe \texttt{cls\_rec} by adding steps
relevant to our task. To extract our features, we will use the
\texttt{step\_tokenize()} function to tokenize the text into words. The
default behavior of the \texttt{step\_tokenize()} function is to
tokenize the text into words, but other token units can be derived and
various options can be added to the function call (as the
\texttt{tokenizers} package is used under the hood). Adding the
\texttt{step\_tokenize()} function to our recipe is seen in
Example~\ref{exm-pda-class-recipe-tokenize}.

\begin{example}[]\protect\hypertarget{exm-pda-class-recipe-tokenize}{}\label{exm-pda-class-recipe-tokenize}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add step to tokenize the text}
\NormalTok{cls\_rec }\OtherTok{\textless{}{-}}
\NormalTok{  base\_rec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tokenize}\NormalTok{(text)}

\CommentTok{\# Preview}
\NormalTok{cls\_rec}
\end{Highlighting}
\end{Shaded}

\end{example}

The recipe object now contains two instructions, one for the outcome
variable and one for the feature variable. The feature variable
instruction specifies that the text should be tokenized into words.

We now need to consider how to engineer the word features. If we add
\texttt{step\_tf()} we will get a matrix of token counts by default. We
also have the option to add \texttt{step\_tfidf()} to get a matrix of
term frequencies weighted by inverse document frequency, which
effectively down weights words that are common across all documents.

We decided in step 1 that we will start with raw token counts, so we
will add \texttt{step\_tf()} to our recipe. This is seen in
Example~\ref{exm-pda-class-recipe-tf}.

\begin{example}[]\protect\hypertarget{exm-pda-class-recipe-tf}{}\label{exm-pda-class-recipe-tf}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add step to tokenize the text}
\NormalTok{cls\_rec }\OtherTok{\textless{}{-}}
\NormalTok{  cls\_rec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tf}\NormalTok{(text)}

\CommentTok{\# Preview}
\NormalTok{cls\_rec}
\end{Highlighting}
\end{Shaded}

\end{example}

To make sure things are in order and that the recipe performs as
expected, we can use the functions \texttt{prep()} and \texttt{bake()}
to inspect the recipe. The \texttt{prep()} function takes a recipe
object and a data frame and returns a \texttt{prep} object. The
\texttt{prep} object contains the recipe and the data frame with the
feature variables engineered according to the recipe. The
\texttt{bake()} function takes a \texttt{prep} object and an optional
new dataset to apply the recipe to. If we only want to see the
application to the training set, we can use the
\texttt{new\_data\ =\ NULL} argument.

In Example~\ref{exm-pda-class-recipe-prep}, we use the \texttt{prep()}
and \texttt{bake()} functions to create a data frame with the feature
variables. We can then inspect the data frame to see if the recipe
performed as expected.

\begin{example}[]\protect\hypertarget{exm-pda-class-recipe-prep}{}\label{exm-pda-class-recipe-prep}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Prep and bake}
\NormalTok{cls\_bake }\OtherTok{\textless{}{-}}
\NormalTok{  cls\_rec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{prep}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# create a prep object}
  \FunctionTok{bake}\NormalTok{(}\AttributeTok{new\_data =} \ConstantTok{NULL}\NormalTok{) }\CommentTok{\# apply to training set}

\CommentTok{\# Preview}
\NormalTok{cls\_bake }\SpecialCharTok{|\textgreater{}} \FunctionTok{dim}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1]  2364 38115
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cls\_bake[}
  \FunctionTok{c}\NormalTok{(}\DecValTok{1000}\SpecialCharTok{:}\DecValTok{1001}\NormalTok{, }\DecValTok{2000}\SpecialCharTok{:}\DecValTok{2001}\NormalTok{),     }\CommentTok{\# selected rows}
  \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1000}\SpecialCharTok{:}\DecValTok{1001}\NormalTok{, }\DecValTok{20000}\SpecialCharTok{:}\DecValTok{20001}\NormalTok{) }\CommentTok{\# selected columns}
\NormalTok{  ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 4 x 5
>   outcome tf_text_actuales tf_text_actualiadad tf_text_iniciáticas
>   <fct>              <int>               <int>               <int>
> 1 Learner                0                   0                   0
> 2 Learner                0                   0                   0
> 3 Native                 0                   0                   0
> 4 Native                 0                   0                   0
> # i 1 more variable: tf_text_iniciativa <int>
\end{verbatim}

\end{example}

The resulting engineered features data frame has 2364 observations and
38115 variables. The first variable is the outcome variable and the
remaining variables are the engineered features. We can see that the
recipe performed as expected and that the feature variables are the
token counts for each word in the text.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{hand-point-up} Tip}

When applying tokenization and feature engineering steps to text data
the result is often contained in a matrix object. Using the
\texttt{recipes} package a data frame with a matrix-like structure is
returned.

Furthermore, the features are prefixed with the variable name and
transformation step labels. In Example~\ref{exm-pda-class-recipe-prep}
we applied term frequency to the \texttt{text} variable. Therefore we
see that our features are prefixed with \texttt{tf\_text\_}.

\end{tcolorbox}

But we should pause. We have a lot of features! One for every single
word in the entire corpus. This is an unweildy number of features for a
model and it is likely that many of these features are not useful for
our classification task. Furthermore, the more features we have, the
more chance these features capture the nuances of these particular
writing samples, thus, we are the more likely we are to overfit the
model. All in all, we need to reduce the number of features.

Our domain knowledge about language can be of help us decide on how to
approach reducing the feature set. On the one hand, we know that most
tokens occur rarely in any sizable corpus. And the lower the frequency,
the more likely that the token may reflect nuances of the speaker or the
content of particular texts rather than generalizable trends. On the
other hand, we know that a relatively small number of the most frequent
words quickly account for a large proportion of the tokens in a corpus.

So just with these two considerations, we can see that we can filter out
many infrequent words and likely have quite a few viable features. Let's
start with an arbitrary threshold of the top 1,000 words by frequency.
We can use the \texttt{step\_tokenfilter()} function to filter out the
top 1,000 words by frequency. This particular step needs to be applied
before the \texttt{step\_tf()} step, so we will add it to our recipe
before the \texttt{step\_tf()} step. This is seen in
Example~\ref{exm-pda-class-recipe-tokenfilter}.

\begin{example}[]\protect\hypertarget{exm-pda-class-recipe-tokenfilter}{}\label{exm-pda-class-recipe-tokenfilter}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Rebuild recipe with tokenfilter step}
\NormalTok{cls\_rec }\OtherTok{\textless{}{-}}
\NormalTok{  base\_rec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tokenize}\NormalTok{(text) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tokenfilter}\NormalTok{(text, }\AttributeTok{max\_tokens =} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tf}\NormalTok{(text)}

\CommentTok{\# Prep and bake}
\NormalTok{cls\_bake }\OtherTok{\textless{}{-}}
\NormalTok{  cls\_rec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{prep}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{bake}\NormalTok{(}\AttributeTok{new\_data =} \ConstantTok{NULL}\NormalTok{)}

\CommentTok{\# Preview}
\NormalTok{cls\_bake }\SpecialCharTok{|\textgreater{}} \FunctionTok{dim}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] 2364 1001
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cls\_bake[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 5 x 10
>   outcome tf_text_10 tf_text_2 tf_text_3 tf_text_4 tf_text_a tf_text_abandonado
>   <fct>        <int>     <int>     <int>     <int>     <int>              <int>
> 1 Learner          0         0         0         0         0                  0
> 2 Learner          0         0         0         0         2                  0
> 3 Learner          0         1         0         0         5                  0
> 4 Learner          0         0         0         0         0                  0
> 5 Learner          0         0         0         0         2                  0
> # i 3 more variables: tf_text_abuela <int>, tf_text_abuelos <int>,
> #   tf_text_aburrido <int>
\end{verbatim}

\end{example}

We now have a manageable set of features. Only during the interrogation
step will we know if they are useful.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{hand-point-up} Tip}

The \texttt{prep()} and \texttt{bake()} functions are useful for
inspecting the recipe and the engineered features, but they are not
required to build a recipe. When a recipe is added to a workflow, the
\texttt{prep()} and \texttt{bake()} functions are called automatically
as part of the process.

\end{tcolorbox}

There is one last step we should add to our recipe which concerns the
skewed nature of word frequency distributions. The magnitude of the
token counts will be highly skewed with a few words having very high
counts and most words having relatively low counts, even for the top
1,000 words. This skew can be problematic for some models, so we will
add a step to log normalize the token counts. This will not change the
rank order, only the magnitude of the differences. Note that we use
\texttt{all\_predictors()} to apply the log transformation to all the
word features, as seen in Example~\ref{exm-pda-class-recipe-log}.

\begin{example}[]\protect\hypertarget{exm-pda-class-recipe-log}{}\label{exm-pda-class-recipe-log}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add log step to recipe}
\NormalTok{cls\_rec }\OtherTok{\textless{}{-}}
\NormalTok{  cls\_rec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_log}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{(), }\AttributeTok{offset =} \DecValTok{1}\NormalTok{) }\CommentTok{\# add 1 to avoid log(0)}

\NormalTok{cls\_rec}
\end{Highlighting}
\end{Shaded}

\end{example}

We are now ready to turn our attention to step 5 of our workflow,
interrogating the data. In this step, we will first select a
classification algorithm, then add this algorithm and our recipe to a
workflow object. We will then use the workflow object to train and
evaluate the model on the training set.

There are many classification algorithms to choose from with their own
strengths and shortcomings. In Table~\ref{tbl-pda-class-algorithms}, we
list some of the most common classification algorithms and their
characteristics.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3889}}@{}}
\caption{Classification
algorithms}\label{tbl-pda-class-algorithms}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Algorithm
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Shortcomings
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Algorithm
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Shortcomings
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Logistic regression & Simple, interpretable, fast & Assumes linear
relationship between features and outcome \\
Naive Bayes & Simple, interpretable, fast & Assumes independence between
features \\
Decision trees & Nonlinear relationships, interpretable & Prone to
overfitting \\
Random forest & Nonlinear relationships, interpretable & Prone to
overfitting \\
Support vector machines & Nonlinear relationships, interpretable & Prone
to overfitting \\
Neural networks & Nonlinear relationships, fast & Prone to overfitting,
difficult to interpret \\
\end{longtable}

In the process of selecting an algorithm, simple, computationally
efficient, and interpretable models are preferred over complex,
computationally expensive, and uninterpretable models, all things being
equal. Only if the performance of the simple model is not good enough
should we move on to a more complex model.

With this end mind, we will start with a simple logistic regression
model to see how well we can classify the texts in the training set with
the features we have engineered. We will use the
\texttt{logistic\_reg()} function from the \texttt{parsnip} package to
specify the logistic regression model. We then select the implementation
engine (\texttt{glm} General Linear Model) and the mode of the model
(\texttt{classification}). The implementation engine is the software
that will be used to fit the model. The mode is the type of model,
either classification or regression. The code is seen in
Example~\ref{exm-pda-class-model-spec}.

\begin{example}[]\protect\hypertarget{exm-pda-class-model-spec}{}\label{exm-pda-class-model-spec}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a model specification}
\NormalTok{cls\_spec }\OtherTok{\textless{}{-}}
  \FunctionTok{logistic\_reg}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"glm"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{)}

\CommentTok{\# Preview}
\NormalTok{cls\_spec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Logistic Regression Model Specification (classification)
> 
> Computational engine: glm
\end{verbatim}

\end{example}

In the \texttt{parsnip} package, model specifications are separate from
model fitting. This allows us to specify the model once and then fit the
model with different datasets. Furthermore, different algorithms will
have different hyperparameters that can be tuned. The code in
Example~\ref{exm-pda-class-model-spec} uses the default hyperparameters
for the logistic regression model.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{hand-point-up} Tip}

The \texttt{parsnip} package provides a consistent interface to many
different models, 105 at the time of writing. You can peruse the list of
models by running \texttt{parsnip::model\_db}.

You can also retrieve the list of potential engines for a given model
specification with the \texttt{show\_engines()} function. For example,
\texttt{show\_engines("logistic\_reg")} will return a tibble with the
engines available for the logistic regression model specification.

\end{tcolorbox}

Now we will combine our recipe and model specification into a workflow
object. The workflow object will allow us to train and evaluate the
model on the training set. We will use the \texttt{workflow()} function
from the \texttt{workflows} package to combine the recipe and model
specification into a workflow object. The code is seen in
Example~\ref{exm-pda-class-workflow}.

\begin{example}[]\protect\hypertarget{exm-pda-class-workflow}{}\label{exm-pda-class-workflow}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a workflow}
\NormalTok{cls\_wf }\OtherTok{\textless{}{-}}
  \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{add\_recipe}\NormalTok{(cls\_rec) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{add\_model}\NormalTok{(cls\_spec)}

\CommentTok{\# Preview}
\NormalTok{cls\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> == Workflow ====================================================================
> Preprocessor: Recipe
> Model: logistic_reg()
> 
> -- Preprocessor ----------------------------------------------------------------
> 4 Recipe Steps
> 
> * step_tokenize()
> * step_tokenfilter()
> * step_tf()
> * step_log()
> 
> -- Model -----------------------------------------------------------------------
> Logistic Regression Model Specification (classification)
> 
> Computational engine: glm
\end{verbatim}

\end{example}

The worflow contains all the preprocessing and the model-specific
parameters we've selected. A workflow object at this stage in the
analysis is not trained. It is a blueprint for a model. We can use the
\texttt{fit()} function to apply the workflow to the training data to
train the model and update our workflow object with the trained model.
The \texttt{fit()} function takes a workflow and a data frame and
returns an updated workflow object which is trained. The code is seen in
Example~\ref{exm-pda-class-workflow-fit}.

\begin{example}[]\protect\hypertarget{exm-pda-class-workflow-fit}{}\label{exm-pda-class-workflow-fit}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit the workflow}
\NormalTok{cls\_wf\_fit }\OtherTok{\textless{}{-}}
  \FunctionTok{fit}\NormalTok{(}
\NormalTok{    cls\_wf,}
    \AttributeTok{data =}\NormalTok{ cls\_train}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\end{example}

The workflow object now contains the trained model. But things did not
go off without a hitch. We receive two warning messages when fitting the
workflow to the training data:

\begin{verbatim}
Warning messages:
1: glm.fit: algorithm did not converge
2: glm.fit: fitted probabilities numerically 0 or 1 occurred
\end{verbatim}

Warning messages are a sign that something may be amiss with the
features we have choosen, the model we have selected, or both! Warning
messages can sometimes be cryptic and steeped in the jargon of the
algorithm. When in doubt it is best to consult the documentation of the
algorithm and/ or seek help from the R community.

The first warning, the `algorithm did not converge', means that given
the data and the model, the algorithm could not estimate a stable
solution. The reason or reasons for this can be varied, but it typically
due to extreme outliers, collinearity, or a small sample size.
Collinearity is when two or more features are highly correlated. Given
we are using words as features, it is likely that there is collinearity
in the data. We will need to address this.

The second warning is `fitted probabilities numerically 0 or 1
occurred'. This means that the model is overfitting the data,
(i.e.~there are one or more features that perfectly predict the
outcome). This very well could be related to the first warning, as a
small set of features may be perfectly predicting the outcome in a way
that will likely not generalize to new data.

The upshot is that this model specification and/ or the feature
engineering needs to be changed. So we step back in the workflow and
make changes that will cascade downstream.

It turns out that a regularized logistic regression model often
addresses the issues this model is experiencing. We will use the
\texttt{glmnet} engine to fit a regularized logistic regression model.
It is a more complex complex approach, but it can often mitigate the
issues of collinearity and overfitting by penalizing features that are
highly correlated and by shrinking the coefficients of features that are
not useful for predicting the outcome.

We will build a new model specification using the
\texttt{logistic\_reg()} function and the \texttt{glmnet} engine. This
time we will use hyperparameters in our \texttt{logistic\_reg()} call to
specify the penality we want to use and the strength of the penality.
The penalty we will use is the LASSO (L1)\footnote{The LASSO (least
  absolute shrinkage and selection operator) is a type of regularization
  that penalizes the absolute value of the coefficients. In essence, it
  smooths the coefficients by shrinking them towards zero to avoid
  coefficients picking up on particularities of the training data that
  will not generalize to new data.}. We now need to decide what value to
use for the strength of the penalty, 0 to 1, where 0 indicates no
penalty and 1 indicates a maximum penalty. We either experiment with
different values one by one or we can implement a range of values in
tandem and then select the best value. We will use the latter approach.

The \texttt{tune} package provides a number of functions for selecting,
or `tuning', hyperparameters. The first is the \texttt{tune()} function
which we add as the argument of the hyperparameter we want to tune in
the model specification, as seen in
Example~\ref{exm-pda-class-model-spec-tune}.

\begin{example}[]\protect\hypertarget{exm-pda-class-model-spec-tune}{}\label{exm-pda-class-model-spec-tune}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a model specification for tuning}
\NormalTok{cls\_spec\_tune }\OtherTok{\textless{}{-}}
  \FunctionTok{logistic\_reg}\NormalTok{(}\AttributeTok{penalty =} \FunctionTok{tune}\NormalTok{(), }\AttributeTok{mixture =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"glmnet"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{)}

\CommentTok{\# Create a workflow}
\NormalTok{cls\_wf\_tune }\OtherTok{\textless{}{-}}
  \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{add\_recipe}\NormalTok{(cls\_rec) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{add\_model}\NormalTok{(cls\_spec\_tune)}
\end{Highlighting}
\end{Shaded}

\end{example}

We now have a workflow that includes the \texttt{tune()} function as a
placeholder for a range of values for the penalty hyperparameter. We use
the \texttt{grid\_regular()} function from the \texttt{dials} package to
specify a grid of values for the penalty hyperparameter. Let's choose a
random set of 10 values, as seen in
Example~\ref{exm-pda-class-model-spec-tune-grid-values}.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{medal} Dive deeper}

The hyperparameters \texttt{penalty} and \texttt{mixture} control which
type(s) of regularization to apply and if there is a mixing of the
types. In the model specification in
Example~\ref{exm-pda-class-model-spec-tune}, the \texttt{penalty} is set
to be tuned and the \texttt{mixture} is set to 1, which means that only
a lasso penalty will be applied.

See the documentation for the \texttt{parsnip::logistic\_reg()} function
for more information.

\end{tcolorbox}

\begin{example}[]\protect\hypertarget{exm-pda-class-model-spec-tune-grid-values}{}\label{exm-pda-class-model-spec-tune-grid-values}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a grid of values for the penalty hyperparameter}
\NormalTok{cls\_grid }\OtherTok{\textless{}{-}}
  \FunctionTok{grid\_regular}\NormalTok{(}
    \FunctionTok{penalty}\NormalTok{(),}
    \AttributeTok{levels =} \DecValTok{10}
\NormalTok{    )}

\CommentTok{\# Preview}
\NormalTok{cls\_grid}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 1
>          penalty
>            <dbl>
>  1 0.0000000001 
>  2 0.00000000129
>  3 0.0000000167 
>  4 0.000000215  
>  5 0.00000278   
>  6 0.0000359    
>  7 0.000464     
>  8 0.00599      
>  9 0.0774       
> 10 1
\end{verbatim}

\end{example}

The 10 values chosen to be in the grid range from nearly zero to 1,
where 0 indicates no penalty and 1 indicates a strong penalty.

Now to perform the tuning and arrive at an optimal value for
\texttt{penalty} we need to create a tuning workflow. We do this by
calling the \texttt{tune\_grid()} function using our tuning model
specification workflow, a resampling object, and our hyperparameter grid
and returns a \texttt{tune\_grid} object.

Now, a resampling object is not something we've seen yet. Resampling is
a strategy that allows us to generate multiple training and testing sets
from a single data set --in this case the training data we split at the
outset. Each variation of the training and testing sets is called a
fold. Which is why this type of resampling is called k-fold
cross-validation. The \texttt{vfold\_cv()} function from the
\texttt{rsample} package takes a data frame and a number of folds and
returns a \texttt{vfold\_cv} object. We will use 10 folds and include
the model specification and the hyperparameter grid in the
\texttt{tune\_grid()} function call. The code is seen in
Example~\ref{exm-pda-class-model-spec-tune-grid-cv}.

\begin{example}[]\protect\hypertarget{exm-pda-class-model-spec-tune-grid-cv}{}\label{exm-pda-class-model-spec-tune-grid-cv}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Create a resampling object}
\NormalTok{cls\_vfold }\OtherTok{\textless{}{-}}
  \FunctionTok{vfold\_cv}\NormalTok{(}
\NormalTok{    cls\_train,}
    \AttributeTok{v =} \DecValTok{10}
\NormalTok{    )}

\CommentTok{\# Tune the model}
\NormalTok{cls\_tune }\OtherTok{\textless{}{-}}
  \FunctionTok{tune\_grid}\NormalTok{(}
\NormalTok{    cls\_wf\_tune,}
    \AttributeTok{resamples =}\NormalTok{ cls\_vfold,}
    \AttributeTok{grid =}\NormalTok{ cls\_grid}
\NormalTok{    )}

\CommentTok{\# Preview}
\NormalTok{cls\_tune}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # Tuning results
> # 10-fold cross-validation 
> # A tibble: 10 x 4
>    splits             id     .metrics          .notes          
>    <list>             <chr>  <list>            <list>          
>  1 <split [2127/237]> Fold01 <tibble [20 x 5]> <tibble [0 x 3]>
>  2 <split [2127/237]> Fold02 <tibble [20 x 5]> <tibble [0 x 3]>
>  3 <split [2127/237]> Fold03 <tibble [20 x 5]> <tibble [0 x 3]>
>  4 <split [2127/237]> Fold04 <tibble [20 x 5]> <tibble [0 x 3]>
>  5 <split [2128/236]> Fold05 <tibble [20 x 5]> <tibble [0 x 3]>
>  6 <split [2128/236]> Fold06 <tibble [20 x 5]> <tibble [0 x 3]>
>  7 <split [2128/236]> Fold07 <tibble [20 x 5]> <tibble [0 x 3]>
>  8 <split [2128/236]> Fold08 <tibble [20 x 5]> <tibble [0 x 3]>
>  9 <split [2128/236]> Fold09 <tibble [20 x 5]> <tibble [0 x 3]>
> 10 <split [2128/236]> Fold10 <tibble [20 x 5]> <tibble [0 x 3]>
\end{verbatim}

\end{example}

The \texttt{cls\_tune} object contains the results of the tuning for
each fold. We can see the results of the tuning for each fold by calling
the \texttt{collect\_metrics()} function on the \texttt{cls\_tune}
object, as seen in
Example~\ref{exm-pda-class-model-spec-tune-grid-collect}.

\begin{example}[]\protect\hypertarget{exm-pda-class-model-spec-tune-grid-collect}{}\label{exm-pda-class-model-spec-tune-grid-collect}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Collect the results of the tuning}
\NormalTok{cls\_tune\_metrics }\OtherTok{\textless{}{-}}
  \FunctionTok{collect\_metrics}\NormalTok{(cls\_tune)}

\CommentTok{\# Visualize metrics}
\NormalTok{cls\_tune }\SpecialCharTok{|\textgreater{}} \FunctionTok{autoplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{prediction_files/figure-pdf/fig-pda-class-model-spec-tune-grid-collect-1.pdf}

}

\caption{\label{fig-pda-class-model-spec-tune-grid-collect}Metrics for
each fold of the tuning process.}

\end{figure}%

\end{example}

The most common metrics for model performance in classification are
accuracy and the area under the receiver operating characteristic curve
(ROC-AUC). Accuracy is the proportion of correct predictions. The
ROC-AUC is a measure of the trade-off between sensitivity and
specificity. Sensitivity is the proportion of true positives that are
correctly identified. Specificity is the proportion of true negatives
that are correctly identified. The ROC-AUC is a measure of how well the
model can distinguish between the two classes.

In the plot of the metrics, we can see that the many of the penalty
values performed similarly, with a drop off in performance at the higher
values. Conveniently, the \texttt{show\_best()} function from the
\texttt{tune} package takes a \texttt{tune\_grid} object and returns the
best performing hyperparameter values. The code is seen in
Example~\ref{exm-pda-class-model-spec-tune-grid-collect-best}.

\begin{example}[]\protect\hypertarget{exm-pda-class-model-spec-tune-grid-collect-best}{}\label{exm-pda-class-model-spec-tune-grid-collect-best}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Show the best performing hyperparameter value}
\NormalTok{cls\_tune }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{show\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"roc\_auc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 5 x 7
>         penalty .metric .estimator  mean     n std_err .config              
>           <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                
> 1 0.00599       roc_auc binary     0.977    10 0.00312 Preprocessor1_Model08
> 2 0.000464      roc_auc binary     0.972    10 0.00320 Preprocessor1_Model07
> 3 0.0000359     roc_auc binary     0.970    10 0.00337 Preprocessor1_Model06
> 4 0.0000000001  roc_auc binary     0.970    10 0.00342 Preprocessor1_Model01
> 5 0.00000000129 roc_auc binary     0.970    10 0.00342 Preprocessor1_Model02
\end{verbatim}

\end{example}

We can make this selection programmatically by using the
\texttt{select\_best()} function. This function needs a metric to select
by. We will use the ROC-AUC and select the best value for the penalty
hyperparameter. The code is seen in
Example~\ref{exm-pda-class-model-spec-tune-grid-collect-select}.

\begin{example}[]\protect\hypertarget{exm-pda-class-model-spec-tune-grid-collect-select}{}\label{exm-pda-class-model-spec-tune-grid-collect-select}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Select the best performing hyperparameter value}
\NormalTok{cls\_best }\OtherTok{\textless{}{-}}
  \FunctionTok{select\_best}\NormalTok{(cls\_tune, }\AttributeTok{metric =} \StringTok{"roc\_auc"}\NormalTok{)}

\CommentTok{\# Preview}
\NormalTok{cls\_best}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 2
>   penalty .config              
>     <dbl> <chr>                
> 1 0.00599 Preprocessor1_Model08
\end{verbatim}

\end{example}

All of that to tune a hyperparameter! Now we can update the model
specification and workflow with the best performing hyperparameter value
using the previous \texttt{cls\_wf\_tune} workflow and the
\texttt{finalize\_workflow()} function. The
\texttt{finalize\_workflow()} function takes a workflow and the selected
parameters and returns an updated \texttt{workflow} object, as seen in
Example~\ref{exm-pda-class-tune-hyperparameters-update-workflow}.

\begin{example}[]\protect\hypertarget{exm-pda-class-tune-hyperparameters-update-workflow}{}\label{exm-pda-class-tune-hyperparameters-update-workflow}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Update model specification}
\NormalTok{cls\_wf\_lasso }\OtherTok{\textless{}{-}}
\NormalTok{  cls\_wf\_tune }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{finalize\_workflow}\NormalTok{(cls\_best)}

\NormalTok{cls\_wf\_lasso}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> == Workflow ====================================================================
> Preprocessor: Recipe
> Model: logistic_reg()
> 
> -- Preprocessor ----------------------------------------------------------------
> 4 Recipe Steps
> 
> * step_tokenize()
> * step_tokenfilter()
> * step_tf()
> * step_log()
> 
> -- Model -----------------------------------------------------------------------
> Logistic Regression Model Specification (classification)
> 
> Main Arguments:
>   penalty = 0.00599484250318942
>   mixture = 1
> 
> Computational engine: glmnet
\end{verbatim}

\end{example}

Our model specification and the worflow are updated with the parameters.

Let's now return to fitting the workflow to the training set as we did
before for the vanilla logistic regression model. As a reminder we are
still working in step 5 of our workflow, interrogating the data. We
identified and addressed potential issues in the model specification,
leaving the feature selection the same.

We again fit the workflow to the training set, as seen in
Example~\ref{exm-pda-class-tune-hyperparameters-fit}.

\begin{example}[]\protect\hypertarget{exm-pda-class-tune-hyperparameters-fit}{}\label{exm-pda-class-tune-hyperparameters-fit}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit the workflow}
\NormalTok{cls\_wf\_lasso\_fit }\OtherTok{\textless{}{-}}
  \FunctionTok{fit}\NormalTok{(}
\NormalTok{    cls\_wf\_lasso,}
    \AttributeTok{data =}\NormalTok{ cls\_train}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\end{example}

No warnings, so that is a good sign! --Or at least a better sign than
before.

Let's evaluate the performance of the model on the training data. The
\texttt{predict()} function takes a trained model specification and a
data frame and returns a data frame with the predicted outcome. We can
join these predicted outcomes with the actual outcomes in the training
data. The \texttt{metrics()} function takes a data frame with the actual
and predicted outcomes and returns a data frame with the metrics for the
model. The code is seen in
Example~\ref{exm-pda-class-tune-hyperparameters-evaluate}.

\begin{example}[]\protect\hypertarget{exm-pda-class-tune-hyperparameters-evaluate}{}\label{exm-pda-class-tune-hyperparameters-evaluate}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Evaluate workflow}
\NormalTok{cls\_lasso\_fit\_preds }\OtherTok{\textless{}{-}}
  \FunctionTok{bind\_cols}\NormalTok{(}
\NormalTok{    cls\_train,}
    \FunctionTok{predict}\NormalTok{(cls\_wf\_lasso\_fit, cls\_train)}
\NormalTok{  )}

\CommentTok{\# Calculate accuracy}
\NormalTok{cls\_lasso\_fit\_preds }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{metrics}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ outcome, }\AttributeTok{estimate =}\NormalTok{ .pred\_class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 2 x 3
>   .metric  .estimator .estimate
>   <chr>    <chr>          <dbl>
> 1 accuracy binary         0.969
> 2 kap      binary         0.932
\end{verbatim}

\end{example}

Again, no warnings, so we have a functioning model. It also has a high
accuracy, but we know that this is not the most reliable metric for the
robustness of the model on new data. Similar to what we did to tune the
hyperparameters, we can use cross-validation to gauge the variability of
the model. The \texttt{fit\_resamples()} function takes a workflow and a
resampling object and returns metrics for each fold. The code is seen in
Example~\ref{exm-pda-class-tune-hyperparameters-evaluate-workflow-cv}.

\begin{example}[]\protect\hypertarget{exm-pda-class-tune-hyperparameters-evaluate-workflow-cv}{}\label{exm-pda-class-tune-hyperparameters-evaluate-workflow-cv}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Cross{-}validate workflow}
\NormalTok{cls\_lasso\_cv }\OtherTok{\textless{}{-}}
\NormalTok{  cls\_wf\_lasso }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit\_resamples}\NormalTok{(}
    \AttributeTok{resamples =}\NormalTok{ cls\_vfold,}
    \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\end{example}

We want to aggregate the metrics across the folds to get a sense of the
variability of the model. The \texttt{collect\_metrics()} function takes
the results of a cross-validation and returns a data frame with the
metrics.

\begin{example}[]\protect\hypertarget{exm-pda-class-tune-hyperparameters-evaluate-workflow-cv-collect}{}\label{exm-pda-class-tune-hyperparameters-evaluate-workflow-cv-collect}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Collect metrics}
\NormalTok{cls\_lasso\_cv }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 2 x 6
>   .metric  .estimator  mean     n std_err .config             
>   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               
> 1 accuracy binary     0.921    10 0.00627 Preprocessor1_Model1
> 2 roc_auc  binary     0.977    10 0.00312 Preprocessor1_Model1
\end{verbatim}

\end{example}

The accuracy has dropped, but is still very high. From these metrics it
appears we have a good candidate model. In many cases, however, there
may be further room for improvement. A good next step to in these cases
is to evaluate the model errors and see if there are any patterns that
can be addressed before evaluating the model on the test set.

For classification tasks, a good place to start is to visualize the
confusion matrix to assess false negatives and false positives. The
\texttt{conf\_mat\_resampled()} function takes a \texttt{fit\_resamples}
object and returns a table (\texttt{tidy\ =\ FALSE}) with the confusion
matrix for the aggregated folds. We can pass this to the
\texttt{autoplot()} function to plot as in
Example~\ref{exm-pda-class-tune-hyperparameters-evaluate-workflow-cv-confusion}.

\begin{example}[]\protect\hypertarget{exm-pda-class-tune-hyperparameters-evaluate-workflow-cv-confusion}{}\label{exm-pda-class-tune-hyperparameters-evaluate-workflow-cv-confusion}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Plot confusion matrix}
\NormalTok{cls\_lasso\_cv }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{conf\_mat\_resampled}\NormalTok{(}\AttributeTok{tidy =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{type =} \StringTok{"heatmap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{prediction_files/figure-pdf/fig-class-tune-hyperparameters-evaluate-workflow-cv-confusion-1.pdf}

}

\caption{\label{fig-class-tune-hyperparameters-evaluate-workflow-cv-confusion}Confusion
matrix for the aggregated folds of the cross-validation.}

\end{figure}%

\end{example}

The top left to bottom right diagonal contains the true positives and
true negatives. The top right to bottom left diagonal contains the false
positives and false negatives. The convention is speak of one class
being the positive class and the other class being the negative class.
In our case, we will consider the positive class to be the `learner'
class and the negative class to be the `natives' class.

We can see that there are more learners falsely predicted to be natives
than the other way around. This may be due to the fact that there are
simply more learners than natives in the data set or this could signal
that there are some learners that are more similar to natives than other
learners. Clearly this can't be the entire explanation as the model is
not perfect, even some natives are classified falsely as learners! But
may be an interesting avenue for further exploration. Maybe these are
learners that are more advanced or have a particular style of writing
that is more similar to natives.

Another perspective often applied to evaluate a model is the Reciever
Operating Characteristic (ROC) curve. The ROC curve is a plot of the
true positive rate (TPR) against the false positive rate (FPR) for
different classification thresholds. To produce the ROC curve, we pass
the resampled fit to the \texttt{collect\_preditions()} function. We
then pass this result to \texttt{roc\_curve()} function. But we want the
results grouped by each fold, so we use \texttt{group\_by(id)}. Finally,
we can pass this to the \texttt{autoplot()} function to plot as in
Example~\ref{exm-pda-class-tune-hyperparameters-evaluate-workflow-cv-roc}.

\begin{example}[]\protect\hypertarget{exm-pda-class-tune-hyperparameters-evaluate-workflow-cv-roc}{}\label{exm-pda-class-tune-hyperparameters-evaluate-workflow-cv-roc}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Plot ROC curve}
\NormalTok{cls\_lasso\_cv }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{collect\_predictions}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(id) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{roc\_curve}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ outcome, .pred\_Learner) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{autoplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{y =} \StringTok{"True positive rate"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"False positive rate"}\NormalTok{,}
    \AttributeTok{color =} \StringTok{"Fold"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{prediction_files/figure-pdf/fig-pda-class-tune-hyperparameters-evaluate-workflow-cv-roc-1.pdf}

}

\caption{\label{fig-pda-class-tune-hyperparameters-evaluate-workflow-cv-roc}ROC
curve for the aggregated folds of the cross-validation.}

\end{figure}%

\end{example}

In a ROC curve plot, each line represents a model's performance for
varying classification probability thresholds (from near 0 to 1, where
0.5 equal chance). The dotted line represents the null model
(\emph{i.e.} random guessing). The further the line is from the dotted
line, along the top left to bottom right diagonal, the better the model.
Thus, the larger the area under the curve (AUC) implies a more robust
model for distinguishing between the two classes.

In
Figure~\ref{fig-pda-class-tune-hyperparameters-evaluate-workflow-cv-roc},
each fold is plotted. We can see that the AUC is quite large for most of
the folds which suggests that our model is robust.

We are now ready to move on to step 7, evaluating the model on the test
set. We will use the \texttt{predict()} function to predict the outcome
on the test set. The \texttt{metrics()} function takes a data frame with
the actual and predicted outcomes and returns a data frame with the
metrics for the model. The code is seen in
Example~\ref{exm-pda-class-tune-hyperparameters-evaluate-test}.

\begin{example}[]\protect\hypertarget{exm-pda-class-tune-hyperparameters-evaluate-test}{}\label{exm-pda-class-tune-hyperparameters-evaluate-test}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# [ ] update to use \textasciigrave{}last\_fit()\textasciigrave{}?}

\NormalTok{cls\_final\_fit }\OtherTok{\textless{}{-}}
  \FunctionTok{last\_fit}\NormalTok{(}
\NormalTok{    cls\_wf\_lasso,}
    \AttributeTok{split =}\NormalTok{ cls\_split}
\NormalTok{  )}

\NormalTok{cls\_final\_fit }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 2 x 4
>   .metric  .estimator .estimate .config             
>   <chr>    <chr>          <dbl> <chr>               
> 1 accuracy binary         0.927 Preprocessor1_Model1
> 2 roc_auc  binary         0.972 Preprocessor1_Model1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Evaluate model on test set}
\NormalTok{cls\_lasso\_fit\_preds\_test }\OtherTok{\textless{}{-}}
  \FunctionTok{bind\_cols}\NormalTok{(}
\NormalTok{    cls\_test,}
    \FunctionTok{predict}\NormalTok{(cls\_wf\_lasso\_fit, cls\_test)}
\NormalTok{  )}

\CommentTok{\# Calculate accuracy}
\NormalTok{cls\_lasso\_fit\_preds\_test }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{metrics}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ outcome, }\AttributeTok{estimate =}\NormalTok{ .pred\_class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 2 x 3
>   .metric  .estimator .estimate
>   <chr>    <chr>          <dbl>
> 1 accuracy binary         0.927
> 2 kap      binary         0.843
\end{verbatim}

\end{example}

The accuracy is as good as the training set, even a tad higher. This is
a good sign that the model is robust as it performs well on both
training and test sets. We can evaluate the confusion matrix on the test
set as well. The code is seen in
Example~\ref{exm-pda-class-tune-hyperparameters-evaluate-test-confusion}.

\begin{example}[]\protect\hypertarget{exm-pda-class-tune-hyperparameters-evaluate-test-confusion}{}\label{exm-pda-class-tune-hyperparameters-evaluate-test-confusion}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Plot confusion matrix}
\NormalTok{cls\_lasso\_fit\_preds\_test }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{conf\_mat}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ outcome, }\AttributeTok{estimate =}\NormalTok{ .pred\_class) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{type =} \StringTok{"heatmap"}\NormalTok{)}

\NormalTok{cls\_final\_fit }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{collect\_predictions}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{conf\_mat}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ outcome, }\AttributeTok{estimate =}\NormalTok{ .pred\_class) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{autoplot}\NormalTok{(}\AttributeTok{type =} \StringTok{"heatmap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{prediction_files/figure-pdf/fig-pda-class-tune-hyperparameters-evaluate-test-confusion-1.pdf}

}

\caption{\label{fig-pda-class-tune-hyperparameters-evaluate-test-confusion-1}Confusion
matrix for the test set.}

\end{figure}%

\begin{figure}[H]

\centering{

\includegraphics{prediction_files/figure-pdf/fig-pda-class-tune-hyperparameters-evaluate-test-confusion-2.pdf}

}

\caption{\label{fig-pda-class-tune-hyperparameters-evaluate-test-confusion-2}Confusion
matrix for the test set.}

\end{figure}%

\end{example}

On the test set the false instances look similar to the distribution on
the training set, with the exception that learners are more likely to be
falsely predicted to be natives than the other way around.

We can inspect the errors on the test set by filtering the data frame to
only include the false instances. I will then select the columns with
the actual outcome, the predicted outcome, the proficiency level, and
the text and separate the predicted outcome to inspect them separately,
as seen in
Example~\ref{exm-pda-class-tune-hyperparameters-evaluate-test-errors}.

\begin{example}[]\protect\hypertarget{exm-pda-class-tune-hyperparameters-evaluate-test-errors}{}\label{exm-pda-class-tune-hyperparameters-evaluate-test-errors}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Inspect errors}
\NormalTok{cls\_lasso\_fit\_preds\_test }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(outcome }\SpecialCharTok{!=}\NormalTok{ .pred\_class) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(outcome, .pred\_class, proficiency, text)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 43 x 4
>    outcome .pred_class proficiency        text                                  
>    <fct>   <fct>       <chr>              <chr>                                 
>  1 Learner Native      Upper beginner     "Un dia, El niño estaba durmiendo cua~
>  2 Learner Native      Lower intermediate "Estoy actualmente en mi segundo año ~
>  3 Learner Native      Lower intermediate "La película “Solas” contiene muchos ~
>  4 Learner Native      Upper intermediate "Un día decidí llevarme a casa una ra~
>  5 Learner Native      Lower advanced     "El año pasado nos fuimos al festivál~
>  6 Learner Native      Lower advanced     "Bueno, el año pasado mi novia y yo v~
>  7 Learner Native      Lower advanced     "Me parece que en actualidad, el mund~
>  8 Learner Native      Lower advanced     "Acabo de visitar Barcelona con mi no~
>  9 Learner Native      Lower advanced     "Podria escribir un lilbro de mis pla~
> 10 Learner Native      Lower advanced     "Un día Miguel y su perro Pepe encont~
> # i 33 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Inspect learners falsely predicted to be natives}
\NormalTok{cls\_lasso\_fit\_preds\_test }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(outcome }\SpecialCharTok{==} \StringTok{"Learner"}\NormalTok{, .pred\_class }\SpecialCharTok{==} \StringTok{"Native"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(outcome, .pred\_class, proficiency, text) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(proficiency, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 5 x 2
>   proficiency            n
>   <chr>              <int>
> 1 Upper advanced        13
> 2 Lower advanced         8
> 3 Lower intermediate     2
> 4 Upper beginner         1
> 5 Upper intermediate     1
\end{verbatim}

\end{example}

\begin{itemize}
\item
  Majority of misclassified learners are advanced, which could be
  expected as they are more similar to natives. There are some beginners
  that are misclassified as natives, which is interesting, and not
  expected. But all models are wrong, but some are useful --George Box.
\item
  Still an open question as to why some natives are classified as
  learners.
\end{itemize}

We can inspect the estimates for the features in the model to gain some
insight into what features are most important for predicting the
outcomes. The \texttt{extract\_fit\_parsnip()} function takes a trained
model specification and returns a data frame with the estimated
coefficients for each feature. The code is seen in
Example~\ref{exm-pda-class-tune-hyperparameters-evaluate-test-estimates}.

\begin{example}[]\protect\hypertarget{exm-pda-class-tune-hyperparameters-evaluate-test-estimates}{}\label{exm-pda-class-tune-hyperparameters-evaluate-test-estimates}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract estimates}
\NormalTok{cls\_wf\_lasso\_fit }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{extract\_fit\_parsnip}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tidy}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1,001 x 3
>    term               estimate penalty
>    <chr>                 <dbl>   <dbl>
>  1 (Intercept)         -1.37   0.00599
>  2 tf_text_10           0      0.00599
>  3 tf_text_2            0      0.00599
>  4 tf_text_3            0      0.00599
>  5 tf_text_4            0      0.00599
>  6 tf_text_a            0.350  0.00599
>  7 tf_text_abandonado   0.0222 0.00599
>  8 tf_text_abuela       0      0.00599
>  9 tf_text_abuelos      0      0.00599
> 10 tf_text_aburrido    -0.108  0.00599
> # i 991 more rows
\end{verbatim}

\end{example}

The estimates are the log odds of the outcome. In a binary
classification task, the log odds of the outcome is the log of the
probability of the outcome divided by the probability of the other
outcome. In our case, the reference outcome is ``Learner'', so negative
log-odds indicate that the feature is associated with the ``Learner''
outcome and positive log-odds indicate that the feature is associated
with the ``Native'' outcome.

The estimates are in log-odds, so we need to exponentiate them to get
the odds. The odds are the probability of the outcome divided by the
probability of the other outcome. The probability of the outcome is the
odds divided by the odds plus one. The code is seen in
Example~\ref{exm-pda-class-tune-hyperparameters-evaluate-test-estimates-probability}.

\begin{example}[]\protect\hypertarget{exm-pda-class-tune-hyperparameters-evaluate-test-estimates-probability}{}\label{exm-pda-class-tune-hyperparameters-evaluate-test-estimates-probability}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate probability}
\NormalTok{cls\_wf\_lasso\_fit }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{extract\_fit\_parsnip}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tidy}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{probability =} \FunctionTok{exp}\NormalTok{(estimate) }\SpecialCharTok{/}\NormalTok{ (}\FunctionTok{exp}\NormalTok{(estimate) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1,001 x 4
>    term               estimate penalty probability
>    <chr>                 <dbl>   <dbl>       <dbl>
>  1 (Intercept)         -1.37   0.00599       0.202
>  2 tf_text_10           0      0.00599       0.5  
>  3 tf_text_2            0      0.00599       0.5  
>  4 tf_text_3            0      0.00599       0.5  
>  5 tf_text_4            0      0.00599       0.5  
>  6 tf_text_a            0.350  0.00599       0.587
>  7 tf_text_abandonado   0.0222 0.00599       0.506
>  8 tf_text_abuela       0      0.00599       0.5  
>  9 tf_text_abuelos      0      0.00599       0.5  
> 10 tf_text_aburrido    -0.108  0.00599       0.473
> # i 991 more rows
\end{verbatim}

\end{example}

So just looking at the snippet of the features returned from
Example~\ref{exm-pda-class-tune-hyperparameters-evaluate-test-estimates-probability},
we can see that the features `a' and `abandonado' are slightly
associated with the ``Native'' outcome nd the other features are neutral
(\texttt{probability} = 0.5).

A quick way to extract the most important features for predicting the
each outcome is to use the \texttt{vi()} function from the \texttt{vip}
package. It takes a trained model specification and returns a data frame
with the most important features. The code is seen in
Example~\ref{exm-pda-class-tune-hyperparameters-evaluate-test-vip}.

\begin{example}[]\protect\hypertarget{exm-pda-class-tune-hyperparameters-evaluate-test-vip}{}\label{exm-pda-class-tune-hyperparameters-evaluate-test-vip}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(vip)}

\NormalTok{conflicted}\SpecialCharTok{::}\FunctionTok{conflicts\_prefer}\NormalTok{(vip}\SpecialCharTok{::}\NormalTok{vi)}

\CommentTok{\# Extract important features}
\NormalTok{var\_importance\_df }\OtherTok{\textless{}{-}}
\NormalTok{  cls\_wf\_lasso\_fit }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{vi}\NormalTok{()}

\CommentTok{\# Preview}
\NormalTok{var\_importance\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1,000 x 3
>    Variable           Importance Sign 
>    <chr>                   <dbl> <chr>
>  1 tf_text_todavia          8.67 NEG  
>  2 tf_text_eeuu             7.20 NEG  
>  3 tf_text_ésta             6.31 POS  
>  4 tf_text_sorpresa         6.31 POS  
>  5 tf_text_sienta           6.30 POS  
>  6 tf_text_arriba           6.29 NEG  
>  7 tf_text_ahí              6.06 POS  
>  8 tf_text_encuentran       5.86 NEG  
>  9 tf_text_mayoría          5.85 NEG  
> 10 tf_text_favorito         5.75 NEG  
> # i 990 more rows
\end{verbatim}

\end{example}

The \texttt{Variable} column contains each feature (with the feature
type and corresponding variable \texttt{tf\_text\_}),
\texttt{Importance} provides the absolute log-odds value, and the
\texttt{Sign} column indicates whether the feature is associated with
the ``NEG'' (``Learner'') or the ``POS'' (``Native'') outcome. We can
recode the \texttt{Variable} and \texttt{Sign} columns to make them more
interpretable and exponentiate the log-odds to get probabilites and then
plot them using \texttt{ggplot()}, as in
Example~\ref{exm-pda-class-tune-hyperparameters-evaluate-test-vip-plot}.

\begin{example}[]\protect\hypertarget{exm-pda-class-tune-hyperparameters-evaluate-test-vip-plot}{}\label{exm-pda-class-tune-hyperparameters-evaluate-test-vip-plot}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Recode variable and sign}
\NormalTok{var\_importance\_df }\OtherTok{\textless{}{-}}
\NormalTok{  var\_importance\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{Feature =} \FunctionTok{str\_remove}\NormalTok{(Variable, }\StringTok{"tf\_text\_"}\NormalTok{),}
    \AttributeTok{Outcome =} \FunctionTok{case\_when}\NormalTok{(Sign }\SpecialCharTok{==} \StringTok{"NEG"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Learner"}\NormalTok{, Sign }\SpecialCharTok{==} \StringTok{"POS"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Native"}\NormalTok{),}
    \AttributeTok{Importance =} \FunctionTok{exp}\NormalTok{(Importance) }\SpecialCharTok{/}\NormalTok{ (}\FunctionTok{exp}\NormalTok{(Importance) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(Outcome, Feature, Importance)}

\CommentTok{\# Plot}
\NormalTok{var\_importance\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_max}\NormalTok{(Importance, }\AttributeTok{n =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(Feature, Importance), }\AttributeTok{y =}\NormalTok{ Importance)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Outcome, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{y =} \StringTok{"Importance"}\NormalTok{, }\AttributeTok{fill =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{prediction_files/figure-pdf/fig-pda-class-tune-hyperparameters-evaluate-test-vip-plot-1.pdf}

}

\caption{\label{fig-pda-class-tune-hyperparameters-evaluate-test-vip-plot}Most
important features for predicting the outcome.}

\end{figure}%

\end{example}

We can inspect
Figure~\ref{fig-pda-class-tune-hyperparameters-evaluate-test-vip-plot},
and qualitatively assess what these features may be telling us about the
differences between the learners and the natives.

In this section we've build a text classifier using a regularized
logistic regression model. We've tuned the hyperparameters to arrive at
a robust model that performs well on both the training and test sets.
We've also evaluated the model errors and inspected the most important
features for predicting the outcome.

The \texttt{tidymodels} package provides a framework for building and
evaluating supervised machine learning models that is modular in nature.
This means, that we can quickly and easily change the model
specification, the features, feature engineering, and the
hyperparameters to arrive at a robust model. If the model is not robust,
we can change models in the model specification. If the features are not
robust, we can change the recipe. If the model is overfitting, we can
tune the hyperparameters.

\subsection{Text regression}\label{sec-pda-text-regression}

We will now turn our attention to the second task in this section, text
regression. In this task, we will use the same original dataset as in
the classification task, but we will predict the placement score based
on the learner writing samples. Let's start by extracting the
observations (only learners) and the relevant variables from the
original data set. The code is seen in Example~\ref{exm-pda-reg-data}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-data}{}\label{exm-pda-reg-data}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract observations and relevant variables}
\NormalTok{reg }\OtherTok{\textless{}{-}}
\NormalTok{  df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(proficiency }\SpecialCharTok{!=} \StringTok{"Native"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\AttributeTok{outcome =}\NormalTok{ placement\_score, proficiency, text)}

\CommentTok{\# Preview}
\NormalTok{reg }\SpecialCharTok{|\textgreater{}} \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 1,906
> Columns: 3
> $ outcome     <dbl> 14.0, 16.3, 16.3, 18.6, 18.6, 18.6, 20.9, 20.9, 20.9, 20.9~
> $ proficiency <chr> "Lower beginner", "Lower beginner", "Lower beginner", "Low~
> $ text        <chr> "Yo vivo es Alanta, Georgia. Atlanta es muy grande ciudad.~
\end{verbatim}

\end{example}

In this task, our outcome variable is numeric so we do not need, or
want, to convert it to a factor. Our predictor variable \texttt{text} is
the same as before. We have already weighed the options for feature
engineering and decided to use the term frequency method (raw counts)
for the top 1,000. Since we are setting up the same recipe as before,
essentially, we can use the same code as before.

Let's first move to step 2, initial split. We will use the
\texttt{initial\_split()} function again, but this time we will not need
to stratify the data as we are not predicting a categorical variable.
The code is seen in Example~\ref{exm-pda-reg-initial-split}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-initial-split}{}\label{exm-pda-reg-initial-split}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Split data}
\NormalTok{reg\_split }\OtherTok{\textless{}{-}}
  \FunctionTok{initial\_split}\NormalTok{(reg, }\AttributeTok{prop =} \FloatTok{0.8}\NormalTok{)}

\CommentTok{\# Training set}
\NormalTok{reg\_train }\OtherTok{\textless{}{-}}
  \FunctionTok{training}\NormalTok{(reg\_split)}

\CommentTok{\# Test set}
\NormalTok{reg\_test }\OtherTok{\textless{}{-}}
  \FunctionTok{testing}\NormalTok{(reg\_split)}
\end{Highlighting}
\end{Shaded}

\end{example}

The training set has 1524 observations and the test set has 382
observations.

Now we can create the recipe to set up the variable relations, select
the features, and engineer the features. The code is seen in
Example~\ref{exm-pda-reg-recipe}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-recipe}{}\label{exm-pda-reg-recipe}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a recipe}
\NormalTok{reg\_rec }\OtherTok{\textless{}{-}}
  \FunctionTok{recipe}\NormalTok{(outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ text, }\AttributeTok{data =}\NormalTok{ reg\_train) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tokenize}\NormalTok{(text) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tokenfilter}\NormalTok{(text, }\AttributeTok{max\_tokens =} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tf}\NormalTok{(text) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_log}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{(), }\AttributeTok{offset =} \DecValTok{1}\NormalTok{)}

\CommentTok{\# Preview}
\NormalTok{reg\_rec}
\end{Highlighting}
\end{Shaded}

\end{example}

At this point we would inspect our recipe to make sure that it looks as
expected and gauge the number of features. But since are using the same
recipe as before, we can skip this step.

We can now proceed to interrogate the data. As before we will want to
start with a simple model and then build up to more complex models.
Let's consider some common algorithms for regression tasks in
Table~\ref{tbl-pda-reg-algorithms}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3889}}@{}}
\caption{Regression
algorithms}\label{tbl-pda-reg-algorithms}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Algorithm
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Shortcomings
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Algorithm
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Strengths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Shortcomings
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Linear regression & Simple, interpretable, fast & Assumes linear
relationship between features and outcome \\
Decision trees & Nonlinear relationships, interpretable & Prone to
overfitting \\
Random forest & Nonlinear relationships, interpretable & Prone to
overfitting \\
Neural networks & Nonlinear relationships, fast & Prone to overfitting,
difficult to interpret \\
\end{longtable}

Let's start with a with a linear regression model in mind for our model
specification. But let's also consider what we learned in our first
attempt to build a logistic regression model using word frequencies. We
learned that the model was overfitting the training data and we needed
to use a regularized model to reduce the variance of the model. So let's
start with a regularized linear regression model.

The \texttt{linear\_reg()} function, just as the
\texttt{logistic\_reg()} function, provides arguments for the
regularization hyperparameters when the \texttt{glmnet} computational
engine is used. Let's tune the \texttt{penalty} hyperparameter in the
same way as before. The code for the process is seen in
Example~\ref{exm-pda-reg-model-spec-tune}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-tune}{}\label{exm-pda-reg-model-spec-tune}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create model specification}
\NormalTok{reg\_spec\_lasso }\OtherTok{\textless{}{-}}
  \FunctionTok{linear\_reg}\NormalTok{(}\AttributeTok{penalty =} \FunctionTok{tune}\NormalTok{(), }\AttributeTok{mixture =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"glmnet"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{)}

\CommentTok{\# Create workflow}
\NormalTok{reg\_wf\_lasso }\OtherTok{\textless{}{-}}
  \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{add\_recipe}\NormalTok{(reg\_rec) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{add\_model}\NormalTok{(reg\_spec\_lasso)}

\CommentTok{\# Create tuning grid}
\NormalTok{reg\_grid }\OtherTok{\textless{}{-}}
  \FunctionTok{grid\_regular}\NormalTok{(}\FunctionTok{penalty}\NormalTok{(), }\AttributeTok{levels =} \DecValTok{10}\NormalTok{)}

\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Create tuning workflow}
\NormalTok{reg\_tune }\OtherTok{\textless{}{-}}
  \FunctionTok{tune\_grid}\NormalTok{(}
\NormalTok{    reg\_wf\_lasso,}
    \AttributeTok{resamples =} \FunctionTok{vfold\_cv}\NormalTok{(reg\_train, }\AttributeTok{v =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{grid =}\NormalTok{ reg\_grid,}
    \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Select best parameter}
\NormalTok{chosen\_penalty }\OtherTok{\textless{}{-}}
\NormalTok{  reg\_tune }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"rmse"}\NormalTok{)}

\CommentTok{\# Update workflow}
\NormalTok{reg\_final\_wf\_lasso }\OtherTok{\textless{}{-}}
\NormalTok{  reg\_wf\_lasso }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{finalize\_workflow}\NormalTok{(chosen\_penalty)}

\NormalTok{reg\_final\_wf\_lasso}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> == Workflow ====================================================================
> Preprocessor: Recipe
> Model: linear_reg()
> 
> -- Preprocessor ----------------------------------------------------------------
> 4 Recipe Steps
> 
> * step_tokenize()
> * step_tokenfilter()
> * step_tf()
> * step_log()
> 
> -- Model -----------------------------------------------------------------------
> Linear Regression Model Specification (regression)
> 
> Main Arguments:
>   penalty = 1
>   mixture = 1
> 
> Computational engine: glmnet
\end{verbatim}

\end{example}

Now we fit this model to the training data and evaluate the performance
using cross-validation. The code is seen in
Example~\ref{exm-pda-reg-model-spec-tune-fit-evaluate}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-tune-fit-evaluate}{}\label{exm-pda-reg-model-spec-tune-fit-evaluate}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Cross{-}validated workflow}
\NormalTok{reg\_cv\_lasso }\OtherTok{\textless{}{-}}
\NormalTok{  reg\_final\_wf\_lasso }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit\_resamples}\NormalTok{(}
    \AttributeTok{resamples =} \FunctionTok{vfold\_cv}\NormalTok{(reg\_train, }\AttributeTok{v =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Collect metrics}
\NormalTok{reg\_cv\_lasso }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 2 x 6
>   .metric .estimator   mean     n std_err .config             
>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               
> 1 rmse    standard   13.3      10  0.348  Preprocessor1_Model1
> 2 rsq     standard    0.659    10  0.0222 Preprocessor1_Model1
\end{verbatim}

\end{example}

Now, the RMSE estimate is 13.3. RMSE is expressed in the same units as
the outcome variable. In this case, the outcome variable is the
placement test score percent. So the RMSE is 13.3 percentage points. The
\(R^2\) (rsq) is 0.659. This means that the model explains 66\% of the
variance in the outcome variable. Taken together, this isn't the best
model.

But how good or bad is it? This is where we can use the null model to
compare the model to. The null model is a model that predicts the mean
of the outcome variable. We can use the \texttt{null\_model()} function
to create a null model and submit it to cross-validation. The code is
seen in Example~\ref{exm-pda-reg-model-spec-tune-fit-evaluate-null}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-tune-fit-evaluate-null}{}\label{exm-pda-reg-model-spec-tune-fit-evaluate-null}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create null model}
\NormalTok{null\_model }\OtherTok{\textless{}{-}}
  \FunctionTok{null\_model}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"parsnip"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{)}

\CommentTok{\# Cross{-}validate null model}
\NormalTok{null\_cv }\OtherTok{\textless{}{-}}
  \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{add\_recipe}\NormalTok{(reg\_rec) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{add\_model}\NormalTok{(null\_model) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit\_resamples}\NormalTok{(}
    \AttributeTok{resamples =} \FunctionTok{vfold\_cv}\NormalTok{(reg\_train, }\AttributeTok{v =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{metrics =} \FunctionTok{metric\_set}\NormalTok{(rmse)}
\NormalTok{  )}

\CommentTok{\# Collect metrics}
\NormalTok{null\_cv }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 6
>   .metric .estimator  mean     n std_err .config             
>   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               
> 1 rmse    standard    22.6    10   0.206 Preprocessor1_Model1
\end{verbatim}

\end{example}

Our regression model performs better than the null model (22.608) which
means that it is picking up on some signal in the data.

Let's visualize the distribution of the predictions and the errors from
our model to see if there are any patterns of interest. We can use the
\texttt{collect\_predictions()} function to extract the predictions of
the cross-validation and plot the true outcome against the predicted
outcome using \texttt{ggplot()}, as in
Example~\ref{exm-pda-reg-lr-eval-rmse}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-lr-eval-rmse}{}\label{exm-pda-reg-lr-eval-rmse}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize predictions}
\NormalTok{reg\_cv\_lasso }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{collect\_predictions}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(outcome, .pred, }\AttributeTok{shape =}\NormalTok{ id)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{position =} \FunctionTok{position\_jitter}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.5}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# trend for each fold}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Truth"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Predicted score"}\NormalTok{,}
    \AttributeTok{shape =} \StringTok{"Fold"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{prediction_files/figure-pdf/fig-pda-reg-lr-eval-rmse-1.pdf}

}

\caption{\label{fig-pda-reg-lr-eval-rmse}Distribution of the RMSE for
the cross-validated linear regression model.}

\end{figure}%

\end{example}

From this plot, we see data points for each predicted and truth value
pair for each of the ten folds. There is a trend line for each fold
which shows the linear relationship between the predicted and truth
values for each fold. The trend lines are more similar than different,
which is a good sign that the model is not wildly overfitting the
training data. Looking closer, however, we can see the errors. Some are
noticeably distant from the linear trend lines, \emph{i.e.} outliers, in
particular for test scores in the lower ranges.

If the \(R^2\) value is in the ballpark, this means that somewhere
around 40\% of the variation is not explained by the frequency of the
top 1,000 words. This is not surprising, as there are many other factors
that contribute to the proficiency level of a text.

We have a model that is performing better than the null model, but it is
not performing well enough to be very useful. We will need to update the
model specification and/ or the features to try to improve the model
fit. Let's start with the model. There are many different model
specifications we could try, but we will likely need to use a more
complex model specification to capture the complexity that we observed
in the errors from the previous model.

Let's try a decision tree model. \textbf{Decision trees} are non-linear
models that are able to model non-linear relationships and interactions
between the features and the outcome and tend to be less influenced by
outliers. These are all desirable characteristics. Decision trees,
however, can be prone to overfitting

Along the spectrum of model complexity, decision trees are more complex
than linear regression models, but less complex than other models such
as neural networks. Furthermore, decision trees are interpretable, which
is a nice feature for an exploratory-oriented analysis.

To implement a new model in \texttt{tidymodels}, we need to create a new
model specification and a new workflow. We will use the
\texttt{decision\_tree()} function from the \texttt{parsnip} package to
create the model specification. The \texttt{decision\_tree()} function
takes no arguments and returns a \texttt{decision\_tree} object. We
create the new model specification in
Example~\ref{exm-pda-reg-model-spec-decision-tree}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-decision-tree}{}\label{exm-pda-reg-model-spec-decision-tree}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create model specification}
\NormalTok{reg\_spec\_tree }\OtherTok{\textless{}{-}}
  \FunctionTok{decision\_tree}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"rpart"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{)}

\NormalTok{reg\_spec\_tree}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Decision Tree Model Specification (regression)
> 
> Computational engine: rpart
\end{verbatim}

\end{example}

We now have a new model specification. We can now create a new workflow.
We will use the same recipe as before, but we will change the model
specification to \texttt{prof\_spec\_tree}. We add this to a new
workflow and fit the workflow to the training data. The code is seen in
Example~\ref{exm-pda-reg-model-spec-decision-tree-workflow}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-decision-tree-workflow}{}\label{exm-pda-reg-model-spec-decision-tree-workflow}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create workflow}
\NormalTok{reg\_wf\_tree }\OtherTok{\textless{}{-}}
  \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{add\_recipe}\NormalTok{(reg\_rec) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{add\_model}\NormalTok{(reg\_spec\_tree)}

\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Cross{-}validated workflow}
\NormalTok{reg\_cv\_tree }\OtherTok{\textless{}{-}}
\NormalTok{  reg\_wf\_tree }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit\_resamples}\NormalTok{(}
    \AttributeTok{resamples =} \FunctionTok{vfold\_cv}\NormalTok{(reg\_train, }\AttributeTok{v =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\end{example}

We can now evaluate the performance of the model using cross-validation.
The code is seen in
Example~\ref{exm-pda-reg-model-spec-decision-tree-workflow-evaluate}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-decision-tree-workflow-evaluate}{}\label{exm-pda-reg-model-spec-decision-tree-workflow-evaluate}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Collect metrics}
\NormalTok{reg\_cv\_tree }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 2 x 6
>   .metric .estimator   mean     n std_err .config             
>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               
> 1 rmse    standard   15.1      10  0.404  Preprocessor1_Model1
> 2 rsq     standard    0.554    10  0.0215 Preprocessor1_Model1
\end{verbatim}

\end{example}

The metrics for the vanilla decision tree are similar to the regularized
linear regression model. The RSME is 15.1 and the \(R^2\) is 0.554. Yet,
if we compare the standard error between the two models, we can see that
the decision tree model has a larger standard error. This means that the
decision tree model is more prone to overfitting the training data.

To minimize overfitting, we can tune hyperparameters of the model.
Regularization is one way to reduce overfitting, as we saw with the
regularized linear regression model. Another way to reduce overfitting
is to reduce the complexity of the model. This can be done by reducing
the number of features or by reducing the number of splits in the
decision tree, known as \textbf{pruning}.

Another approach is to implement a random forest model. A \textbf{random
forest} is an ensemble model that combines multiple decision trees to
make a prediction. A random forest is a type of ensemble model which
means that it combines multiple models to make a prediction. In addition
to multiple decision trees, random forests also perform random feature
selection. This helps to reduce the correlation between the decision
trees and thus reduces the variance of the model.

Let's try a random forest model. We will use the \texttt{rand\_forest()}
function from the \texttt{parsnip} package to create the model
specification. The \texttt{rand\_forest()} function takes no arguments
and returns a \texttt{rand\_forest} object. We will select the
\texttt{ranger} engine and add the \texttt{importance} argument to
ensure that we can extract feature importance if this model proves to be
useful. We create the new model specification in
Example~\ref{exm-pda-reg-model-spec-random-forest}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-random-forest}{}\label{exm-pda-reg-model-spec-random-forest}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create model specification}
\NormalTok{reg\_spec\_rf }\OtherTok{\textless{}{-}}
  \FunctionTok{rand\_forest}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"ranger"}\NormalTok{, }\AttributeTok{importance =} \StringTok{"impurity"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{)}

\NormalTok{reg\_spec\_rf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Random Forest Model Specification (regression)
> 
> Engine-Specific Arguments:
>   importance = impurity
> 
> Computational engine: ranger
\end{verbatim}

\end{example}

We can now update the workflow to use the new model specification. The
code is seen in
Example~\ref{exm-pda-reg-model-spec-random-forest-workflow}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-random-forest-workflow}{}\label{exm-pda-reg-model-spec-random-forest-workflow}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Update workflow}
\NormalTok{reg\_wf\_rf }\OtherTok{\textless{}{-}}
\NormalTok{  reg\_wf\_tree }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{update\_model}\NormalTok{(reg\_spec\_rf)}

\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Cross{-}validated workflow}
\NormalTok{reg\_cv\_rf }\OtherTok{\textless{}{-}}
\NormalTok{  reg\_wf\_rf }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit\_resamples}\NormalTok{(}
    \AttributeTok{resamples =} \FunctionTok{vfold\_cv}\NormalTok{(reg\_train, }\AttributeTok{v =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\end{example}

We can now evaluate the performance of the model using cross-validation.
The code is seen in
Example~\ref{exm-pda-reg-model-spec-random-forest-workflow-evaluate}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-random-forest-workflow-evaluate}{}\label{exm-pda-reg-model-spec-random-forest-workflow-evaluate}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Collect metrics}
\NormalTok{reg\_cv\_rf }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 2 x 6
>   .metric .estimator   mean     n std_err .config             
>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               
> 1 rmse    standard   13.1      10  0.305  Preprocessor1_Model1
> 2 rsq     standard    0.680    10  0.0133 Preprocessor1_Model1
\end{verbatim}

\end{example}

The random forest model performs better than the decision tree model and
the regularized linear regression model. The RSME is 13.1 and the
\(R^2\) is 0.68. We also see that the standard error is the lowest of
the models we have tried so far. This means that the random forest model
is less prone to overfitting the training data.

Before we settle on this model, let's try one more model, a support
vector machine (SVM). A \textbf{support vector machine} is a supervised
machine learning model that can be used for both classification and
regression. It is a non-parametric method, which means that it does not
make any assumptions about the underlying distribution of the data. It
is also a method which can be better suited for high-dimensional data
where many values are zero, that is, sparse data, which is often the
case with text data.

Let's again specify the model. We will use the \texttt{svm\_linear()}
function from the \texttt{parsnip} package to create the model
specification and we will use the \texttt{LiblineaR} computational
engine. We create the new model specification in
Example~\ref{exm-pda-reg-model-spec-svm}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-svm}{}\label{exm-pda-reg-model-spec-svm}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create model specification}
\NormalTok{reg\_spec\_svm }\OtherTok{\textless{}{-}}
  \FunctionTok{svm\_linear}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"LiblineaR"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{)}

\NormalTok{reg\_spec\_svm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Linear Support Vector Machine Model Specification (regression)
> 
> Computational engine: LiblineaR
\end{verbatim}

\end{example}

Now let's update the workflow to use the new model specification. The
code is seen in Example~\ref{exm-pda-reg-model-spec-svm-workflow}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-svm-workflow}{}\label{exm-pda-reg-model-spec-svm-workflow}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Update workflow}
\NormalTok{reg\_wf\_svm }\OtherTok{\textless{}{-}}
\NormalTok{  reg\_wf\_rf }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{update\_model}\NormalTok{(reg\_spec\_svm)}

\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Cross{-}validated workflow}
\NormalTok{reg\_cv\_svm }\OtherTok{\textless{}{-}}
\NormalTok{  reg\_wf\_svm }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit\_resamples}\NormalTok{(}
    \AttributeTok{resamples =} \FunctionTok{vfold\_cv}\NormalTok{(reg\_train, }\AttributeTok{v =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\end{example}

Let's evaluate the cross-validation performance of the model. The code
is seen in Example~\ref{exm-pda-reg-model-spec-svm-workflow-evaluate}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-svm-workflow-evaluate}{}\label{exm-pda-reg-model-spec-svm-workflow-evaluate}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Collect metrics}
\NormalTok{reg\_cv\_svm }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 2 x 6
>   .metric .estimator   mean     n std_err .config             
>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               
> 1 rmse    standard   14.7      10  0.353  Preprocessor1_Model1
> 2 rsq     standard    0.625    10  0.0152 Preprocessor1_Model1
\end{verbatim}

\end{example}

This SVR is not performing better than the linear regression and random
forest models and the standard error is not particularly low. So we will
not pursue this model further.

So in summary, we've tried four different model specifications. The
regularized linear regression model, the decision tree model, the random
forest model, and the support vector machine model. The random forest
model performed the best. But there stands to be improvement, but it
looks as if we may need to update the features.

In the recipe we have used until this point we have limited the word
features to 1,000 and used the raw counts of the words. The rationale
for this was that we wanted to limit the number of features to reduce
the complexity of the model while still capturing the most important
features. But we may have limited the features too much, at least now
that we are comparing features between the same population (learners). A
potentially more informative feature would be the TF-IDF score. This
score is a measure of how important a word is to a document in a
collection or corpus. Specifically, it is the product of the term
frequency and the inverse document frequency. The more dispersed a
feature is across the corpus, the lower the TF-IDF score. The more
concentrated a feature is across the corpus, the higher the TF-IDF
score, and the more informative the feature is for distinguishing
between documents.

So we can use the \texttt{step\_tfidf()} function to update the recipe.
The code is seen in Example~\ref{exm-pda-reg-recipe-tfidf}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-recipe-tfidf}{}\label{exm-pda-reg-recipe-tfidf}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a recipe}
\NormalTok{reg\_rec }\OtherTok{\textless{}{-}}
  \FunctionTok{recipe}\NormalTok{(outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ text, }\AttributeTok{data =}\NormalTok{ reg\_train) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tokenize}\NormalTok{(text) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tokenfilter}\NormalTok{(text, }\AttributeTok{max\_tokens =} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tfidf}\NormalTok{(text, }\AttributeTok{smooth\_idf =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Preview}
\NormalTok{reg\_rec}
\end{Highlighting}
\end{Shaded}

\end{example}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{hand-point-up} Tip}

Two things to note about the code in
Example~\ref{exm-pda-reg-recipe-tfidf}. First, the
\texttt{step\_tfidf()} function by default add a smoothing term to the
inverse document frequency (IDF) calculation. This setting has the
effect of reducing the influence of the IDF calculation. Thus, terms
that appear in many (or all) documents will not be downweighted as much
as they would be if the smoothing term was not added. For our purposes,
we want to downweight or eliminate the influence of the most frequent
terms, so we will set \texttt{smooth\_idf\ =\ FALSE}.

Second, the log-transformation is not necessary when using TF-IDF scores
as normalization is built into the TF-IDF calculation.

\end{tcolorbox}

Another change we can explore is to increase the number of features. But
how many features should we use? We can turn to the \texttt{tune()}
function to help us answer this question. Let's add the \texttt{tune()}
function to the recipe and tune the \texttt{max\_tokens} hyperparameter.
The code is seen in Example~\ref{exm-pda-reg-recipe-tune}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-recipe-tune}{}\label{exm-pda-reg-recipe-tune}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a recipe}
\NormalTok{reg\_rec }\OtherTok{\textless{}{-}}
  \FunctionTok{recipe}\NormalTok{(outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ text, }\AttributeTok{data =}\NormalTok{ reg\_train) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tokenize}\NormalTok{(text) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tokenfilter}\NormalTok{(text, }\AttributeTok{max\_tokens =} \FunctionTok{tune}\NormalTok{()) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tfidf}\NormalTok{(text, }\AttributeTok{smooth\_idf =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Preview}
\NormalTok{reg\_rec}
\end{Highlighting}
\end{Shaded}

\end{example}

Let's tune the \texttt{max\_tokens} with the random forest model
specification that we created earlier. We will create a tuning workflow,
which will tune the \texttt{max\_tokens} hyperparameter on the training
data using cross-validation. The code is seen in
Example~\ref{exm-pda-reg-model-spec-random-forest-workflow-tune}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-random-forest-workflow-tune}{}\label{exm-pda-reg-model-spec-random-forest-workflow-tune}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create tuning workflow}
\NormalTok{tune\_wf }\OtherTok{\textless{}{-}}
  \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{add\_recipe}\NormalTok{(reg\_rec) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{add\_model}\NormalTok{(reg\_spec\_rf)}

\NormalTok{tune\_wf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> == Workflow ====================================================================
> Preprocessor: Recipe
> Model: rand_forest()
> 
> -- Preprocessor ----------------------------------------------------------------
> 3 Recipe Steps
> 
> * step_tokenize()
> * step_tokenfilter()
> * step_tfidf()
> 
> -- Model -----------------------------------------------------------------------
> Random Forest Model Specification (regression)
> 
> Engine-Specific Arguments:
>   importance = impurity
> 
> Computational engine: ranger
\end{verbatim}

\end{example}

The \texttt{tune\_grid()} function takes a workflow, a resampling
method, a tuning grid, and a set of metrics. The tuning grid is a set of
hyperparameters and their values. The \texttt{tune\_grid()} function
will tune the hyperparameters on the training data using
cross-validation. The code is seen in
Example~\ref{exm-pda-reg-model-spec-random-forest-workflow-tune-grid}.
This is a computationally intensive process, as it will train a 10
folds, for each hyperparameter value. Each fold in the random forest is
500 trees, so this will train 5,000 trees for each hyperparameter value!
So that is 30,000 trees in total. This code will take some time to run.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-random-forest-workflow-tune-grid}{}\label{exm-pda-reg-model-spec-random-forest-workflow-tune-grid}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create tuning grid}
\NormalTok{tune\_grid }\OtherTok{\textless{}{-}}
  \FunctionTok{grid\_regular}\NormalTok{(}\FunctionTok{max\_tokens}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(}\DecValTok{500}\NormalTok{, }\DecValTok{3500}\NormalTok{)), }\AttributeTok{levels =} \DecValTok{5}\NormalTok{)}

\NormalTok{tune\_grid}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 5 x 1
>   max_tokens
>        <int>
> 1        500
> 2       1250
> 3       2000
> 4       2750
> 5       3500
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Create tuning workflow}
\NormalTok{tune }\OtherTok{\textless{}{-}}
  \FunctionTok{tune\_grid}\NormalTok{(}
\NormalTok{    tune\_wf,}
    \AttributeTok{resamples =} \FunctionTok{vfold\_cv}\NormalTok{(reg\_train, }\AttributeTok{v =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{grid =}\NormalTok{ tune\_grid,}
    \AttributeTok{metrics =} \FunctionTok{metric\_set}\NormalTok{(rmse, rsq),}
    \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  )}

\NormalTok{tune}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # Tuning results
> # 10-fold cross-validation 
> # A tibble: 10 x 5
>    splits             id     .metrics          .notes           .predictions
>    <list>             <chr>  <list>            <list>           <list>      
>  1 <split [1371/153]> Fold01 <tibble [10 x 5]> <tibble [0 x 3]> <tibble>    
>  2 <split [1371/153]> Fold02 <tibble [10 x 5]> <tibble [0 x 3]> <tibble>    
>  3 <split [1371/153]> Fold03 <tibble [10 x 5]> <tibble [0 x 3]> <tibble>    
>  4 <split [1371/153]> Fold04 <tibble [10 x 5]> <tibble [0 x 3]> <tibble>    
>  5 <split [1372/152]> Fold05 <tibble [10 x 5]> <tibble [0 x 3]> <tibble>    
>  6 <split [1372/152]> Fold06 <tibble [10 x 5]> <tibble [0 x 3]> <tibble>    
>  7 <split [1372/152]> Fold07 <tibble [10 x 5]> <tibble [0 x 3]> <tibble>    
>  8 <split [1372/152]> Fold08 <tibble [10 x 5]> <tibble [0 x 3]> <tibble>    
>  9 <split [1372/152]> Fold09 <tibble [10 x 5]> <tibble [0 x 3]> <tibble>    
> 10 <split [1372/152]> Fold10 <tibble [10 x 5]> <tibble [0 x 3]> <tibble>
\end{verbatim}

\end{example}

Just as we did for the \texttt{penalty} hyperparameter, we can visualize
the performance of the model as a function of the \texttt{max\_tokens}
hyperparameter. The code is seen in
Example~\ref{exm-pda-reg-model-spec-random-forest-workflow-tune-plot}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-random-forest-workflow-tune-plot}{}\label{exm-pda-reg-model-spec-random-forest-workflow-tune-plot}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize tuning results}
\NormalTok{tune }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{collect\_metrics}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(max\_tokens, mean)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}
    \AttributeTok{breaks =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{pretty\_breaks}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{comma}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ .metric, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Max tokens"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{prediction_files/figure-pdf/fig-pda-reg-model-spec-random-forest-workflow-tune-plot-1.pdf}

}

\caption{\label{fig-pda-reg-model-spec-random-forest-workflow-tune-plot}Performance
of the random forest model as a function of the max\_tokens
hyperparameter.}

\end{figure}%

\end{example}

We added a range of maximum token values to the tuning grid to see if
there is a point of diminishing returns. We can see that the performance
of the model actually appears to improve, not with more tokens, but
instead with less. Our lowest value of 500 tokens appears to perform the
best. This is interesting, as it suggests that there are a small number
of words that are most informative for predicting the placement test
score. It also suggests that individual words, on the whole, are not
very informative for predicting the placement test score.

We can now select the best hyperparameter value. The code is seen in
Example~\ref{exm-pda-reg-model-spec-random-forest-workflow-tune-select}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-random-forest-workflow-tune-select}{}\label{exm-pda-reg-model-spec-random-forest-workflow-tune-select}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Select best parameter}
\NormalTok{chosen\_max\_tokens }\OtherTok{\textless{}{-}}
\NormalTok{  tune }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"rmse"}\NormalTok{)}

\CommentTok{\# Update workflow}
\NormalTok{reg\_final\_rf }\OtherTok{\textless{}{-}}
\NormalTok{  tune\_wf }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{finalize\_workflow}\NormalTok{(chosen\_max\_tokens)}

\NormalTok{reg\_final\_rf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> == Workflow ====================================================================
> Preprocessor: Recipe
> Model: rand_forest()
> 
> -- Preprocessor ----------------------------------------------------------------
> 3 Recipe Steps
> 
> * step_tokenize()
> * step_tokenfilter()
> * step_tfidf()
> 
> -- Model -----------------------------------------------------------------------
> Random Forest Model Specification (regression)
> 
> Engine-Specific Arguments:
>   importance = impurity
> 
> Computational engine: ranger
\end{verbatim}

\end{example}

We can now fit the model to the training data and evaluate the
performance using cross-validation. The code is seen in
Example~\ref{exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate}{}\label{exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Cross{-}validated workflow}
\NormalTok{reg\_cv\_rf }\OtherTok{\textless{}{-}}
\NormalTok{  reg\_final\_rf }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit\_resamples}\NormalTok{(}
    \AttributeTok{resamples =} \FunctionTok{vfold\_cv}\NormalTok{(reg\_train, }\AttributeTok{v =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Collect metrics}
\NormalTok{reg\_cv\_rf }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 2 x 6
>   .metric .estimator   mean     n std_err .config             
>   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               
> 1 rmse    standard   12.8      10  0.221  Preprocessor1_Model1
> 2 rsq     standard    0.695    10  0.0137 Preprocessor1_Model1
\end{verbatim}

\end{example}

We've improved the model, but not very much. We can see that the RMSE is
13.1 and the \(R^2\) is 0.68. The standard error is NA. This is the
lowest standard error we have seen so far, but it is still not as low as
we would like.

Let's now visualize the distribution of the predictions and the errors
from our word features model to see if there are any patterns of
interest. We can use the \texttt{collect\_predictions()} function to
extract the predictions of the cross-validation and plot the true
outcome agains the predicted outcome using \texttt{ggplot()}, as in
Example~\ref{exm-pda-reg-lr-eval-rmse}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-lr-eval-rmse}{}\label{exm-pda-reg-lr-eval-rmse}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize predictions}
\NormalTok{reg\_cv\_rf }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{collect\_predictions}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(outcome, .pred, }\AttributeTok{shape =}\NormalTok{ id)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{position =} \FunctionTok{position\_jitter}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.5}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# trend for each fold}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Truth"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Predicted score"}\NormalTok{,}
    \AttributeTok{shape =} \StringTok{"Fold"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{prediction_files/figure-pdf/fig-pda-reg-rf-eval-rmse-1.pdf}

}

\caption{\label{fig-pda-reg-rf-eval-rmse}Distribution of the RMSE for
the cross-validated linear regression model.}

\end{figure}%

\end{example}

There appears to be more cohesion in the predictions, overall. The
errors however seem visually to be larger for the lower scores.

At this point we can either consider this model to be good enough or we
can try to improve it further. Let's take one more shot at improving the
features by including bigrams. To include words (unigrams) and bigrams,
we can modify the \texttt{step\_tokenize()} function in our recipe. We
add the \texttt{token\ =\ "ngrams"} argument and specify the
\texttt{options} as \texttt{list(n\ =\ 2,\ n\_min\ =\ 1)}. This will
create unigrams and bigrams. The code is seen in
Example~\ref{exm-pda-reg-recipe-tfidf-bigrams}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-recipe-tfidf-bigrams}{}\label{exm-pda-reg-recipe-tfidf-bigrams}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a recipe}
\NormalTok{reg\_rec }\OtherTok{\textless{}{-}}
  \FunctionTok{recipe}\NormalTok{(outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ text, }\AttributeTok{data =}\NormalTok{ reg\_train) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tokenize}\NormalTok{(text, }\AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{, }\AttributeTok{options =} \FunctionTok{list}\NormalTok{(}\AttributeTok{n =} \DecValTok{2}\NormalTok{, }\AttributeTok{n\_min =} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tokenfilter}\NormalTok{(text, }\AttributeTok{max\_tokens =} \FunctionTok{tune}\NormalTok{()) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{step\_tfidf}\NormalTok{(text, }\AttributeTok{smooth\_idf =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Preview}
\NormalTok{reg\_rec}
\end{Highlighting}
\end{Shaded}

\end{example}

Since the features have changed, we won't assume that our tuning of the
\texttt{max\_tokens} is still valid. So we will tune the
\texttt{max\_tokens} hyperparameter again, as in
Example~\ref{exm-pda-reg-recipe-tune} through
Example~\ref{exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate}.
For the sake of brevity, we will not show the code here. We will simply
show the results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create workflow}
\NormalTok{reg\_wf\_rf }\OtherTok{\textless{}{-}}
  \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{add\_recipe}\NormalTok{(reg\_rec) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{add\_model}\NormalTok{(reg\_spec\_rf)}

\CommentTok{\# Create tuning grid}
\NormalTok{tune\_grid }\OtherTok{\textless{}{-}}
  \FunctionTok{grid\_regular}\NormalTok{(}\FunctionTok{max\_tokens}\NormalTok{(}\AttributeTok{range =} \FunctionTok{c}\NormalTok{(}\DecValTok{500}\NormalTok{, }\DecValTok{3500}\NormalTok{)), }\AttributeTok{levels =} \DecValTok{5}\NormalTok{)}

\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# Create tuning workflow}
\NormalTok{tune }\OtherTok{\textless{}{-}}
  \FunctionTok{tune\_grid}\NormalTok{(}
\NormalTok{    reg\_wf\_rf,}
    \AttributeTok{resamples =} \FunctionTok{vfold\_cv}\NormalTok{(reg\_train, }\AttributeTok{v =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{grid =}\NormalTok{ tune\_grid,}
    \AttributeTok{metrics =} \FunctionTok{metric\_set}\NormalTok{(rmse, rsq),}
    \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Select best parameter}
\NormalTok{chosen\_max\_tokens }\OtherTok{\textless{}{-}}
\NormalTok{  tune }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select\_best}\NormalTok{(}\AttributeTok{metric =} \StringTok{"rmse"}\NormalTok{)}

\CommentTok{\# Update workflow}
\NormalTok{reg\_final\_rf }\OtherTok{\textless{}{-}}
\NormalTok{  reg\_wf\_rf }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{finalize\_workflow}\NormalTok{(chosen\_max\_tokens)}

\CommentTok{\# Cross{-}validated workflow}
\NormalTok{reg\_cv\_rf }\OtherTok{\textless{}{-}}
\NormalTok{  reg\_final\_rf }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit\_resamples}\NormalTok{(}
    \AttributeTok{resamples =} \FunctionTok{vfold\_cv}\NormalTok{(reg\_train, }\AttributeTok{v =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{control =} \FunctionTok{control\_resamples}\NormalTok{(}\AttributeTok{save\_pred =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Collect metrics}
\NormalTok{estimates }\OtherTok{\textless{}{-}}
\NormalTok{  reg\_cv\_rf }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{collect\_metrics}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The tuning of \texttt{max\_tokens} selected 2,000 features as the
optimal number. The cross-validation of the training dataset produced an
RMSE of 12.8 and an \(R^2\) of 0.698. The standard error is NA. The
upshot is that by including bigrams we have not improved, or worsened,
the model. Including the unigrams and bigrams together may be more
informative for the exploration of the features.

We could continue to try to improve the model, but at this point we have
a model that is performing better than the null model and is performing
better than the other models we have tried. So we will consider this
model to be good enough.

Let's now fit the 1-2 gram model to the testing data and evaluate the
performance on the testing set. The code is seen in
Example~\ref{exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate-test}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate-test}{}\label{exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate-test}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit model on training data}
\NormalTok{reg\_final\_rf\_fit }\OtherTok{\textless{}{-}}
\NormalTok{  reg\_final\_rf }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit}\NormalTok{(reg\_train)}

\CommentTok{\# Predict on testing data}
\NormalTok{reg\_final\_rf\_pred }\OtherTok{\textless{}{-}}
  \FunctionTok{bind\_cols}\NormalTok{(}
\NormalTok{    reg\_test,}
    \FunctionTok{predict}\NormalTok{(reg\_final\_rf\_fit, reg\_test)}
\NormalTok{  )}

\CommentTok{\# Evaluate performance}
\NormalTok{reg\_final\_rf\_pred }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{metrics}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ outcome, }\AttributeTok{estimate =}\NormalTok{ .pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 3 x 3
>   .metric .estimator .estimate
>   <chr>   <chr>          <dbl>
> 1 rmse    standard      12.9  
> 2 rsq     standard       0.695
> 3 mae     standard       9.84
\end{verbatim}

\end{example}

We can now visualize the feature importance of the model. The code is
seen in
Example~\ref{exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate-test-feature-importance}.

\begin{example}[]\protect\hypertarget{exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate-test-feature-importance}{}\label{exm-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate-test-feature-importance}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract predictions}
\NormalTok{reg\_final\_rf\_fit }\SpecialCharTok{|\textgreater{}}
\NormalTok{  vip}\SpecialCharTok{::}\FunctionTok{vi}\NormalTok{(}\AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{)  }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Variable =} \FunctionTok{str\_replace}\NormalTok{(Variable, }\StringTok{"\^{}tfidf\_text\_"}\NormalTok{, }\StringTok{""}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_max}\NormalTok{(Importance, }\AttributeTok{n =} \DecValTok{20}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \CommentTok{\# reorder variables by importance}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\FunctionTok{reorder}\NormalTok{(Variable, Importance), Importance)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Feature"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Importance"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{prediction_files/figure-pdf/fig-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate-test-feature-importance-1.pdf}

}

\caption{\label{fig-pda-reg-model-spec-random-forest-workflow-tune-select-fit-evaluate-test-feature-importance}Feature
importance of the random forest model.}

\end{figure}%

\end{example}

\section*{Activities}\label{activities-7}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-9.html}{Building
predictive models}\\
\textbf{How}: Read Recipe 9 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: \faIcon{wrench} \ldots{}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-09}{Text
classification}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 9.\\
\textbf{Why}: \faIcon{wrench} \ldots{}

\end{tcolorbox}

\section*{Summary}\label{summary-8}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we have learned about supervised machine learning. We
have learned about the different types of supervised machine learning
methods and how they can be used to predict and classify. We have also
learned about the different types of data structures that are used in
supervised machine learning. Finally, we have learned about the
different types of evaluation metrics that are used to evaluate the
performance of supervised machine learning models.

\chapter{Infer}\label{sec-inference}

\begin{quote}
People generally see what they look for, and hear what they listen for.

--- Harper Lee, To Kill a Mockingbird
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Identify the research goals of inferential data analysis
\item
  Describe the workflow for inferential data analysis
\item
  Indicate the importance of quantifying uncertainty in inferential data
  analysis
\end{itemize}

\end{tcolorbox}

In this chapter, we consider approaches to deriving knowledge from
information which can be generalized to the population from which the
data is sampled. This process is known as statistical inference. The
discussion here implements descriptive assessments, statistical tests,
and evaluation procedures for a series of contexts which are common in
the analysis of corpus-based data. During our treatment of these
contexts, we will establish a foundational understanding of the null
hypothesis signficance testing (NHST) framework using a simulation-based
approach. This approach is more intuitive, easier to implement, and
provides a better conceptual understanding of the statistical designs
and analyses. The structure of the chapter groups significance testing
scenarios by reponse and explanatory variables, highlighting the
commonalities and differences that arise in the statistical designs and
analyses.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{terminal} Lessons}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Advanced
Tables}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: \faIcon{wrench} TBD

\end{tcolorbox}

\section{Orientation}\label{sec-ida-orientation}

The aim of this section is to provide an overview of inferential data
analysis (IDA). First we consider the research goal of inferential
analysis pointing to key differences with exploratory data analysis
(EDA) and predictive data analysis (PDA). Next we outline the Null
Hypothesis Signficance Testing (NHST) approach to IDA, highlighting the
key steps in the workflow used to conduct analysis.

\subsection{Research goal}\label{sec-ida-research-goal}

In contrast to exploratory and predictive analyses, inference is not a
data-driven endeavor. Rather, the goal of inferential data analysis
(IDA) is to make claims about the population and assess the extent to
which the data supports those claims. This implicates two key
methodological restrictions which are not in play in other analysis
methods.

First, the research question and expected findings are formulated
\emph{before} the data is analyzed, in fact strictly speaking this
should take place even before data collection. This helps ensure that
the data is aligned with the research question and that the data is
representative of the population and that the analysis has a targeted
focus and does not run the risk of becoming a `just-so' story\footnote{``Hypothesis
  After Result is Known'' (HARKing) involves selectively analyzing data,
  trying different variables or combinations until a significant p-value
  is obtained, or stopping data collection when a significant result is
  found (\citeproc{ref-Kerr1998}{Kerr 1998}).} or a
`significance-finding' mission\footnote{``p-hacking'' is the practice of
  running multiple tests until a statistically significant result is
  found. This practice violates the principles of significance testing
  (\citeproc{ref-Head2015}{Head et al. 2015}).} both of which violate
the principles of significance testing.

Second, the data used in IDA is only used once. That is to say, that the
entire dataset is used a single time to statistically interrogate the
relationship(s) of interest. In both EDA and PDA the data can be
approached in different ways and the results of the analysis can be used
to inform the next steps in the analysis. In IDA, however, the data is
used to test a specific hypothesis and the results of the analysis are
interpreted in the context of that hypothesis.

\subsection{Approach}\label{sec-ida-approach}

The methodological approach to inferential data analysis (IDA) is the
most straightforward of the analysis types covered in this textbook. As
the research goal is to test a claim, the steps necessary are fewer than
in EDA or PDA, where the exploratory nature of these approaches includes
various possible iterations. The workflow for IDA is shown in
Table~\ref{tbl-ida-workflow}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7500}}@{}}
\caption{Workflow for inferential data
analysis}\label{tbl-ida-workflow}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Identify & Identify and map the hypothesis statement to the
appropriate response and explanatory variables \\
2 & Inspect & Assess the distribution of the variable(s) with the
appropriate descriptive statistics and visualizations. \\
3 & Interrogate & Apply the appropriate statistical procedure to the
dataset. \\
4 & Interpret & Review the statistical results and interpret them in the
context of the hypothesis. \\
\end{longtable}

Based on the hypothesis statement, we first identify and operationalize
the variables. The response variable whose variation we aim to explain.
In most statistical designs, one or more explanatory variables are
included in the analysis in an attempt to gauge the extent to which
these variables account for the variation in the response variable. For
both reponse and explanatory variables, it is key to confirm that your
operationalization of the variables is well-defined and that the data is
aligned with the operationalization.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{lightbulb} Consider this}

\begin{itemize}
\tightlist
\item[$\square$]
  Update this callout
\end{itemize}

Consider the following hypothesis statements:

\begin{itemize}
\tightlist
\item
  \emph{The length of the recipient clause is related to the realization
  of the recipient clause.}
\item
  \emph{The length of the recipient clause is related to the realization
  of the recipient clause in spoken language but not in written
  language.}
\item
  \emph{The length of the recipient clause is related to the realization
  of the recipient clause in spoken language but not in written language
  when the length of the theme clause is controlled for.}
\end{itemize}

What are the explanatory and/ or response variables in each of these
statements? What are the operationalizations of these variables? What
are the implications for the data that is used to test these hypotheses?

\end{tcolorbox}

Next, we determine the informational values of the variables. The
informational value of each variable will condition how we approach
visualization, interrogation, and ultimately interpretation of the
results. Note, that some informational types can be converted to other
types, specifically higer-order types can be converted to lower-order
types. For example, a continuous variable can be converted to a
categorical variable, but not vice versa. It is preferrable, however, to
use the highest informational value of a variable. Simplifying data
results in a loss of information --which will result in a loss of
information and hence statistical power which may lead to results that
obscure meaningful patterns in the data (\citeproc{ref-Baayen2004}{R.
Harald Baayen 2004}).

With our design in place, we can now inspect the data. This involves
assessing the distribution of the variables using descriptive statistics
and visualizations. The goal of this step is to confirm the integrity of
the data (missing data, anomalies, etc.), identify general patterns in
the data, and identify potential outliers. As much as this is a
verification step, it also serves to provide a sense of the data and the
extent to which the data aligns with the hypothesis. This is
particularly true when statistical designs are complex and involve
multiple explanatory variables. An appropriate visualization provides
context for interpreting the results of the statistical analysis.

Interrogating the data involves applying the appropriate statistical
procedure to the dataset. In the Null Hypothesis Significance Testing
(NHST) paradigm, this process includes calculating a statistic from the
data, comparing it to a null hypothesis distribution, and measuring the
evidence against the null hypothesis. The null hypothesis distribution
is a distribution of statistic values that we would expect if the null
hypothesis were true, \emph{i.e.} that there is no difference or
relationship between the explanatory and/ or response variables. By
comparing the observed statistic to the null hypothesis distribution, we
can determine the likelihood of observing the observed statistic if the
null hypothesis were true. This likelihood is known as the p-value. When
the p-value is below a pre-determined threshold, typically 0.05, the
result is considered statistically significant. This means that the
observed statistic is sufficiently different from the null hypothesis
distribution that we can reject the null hypothesis.

Now let's consider how to approach interpreting the results from a
statistical test. The p-value provides a probability that the results of
our statistical test could be explained by the null hypothesis. When
this probability crosses above the threshold of .05, the result is
considered statistically significant, otherwise we have a `null result'
(i.e.~non-significant).

However, this sets up a binary distinction that can be problematic. On
the one hand, what is one to do if a test returns a p-value of .051 or
something `marginally significant'? According to standard practice these
results would not be statistically significant. On the other hand, if we
get a statistically significant result, do we move on --case closed? To
address both of these issues, it is important to calculate a confidence
interval for the test statistic. The confidence interval is the range of
values for our test statistic that we would expect the true statistic
value to fall within some level of uncertainty. Again, 95\% is the most
common level of uncertainty. The upper and lower bounds of this range
are called the confidence limits for the test statistic.

Used in conjunction with p-values, confidence intervals can provide a
more nuanced interpretation of the results of a statistical test. For
example, if we get a p-value of .051, but the confidence interval is
very narrow, we can be more confident that the results are reliable.
Conversely, if we get a p-value of .049, but the confidence interval is
very wide, we can be less confident that the results are reliable. If
our confidence interval contains the null value, then even a significant
p-value will require a more nuanced interpretation.

It is important to underscore that the purpose of IDA is to draw
conclusions from a dataset which are generalizable to the population.
These conclusions require that there are rigorous measures to ensure
that the results of the analysis do not overgeneralize (suggest there is
a relationship when there is not one) and balance that with the fact
that we don't want to undergeneralize (miss the fact that there is an
relationship in the population, but our analysis was not capable of
detecting it).

\section{Analysis}\label{sec-ida-analysis}

In this section, we will discuss the practical application of
inferential data analysis. The discussion will be divided into two
sections based on the type of response variable: categorical and
numeric. We will then explore specific designs for univariate,
bivariate, and multivariate tests. Throughout this process, we will
focus on null hypothesis testing, covering concepts and methods that are
relevant to significance testing and the interpretation of results.

Two datasets are included in this section to provide a context for the
inferential data analysis in both categorical and numeric cases. The
\texttt{dative} dataset is a subset of the \texttt{languageR::dative}
dataset (\citeproc{ref-R-languageR}{R. H. Baayen and Shafaei-Bajestan
2019}). It contains over 3k observations describing the realization of
the recipient clause in English dative constructions. The
\texttt{fillers} dataset is drawn from the Switchboard Dialog Act Corpus
(\citeproc{ref-SWDA2008}{University of Colorado Boulder 2008}). This
dataset contains //FIXME \ldots{} observations summarizing the use of
fillers by speaker in the corpus.

We will learn and implement null significance testing using a
simulation-based method. In contrast to theory-based methods,
simulation-based methods tend to be more intuitive, easier to implement,
and provide a better conceptual understanding of the statistical designs
and analyses (\citeproc{ref-Morris2019}{Morris, White, and Crowther
2019}; \citeproc{ref-Rossman2014a}{Rossman and Chance 2014}). The steps
for implementing a simulation-based approach to significance testing are
outlined in Table~\ref{tbl-ida-simulation}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6500}}@{}}
\caption{Simulation-based approach to significance
testing}\label{tbl-ida-simulation}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Specify & Specify the variables of interest and their
relationship \\
2 & Calculate & Calculate the observed statistic \\
3 & Hypothesize & Generate the null hypothesis distribution \\
4 & Visualize & Visualize the null hypothesis distribution \\
5 & Get p-value & Calculate the p-value \\
6 & Get confidence interval & Calculate the confidence interval \\
\end{longtable}

The \texttt{infer} package (\citeproc{ref-R-infer}{Bray et al. 2024})
provides a tidyverse-friendly framework to implement simulation-based
methods for statistical inference. Designed to be used in conjunction
with the \texttt{tidyverse} (\citeproc{ref-R-tidyverse}{Wickham 2023b}),
\texttt{infer} provides a set of functions that can be used to specify
the variables of interest, calculate the observed statistic, generate
the null hypothesis distribution, visualize the null hypothesis
distribution, calculate the p-value, and calculate the confidence
interval.

Let's load the necessary packages and import the datasets we will use in
this section, as seen in Example~\ref{exm-ida-setup}.

\begin{example}[]\protect\hypertarget{exm-ida-setup}{}\label{exm-ida-setup}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load packages}
\FunctionTok{library}\NormalTok{(infer)      }\CommentTok{\# for statistical inference}
\FunctionTok{library}\NormalTok{(skimr)      }\CommentTok{\# for descriptive statistics}
\FunctionTok{library}\NormalTok{(janitor)    }\CommentTok{\# for cross{-}tabulation}

\CommentTok{\# Load datasets}
\NormalTok{dative\_df }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"../data/derived/dative.csv"}\NormalTok{)}

\NormalTok{fillers\_df }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"../data/derived/fillers.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

\subsection{Categorical}\label{sec-ida-categorical}

Here we demonstrate the application of inferential data analysis (IDA)
to categorical response variables. This will include various common
statistical designs and analyses. In Table~\ref{tbl-ida-cat-design}, we
see common design scenarios, the variables involved, and the statistic
used in the analysis.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2000}}@{}}
\caption{Statistical test designs for categorical response
variables}\label{tbl-ida-cat-design}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Scenario
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Dependent variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Independent variable(s)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Statistical test
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Scenario
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Dependent variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Independent variable(s)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Statistical test
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Univariate & Categorical & None & Proportion \\
Bivariate & Categorical & Categorical & Difference in proportions \\
Bivariate & Categorical (\textgreater2 levels) & Categorical
(\textgreater2 levels) & Chi-square \\
Multivariate & Categorical & Categorical or Numeric (\textgreater1
variables) & Logistic regression \\
\end{longtable}

We will use the \texttt{dative\_df} dataset to illustrate each of these
designs and analyses. To familiarize ourselves with the dataset, let's
consider the data dictionary in Table~\ref{tbl-ida-cat-data-dict}.

\begin{longtable}[t]{llll}

\caption{\label{tbl-ida-cat-data-dict}Data dictionary for the
\texttt{dative\_df} dataset.}

\tabularnewline

\toprule
variable & name & variable\_type & description\\
\midrule
realization\_of\_rcp & Realization of RCP & categorical & The realization of the recipient (NP/ PP)\\
modality & Modality & categorical & The modality of the utterance (spoken/ written)\\
length\_of\_rcp & Length of RCP & numeric & The length of the recipient (number of words)\\
length\_of\_thm & Length of THM & numeric & The length of the theme (number of words)\\
\bottomrule

\end{longtable}

We see that this dataset has four variables, two categorical and two
numeric. In our demonstrations we are going to use the
\texttt{realization\_of\_rcp} as the response variable, the variable
whose variation we are investigating.

For a bit more context, a dative is the phrase which reflects the entity
that takes the recipient role in a ditransitive clause. In English, the
recipient (dative) can be realized as either a prepositional phrase (PP)
as seen in (1) or as a noun phrase (NP) as seen in (2) below.

Example utterances:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  John gave the book {[}to Mary \textsubscript{PP}{]}.
\item
  John gave {[}Mary \textsubscript{NP}{]} the book.
\end{enumerate}

Together these two syntactic options are known as the Dative Alternation
Bresnan et al. (\citeproc{ref-Bresnan2007}{2007}).

From the large set of variables in the original dataset, I've selected
three explanatory variables to use in our demonstrations. These
variables are:

\begin{itemize}
\tightlist
\item
  \texttt{modality}: either written or spoken
\item
  \texttt{length\_of\_rcp}: the length of the recipient clause (in
  words)
\item
  \texttt{length\_of\_thm}: the length of the theme clause (in words)
\end{itemize}

In preparation for statistical analysis, we need to convert the
character variables to factors. Let's do that and preview the dataset,
as seen in Example~\ref{exm-ida-cat-setup-factors}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-setup-factors}{}\label{exm-ida-cat-setup-factors}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Convert character variables to factors}
\NormalTok{dative\_df }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.character), factor))}

\CommentTok{\# Preview}
\FunctionTok{glimpse}\NormalTok{(dative\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 3,263
> Columns: 4
> $ realization_of_rcp <fct> NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP,~
> $ modality           <fct> written, written, written, written, written, writte~
> $ length_of_rcp      <dbl> 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 4, ~
> $ length_of_thm      <dbl> 14, 3, 13, 5, 3, 4, 4, 1, 11, 2, 3, 3, 5, 2, 8, 2, ~
\end{verbatim}

\end{example}

We can see that the dataset includes 3263 observations. We will take a
closer look a the descriptive statistics for the variables as we prepare
for each analysis.

\subsubsection{Univariate analysis}\label{sec-ida-cat-univariate}

The univariate analysis is the simplest statistical design and analysis.
It includes only one variable. The goal is to describe the distribution
of the levels of the variable. The \texttt{realization\_of\_rcp}
variable has two levels: NP and PP. A potential research question is:

\begin{itemize}
\tightlist
\item
  Is there a difference in the proportion of NP and PP realizations of
  the recipient clause?
\end{itemize}

This question can be answered using a difference in proportion test. The
null hypothesis is that there is no difference in the proportion of NP
and PP realizations of the recipient clause. The alternative hypothesis
is that there is a difference in the proportion of NP and PP
realizations of the recipient clause.

Before we get into statistical analysis, it is always a good idea to
cross-tabulate or visualize the question, depending on the complexity of
the relationship. In Example~\ref{exm-ida-cat-univariate-tbl}, we see
the code the shows the distribution of the levels of the
\texttt{realization\_of\_rcp} variable in a cross-table.

\begin{example}[]\protect\hypertarget{exm-ida-cat-univariate-tbl}{}\label{exm-ida-cat-univariate-tbl}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Cross{-}tabulation of \textasciigrave{}realization\_of\_rcp\textasciigrave{}}
\NormalTok{dative\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tabyl}\NormalTok{(realization\_of\_rcp) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{(}\AttributeTok{digits =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kable}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kable\_styling}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[t]{lrl}

\caption{\label{tbl-ida-cat-univariate}Distribution of the levels of the
\texttt{realization\_of\_rcp} variable.}

\tabularnewline

\toprule
realization\_of\_rcp & n & percent\\
\midrule
NP & 2414 & 73.98\%\\
PP & 849 & 26.02\%\\
\bottomrule

\end{longtable}

\end{example}

From Table~\ref{tbl-ida-cat-univariate}, we see that the proportion of
NP realizations of the recipient clause is higher than the proportion of
PP realizations of the recipient clause. However, we cannot conclude
that there is a difference in the proportion of NP and PP realizations
of the recipient clause. We need to conduct a statistical test to
determine if the difference is statistically significant.

To determine if the distribution of the levels of the
\texttt{realization\_of\_rcp} variable is different from what we would
expect if the null hypothesis were true, we need to calculate the
difference observed in the sample and compare it to the differences
observed in many samples where the null hypothesis is true.

First, let's calculate the proportion of NP and PP realizations of the
recipient clause in the sample. We turn to the \texttt{specify()}
function from the \texttt{infer} package to specify the variable of
interest. In this case, we only have the response variable. Furthermore,
the argument \texttt{success} specifies the level of the response
variable that we will use as the `success'. The term `success' is used
because the \texttt{specify()} function was designed for binomial
variables where the levels are `success' and `failure', as seen in
Example~\ref{exm-ida-cat-specify}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-specify}{}\label{exm-ida-cat-specify}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Specify the variable of interest}
\NormalTok{dative\_spec }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{specify}\NormalTok{(}
    \AttributeTok{response =}\NormalTok{ realization\_of\_rcp,}
    \AttributeTok{success =} \StringTok{"NP"}
\NormalTok{  )}

\NormalTok{dative\_spec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Response: realization_of_rcp (factor)
> # A tibble: 3,263 x 1
>    realization_of_rcp
>    <fct>             
>  1 NP                
>  2 NP                
>  3 NP                
>  4 NP                
>  5 NP                
>  6 NP                
>  7 NP                
>  8 NP                
>  9 NP                
> 10 NP                
> # i 3,253 more rows
\end{verbatim}

\end{example}

The \texttt{dative\_spec} is a data frame with attributes which are used
by the \texttt{infer} package to maintain information about the
statistical design for the analysis. In this case, we only have
information about what the response variable is.

The next step to calculating the observed proportion is to calculate the
proportion statistic. The \texttt{calculate()} function from the
\texttt{infer} package is used to calculate the proportion statistic.
The \texttt{calculate()} function takes the \texttt{dative\_spec} data
frame as an argument. The \texttt{calculate()} function returns a data
frame with the proportion statistic in the \texttt{stat} column, as seen
in Example~\ref{exm-ida-cat-calculate}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-calculate}{}\label{exm-ida-cat-calculate}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the proportion statistic}
\NormalTok{dative\_obs }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"prop"}\NormalTok{)}

\NormalTok{dative\_obs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Response: realization_of_rcp (factor)
> # A tibble: 1 x 1
>    stat
>   <dbl>
> 1 0.740
\end{verbatim}

\end{example}

Note, that the observed statistic, proportion, is the same as the
proportion we calculated in Table~\ref{tbl-ida-cat-univariate}. In such
a simple example, the summary statistic and the observed statistic are
the same. But this simple example shows how choosing the `success' level
of the response variable is important. If we had chosen the `PP' level
as the `success' level, then the observed statistic would be the
proportion of PP realizations of the recipient clause. There is nothing
wrong with choosing the `PP' level as the `success' level, but it would
change the direction of the observed statistic.

Now that we have the observed statistic, our goal will be to determine
if the observed statistic is different from what we would expect if the
null hypothesis were true. To do this, we simulate many samples where
the null hypothesis is true. Simulation means that we will randomly
sample from the \texttt{dative\_df} data frame many times. Note that
sampling the dataset in this way is much more feasible with a computer
than it would be to do so by hand. This is one of the advantages of
using a computer for statistical analysis.

The only addition decision we need to make is how the sampling takes
place. Since \texttt{realization\_of\_rcp} is a variable with only two
levels, the null hypothesis that both levels are equally likely. In
other words, in a null hypothesis world, NP and PP we would expect the
proportions to roughly be 50/50.

To formulize this hypothesis with \texttt{infer} we use the
\texttt{hypothesize()} function and set the null hypothesis to ``point''
and the proportion to 0.5. Then we can \texttt{generate()} a number of
samples, say 1,000, drawn from our 50/50 world. Finally, the ``prop''
(proportion) statistic is \texttt{calculate()}d for each of the 1,000
samples and returned in a data frame, as seen in
Example~\ref{exm-ida-cat-null-hypothesis}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-null-hypothesis}{}\label{exm-ida-cat-null-hypothesis}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate the null hypothesis distribution}
\NormalTok{dative\_null }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"point"}\NormalTok{, }\AttributeTok{p =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"draw"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"prop"}\NormalTok{)}

\NormalTok{dative\_null}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Response: realization_of_rcp (factor)
> Null Hypothesis: point
> # A tibble: 1,000 x 2
>    replicate  stat
>        <int> <dbl>
>  1         1 0.496
>  2         2 0.499
>  3         3 0.509
>  4         4 0.496
>  5         5 0.492
>  6         6 0.499
>  7         7 0.514
>  8         8 0.499
>  9         9 0.504
> 10        10 0.501
> # i 990 more rows
\end{verbatim}

\end{example}

The result of Example~\ref{exm-ida-cat-null-hypothesis} is a data frame
with as many rows as there are samples. Each row contains the proportion
statistic for each sample drawn from the hypothesized distribution that
the proportion of NP realizations of the recipient clause is 0.5.

To appreciate the null hypothesis distribution, we can visualize it
using a histogram. The \texttt{infer} package provides a convenient
\texttt{visualize()} function for visualizing distributions, as seen in
Example~\ref{exm-ida-cat-null-hypothesis}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-null-hypothesis}{}\label{exm-ida-cat-null-hypothesis}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize the null hypothesis distribution}
\NormalTok{dative\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-cat-null-hypothesis-1.pdf}

}

\caption{\label{fig-ida-cat-null-hypothesis}Null hypothesis distribution
of the proportion of NP realizations of the recipient clause.}

\end{figure}%

\end{example}

On the x-axis is the proportion statistic of NP realizations of the
recipient clause that we would expect if the null hypothesis were true.
For the 1,000 samples, the proportion statistic ranges from 0.47, 0.52.
Importantly we can appreciate that the most of the proportion statistics
are around 0.5, which is what we would expect if the null hypothesis
were true. But there is variation, as we would also expect.

Why would we expect variation? Consider the following analogy. If we
were to flip a fair coin 10 times, we would expect to get 5 heads and 5
tails. But this doesn't always happen. Sometimes we get 6 heads and 4
tails. Sometimes we get 7 heads and 3 tails, and so on. As the number of
flips increases, however, we would expect the proportion of heads to be
closer to 0.5, but there would still be variation. The same is true for
the null hypothesis distribution. As the number of samples increases, we
would expect the proportion of NP realizations of the recipient clause
to be closer to 0.5, but there would still be variation. The question is
whether the observed statistic we obtained from our data, in
Example~\ref{exm-ida-cat-calculate}, is within some level of variation
that we would expect if the null hypothesis were true.

Let's visualize the observed statistic on the null hypothesis
distribution, as seen in Figure~\ref{fig-ida-cat-null-hypothesis-obs},
to gauge whether the observed statistic is within some level of
variation that we would expect if the null hypothesis were true.

\begin{example}[]\protect\hypertarget{exm-ida-cat-null-hypothesis-obs}{}\label{exm-ida-cat-null-hypothesis-obs}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize the null hypothesis distribution with the observed statistic}
\NormalTok{dative\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# note we are adding a visual layer \textasciigrave{}+\textasciigrave{}}
  \FunctionTok{geom\_vline}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ dative\_obs,      }\CommentTok{\# the data is the observed statistic}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ stat), }\CommentTok{\# the x{-}axis is the proportion statistic}
    \AttributeTok{color =} \StringTok{"red"}\NormalTok{,          }\CommentTok{\# the color of the line is red}
    \AttributeTok{linewidth =} \DecValTok{2}           \CommentTok{\# the size of the line is 2}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-cat-null-hypothesis-obs-1.pdf}

}

\caption{\label{fig-ida-cat-null-hypothesis-obs}Null hypothesis
distribution of the proportion of NP realizations of the recipient
clause with the observed statistic.}

\end{figure}%

\end{example}

Just from a visual inspection, we can see that the observed statistic
lies far away from the null distribution, far right of the right tail.
This suggests that the observed statistic is not within some level of
variation that we would expect if the null hypothesis were true.

But we need to quantify this. We need to calculate the probability of
observing the observed statistic or a more extreme statistic if the null
hypothesis were true. This is called the p-value. The p-value is
calculated by counting the number of samples in the null hypothesis
distribution that are more extreme than expected within some level of
uncertainty. 95\% is the most common level of uncertainty, which is
called the alpha level. The remain 5\% of the distribution is the space
where the likelihood that the null hypothesis accounts for the statistic
is below This means that if the p-value is less than 0.05, then we
reject the null hypothesis. If the p-value is greater than 0.05, then we
fail to reject the null hypothesis.

The \texttt{shade\_p\_value()} function from the \texttt{infer} package
will plot our observed statistic and shade the sample statistics that
fall within the alpha level, as seen in
Example~\ref{exm-ida-cat-null-hypothesis-obs-pval}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-null-hypothesis-obs-pval}{}\label{exm-ida-cat-null-hypothesis-obs-pval}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize the null hypothesis distribution with the observed statistic and p{-}value}

\NormalTok{dative\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_p\_value}\NormalTok{(}
\NormalTok{    dative\_obs, }\CommentTok{\# the observed statistic}
    \AttributeTok{direction =} \StringTok{"two{-}sided"}\NormalTok{, }\CommentTok{\# the direction of the alternative hypothesis}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-cat-null-hypothesis-obs-pval-1.pdf}

}

\caption{\label{fig-ida-cat-null-hypothesis-obs-pval}Null hypothesis
distribution of the proportion of NP realizations of the recipient
clause with the observed statistic and p-value.}

\end{figure}%

\end{example}

The two plots we've generated look identical, we cannot appreciate the
shading of the p-value range, as our statistic is so far away from the
null hypothesis distribution. But if we were to change the observed
statistic, to say \(0.51\), we would see the shading, as seen in
Example~\ref{exm-ida-cat-null-hypothesis-obs-pval-alpha}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-null-hypothesis-obs-pval-alpha}{}\label{exm-ida-cat-null-hypothesis-obs-pval-alpha}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize the null hypothesis distribution with the observed statistic and p{-}value}
\NormalTok{dative\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_p\_value}\NormalTok{(}
    \AttributeTok{obs\_stat =} \FloatTok{0.51}\NormalTok{, }\CommentTok{\# the observed statistic}
    \AttributeTok{direction =} \StringTok{"two{-}sided"}\NormalTok{, }\CommentTok{\# the direction of the alternative hypothesis}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-cat-null-hypothesis-obs-pval-alpha-1.pdf}

}

\caption{\label{fig-ida-cat-null-hypothesis-obs-pval-alpha}Null
hypothesis distribution of the proportion of NP realizations of the
recipient clause with the observed statistic and p-value with alpha
level of 0.1.}

\end{figure}%

\end{example}

Now we can appreciate the shading of the p-value range. We can now talk
about the ``two-sided'' direction we used. The direction of the
alternative hypothesis is important because it determines the p-value
range. The ``two-sided'' direction means that we are interested in the
proportion of NP realizations of the recipient clause being different
from 0.5. If we were only interested in the proportion of NP
realizations of the recipient clause being greater than 0.5, then we
would use the ``greater'' direction. If we were only interested in the
proportion of NP realizations of the recipient clause being less than
0.5, then we would use the ``less'' direction.

Let's visualize our hypothetical observed statistic of 0.51 on the null
hypothesis distribution were the alternative hypothesis is that the
proportion of NP realizations of the recipient clause is greater than
0.5, as seen in
Example~\ref{exm-ida-cat-null-hypothesis-obs-pval-greater}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-null-hypothesis-obs-pval-greater}{}\label{exm-ida-cat-null-hypothesis-obs-pval-greater}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize the null hypothesis distribution with the observed statistic and p{-}value}
\NormalTok{dative\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_p\_value}\NormalTok{(}
    \FloatTok{0.51}\NormalTok{, }\CommentTok{\# the observed statistic}
    \AttributeTok{direction =} \StringTok{"greater"}\NormalTok{, }\CommentTok{\# the direction of the alternative hypothesis}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-cat-null-hypothesis-obs-pval-greater-1.pdf}

}

\caption{\label{fig-ida-cat-null-hypothesis-obs-pval-greater}Null
hypothesis distribution of the proportion of NP realizations of the
recipient clause with the observed statistic and p-value with alpha
level of 0.1 and alternative hypothesis that the proportion of NP
realizations of the recipient clause is greater than 0.5.}

\end{figure}%

\end{example}

As you can now see, the p-value range is dependent on the null
hypothesis distribution and the direction of the alternative hypothesis
as well as the number of samples. The more samples we have, the more
precise the p-value range will likely be.

With \texttt{infer} we can calculate the p-value using the
\texttt{get\_p\_value()} function. Let's calculate the p-value for our
observed statistic and the hypothetical observed statistic of 0.51, as
seen in Example~\ref{exm-ida-cat-p-value}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-p-value}{}\label{exm-ida-cat-p-value}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the p{-}value (observed statistic)}
\NormalTok{dative\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{get\_p\_value}\NormalTok{(}
\NormalTok{    dative\_obs, }\CommentTok{\# the observed statistic}
    \AttributeTok{direction =} \StringTok{"two{-}sided"} \CommentTok{\# the direction of the alternative hypothesis}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 1
>   p_value
>     <dbl>
> 1       0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the p{-}value (hypothetical observed statistic)}
\NormalTok{dative\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{get\_p\_value}\NormalTok{(}
    \FloatTok{0.51}\NormalTok{, }\CommentTok{\# the observed statistic}
    \AttributeTok{direction =} \StringTok{"two{-}sided"} \CommentTok{\# the direction of the alternative hypothesis}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 1
>   p_value
>     <dbl>
> 1   0.214
\end{verbatim}

\end{example}

The p-value for our observed statistic is reported as \(0\), with a
warning that the p-value estimate is contingent on the number of samples
we generate in the null distribution. 1,000 is a reasonable number of
samples, so we likely have a statistically significant result at the
alpha level of 0.05.

The p-value for our hypothetical observed statistic of 0.51 is reported
as larger than \(0.05\), which provides evidence that our hypothetical
observed statistic is not sufficiently different from the variation
expected in the null distribution to reject the null hypothesis, i.e.~we
would not have a significant result at the alpha level of 0.05.

The p-value is one, traditionally very common, estimate of uncertainty.
Another estimate of uncertainty is the confidence interval. The
confidence interval is the range of values for our test statistic that
we would expect the true statistic value to fall within some level of
uncertainty. Again, 95\% is the most common level of uncertainty. The
upper and lower bounds of this range are called the confidence limits
for the test statistic. The confidence interval is calculated by
calculating the confidence limits for the test statistic for many
samples from the observed data. But instead of generating a null
hypothesis distribution, we generate a distribution based on resampling
from the observed data. This is called the bootstrap distribution. The
bootstrap distribution is generated by resampling from the observed
data, with replacement, many times. This simulates the process of
sampling from the population many times. Each time the test statistic is
generated for each sample. The confidence limits are the 2.5th and
97.5th percentiles of the bootstrap distribution. The confidence
interval is the range between the confidence limits.

In Example~\ref{exm-ida-cat-confidence-interval}, we see the code for
calculating the confidence interval for our observed statistic.

\begin{example}[]\protect\hypertarget{exm-ida-cat-confidence-interval}{}\label{exm-ida-cat-confidence-interval}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate boostrap distribution}
\NormalTok{dative\_boot }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"prop"}\NormalTok{)}

\NormalTok{dative\_ci }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_boot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{get\_confidence\_interval}\NormalTok{(}\AttributeTok{level =} \FloatTok{0.95}\NormalTok{) }\CommentTok{\# 95\% confidence interval}

\NormalTok{dative\_ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 2
>   lower_ci upper_ci
>      <dbl>    <dbl>
> 1    0.724    0.754
\end{verbatim}

\end{example}

Let's visualize the confidence interval for our bootstrapped samples, as
seen in Example~\ref{exm-ida-cat-confidence-interval-visualize}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-confidence-interval-visualize}{}\label{exm-ida-cat-confidence-interval-visualize}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize the bootstrap distribution with the confidence interval}
\NormalTok{dative\_boot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_confidence\_interval}\NormalTok{(}
\NormalTok{    dative\_ci }\CommentTok{\# the confidence interval}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-cat-confidence-interval-visualize-1.pdf}

}

\caption{\label{fig-ida-cat-confidence-interval-visualize}Bootstrap
distribution of the proportion of NP realizations of the recipient
clause with the confidence interval.}

\end{figure}%

\end{example}

The confidence level is the probability that the confidence interval
contains the true value. The confidence level is typically set to 0.95
in the social sciences. This means that if the confidence interval
contains the null hypothesis value, then we fail to reject the null
hypothesis. If the confidence interval does not contain the null
hypothesis value, then we reject the null hypothesis.

Our stat is 0.74 and the confidence interval is 0.724 to 0.754. The
confidence interval does not contain the null hypothesis value of 0.5,
which provides evidence that the proportion of NP realizations of the
recipient clause is different from 0.5.

Confidence intervals are often misinterpreted. Confidence intervals are
not the probability that the true value is within the range. The true
value is either within the range or not. The confidence interval is the
probability that the range contains the true value. This is a subtle but
important distinction.

Interpreted correctly confidence intervals can enhance our understanding
of the uncertainty of our test statistic and reduces the interpretation
of p-values (which are based on a relatively arbitrary alpha level) as a
binary decision, significant or not significant. Instead, confidence
intervals encourage us to think about the uncertainty of our test
statistic as a range of values that we would expect the true value to
fall within some level of uncertainty.

\subsubsection{Bivariate analysis}\label{sec-ida-cat-bivariate}

The univarite case is not very interesting or common in statistical
inference, but it is a good place to start to understand the process and
the logic of statistical inference. The bivariate case, on the other
hand, is much more common and interesting. The bivariate case includes
two variables. The goal is to describe the relationship between the two
variables.

In the \texttt{dative\_df} dataset, we can imagine asking the question:

\begin{itemize}
\tightlist
\item
  Is there a difference in the proportion of NP and PP realizations of
  the recipient clause by modality?
\end{itemize}

This question can be answered using a difference in proportions test, as
both variables are binomial (have two levels). The null hypothesis is
that there is no difference in the proportion of NP and PP realizations
of the recipient clause by modality. The alternative hypothesis is that
there is a difference in the proportion of NP and PP realizations of the
recipient clause by modality.

We can cross-tabulate or visualize, but let's visualize this
relationship in this case. In Example~\ref{exm-ida-cat-bivariate-vis},
we see the code the shows the distribution of the levels of the
\texttt{realization\_of\_rcp} variable by the levels of the
\texttt{modality} variable.

\begin{example}[]\protect\hypertarget{exm-ida-cat-bivariate-vis}{}\label{exm-ida-cat-bivariate-vis}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize the relationship between \textasciigrave{}realization\_of\_rcp\textasciigrave{} and \textasciigrave{}modality\textasciigrave{}}
\NormalTok{dative\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ realization\_of\_rcp, }\AttributeTok{fill =}\NormalTok{ modality)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"fill"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Realization of recipient clause"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Proportion"}\NormalTok{,}
    \AttributeTok{fill =} \StringTok{"Modality"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-cat-bivariate-1.pdf}

}

\caption{\label{fig-ida-cat-bivariate}Distribution of the levels of the
\texttt{realization\_of\_rcp} variable by the levels of the
\texttt{modality} variable.}

\end{figure}%

\end{example}

From our visualization, we can see that the proportion of NP
realizations of the recipient clause is higher in both modalities, as we
might expect from our univariate analysis. However, the proportion
appears to be different with the spoken modality having a higher
proportion of NP realizations of the recipient clause than the written
modality. But we cannot conclude that there is a difference in the
proportion of NP and PP realizations of the recipient clause by
modality. We need to conduct a statistical test to determine if the
difference is statistically significant.

\begin{example}[]\protect\hypertarget{exm-ida-cat-bivariate-tbl}{}\label{exm-ida-cat-bivariate-tbl}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dative\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tabyl}\NormalTok{(realization\_of\_rcp, modality) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# cross{-}tabulate}
  \FunctionTok{adorn\_totals}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"row"}\NormalTok{, }\StringTok{"col"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# provide row and column totals}
  \FunctionTok{adorn\_percentages}\NormalTok{(}\StringTok{"col"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# add percentages to the columns}
  \FunctionTok{adorn\_pct\_formatting}\NormalTok{(}\AttributeTok{rounding =} \StringTok{"half up"}\NormalTok{, }\AttributeTok{digits =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# round the digits}
  \FunctionTok{adorn\_ns}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# add observation number}
  \FunctionTok{adorn\_title}\NormalTok{(}\StringTok{"combined"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# add a header title}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{booktabs =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# pretty table)}
  \FunctionTok{kable\_styling}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[t]{llll}

\caption{\label{tbl-ida-cat-bivariate}Contingency table for
\texttt{realization\_of\_rcp} and \texttt{modality}.}

\tabularnewline

\toprule
realization\_of\_rcp/modality & spoken & written & Total\\
\midrule
NP & 79\% (1,859) & 61\% (555) & 74\% (2,414)\\
PP & 21\%   (501) & 39\% (348) & 26\%   (849)\\
Total & 100\% (2,360) & 100\% (903) & 100\% (3,263)\\
\bottomrule

\end{longtable}

\end{example}

To determine if the distribution of the levels of the
\texttt{realization\_of\_rcp} variable by the levels of the
\texttt{modality} variable is different from what we would expect if the
null hypothesis were true, we need to calculate the difference observed
in the sample and compare it to the differences observed in many samples
where the null hypothesis is true.

The \texttt{infer} package provides a pipeline which maintains a
consistent workflow for statistical inference. As such, the procedure is
very similar to the univarite analysis we performed, with some
adjustments. Let's focus on the adjustments. First, our
\texttt{specify()} call needs to include the relationship between two
variables: \texttt{realization\_of\_rcp} and \texttt{modality}. The
\texttt{response} argument is the response variable, which is
\texttt{realization\_of\_rcp}. The \texttt{explanatory} argument is the
explanatory variable, which is \texttt{modality}.

There are two approaches to specifying the relationship between the
response and explanatory variables. The first approach is to specify the
response variable and the explanatory variable separately as values of
the arguments \texttt{response} and \texttt{explanatory}. The second
approach is to specify the response variable and the explanatory
variable as a formula using the \texttt{\textasciitilde{}} operator. The
formula approach is more flexible and allows for more complex
relationships between the response and explanatory variables. In
Example~\ref{exm-ida-cat-specify-bivariate}, we see the code for the
\texttt{specify()} call using the formula approach.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{hand-point-up} Tip}

The formula syntax \texttt{y\ \textasciitilde{}\ x} can be read as `y'
as a function of `x'.

\end{tcolorbox}

\begin{example}[]\protect\hypertarget{exm-ida-cat-specify-bivariate}{}\label{exm-ida-cat-specify-bivariate}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Specify the relationship between the response and explanatory variables}
\NormalTok{dative\_spec }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{specify}\NormalTok{(}
\NormalTok{    realization\_of\_rcp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ modality,}
    \AttributeTok{success =} \StringTok{"NP"}
\NormalTok{  )}

\NormalTok{dative\_spec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Response: realization_of_rcp (factor)
> Explanatory: modality (factor)
> # A tibble: 3,263 x 2
>    realization_of_rcp modality
>    <fct>              <fct>   
>  1 NP                 written 
>  2 NP                 written 
>  3 NP                 written 
>  4 NP                 written 
>  5 NP                 written 
>  6 NP                 written 
>  7 NP                 written 
>  8 NP                 written 
>  9 NP                 written 
> 10 NP                 written 
> # i 3,253 more rows
\end{verbatim}

\end{example}

The \texttt{dative\_spec} now contains attributes about the response and
explanatory variables encoded into the data frame.

We now calculate the observed statistic with \texttt{calculate()}, as
seen in Example~\ref{exm-ida-cat-calculate-bivariate}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-calculate-bivariate}{}\label{exm-ida-cat-calculate-bivariate}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the observed statistic}
\NormalTok{dative\_obs }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in props"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"spoken"}\NormalTok{, }\StringTok{"written"}\NormalTok{))}

\NormalTok{dative\_obs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Response: realization_of_rcp (factor)
> Explanatory: modality (factor)
> # A tibble: 1 x 1
>    stat
>   <dbl>
> 1 0.173
\end{verbatim}

\end{example}

Two differences are that our statistic is now a difference in
proportions and that we are asked to specify the ordere of the levels of
\texttt{modality}. The statistic is clear, we are investigating whether
the proportion of NP realizations of the recipient clause is different
between the spoken and written modalities. The order of the levels of
\texttt{modality} is important because it determines the direction of
the alternative hypothesis, specifically how the statistic is calculated
(the order of the subtraction).

So our observed statistic 0.17 is the proportion of NP realizations of
the recipient clause in the spoken modality minus the proportion of NP
realizations of the recipient clause in the written modality, so the NP
realization appears 17\% more in the spoken modality compared to the
written modality.

The question remains, is this difference statistically significant? To
answer this question, we generate the null hypothesis distribution,
visualize the p-value range, and calculate the p-value, as seen in
Example~\ref{exm-ida-cat-null-hypothesis-bivariate}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-null-hypothesis-bivariate}{}\label{exm-ida-cat-null-hypothesis-bivariate}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate the null hypothesis distribution}
\NormalTok{dative\_null }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"permute"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in props"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"spoken"}\NormalTok{, }\StringTok{"written"}\NormalTok{))}

\CommentTok{\# Visualize the null hypothesis distribution with the observed statistic and p{-}value}
\NormalTok{dative\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_p\_value}\NormalTok{(}
\NormalTok{    dative\_obs, }\CommentTok{\# the observed statistic}
    \AttributeTok{direction =} \StringTok{"two{-}sided"}\NormalTok{, }\CommentTok{\# the direction of the alternative hypothesis}
\NormalTok{  )}

\CommentTok{\# Calculate the p{-}value}
\NormalTok{dative\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{get\_p\_value}\NormalTok{(}
\NormalTok{    dative\_obs, }\CommentTok{\# the observed statistic}
    \AttributeTok{direction =} \StringTok{"two{-}sided"} \CommentTok{\# the direction of the alternative hypothesis}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 1
>   p_value
>     <dbl>
> 1       0
\end{verbatim}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-cat-null-hypothesis-bivariate-1.pdf}

}

\caption{\label{fig-ida-cat-null-hypothesis-bivariate}Null hypothesis
distribution of the difference in proportions of NP realizations of the
recipient clause by modality with the observed statistic and p-value.}

\end{figure}%

\end{example}

Note when generating the null hypothesis distribution, we use the
\texttt{hypothesize()} function with the \texttt{null} argument set to
``independence''. This is because we are interested in the relationship
between the response and explanatory variables. The null hypothesis is
that there is no relationship between the response and explanatory
variables. When generating the samples, we use the permutation approach,
which randomly shuffles the response variable values for each sample.
This simulates the null hypothesis that there is no relationship between
the response and explanatory variables.

When we plot the null distribution, we see that the statistic values
cluster around \(0\), which is what we expect if there is no difference.
We also see that our observed statistic is far away from the null
distribution, which suggests that there is a difference in the
proportion of NP realizations of the recipient clause by modality.

The p-value underscores this apparent difference. The p-value is
reported as \(0\). To provide some context, we will generate a
confidence interval for our observed statistic using the bootstrap
method, as seen in
Example~\ref{exm-ida-cat-confidence-interval-bivariate}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-confidence-interval-bivariate}{}\label{exm-ida-cat-confidence-interval-bivariate}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate boostrap distribution}
\NormalTok{dative\_boot }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in props"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"spoken"}\NormalTok{, }\StringTok{"written"}\NormalTok{))}

\CommentTok{\# Calculate the confidence interval}
\NormalTok{dative\_ci }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_boot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{get\_confidence\_interval}\NormalTok{(}\AttributeTok{level =} \FloatTok{0.95}\NormalTok{)}

\CommentTok{\# Visualize the bootstrap distribution with the confidence interval}
\NormalTok{dative\_boot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_confidence\_interval}\NormalTok{(}
\NormalTok{    dative\_ci }\CommentTok{\# the confidence interval}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-cat-confidence-interval-bivariate-1.pdf}

}

\caption{\label{fig-ida-cat-confidence-interval-bivariate}Bootstrap
distribution of the difference in proportions of NP realizations of the
recipient clause by modality with the confidence interval.}

\end{figure}%

\end{example}

The confidence interval does not contain the null hypothesis value of 0,
which provides evidence that the proportion of NP realizations of the
recipient clause is different between the spoken and written modalities.

\subsubsection{Multivariate Analysis}\label{multivariate-analysis}

In many scenarios, it is common to have multiple explanatory variables
that need to be considered. In such cases, logistic regression is a
suitable modeling technique. Logistic regression allows for the
inclusion of both categorical and continuous explanatory variables. The
primary objective of using logistic regression is to assess the
association between these variables and the dependent variable. By
analyzing this relationship, we can determine how changes in the
explanatory variables influence the probability of the outcome
occurring.

In our \texttt{dative\_df} dataset, we can imagine asking the question:

\begin{itemize}
\tightlist
\item
  Is there a difference in the proportion of NP and PP realizations of
  the recipient clause by modality and length ratio?
\end{itemize}

The length ratio gets at the length of the recipient clause relative to
the length of the theme clause. This ratio is an operationalization of a
phenomenon known as `Heavy NP' shift. There are many ways to
operationalize this phenomenon, but the length ratio is a simple method
to approximate the phenomenon. It attempts to capture the idea that the
longer the theme clause is relative to the recipient clause, the more
likely the recipient clause will be realized as an NP --in other words,
when the theme is relatively longer than the recipient, the theme is
ordered last in the sentence, and the recipient is ordered first in the
sentence and takes the form of an NP (instead of a PP).

For example,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  John gave the book {[}to Mary{]}. (PP)
\item
  John gave {[}Mary{]} the large book that I showed you in class
  yesterday. (NP)
\end{enumerate}

The prediction, then, is that the example in (3) would be less likely
than (2) because the theme is relatively longer than the recipient.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  John gave the book that I showed you in class yesterday {[}to Mary{]}.
  (PP)
\end{enumerate}

Let's consider this variable \texttt{length\_ratio} and
\texttt{modality} together as explanatory variables for the realizations
of the recipient clause \texttt{realization\_of\_rcp}.

Let's create the \texttt{length\_ratio} variable by dividing the
\texttt{length\_of\_thm} by the \texttt{length\_of\_rcp}. This will give
us values larger than 1 when the theme is longer than the recipient. In
Example~\ref{exm-ida-cat-create-length-ratio}, we see the code for
creating the \texttt{length\_ratio} variable.

\begin{example}[]\protect\hypertarget{exm-ida-cat-create-length-ratio}{}\label{exm-ida-cat-create-length-ratio}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create the \textasciigrave{}length\_ratio\textasciigrave{} variable}
\NormalTok{dative\_df }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{length\_ratio =}\NormalTok{ length\_of\_thm }\SpecialCharTok{/}\NormalTok{ length\_of\_rcp}
\NormalTok{  )}

\CommentTok{\# Preview}
\NormalTok{dative\_df }\SpecialCharTok{|\textgreater{}} \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 3,263
> Columns: 5
> $ realization_of_rcp <fct> NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP,~
> $ modality           <fct> written, written, written, written, written, writte~
> $ length_of_rcp      <dbl> 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 4, ~
> $ length_of_thm      <dbl> 14, 3, 13, 5, 3, 4, 4, 1, 11, 2, 3, 3, 5, 2, 8, 2, ~
> $ length_ratio       <dbl> 14.00, 1.50, 13.00, 5.00, 1.50, 2.00, 2.00, 1.00, 1~
\end{verbatim}

\end{example}

Let's visualize the relationship between \texttt{realization\_of\_rcp}
and \texttt{length\_ratio} separately and then together with
\texttt{modality}, as seen in
Example~\ref{exm-ida-cat-bivariate-vis-length-ratio}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-bivariate-vis-length-ratio}{}\label{exm-ida-cat-bivariate-vis-length-ratio}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize the relationship between \textasciigrave{}realization\_of\_rcp\textasciigrave{} and \textasciigrave{}length\_ratio\textasciigrave{}}
\NormalTok{p1 }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ realization\_of\_rcp, }\AttributeTok{y =}\NormalTok{ length\_ratio)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Realization of recipient clause"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Length ratio"}
\NormalTok{  )}

\NormalTok{p1}

\CommentTok{\# Visualize the relationship between \textasciigrave{}realization\_of\_rcp\textasciigrave{} and \textasciigrave{}length\_ratio\textasciigrave{} by \textasciigrave{}modality\textasciigrave{}}
\NormalTok{p1 }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ modality)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-cat-bivariate-length-ratio-1.pdf}

}

\subcaption{\label{fig-ida-cat-bivariate-length-ratio-1}Length ratio by
realization of recipient clause}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-cat-bivariate-length-ratio-2.pdf}

}

\subcaption{\label{fig-ida-cat-bivariate-length-ratio-2}Length ratio by
realization of recipient clause by modality}

\end{minipage}%

\caption{\label{fig-ida-cat-bivariate-length-ratio}Distribution of the
levels of the \texttt{realization\_of\_rcp} variable by the levels of
the \texttt{modality} variable and \texttt{length\_ratio} variable.}

\end{figure}%

\end{example}

Before jumping in here, the outliers should catch our attention. Each
boxplot has a number of outliers. These outliers are likely due to the
fact that the \texttt{length\_ratio} variable is built upon the
\texttt{length\_of\_thm} and \texttt{length\_of\_rcp} variables, which
are both highly skewed. This is a common problem in statistical
inference. The solution is to transform the variables to reduce the
skewness. In this case, we can use the log transformation, as seen in
Example~\ref{exm-ida-cat-log-transform}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-log-transform}{}\label{exm-ida-cat-log-transform}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Log transform the \textasciigrave{}length\_ratio\textasciigrave{} variable}
\NormalTok{dative\_df }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{length\_ratio\_log =} \FunctionTok{log10}\NormalTok{(length\_ratio)}
\NormalTok{  )}

\NormalTok{dative\_df }\SpecialCharTok{|\textgreater{}} \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 3,263
> Columns: 6
> $ realization_of_rcp <fct> NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP,~
> $ modality           <fct> written, written, written, written, written, writte~
> $ length_of_rcp      <dbl> 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 4, ~
> $ length_of_thm      <dbl> 14, 3, 13, 5, 3, 4, 4, 1, 11, 2, 3, 3, 5, 2, 8, 2, ~
> $ length_ratio       <dbl> 14.00, 1.50, 13.00, 5.00, 1.50, 2.00, 2.00, 1.00, 1~
> $ length_ratio_log   <dbl> 1.146, 0.176, 1.114, 0.699, 0.176, 0.301, 0.301, 0.~
\end{verbatim}

\end{example}

Now we can plot the relationships in
Figure~\ref{fig-ida-cat-bivariate-length-ratio} with the log-transformed
\texttt{length\_ratio}, seen in
Example~\ref{exm-ida-cat-bivariate-vis-length-ratio-log}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-bivariate-vis-length-ratio-log}{}\label{exm-ida-cat-bivariate-vis-length-ratio-log}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize the relationship between \textasciigrave{}realization\_of\_rcp\textasciigrave{} and \textasciigrave{}length\_ratio\textasciigrave{}}
\NormalTok{p1 }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ realization\_of\_rcp, }\AttributeTok{y =}\NormalTok{ length\_ratio\_log)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Realization of recipient clause"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Length ratio (log)"}
\NormalTok{  )}

\NormalTok{p1}

\CommentTok{\# Visualize the relationship between \textasciigrave{}realization\_of\_rcp\textasciigrave{} and \textasciigrave{}length\_ratio\textasciigrave{} by \textasciigrave{}modality\textasciigrave{}}
\NormalTok{p1 }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ modality)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-cat-bivariate-length-ratio-log-1.pdf}

}

\subcaption{\label{fig-ida-cat-bivariate-length-ratio-log-1}Length ratio
by realization of recipient clause}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-cat-bivariate-length-ratio-log-2.pdf}

}

\subcaption{\label{fig-ida-cat-bivariate-length-ratio-log-2}Length ratio
by realization of recipient clause by modality}

\end{minipage}%

\caption{\label{fig-ida-cat-bivariate-length-ratio-log}Distribution of
the levels of the \texttt{realization\_of\_rcp} variable by the levels
of the \texttt{modality} variable and \texttt{length\_ratio} variable
with log transformation.}

\end{figure}%

\end{example}

The log transformation has reduced the skewness of the
\texttt{length\_ratio} variable. Note that the log transformation does
not change the rank order of the values, but it does change the scale of
the values (the distance between the values) by reducing the magnitude
of the values. The boxplots in
Figure~\ref{fig-ida-cat-bivariate-length-ratio-log} still have outliers,
but there are fewer outliers and they are not as extreme.

Now we can return to our question:

\begin{itemize}
\tightlist
\item
  Is there a difference in the proportion of NP and PP realizations of
  the recipient clause by modality and length ratio?
\end{itemize}

We can answer this question using a logistic regression. The null
hypothesis is that there is no difference in the proportion of NP and PP
realizations of the recipient clause by modality and length ratio. The
alternative hypothesis is that there is a difference in the proportion
of NP and PP realizations of the recipient clause by modality and length
ratio.

Let's calculate the statistics (not statistic) for our logistic
regression by specifying the relationship between the response and
explanatory variables and then using \texttt{fit()} to fit the logistic
regression model, as seen in
Example~\ref{exm-ida-cat-logistic-regression}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-logistic-regression}{}\label{exm-ida-cat-logistic-regression}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Specify the relationship between the response and explanatory variables}
\NormalTok{dative\_spec }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{specify}\NormalTok{(}
\NormalTok{    realization\_of\_rcp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ modality }\SpecialCharTok{+}\NormalTok{ length\_ratio\_log}
\NormalTok{  )}

\CommentTok{\# Fit the logistic regression model}
\NormalTok{dative\_fit }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit}\NormalTok{()}

\NormalTok{dative\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 3 x 2
>   term             estimate
>   <chr>               <dbl>
> 1 intercept          -0.563
> 2 modalitywritten     1.01 
> 3 length_ratio_log   -3.76
\end{verbatim}

\end{example}

Note I pointed out statistics, not statistic. In logistic regression
models, there the number of statistic reported depends on the number of
explanatory variables. If there are two variables there will be at least
three terms, one for each variable and the intercept term. If one or
more variables are categorical, however, there will be additional terms
when the categorical variable has three or more levels.

In our case, the \texttt{modality} variable has two levels, so there are
three terms. The first term is the intercept term, which is the log odds
of the proportion of NP realizations of the recipient clause in the
written modality when the \texttt{length\_ratio} is 1. The second term
is the log odds of the proportion of NP realizations of the recipient
clause in the spoken modality when the \texttt{length\_ratio} is 1. The
third term is the log odds of the proportion of NP realizations of the
recipient clause when the \texttt{length\_ratio} is 1 in the written
modality. Notable the spoken modality does not explicitly appear but is
implicitly represented the \texttt{modalitywritten} term statistic. It
is used as the reference level for the \texttt{modality} variable.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{hand-point-up} Tip}

The reference level in R is assumed to be the first level
alphabetically, unless otherwise specified. We can override this default
by using the \texttt{fct\_relevel()} function from the \texttt{forcats}
package. The reason we would want to do this is to make the reference
level more interpretable. In our case, we would want to make the spoken
modality the reference level it allows us to estimate the difference of
the proportion of NP realizations of the recipient as a positive value.
Remember that in Figure~\ref{fig-ida-cat-bivariate-length-ratio-log-2},
the proportion of NP realizations of the recipient clause is higher in
the written modality than in the spoken modality. If we were to use the
written modality as the reference level, the difference would be
negative. Not that we couldn't interpret this, but working with positive
integers is easier to interpret.

\end{tcolorbox}

The statistics returned in logistic regression are log odds. Odds are
the ratio of the probability of an event occurring to the probability of
an event not occurring. Log odds are the natural log of the odds. If we
would like to return the statistic as the odds ratio, we can use the
\texttt{exp()} function, as seen in
Example~\ref{exm-ida-cat-logistic-regression-odds-ratio}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-logistic-regression-odds-ratio}{}\label{exm-ida-cat-logistic-regression-odds-ratio}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the odds ratio by exponentiating the log odds}
\NormalTok{dative\_fit }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_fit }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{odds\_ratio =} \FunctionTok{exp}\NormalTok{(estimate)}
\NormalTok{  )}

\NormalTok{dative\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 3 x 3
>   term             estimate odds_ratio
>   <chr>               <dbl>      <dbl>
> 1 intercept          -0.563     0.569 
> 2 modalitywritten     1.01      2.75  
> 3 length_ratio_log   -3.76      0.0233
\end{verbatim}

\end{example}

\begin{itemize}
\tightlist
\item[$\square$]
  TODO: Interpret the odds ratio
\end{itemize}

The log odds for \texttt{writtenmodality} is 1.01. This means that the
odds of the proportion of NP realizations of the recipient clause in the
written modality when the \texttt{length\_ratio} is 1 is 2.75 times the
odds of the proportion of NP realizations of the recipient clause in the
spoken modality when the \texttt{length\_ratio} is 1.

On the other hand, the log odds for \texttt{length\_ratio} is -3.76. In
this case the \texttt{length\_ratio} log odds can be interpreted as the
log odds of the proportion of NP realizations of the recipient clause as
a function of the \texttt{length\_ratio} when the \texttt{modality} is
the spoken modality. This is because the \texttt{modality} variable is
the reference level.

? \texttt{length\_ratio\_log} has was log-transformed with a base of 10.
This means that the odds ratio can be exponentiated with a base of 10,
to return the odds ratio as a function of the \texttt{length\_ratio}
values, as seen in
Example~\ref{exm-ida-cat-logistic-regression-odds-ratio-10}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-logistic-regression-odds-ratio-10}{}\label{exm-ida-cat-logistic-regression-odds-ratio-10}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the odds ratio by exponentiating the log odds with a base of 10}
\NormalTok{dative\_fit }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_fit }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{odds\_ratio =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      term }\SpecialCharTok{==} \StringTok{"length\_ratio\_log"} \SpecialCharTok{\textasciitilde{}} \DecValTok{10}\SpecialCharTok{\^{}}\NormalTok{odds\_ratio,}
      \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \FunctionTok{exp}\NormalTok{(estimate)}
\NormalTok{    )}
\NormalTok{  )}

\NormalTok{dative\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 3 x 3
>   term             estimate odds_ratio
>   <chr>               <dbl>      <dbl>
> 1 intercept          -0.563      0.569
> 2 modalitywritten     1.01       2.75 
> 3 length_ratio_log   -3.76       1.06
\end{verbatim}

\end{example}

So the odds ratio for \texttt{length\_ratio} is 1.055 when the
\texttt{modality} is the written modality.

So our logistic regression model as specified considers each explanatory
variable independently, controlling for the other explanatory variable.
This is an additive model, which is what we stated in our formula
\texttt{y\ \textasciitilde{}\ x1\ +\ x2}. We can also specify an
interaction between the explanatory variables, as seen in
Example~\ref{exm-ida-cat-logistic-regression-interaction}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-logistic-regression-interaction}{}\label{exm-ida-cat-logistic-regression-interaction}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Specify the relationship between the response and explanatory variables}
\NormalTok{dative\_inter\_spec }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{specify}\NormalTok{(}
\NormalTok{    realization\_of\_rcp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ modality }\SpecialCharTok{*}\NormalTok{ length\_ratio\_log}
\NormalTok{  )}

\NormalTok{dative\_inter\_spec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Response: realization_of_rcp (factor)
> Explanatory: modality (factor), length_ratio_log (numeric)
> # A tibble: 3,263 x 3
>    realization_of_rcp modality length_ratio_log
>    <fct>              <fct>               <dbl>
>  1 NP                 written             1.15 
>  2 NP                 written             0.176
>  3 NP                 written             1.11 
>  4 NP                 written             0.699
>  5 NP                 written             0.176
>  6 NP                 written             0.301
>  7 NP                 written             0.301
>  8 NP                 written             0    
>  9 NP                 written             1.04 
> 10 NP                 written             0.301
> # i 3,253 more rows
\end{verbatim}

\end{example}

Replacing the \texttt{+} with a \texttt{*} tells the model to consider
the interaction between the explanatory variables. The interaction is
the effect of one explanatory variable on the response variable is
dependent on the other explanatory variable. In our case, the
interaction is the effect of the \texttt{length\_ratio} on the
proportion of NP realizations of the recipient clause is dependent on
the \texttt{modality}.

An model with an interaction changes the terms and the estimates. In
Example~\ref{exm-ida-cat-logistic-regression-interaction-terms}, we see
the terms for the logistic regression model with an interaction.

\begin{example}[]\protect\hypertarget{exm-ida-cat-logistic-regression-interaction-terms}{}\label{exm-ida-cat-logistic-regression-interaction-terms}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit the logistic regression model}
\NormalTok{dative\_inter\_fit }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_inter\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit}\NormalTok{()}

\NormalTok{dative\_inter\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 4 x 2
>   term                             estimate
>   <chr>                               <dbl>
> 1 intercept                          -0.549
> 2 modalitywritten                     0.958
> 3 length_ratio_log                   -3.88 
> 4 modalitywritten:length_ratio_log    0.317
\end{verbatim}

\end{example}

The additional term \texttt{modalitywritten:length\_ratio\_log} is the
interaction term. We also see the log odds estimates have changed for
the previous terms. This is because this interaction draws some of the
explanatory power from the other terms. Whether or not we run an
interaction model depends on our research question. Whether or not the
interaction term, or the other main terms, adds evidence to reject the
null hypothesis, is the next step to consider.

Let's return to our additive model and generate the null hypothesis
distribution, visualize the p-value range, and calculate the p-value for
each of the terms, as seen in
Example~\ref{exm-ida-cat-null-hypothesis-logistic-regression}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-null-hypothesis-logistic-regression}{}\label{exm-ida-cat-null-hypothesis-logistic-regression}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate the null hypothesis distribution}
\NormalTok{dative\_null }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"permute"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit}\NormalTok{()}

\CommentTok{\# Visualize the null hypothesis distribution with the observed statistics}
\NormalTok{dative\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_p\_value}\NormalTok{(}
\NormalTok{    dative\_fit, }\CommentTok{\# the observed statistics}
    \AttributeTok{direction =} \StringTok{"two{-}sided"} \CommentTok{\# the direction of the alternative hypothesis}
\NormalTok{  )}

\CommentTok{\# Calculate the p{-}value}
\NormalTok{dative\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{get\_p\_value}\NormalTok{(}
\NormalTok{    dative\_fit, }\CommentTok{\# the observed statistics}
    \AttributeTok{direction =} \StringTok{"two{-}sided"} \CommentTok{\# the direction of the alternative hypothesis}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 3 x 2
>   term             p_value
>   <chr>              <dbl>
> 1 intercept              0
> 2 length_ratio_log       0
> 3 modalitywritten        0
\end{verbatim}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-cat-null-hypothesis-logistic-regression-1.pdf}

}

\caption{\label{fig-ida-cat-null-hypothesis-logistic-regression}Null
hypothesis distribution of the proportion of NP realizations of the
recipient clause by modality and length ratio with the observed
statistic and p-value.}

\end{figure}%

\end{example}

It appears that our main effects, \texttt{modality} and
\texttt{length\_ratio}, are statistically significant. Let's generate
the confidence intervals for each of the terms, as seen in
Example~\ref{exm-ida-cat-confidence-interval-logistic-regression}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-confidence-interval-logistic-regression}{}\label{exm-ida-cat-confidence-interval-logistic-regression}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate boostrap distribution}
\NormalTok{dative\_boot }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit}\NormalTok{()}

\CommentTok{\# Calculate the confidence interval}
\NormalTok{dative\_ci }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_boot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{get\_confidence\_interval}\NormalTok{(}
    \AttributeTok{point\_estimate =}\NormalTok{ dative\_fit, }\CommentTok{\# the observed statistics}
    \AttributeTok{level =} \FloatTok{0.95}
\NormalTok{  )}

\CommentTok{\# Visualize the bootstrap distribution with the confidence interval}
\NormalTok{dative\_boot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_confidence\_interval}\NormalTok{(}
\NormalTok{    dative\_ci }\CommentTok{\# the confidence interval}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-cat-confidence-interval-logistic-regression-1.pdf}

}

\caption{\label{fig-ida-cat-confidence-interval-logistic-regression}Bootstrap
distribution of the proportion of NP realizations of the recipient
clause by modality and length ratio with the confidence interval.}

\end{figure}%

\end{example}

The confidence intervals for the main effects, \texttt{modality} and
\texttt{length\_ratio}, do not contain the null hypothesis value of 0,
which provides evidence that each of the explanatory variables is
related to the proportion of NP realizations of the recipient clause.

We can quantify the effect size of each of the explanatory variables
using the odds ratio to calculate the \(r\) and \(R^2\) values. We can
use the \texttt{logoddsratio\_to\_r()} function from \texttt{effectsize}
package to calculate the \(r\) value and then square the \(r\) value to
calculate the \(R^2\) value, as seen in
Example~\ref{exm-ida-cat-effect-size-logistic-regression}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-effect-size-logistic-regression}{}\label{exm-ida-cat-effect-size-logistic-regression}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the effect size}
\NormalTok{dative\_fit }\OtherTok{\textless{}{-}}
\NormalTok{  dative\_fit }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{r =} \FunctionTok{logoddsratio\_to\_r}\NormalTok{(estimate),}
    \AttributeTok{r\_squared =}\NormalTok{ r}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{  )}

\NormalTok{dative\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 3 x 5
>   term             estimate odds_ratio      r r_squared
>   <chr>               <dbl>      <dbl>  <dbl>     <dbl>
> 1 intercept          -0.563      0.569 -0.153    0.0235
> 2 modalitywritten     1.01       2.75   0.269    0.0722
> 3 length_ratio_log   -3.76       1.06  -0.720    0.518
\end{verbatim}

\end{example}

Beyond the estimates to quantify the likelihood that the null hypothesis
is true, the effect size provides a measure of the strength of the
relationship between the response and explanatory variables. The \(r\)
value is the correlation coefficient, which is a measure of the strength
of the linear relationship between the response and explanatory
variables. The \(R^2\) value is the coefficient of determination, which
is a measure of the proportion of the variance in the response variable
that is explained by the explanatory variables.

As you can imagine, the larger the \(R^2\) value, the stronger the
relationship between the response and explanatory variables. From our
effect size evaluation, the \texttt{length\_ratio} variable has a
stronger relationship with the proportion of NP realizations of the
recipient clause than the \texttt{modality} variable.

Now, knowing ``how'' strong a given value is can be a little less
intuitive. The \texttt{effectsize} package provides some guidelines for
interpreting effect size measures. Let's use the
\texttt{interpret\_r2()} function to interpret the \(R^2\) value, as
seen in Example~\ref{exm-ida-cat-interpret-r2}.

\begin{example}[]\protect\hypertarget{exm-ida-cat-interpret-r2}{}\label{exm-ida-cat-interpret-r2}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Interpret the R\^{}2 value}
\NormalTok{dative\_fit }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{r2\_interpretation =} \FunctionTok{interpret\_r2}\NormalTok{(r\_squared)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 3 x 6
>   term             estimate odds_ratio      r r_squared r2_interpretation
>   <chr>               <dbl>      <dbl>  <dbl>     <dbl> <effctsz_>       
> 1 intercept          -0.563      0.569 -0.153    0.0235 weak             
> 2 modalitywritten     1.01       2.75   0.269    0.0722 weak             
> 3 length_ratio_log   -3.76       1.06  -0.720    0.518  substantial
\end{verbatim}

\end{example}

While the effects for \texttt{modality} and \texttt{length\_ratio} are
statistically significant, the effect size for \texttt{modality} is
small and the effect size for \texttt{length\_ratio} is much stronger.
This can help inform our interpretation of the results.

\subsection{Numeric}\label{sec-ida-numeric}

We now turn our attention to the analysis scenarios where the response
variable is numeric. Just as for categorical variables, we can have
univariate, bivariate, and multivariate analysis scenarios. The
statistical tests for numeric variables are summarized in
Table~\ref{tbl-ida-num-design}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1370}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2740}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3425}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}@{}}
\caption{Statistical test design for numeric
variables}\label{tbl-ida-num-design}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Scenario
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Dependent variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Independent variable(s)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Statistical test
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Scenario
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Dependent variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Independent variable(s)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Statistical test
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Univariate & Numeric & None & Mean \\
Bivariate & Numeric & Numeric & Correlation \\
Bivariate & Numeric & Categorical & Difference in means \\
Multivariate & Numeric & Numeric or Categorical (2+) & Linear
regression \\
\end{longtable}

The dataset we will use here is \texttt{fillers\_df}. The data
dictionary is found in Table~\ref{tbl-ida-num-data-dict}.

\begin{longtable}[t]{llll}

\caption{\label{tbl-ida-num-data-dict}Data dictionary for the
\texttt{fillers\_df} dataset}

\tabularnewline

\toprule
variable & name & variable\_type & description\\
\midrule
speaker\_id & Speaker ID & numeric & Unique identifier for each speaker\\
age & Age & numeric & Age of the speaker in years\\
sex & Sex & categorical & Gender of the speaker\\
education & Education & ordinal & Level of education attained by the speaker\\
fillers\_per\_100 & Fillers per 100 & numeric & Number of filler words used per 100 utterances\\
\addlinespace
total\_fillers & Total Fillers & numeric & Total number of filler words used\\
total\_utterances & Total Utterances & numeric & Total number of utterances made by the speaker\\
\bottomrule

\end{longtable}

We see the dataset has {[}FOUR FIXME{]} variables. The
\texttt{fillers\_per\_100} will be used as our reponse variable and
corresponds to the rate of filler usage per speaker, normalized by the
number of utterances. The other variables we will consider as
explanatory variables are \texttt{age}, \texttt{sex}, and
\texttt{education}, providing us a mix of numeric and categorical
variables.

The context for these analysis demonstrations comes from the
socio-linguistic literature on the use of filled pauses. Filled pauses
have often been associated with a type of disfluency; speech errors that
occur during speech production. However, some authors have argued that
filled pauses can act as sociolinguistic markers of socio-demographic
characterstics of speakers, such as gender, age, and educational level
(\citeproc{ref-Shriberg1994}{Shriberg 1994};
\citeproc{ref-Tottie2011}{Tottie 2011}).

Let's prepare these datasets for analysis by converting the categorical
variables to factors and ordering the \texttt{education} variable, as
seen in Example~\ref{exm-ida-num-factors}.

\begin{example}[]\protect\hypertarget{exm-ida-num-factors}{}\label{exm-ida-num-factors}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Establish level names and order for the \textasciigrave{}education\textasciigrave{} variable}
\NormalTok{edu\_levels }\OtherTok{\textless{}{-}}
  \FunctionTok{c}\NormalTok{(}\StringTok{"More Than College"}\NormalTok{,}
    \StringTok{"College"}\NormalTok{,}
    \StringTok{"Less Than College"}\NormalTok{,}
    \StringTok{"Less Than High School"}\NormalTok{,}
    \StringTok{"Unknown"}\NormalTok{)}

\CommentTok{\# Convert categorical variables to factors}
\NormalTok{fillers\_df }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{sex =} \FunctionTok{factor}\NormalTok{(sex),}
    \AttributeTok{education =} \FunctionTok{factor}\NormalTok{(education, }\AttributeTok{levels =}\NormalTok{ edu\_levels, }\AttributeTok{ordered =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Preview}
\FunctionTok{glimpse}\NormalTok{(fillers\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 441
> Columns: 7
> $ speaker_id       <dbl> 1000, 1001, 1002, 1004, 1005, 1007, 1008, 1010, 1011,~
> $ age              <dbl> 38, 52, 29, 34, 36, 27, 53, 60, 28, 35, 45, 25, 47, 3~
> $ sex              <fct> Female, Male, Female, Female, Female, Female, Female,~
> $ education        <ord> Less Than College, More Than College, College, Colleg~
> $ fillers_per_100  <dbl> 2.14, 25.34, 4.13, 2.41, 3.79, 0.00, 8.33, 1.82, 5.22~
> $ total_fillers    <dbl> 44, 263, 54, 45, 129, 0, 27, 2, 54, 130, 135, 14, 152~
> $ total_utterances <dbl> 2052, 1038, 1308, 1866, 3400, 278, 324, 110, 1034, 20~
\end{verbatim}

\end{example}

Our \texttt{fillers\_df} dataset has 441 observations. Again, we will
postpone more descriptive statistics for treatment in the particular
scenarios.

\subsubsection{Univariate analysis}\label{univariate-analysis}

In hypothesis testing, the analysis of a single variable is directed at
determining whether or not the distribution or statistic of the variable
differs from some expected distribution or statistic. In the case of a
single categorical variable with two levels (as in the datives case), we
sampled from a binomial distribution by chance. In the case of a single
numeric variable, we can sample and compare the observed distribution to
a theoretical distribution. When approaching hypothesis testing from a
theoretical perspective, it is often necessary to assess how well a
numeric variable fits the normal distribution as many statistical tests
assume that the data are normally distributed. However, we have adopted
the simulation-based approach to hypothesis testing, which does not
require that the data fit the normal distribution, or any other
distribution for that matter.

The other approach to analyzing a single numeric variable is to compare
an observed statistic to an expected statistic. This approach requires a
priori knowledge of the expected statistic. For example, imagine we are
interested testing the hypothesis that the length of words in a medical
corpus tend to be longer than the average length of words in English. We
would then calculate the observed mean for the length of words in the
medical corpus and then generate a null distribution of means for the
length of words in English, as in
Example~\ref{exm-ida-num-uni-null-mean}.

\begin{example}[]\protect\hypertarget{exm-ida-num-uni-null-mean}{}\label{exm-ida-num-uni-null-mean}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Observed mean}
\NormalTok{obs\_mean }\OtherTok{\textless{}{-}}
\NormalTok{  medical\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{response =}\NormalTok{ word\_length) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"mean"}\NormalTok{)}

\CommentTok{\# Null distribution of means}
\NormalTok{null\_mean }\OtherTok{\textless{}{-}}
\NormalTok{  medical\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{response =}\NormalTok{ word\_length) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"point"}\NormalTok{, }\AttributeTok{mu =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"draw"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"mean"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

Note that instead of a \texttt{p\ =} argument, as was used in the
\texttt{hypothesize()} step to generate a null distribution of
proportions, we use a \texttt{mu\ =} argument in
Example~\ref{exm-ida-num-uni-null-mean} to specify the expected mean.
The rest of the hypothesis testing workflow is the same as for the null
distribution of proportions.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

The mean \texttt{mu} is not the only statistic we can specify for a
numeric variable. We can also specify the median \texttt{med}, or the
standard deviation \texttt{sigma}.

\end{tcolorbox}

In our case, we do not have a priori knowledge of the expected statistic
for the \texttt{fillers\_per\_100} variable, so we will not pursue this
approach. However, when looking at the distribution of a numeric
variable, it is useful to look at the descriptive statistics the mean,
median, and standard deviation in order to get a sense of the central
tendency and spread of the data.

We can use the \texttt{skim()} function to get the mean, median, and
standard deviation of the \texttt{fillers\_per\_100} variable, as seen
in Example~\ref{exm-ida-num-uni-summary}.

\begin{example}[]\protect\hypertarget{exm-ida-num-uni-summary}{}\label{exm-ida-num-uni-summary}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{skim}\NormalTok{(fillers\_per\_100)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> -- Data Summary ------------------------
>                            Values    
> Name                       fillers_df
> Number of rows             441       
> Number of columns          7         
> _______________________              
> Column type frequency:               
>   numeric                  1         
> ________________________             
> Group variables            None      
> 
> -- Variable type: numeric ------------------------------------------------------
>   skim_variable   n_missing complete_rate mean   sd p0  p25  p50  p75 p100 hist 
> 1 fillers_per_100         0             1 9.19 7.03  0 4.11 7.95 13.2 52.8 ▇▃▁▁▁
\end{verbatim}

\end{example}

The mean is larger than the median which suggests that the distribution
is right-skewed. Furthermore, the standard deviation is large relative
to the mean, which suggests that there is a lot of variability in the
data.

To get a sense of the distribution of the \texttt{fillers\_per\_100}
variable, we can plot a histogram and density plot, as seen in
Example~\ref{exm-ida-num-uni-hist-dens}.

\begin{example}[]\protect\hypertarget{exm-ida-num-uni-hist-dens}{}\label{exm-ida-num-uni-hist-dens}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Histogram}
\NormalTok{fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fillers\_per\_100)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Fillers per 100 utterances"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}

\CommentTok{\# Density plot}
\NormalTok{fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fillers\_per\_100)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Fillers per 100 utterances"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Density"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-uni-hist-dens-1.pdf}

}

\subcaption{\label{fig-ida-num-uni-hist-dens-1}Histogram}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-uni-hist-dens-2.pdf}

}

\subcaption{\label{fig-ida-num-uni-hist-dens-2}Density plot}

\end{minipage}%

\caption{\label{fig-ida-num-uni-hist-dens}Histogram and density plot of
the \texttt{fillers\_per\_100} variable}

\end{figure}%

\end{example}

The distribution of \texttt{fillers\_per\_100} is indeed skewed to the
right. We might have predicted this given that we are working with ratio
based on count data, perhaps not. In any case, the skewing we observe
tends to compress the distribution and may make it difficult to see any
patterns. To mitigate this, we can log transform the variable. But we
will run into a problem if we have any speakers who do not use any
fillers at all as these speakers will have a value of zero, as we can
see in Figure~\ref{fig-ida-num-uni-hist-dens-2}. The log of zero is
undefined. So we need to address this.

Eliminating the speakers who do not use any fillers at all is one
option. This is quite extreme as we may lose quite a few speakers and it
is not clear that removing data in this way will not cause inordinate
bias in the results as these speakers may be different in some way from
the rest of the speakers. Looking at the speakers with zero fillers in
Example~\ref{exm-ida-num-uni-zero-fillers}, we can see that there is
some potential for bias.

\begin{example}[]\protect\hypertarget{exm-ida-num-uni-zero-fillers}{}\label{exm-ida-num-uni-zero-fillers}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(fillers\_per\_100 }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tabyl}\NormalTok{(education, sex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>              education Female Male
>      More Than College      3   14
>                College     16   11
>      Less Than College      2    0
>  Less Than High School      1    0
>                Unknown      1    0
\end{verbatim}

\end{example}

Another approach is to add a small value to the
\texttt{fillers\_per\_100} variable, for all speakers. This will allow
us to log transform the variable and will likely not have any (or very
little) impact on the results. It also allows us to keep these speakers.

Adding values can be done in one of two ways. We can add a small
constant value to all speakers, or we can add a small random value to
all speakers. The former is easier to implement, but means that we will
still have a spike in the distribution at the value of the constant.
Since we do not expect that speakers that did not use fillers at all
would never do so and that when they do we would not expect them to be
at exactly the same rate as other speakers, we can add a small random
value to all speakers.

In R, we can use the \texttt{jitter()} function to add a small amount of
random noise to the variable. Note, however, this random noise can be
positive or negative. When a negative value is added to a zero value, we
are still in trouble. So we need to make sure that none of the jitter
produces negative values. We can do this by simply taking the absolute
value of the jittered variable with the \texttt{abs()} function. Let's
see how this works in Example~\ref{exm-ida-num-uni-jitter}.

\begin{example}[]\protect\hypertarget{exm-ida-num-uni-jitter}{}\label{exm-ida-num-uni-jitter}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}

\CommentTok{\# Add jitter to fillers}
\NormalTok{fillers\_df }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{fillers\_per\_100\_jitter =} \FunctionTok{abs}\NormalTok{(}\FunctionTok{jitter}\NormalTok{(fillers\_per\_100)))}

\NormalTok{fillers\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 441 x 8
>    speaker_id   age sex    education         fillers_per_100 total_fillers
>         <dbl> <dbl> <fct>  <ord>                       <dbl>         <dbl>
>  1       1000    38 Female Less Than College            2.14            44
>  2       1001    52 Male   More Than College           25.3            263
>  3       1002    29 Female College                      4.13            54
>  4       1004    34 Female College                      2.41            45
>  5       1005    36 Female College                      3.79           129
>  6       1007    27 Female College                      0                0
>  7       1008    53 Female Less Than College            8.33            27
>  8       1010    60 Male   Less Than College            1.82             2
>  9       1011    28 Female College                      5.22            54
> 10       1013    35 Female College                      6.23           130
> # i 431 more rows
> # i 2 more variables: total_utterances <dbl>, fillers_per_100_jitter <dbl>
\end{verbatim}

\end{example}

The results from Example~\ref{exm-ida-num-uni-jitter} show that the
\texttt{fillers\_per\_100\_jitter} variable has been added to the
\texttt{fillers\_df} dataset and that zero values for
\texttt{fillers\_per\_100} now have a small amount of random noise added
to them. Note, that the other values also have a small amount of random
noise added to them, but it is so small that rounding to 2 decimal
places makes it look like nothing has changed.

Now let's return to log transforming the
\texttt{fillers\_per\_100\_jitter} variable. We can do this with the
\texttt{log()} function. Let's see how this works in
Example~\ref{exm-ida-num-uni-log}.

\begin{example}[]\protect\hypertarget{exm-ida-num-uni-log}{}\label{exm-ida-num-uni-log}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Log transform fillers (with jitter)}
\NormalTok{fillers\_df }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{fillers\_per\_100\_log =} \FunctionTok{log}\NormalTok{(fillers\_per\_100\_jitter))}

\NormalTok{fillers\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 441 x 9
>    speaker_id   age sex    education         fillers_per_100 total_fillers
>         <dbl> <dbl> <fct>  <ord>                       <dbl>         <dbl>
>  1       1000    38 Female Less Than College            2.14            44
>  2       1001    52 Male   More Than College           25.3            263
>  3       1002    29 Female College                      4.13            54
>  4       1004    34 Female College                      2.41            45
>  5       1005    36 Female College                      3.79           129
>  6       1007    27 Female College                      0                0
>  7       1008    53 Female Less Than College            8.33            27
>  8       1010    60 Male   Less Than College            1.82             2
>  9       1011    28 Female College                      5.22            54
> 10       1013    35 Female College                      6.23           130
> # i 431 more rows
> # i 3 more variables: total_utterances <dbl>, fillers_per_100_jitter <dbl>,
> #   fillers_per_100_log <dbl>
\end{verbatim}

\end{example}

Let's now plot the log-transformed variable, as seen in
Example~\ref{exm-ida-num-uni-log-plot}.

\begin{example}[]\protect\hypertarget{exm-ida-num-uni-log-plot}{}\label{exm-ida-num-uni-log-plot}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Histogram}
\NormalTok{fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fillers\_per\_100\_log)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Fillers per 100 utterances (log)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}

\CommentTok{\# Density plot}
\NormalTok{fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fillers\_per\_100\_log)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Fillers per 100 utterances (log)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Density"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-uni-log-plot-1.pdf}

}

\subcaption{\label{fig-ida-num-uni-log-plot-1}Histogram}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-uni-log-plot-2.pdf}

}

\subcaption{\label{fig-ida-num-uni-log-plot-2}Density plot}

\end{minipage}%

\caption{\label{fig-ida-num-uni-log-plot}Histogram and density plot of
the \texttt{fillers\_per\_100\_log} variable}

\end{figure}%

\end{example}

The distribution of the log-transformed variable is more spread out now,
but the zero-filler speakers do show a low-level spike in the left tail
of the distribution. Jitter and log-transformation, however, smooth over
their effect to a large degree.

Let's drop some of the variables we no longer need and proceed with the
log-transformed variable, as seen in Example~\ref{exm-ida-num-uni-drop}.

\begin{example}[]\protect\hypertarget{exm-ida-num-uni-drop}{}\label{exm-ida-num-uni-drop}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Drop variables}
\NormalTok{fillers\_df }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_df }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{select}\NormalTok{(sex, education, age, fillers\_per\_100\_log)}
\end{Highlighting}
\end{Shaded}

\end{example}

\subsubsection{Bivariate analysis}\label{bivariate-analysis}

When considering a numeric reponses variable and another variable, it is
key to consider the nature of the other variable. If it is a categorical
variable with two levels, then we can compare a statistic between the
two groups (mean or median). If it is categorical with more than two
levels, the F statistic is used to compare the means. Finally, if it is
a numeric variable, then we can use a correlation test to see if there
is an association between the two variables.

Let's approach each of these bivariate scenarios in turn.

The \texttt{datives\_df} contains the \texttt{sex} variable which is a
categorical variable with two levels. According to the literature,
filled pauses are associated with differences between men and women
Tottie (\citeproc{ref-Tottie2014}{2014}). Sex is one of the variables
associated with the rate of filled pauses. The findings suggest that men
use fillers at a higher rate than women. Let's test to see if this holds
for the SWDA data.

Let's first explore the distribution from a descriptive point of view.
With a numeric response variable \texttt{fillers\_per\_100\_log} and a
categorical explanatory variable \texttt{sex}, a boxplot is a natural
fit. Let's add a density plot with a \texttt{sex} overlay in
Example~\ref{exm-ida-num-bi-vis}.

\begin{example}[]\protect\hypertarget{exm-ida-num-bi-vis}{}\label{exm-ida-num-bi-vis}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ fillers\_per\_100\_log, }\AttributeTok{x =}\NormalTok{ sex)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{y =} \StringTok{"Filler use (log)"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Sex"}
\NormalTok{  )}

\NormalTok{fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fillers\_per\_100\_log, }\AttributeTok{fill =}\NormalTok{ sex)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Filler use (log)"}\NormalTok{,}
    \AttributeTok{fill =} \StringTok{"Sex"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-bi-vis-1.pdf}

}

\subcaption{\label{fig-ida-num-bi-vis-1}Boxplot}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-bi-vis-2.pdf}

}

\subcaption{\label{fig-ida-num-bi-vis-2}Density plot}

\end{minipage}%

\caption{\label{fig-ida-num-bi-vis}Visualizations of the
\texttt{fillers\_per\_100\_log} variable by \texttt{sex}}

\end{figure}%

\end{example}

Looking at the boxplot in Figure~\ref{fig-ida-num-bi-vis-1}, we see that
the appears to be an overall higher rate of filler use for men, compared
to women. We also can see that the random noise added to zero-rate
speakers appear as outliers in the left tail. The density plot in
Figure~\ref{fig-ida-num-bi-vis-2} confirms this and allows us to see the
overlap between the two groups. We see that the density distribution is
not as concentrated and is slightly lower than that of men.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{Consider this}

If we focus on the zero-rate outliers, we see there is also a difference
in the distribution. Can this difference be attributed to differences
between men and women? Why or why not?

Does your answer to this question have an effect on how we interpret the
differences between men and women in the main distribution?

\end{tcolorbox}

Let's follow the simulation-based hypothesis testing workflow and
investigate if the apparent difference between men and women is
statistically significant, or expected by chance. The first steps are
found in Example~\ref{exm-ida-num-bi-sex-null}.

\begin{example}[]\protect\hypertarget{exm-ida-num-bi-sex-null}{}\label{exm-ida-num-bi-sex-null}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Specify the relationship}
\NormalTok{fillers\_spec }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{specify}\NormalTok{(fillers\_per\_100\_log }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sex) }\CommentTok{\# response \textasciitilde{} explanatory}

\CommentTok{\# Observed statistic}
\NormalTok{fillers\_obs }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_spec }\SpecialCharTok{|\textgreater{}}
  \CommentTok{\# diff in means, Male {-} Female}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in means"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"Male"}\NormalTok{, }\StringTok{"Female"}\NormalTok{))}

\CommentTok{\# Null distribution}
\NormalTok{fillers\_null }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# independence = no relationship}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"permute"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# permute = shuffle}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in means"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"Male"}\NormalTok{, }\StringTok{"Female"}\NormalTok{))}

\CommentTok{\# Visualize the null distribution w/ observed statistic}
\NormalTok{fillers\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_p\_value}\NormalTok{(}\AttributeTok{obs\_stat =}\NormalTok{ fillers\_obs, }\AttributeTok{direction =} \StringTok{"greater"}\NormalTok{) }\CommentTok{\# expect greater}

\CommentTok{\# Calculate the p{-}value}
\NormalTok{fillers\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{get\_p\_value}\NormalTok{(}\AttributeTok{obs\_stat =}\NormalTok{ fillers\_obs, }\AttributeTok{direction =} \StringTok{"greater"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 1
>   p_value
>     <dbl>
> 1   0.033
\end{verbatim}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-bi-sex-null-1.pdf}

}

\caption{\label{fig-ida-num-bi-sex-null}Null distribution of the
\texttt{fillers\_per\_100\_log} variable by \texttt{sex}}

\end{figure}%

\end{example}

From the analysis performed in Example~\ref{exm-ida-num-bi-sex-null}, we
can reject the null hypothesis that there is no difference between the
rate of filler use between men and women, as the p-value is greater less
than 0.05.

To further assess the uncertainty of the observed statistic, and the
robustness of the difference, we calculate a confidence interval, as
seen in Example~\ref{exm-ida-num-bi-sex-ci}.

\begin{example}[]\protect\hypertarget{exm-ida-num-bi-sex-ci}{}\label{exm-ida-num-bi-sex-ci}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Resampling distribution}
\NormalTok{fillers\_boot }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in means"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"Male"}\NormalTok{, }\StringTok{"Female"}\NormalTok{))}

\CommentTok{\# Calculate the confidence interval}
\NormalTok{fillers\_ci }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_boot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{get\_ci}\NormalTok{(}\AttributeTok{level =} \FloatTok{0.95}\NormalTok{)}

\NormalTok{fillers\_ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 2
>   lower_ci upper_ci
>      <dbl>    <dbl>
> 1 -0.00378     1.05
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize the confidence interval}
\NormalTok{fillers\_boot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_ci}\NormalTok{(fillers\_ci)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-bi-sex-ci-1.pdf}

}

\caption{\label{fig-ida-num-bi-sex-ci}Confidence interval generated from
bootstrap resampling}

\end{figure}%

\end{example}

The confidence interval includes 0, which suggests that the observed
difference is questionable. It is of note, however, that the majority of
the interval is above 0, which provides some evidence that the observed
difference is not due to chance.

This result highlights how p-values and confidence intervals together
can provide a more nuanced picture of the data.

The second bivariate scenario we can consider is when the explanatory
variable is categorical with more than two levels. In this case, we can
use the F statistic to compare the means of the different levels. The
\texttt{education} variable in the \texttt{fillers\_df} dataset is a
categorical variable with five levels. Tottie
(\citeproc{ref-Tottie2011}{2011}) suggests that more educated speakers
use more fillers than less educated speakers. Let's test this
hypothesis.

First, we visualize the distribution of the
\texttt{fillers\_per\_100\_log} variable by \texttt{education}, as seen
in Example~\ref{exm-ida-num-bi-edu-vis}.

\begin{example}[]\protect\hypertarget{exm-ida-num-bi-edu-vis}{}\label{exm-ida-num-bi-edu-vis}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Boxplot}
\NormalTok{fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ fillers\_per\_100\_log, }\AttributeTok{x =}\NormalTok{ education)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{y =} \StringTok{"Filler use (log)"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Education"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-bi-edu-vis-1.pdf}

}

\caption{\label{fig-ida-num-bi-edu-vis}Visualizations of the
\texttt{fillers\_per\_100\_log} variable by \texttt{education}}

\end{figure}%

\end{example}

The boxplot in Figure~\ref{fig-ida-num-bi-edu-vis} does not point to any
obvious differences between the levels of the \texttt{education}
variable. There are a fair number of outliers, however, in the two most
educated groups. These outliers are likely due to the random noise added
to the 0-rate speakers and it is interesting that they are concentrated
in the two most educated groups.

Let's now submit these variables to the simulation-based hypothesis
testing workflow to quantify the uncertainty of the observed statistic
and determine if the observed difference is statistically significant.
The first steps are found in Example~\ref{exm-ida-num-bi-edu-null}.

\begin{example}[]\protect\hypertarget{exm-ida-num-bi-edu-null}{}\label{exm-ida-num-bi-edu-null}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Specify the relationship}
\NormalTok{fillers\_spec }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{specify}\NormalTok{(fillers\_per\_100\_log }\SpecialCharTok{\textasciitilde{}}\NormalTok{ education) }\CommentTok{\# response \textasciitilde{} explanatory}

\CommentTok{\# Observed statistic}
\NormalTok{fillers\_obs }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"F"}\NormalTok{) }\CommentTok{\# F = variance between groups / variance within groups}

\CommentTok{\# Null distribution}
\NormalTok{fillers\_null }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# independence = no relationship}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"permute"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# permute = shuffle}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"F"}\NormalTok{)}

\CommentTok{\# Visualize the null distribution w/ observed statistic}
\NormalTok{fillers\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_p\_value}\NormalTok{(}\AttributeTok{obs\_stat =}\NormalTok{ fillers\_obs, }\AttributeTok{direction =} \StringTok{"two{-}sided"}\NormalTok{) }\CommentTok{\# expect greater}

\CommentTok{\# Calculate the p{-}value}
\NormalTok{fillers\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{get\_p\_value}\NormalTok{(}\AttributeTok{obs\_stat =}\NormalTok{ fillers\_obs, }\AttributeTok{direction =} \StringTok{"two{-}sided"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 1
>   p_value
>     <dbl>
> 1   0.426
\end{verbatim}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-bi-edu-1.pdf}

}

\caption{\label{fig-ida-num-bi-edu}Null distribuition of the
\texttt{fillers\_per\_100\_log} variable by \texttt{education}}

\end{figure}%

\end{example}

The analysis in Example~\ref{exm-ida-num-bi-edu-null} suggests that the
observed difference between the means of the different levels of the
\texttt{education} variable not significantly different from what we
would expect by chance.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

The p-value in Example~\ref{exm-ida-num-bi-edu-null} was calculated
using a two-sided test, which is appropriate when the expected
directionality is not known. In this case, while we do have an expected
directionality, the visualizations strongly suggest that the observed
difference is not in line with our expectations. To account for this
uncertainty and to be conservative, we choose to use a two-sided test.
This allows us to remain open to the possibility that the observed
difference may actually be in the opposite direction, rather than solely
focusing on our initial expectation. However, it's important to note
that the decision to use a two-sided test should also consider factors
such as the specific research question and the context of the analysis.

\end{tcolorbox}

Let's now calculate a confidence interval to assess the uncertainty of
the observed statistic, as seen in Example~\ref{exm-ida-num-bi-edu-ci}.

\begin{example}[]\protect\hypertarget{exm-ida-num-bi-edu-ci}{}\label{exm-ida-num-bi-edu-ci}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Resampling distribution}
\NormalTok{fillers\_boot }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"F"}\NormalTok{)}

\CommentTok{\# Calculate the confidence interval}
\NormalTok{fillers\_ci }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_boot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{get\_ci}\NormalTok{(}\AttributeTok{level =} \FloatTok{0.95}\NormalTok{)}

\NormalTok{fillers\_ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 2
>   lower_ci upper_ci
>      <dbl>    <dbl>
> 1    0.123     4.57
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize the confidence interval}
\NormalTok{fillers\_boot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_ci}\NormalTok{(fillers\_ci)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-bi-edu-ci-1.pdf}

}

\caption{\label{fig-ida-num-bi-edu-ci}Confidence interval generated from
bootstrap resampling}

\end{figure}%

\end{example}

In Example~\ref{exm-ida-num-bi-edu-ci}, we see that we are in the
opposite situation --the p-value is not significant and but the
confidence interval does not include 0. So how do we interpret this?
Remember, the p-value is the probability of observing a statistic as
extreme or more extreme than the observed statistic, given that the null
hypothesis is true. The confidence interval is the range of values that
we are 95\% confident contains the true population parameter. We should
take into consideration two aspects: (1) the confidence interval has a
large range (the interval is wide) and (2) that the lower limit is near
0. Take together and in addition to the p-value, we can conclude that
the observed difference is not statistically significant, and if there
is a difference, it is likely to be small or negligible.

The last bivariate case is when the explanatory variable is numeric. In
this case, we can use a correlation test to see if there is an
association between the two variables. The \texttt{age} variable in the
\texttt{fillers\_df} dataset is a numeric variable. Tottie
(\citeproc{ref-Tottie2011}{2011}) suggests that older speakers use more
fillers than younger speakers. Let's test this hypothesis.

First, we visualize the distribution of the
\texttt{fillers\_per\_100\_log} variable by \texttt{age}, as seen in
Example~\ref{exm-ida-num-bi-age-vis}.

\begin{example}[]\protect\hypertarget{exm-ida-num-bi-age-vis}{}\label{exm-ida-num-bi-age-vis}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Scatterplot}
\NormalTok{fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ fillers\_per\_100\_log)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Age"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Filler use (log)"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-bi-age-vis-1.pdf}

}

\caption{\label{fig-ida-num-bi-age-vis}Scatterplot of the
\texttt{fillers\_per\_100\_log} variable by \texttt{age}}

\end{figure}%

\end{example}

The trend line in Figure~\ref{fig-ida-num-bi-age-vis} does not suggest
that there is a strong relationship between the two variables, or even
if there is a relationship at all. We can see that the rate of filler
use appears constant across the age range.

Let's now submit these variables to the simulation-based hypothesis
testing workflow to quantify the uncertainty of the observed statistic
and determine if the observed difference is statistically significant.
The first steps are found in Example~\ref{exm-ida-num-bi-age-null}.

\begin{example}[]\protect\hypertarget{exm-ida-num-bi-age-null}{}\label{exm-ida-num-bi-age-null}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Specify the relationship}
\NormalTok{fillers\_spec }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{specify}\NormalTok{(fillers\_per\_100\_log }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age) }\CommentTok{\# response \textasciitilde{} explanatory}

\CommentTok{\# Observed statistic}
\NormalTok{fillers\_obs }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"correlation"}\NormalTok{) }\CommentTok{\# correlation = r}

\CommentTok{\# Null distribution}
\NormalTok{fillers\_null }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# independence = no relationship}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"permute"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# permute = shuffle}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"correlation"}\NormalTok{)}

\CommentTok{\# Visualize the null distribution w/ observed statistic}
\NormalTok{fillers\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_p\_value}\NormalTok{(}\AttributeTok{obs\_stat =}\NormalTok{ fillers\_obs, }\AttributeTok{direction =} \StringTok{"two{-}sided"}\NormalTok{) }\CommentTok{\# expect greater}

\CommentTok{\# Calculate the p{-}value}
\NormalTok{fillers\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{get\_p\_value}\NormalTok{(}\AttributeTok{obs\_stat =}\NormalTok{ fillers\_obs, }\AttributeTok{direction =} \StringTok{"two{-}sided"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 1
>   p_value
>     <dbl>
> 1   0.762
\end{verbatim}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-bi-age-null-1.pdf}

}

\caption{\label{fig-ida-num-bi-age-null}Null distribution of the
\texttt{fillers\_per\_100\_log} variable by \texttt{age}}

\end{figure}%

\end{example}

This is a very large p-value and underscores what the scatterplot in
Figure~\ref{fig-ida-num-bi-age-vis} suggests --there is no relationship
between the two variables.

For completeness, we calculate a confidence interval to assess the
uncertainty of the observed statistic, as seen in
Example~\ref{exm-ida-num-bi-age-ci}.

\begin{example}[]\protect\hypertarget{exm-ida-num-bi-age-ci}{}\label{exm-ida-num-bi-age-ci}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Resampling distribution}
\NormalTok{fillers\_boot }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"correlation"}\NormalTok{)}

\CommentTok{\# Calculate the confidence interval}
\NormalTok{fillers\_ci }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_boot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{get\_ci}\NormalTok{(}\AttributeTok{level =} \FloatTok{0.95}\NormalTok{)}

\NormalTok{fillers\_ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 2
>   lower_ci upper_ci
>      <dbl>    <dbl>
> 1  -0.0867    0.120
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize the confidence interval}
\NormalTok{fillers\_boot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_ci}\NormalTok{(fillers\_ci)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-bi-age-ci-1.pdf}

}

\caption{\label{fig-ida-num-bi-age-ci}Confidence interval generated from
bootstrap resampling}

\end{figure}%

\end{example}

The confidence interval includes 0, which suggests that the observed
difference is questionable. But in contrast to the confidence interval
in Example~\ref{exm-ida-num-bi-edu-ci}, the interval is narrow
suggesting more strongly that the observed difference is not due to
chance.

\subsubsection{Multivariate analysis}\label{multivariate-analysis-1}

While bivariate analysis is useful for exploring the relationship
between two variables, it is often the case that we want to consider
relationships between more than two variables. In this case, we can use
multivariate analysis. Linear regression is a common multivariate
analysis technique.

In linear regression, we are interested in predicting the value of a
numeric response variable based on the values of the explanatory
variables. The contribution of the explanatory variables can be
considered individually, as an interaction, or as a combination of both.

Let's now pick up the \texttt{fillers\_type\_df} dataset and explore the
hypothesis that the rate of filler use varies by the type of filler
across the socio-demographic variables \texttt{sex}, \texttt{education},
and \texttt{age}. To do this we will use R formula syntax to specify the
variables we want to include in the model.

\begin{itemize}
\tightlist
\item
  Simple effects:
  \texttt{response\ \textasciitilde{}\ explanatory\_1\ +\ explanatory\_2}:
  the response variable as a function of each explanatory variable.
\item
  Interaction effects:
  \texttt{response\ \textasciitilde{}\ explanatory\_1:explanatory\_2}:
  the response variable as a function of the interaction between the two
  explanatory variables.
\item
  Simple and interaction effects:
  \texttt{response\ \textasciitilde{}\ explanatory\_1\ *\ explanatory\_2}:
  the response variable as a function of each explanatory variable and
  the interaction between the two explanatory variables.
\end{itemize}

Let's test the hypothesis that men and women differ in the rates that
they use the filler types
(\texttt{fillers\_per\_100\_log\ \textasciitilde{}\ filler\_type\ *\ sex}).
A plot will help us begin to understand the potential relationships. In
Example~\ref{exm-ida-multi-sex-plot}, we use a boxplot to visualize the
relationship between the \texttt{fillers\_per\_100\_log} variable and
the \texttt{filler\_type} variable, with a \texttt{sex} overlay.

\begin{example}[]\protect\hypertarget{exm-ida-multi-sex-plot}{}\label{exm-ida-multi-sex-plot}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Boxplot \textasciigrave{}filler\_type\textasciigrave{}}
\NormalTok{fillers\_type\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ fillers\_per\_100\_log, }\AttributeTok{x =}\NormalTok{ filler\_type)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Filler type"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Fillers per 100 (log)"}
\NormalTok{  )}


\CommentTok{\# Boxplot \textasciigrave{}filler\_type\textasciigrave{} and \textasciigrave{}sex\textasciigrave{}}
\NormalTok{fillers\_type\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ fillers\_per\_100\_log, }\AttributeTok{x =}\NormalTok{ filler\_type, }\AttributeTok{fill =}\NormalTok{ sex)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{notch =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Filler type"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Fillers per 100 (log)"}\NormalTok{,}
    \AttributeTok{fill =} \StringTok{"Sex"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-multi-sex-plot-1.pdf}

}

\subcaption{\label{fig-ida-multi-sex-plot-1}Boxplot by
\texttt{filler\_type}}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-multi-sex-plot-2.pdf}

}

\subcaption{\label{fig-ida-multi-sex-plot-2}Boxplot by
\texttt{filler\_type} and \texttt{sex}}

\end{minipage}%

\caption{\label{fig-ida-multi-sex-plot}Boxplot of the
\texttt{fillers\_per\_100\_log} variable by \texttt{filler\_type} and
\texttt{sex}}

\end{figure}%

\end{example}

Let's interpret the boxplot in Figure~\ref{fig-ida-multi-sex-plot}.
Focusing on Figure~\ref{fig-ida-multi-sex-plot-1} first, we see that the
filler `uh' is more frequent than `um' as the median is distinct and the
confidence intervals do not overlap. Now, looking at
Figure~\ref{fig-ida-multi-sex-plot-2}, we see the same distinction
between `uh' and `um', but we also see that the difference between the
use of `uh' and `um' is different for males and females. This type of
relationship is called an interaction effect. In this case the
interaction effect goes in the same direction but the magnitude of the
difference is different.

The upshot, men and women both use `uh' more than `um' but men are even
more likely to use `uh' over `um' than women.

Let's test this effect using the \texttt{infer} workflow. Calculating
the observed statistics for the simple and interaction effects is very
similar to other designs, except instead of \texttt{calculate()} to
derive our statistics we will use the \texttt{fit()} function, just as
we did for logistic regression. Let's go ahead calculate the observed
statistics first, as seen in Example~\ref{exm-ida-num-multi-spec}.

\begin{example}[]\protect\hypertarget{exm-ida-num-multi-spec}{}\label{exm-ida-num-multi-spec}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Specify the relationship}
\NormalTok{fillers\_type\_spec }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_type\_df }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{specify}\NormalTok{(fillers\_per\_100\_log }\SpecialCharTok{\textasciitilde{}}\NormalTok{ filler\_type }\SpecialCharTok{*}\NormalTok{ sex)}

\CommentTok{\# Observed statistics}
\NormalTok{fillers\_type\_obs }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_type\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit}\NormalTok{()}

\NormalTok{fillers\_type\_obs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 4 x 2
>   term                  estimate
>   <chr>                    <dbl>
> 1 intercept                0.551
> 2 filler_typeum           -2.16 
> 3 sexMale                  0.657
> 4 filler_typeum:sexMale   -1.84
\end{verbatim}

\end{example}

The terms in the output from Example~\ref{exm-ida-num-multi-spec}
provide information as to what the reference levels are. For example,
\texttt{filler\_typeum} tells us that the `uh' level is the reference
for \texttt{filler\_type} and by the same logic, `Female' is the
reference for \texttt{sex}. These terms provide our simple effect
statistics. Each can be understood as the difference between the
reference level when the other variables are held constant. Our response
variable is log transformed, so it is not directly interpretable beyond
the fact that smaller units are lower rates of filler use and larger
units are higher rates of filler use. So `um' is used less than `uh' and
men use more fillers than women.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

The statistic returned from a linear regression is the difference in the
means of the two levels with the other variables held constant. In this
way, the statistic is a measure of the slope of the line between the two
levels. This slope quantifies the magnitude of the difference between
the two levels.

We can also calculate the effect size by standardizing the statistics
(coefficients). This is done by dividing the coefficient by the standard
deviation of the response variable. This allows us to compare the effect
sizes of different variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Standardized effect size}
\NormalTok{fillers\_type\_df  }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(filler\_type) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{type\_sd =} \FunctionTok{sd}\NormalTok{(fillers\_per\_100\_log)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

The interaction term \texttt{fillertypeum:sexMale} is the difference in
the rate of fillers for this combination compared to the reference level
combination (`uh' and `Female'). In this case, the observed rate is
lower.

We now need to generate a null distribution to compare the observed
statistics to. We will again use the permutation method, but since there
is an interaction effect, we need to shuffle the \texttt{filler\_type}
and \texttt{sex} variables together. This ensures that any relationship
between the two variables is removed. Let's see how this works in
Example~\ref{exm-ida-num-multi-null}.

\begin{example}[]\protect\hypertarget{exm-ida-num-multi-null}{}\label{exm-ida-num-multi-null}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Null distribution}
\NormalTok{fillers\_type\_null }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_type\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"permute"}\NormalTok{, }\AttributeTok{variables =} \FunctionTok{c}\NormalTok{(}\StringTok{"filler\_type"}\NormalTok{, }\StringTok{"sex"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit}\NormalTok{()}

\CommentTok{\# Visualize the null distribution w/ observed statistics}
\NormalTok{fillers\_type\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_p\_value}\NormalTok{(}\AttributeTok{obs\_stat =}\NormalTok{ fillers\_type\_obs, }\AttributeTok{direction =} \StringTok{"two{-}sided"}\NormalTok{)}

\CommentTok{\# Calculate the p{-}values}
\NormalTok{fillers\_type\_null }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{get\_p\_value}\NormalTok{(}\AttributeTok{obs\_stat =}\NormalTok{ fillers\_type\_obs, }\AttributeTok{direction =} \StringTok{"two{-}sided"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 4 x 2
>   term                  p_value
>   <chr>                   <dbl>
> 1 filler_typeum           0    
> 2 filler_typeum:sexMale   0    
> 3 intercept               0    
> 4 sexMale                 0.066
\end{verbatim}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-multi-null-1.pdf}

}

\caption{\label{fig-ida-num-multi-null}Null distribution of the
\texttt{fillers\_per\_100\_log} variable by \texttt{filler\_type} and
\texttt{sex}}

\end{figure}%

\end{example}

For the simple effects, we see that \texttt{filler\_type} is significant
but \texttt{sex} is not. Remember, when we only considered \texttt{sex}
in isolation in the bivariate case, we found it to be significant. So
why is it not signficant now? It is important to remember that in every
statistical design, there are other factors that are not considered.
When these are not in the model, our effects may appear to account for
more of the variance than they actually do. In this case, the
\texttt{filler\_type} variable is accounting for some of the variance
that \texttt{sex} was accounting for in the bivariate case, enough, it
appears, to make \texttt{sex} not significant as a simple effect.

Our interaction effect is also significant meaning the observed
difference we visualized in Figure~\ref{fig-ida-multi-sex-plot} is
likely not due to chance. The upshot, both men and women use more `uh'
compared to `um' but mens' difference in use is larger than womens'.

As always, let's calculate a confidence interval to assess the
uncertainty of the observed statistic, as seen in
Example~\ref{exm-ida-num-multi-sex-ci}.

\begin{example}[]\protect\hypertarget{exm-ida-num-multi-sex-ci}{}\label{exm-ida-num-multi-sex-ci}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Resampling distribution}
\NormalTok{fillers\_type\_boot }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_type\_spec }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit}\NormalTok{()}

\CommentTok{\# Calculate the confidence intervals}
\NormalTok{fillers\_type\_ci }\OtherTok{\textless{}{-}}
\NormalTok{  fillers\_type\_boot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{get\_ci}\NormalTok{(}\AttributeTok{level =} \FloatTok{0.95}\NormalTok{, }\AttributeTok{point\_estimate =}\NormalTok{ fillers\_type\_obs)}

\NormalTok{fillers\_type\_ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 4 x 3
>   term                  lower_ci upper_ci
>   <chr>                    <dbl>    <dbl>
> 1 filler_typeum          -2.75     -1.54 
> 2 filler_typeum:sexMale  -2.68     -0.934
> 3 intercept               0.168     0.909
> 4 sexMale                 0.0947    1.22
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize the confidence intervals}
\NormalTok{fillers\_type\_boot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{visualize}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{shade\_ci}\NormalTok{(fillers\_type\_ci)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{inference_files/figure-pdf/fig-ida-num-multi-sex-ci-1.pdf}

}

\caption{\label{fig-ida-num-multi-sex-ci}Confidence intervals generated
from bootstrap resampling}

\end{figure}%

\end{example}

From the confidence intervals, we see that zero is not included in any
of the intervals, which suggests that the observed differences are not
due to chance. Interpreting the width and the proximity to zero,
however, suggests that the observed differences for
\texttt{filler\_type} are stronger than for \texttt{sex}, which did not
result in a significant simple effect. The interaction effect is also
significant, but the confidence interval is quite wide and approximates
zero. This should raise some questions about the robustness of the
observed effect.

\section*{Activities}\label{activities-8}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-10.html}{Building
inference models} \textbf{How}: Read Recipe 10 and participate in the
Hypothes.is online social annotation.\\
\textbf{Why}: \faIcon{wrench} TBD

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-10}{Statistical
inference} \textbf{How}: Clone, fork, and complete the steps in Lab
10.\\
\textbf{Why}: \faIcon{wrench} TBD

\end{tcolorbox}

\section*{Summary}\label{summary-9}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

\ldots{}

In sum, in this section we explored the process of statistical
inference, specifically the process of hypothesis testing with
categorical responses variables. We started with a univariate analysis,
which, although not very interesting, allowed us to focus on the process
and the logic of statistical inference. We then moved on to bivariate
and multivariate analyses. We see that the process is very similar, but
the complexity increases with the number of explanatory variables. In
each case, we applied a simulation-based approach to statistical
inference which may differ from the traditional approach you may have
learned in your statistics courses. But as you can appreciate, the
simulation-based approach is highly flexible, more intiutive, and more
transparent. The \texttt{infer} package provides a consistent set of
functions and pipeline to maintain a consistent workflow. If you have
previous experience with R or find resources on statistical inference
using R, you will likely find the traditional, theory-based, approach. I
encourage you to explore the both approaches. They have their strengths
and weaknesses, but I think you will find the simulation-based approach
more intuitive and transparent as a beginner.

\ldots{}

In this chapter we have discussed various approaches to conducting
inferential data analysis. Each configuration, however, always includes
a descriptive assessment, statistical interrogation, and an evaluation
of the results. We considered univariate, bivariate, and multivariate
analyses using both categorical and non-categorical dependent variables
to explore the similarities and differences between these approaches.

\part{Communication}

In this section, I cover the steps in presenting the findings of the
research both as a research document and as a reproducible research
project. Both research documents and reproducible projects are
fundamental components of modern scientific inquiry. On the one hand a
research document provides readers a detailed summary of the main import
of the research study. On the other hand making the research project
available to interested readers ensures that the scientific community
can gain insight into the process implemented in the research and thus
enables researchers to vet and extend this research to build a more
robust and verifiable research base.

\chapter{Contribute}\label{sec-contributing}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, toptitle=1mm, leftrule=.75mm, colbacktitle=quarto-callout-caution-color!10!white, coltitle=black, breakable, colframe=quarto-callout-caution-color-frame, bottomrule=.15mm, titlerule=0mm, rightrule=.15mm, opacitybacktitle=0.6, arc=.35mm, toprule=.15mm, bottomtitle=1mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, opacityback=0]

Under development.

\end{tcolorbox}

\begin{quote}
The reproducibility of studies and the ability to follow up on the work
of others is key for innovation in science and engineering.

--- Leland Wilkinson
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\item
  \ldots{}
\end{itemize}

\end{tcolorbox}

In this chapter, we first examine the role of reporting your research,
highlighting the benefits of presenting one's work as a means to develop
and refine ideas and conclusions. We then discuss the purpose of a
research document, focusing on how its structure effectively conveys the
project's rationale, goals, procedures, results, and findings.

Whether for other researchers or for your future self, creating research
that is well-documented and reproducible is a fundamental part of
conducting modern scientific inquiry. In this chapter we will also
emphasize the importance of this endeavor and outline strategies for
ensuring your research project is reproducible. This will include
directory and file structure, key documentation files as well as how to
ensure a reproducible computing environment.

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{terminal} Lessons}

\textbf{What}: \faIcon{wrench}
\href{https://github.com/qtalr/lessons}{TBA}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: \faIcon{wrench} To \ldots{}

\end{tcolorbox}

\section{Goals}\label{sec-contr-goals}

\ldots{}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.8000}}@{}}
\caption{This is a table too wide for the
page.}\label{tbl-too-wide}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sentence\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sentence
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sentence\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sentence
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 1 & \textgreater Hotel California Fact: Sound is a vibration. \\
1 & 2 & Sound travels as a mechanical wave through a medium, and in
space, there is no medium. \\
1 & 3 & So when my shuttle malfunctioned and the airlocks didn't keep
the air in, I heard nothing. \\
1 & 4 & After the first whoosh of the air being sucked away, there was
lightning, but no thunder. \\
1 & 5 & Eyes bulging in panic, but no screams. \\
1 & 6 & Quiet and peaceful, right? \\
1 & 7 & Such a relief to never again hear my crewmate Jesse natter about
his girl back on Earth and \ldots{} \\
1 & 8 & I swore, if I ever had to see a photo of him in a skimpy bathing
suit again, giving the came\ldots{} \\
1 & 9 & Metaphorically, of course. \\
\end{longtable}

\section{Write ups}\label{sec-contr-write-ups}

\subsection{Contents}\label{contents}

\begin{itemize}
\tightlist
\item
  Introduction

  \begin{itemize}
  \tightlist
  \item
    Importance of clear and effective communication in research

    \begin{itemize}
    \tightlist
    \item
      Enhances understanding of the research
    \item
      Facilitates collaboration and interdisciplinary work
    \end{itemize}
  \item
    Overview of the chapter content

    \begin{itemize}
    \tightlist
    \item
      Research presentations
    \item
      Research document structure
    \item
      Citations and references
    \item
      Figures and tables
    \item
      Field-specific and publishing house formats
    \end{itemize}
  \end{itemize}
\item
  Research Presentations

  \begin{itemize}
  \tightlist
  \item
    Importance of research presentations in academia and professional
    contexts

    \begin{itemize}
    \tightlist
    \item
      Sharing research findings
    \item
      Building a professional reputation
    \end{itemize}
  \item
    Benefits of presenting research work

    \begin{itemize}
    \tightlist
    \item
      Developing and refining ideas and conclusions through feedback and
      discussion
    \item
      Receiving feedback and suggestions from peers and experts

      \begin{itemize}
      \tightlist
      \item
        Conducting peer review can help ensure that the research is
        conducted appropriately and transparently. Peer review can also
        identify potential errors or biases in the research design or
        analysis, and can provide suggestions for improving the study's
        reproducibility
      \end{itemize}
    \item
      Enhancing communication and presentation skills
    \item
      Building professional network and fostering collaboration
    \end{itemize}
  \item
    Key elements of an effective research presentation

    \begin{itemize}
    \tightlist
    \item
      Structure and organization: logical flow, clear sections, concise
      points
    \item
      Visual design and aesthetics: use of color, fonts, visuals, and
      layout
    \item
      Delivery and engagement: pacing, tone, body language, and audience
      interaction
    \end{itemize}
  \item
    Using R for creating research presentations

    \begin{itemize}
    \tightlist
    \item
      RMarkdown and its integration with presentation formats (e.g.,
      xaringan, slidify)
    \item
      Incorporating data visualization, tables, and other R-generated
      content seamlessly
    \end{itemize}
  \item
    Tips for delivering engaging and memorable presentations

    \begin{itemize}
    \tightlist
    \item
      Practice and preparation
    \item
      Storytelling and relatability
    \item
      Addressing diverse audience backgrounds
    \end{itemize}
  \end{itemize}
\item
  Research Documents

  \begin{itemize}
  \tightlist
  \item
    Purpose of a research document

    \begin{itemize}
    \tightlist
    \item
      Communicate research methodology and findings
    \item
      Contribute to the body of knowledge
    \end{itemize}
  \item
    Benefits of writing a research document

    \begin{itemize}
    \tightlist
    \item
      Clarifying and organizing thoughts and ideas
    \item
      Providing a comprehensive record of research work
    \item
      Enhancing writing and argumentation skills
    \item
      Contributing to the body of knowledge in a field
    \item
      Establishing professional credibility and visibility
    \end{itemize}
  \item
    Components of a research document

    \begin{itemize}
    \tightlist
    \item
      Title: concise and informative
    \item
      Abstract: brief summary of the research
    \item
      Introduction: background, research question, and objectives
    \item
      Methodology: description of the data, tools, and analysis
      techniques
    \item
      Results: presentation of the findings
    \item
      Discussion: interpretation and implications of the results
    \item
      Conclusion: summary and future directions
    \item
      References: list of cited sources
    \end{itemize}
  \item
    Tips for organizing and structuring research documents

    \begin{itemize}
    \tightlist
    \item
      Outlining the main sections
    \item
      Maintaining logical flow and consistency
    \item
      Using clear and concise language
    \end{itemize}
  \end{itemize}
\item
  Citations and References

  \begin{itemize}
  \tightlist
  \item
    Importance of proper citation and referencing

    \begin{itemize}
    \tightlist
    \item
      Acknowledge the work of others
    \item
      Demonstrate the foundation of your research
    \end{itemize}
  \item
    Different citation styles (APA, MLA, Chicago, etc.)

    \begin{itemize}
    \tightlist
    \item
      Overview of common styles
    \item
      Choosing the appropriate style for your discipline
    \end{itemize}
  \item
    Using R and Quarto for managing citations and references

    \begin{itemize}
    \tightlist
    \item
      Integration with reference managers (e.g., Zotero, Mendeley)
    \item
      Automating citation formatting
    \end{itemize}
  \item
    Integration of citations and references in research documents

    \begin{itemize}
    \tightlist
    \item
      In-text citations
    \item
      Reference list or bibliography
    \end{itemize}
  \end{itemize}
\item
  Figures and Tables

  \begin{itemize}
  \tightlist
  \item
    Importance of visual representations in research reporting

    \begin{itemize}
    \tightlist
    \item
      Aid in understanding complex data
    \item
      Enhance the readability of the document
    \end{itemize}
  \item
    Creating and customizing figures and tables using R and Quarto

    \begin{itemize}
    \tightlist
    \item
      ggplot2 for creating visualizations
    \item
      kable and gt for generating tables
    \end{itemize}
  \item
    Guidelines for labeling, formatting, and presenting figures and
    tables

    \begin{itemize}
    \tightlist
    \item
      Clear and informative titles and labels
    \item
      Consistent formatting
    \item
      Appropriate use of color, fonts, and layout
    \item
      Proper referencing and integration in the text
    \item
      Accessibility considerations for diverse readers
    \end{itemize}
  \end{itemize}
\item
  Field-specific and Publishing House Formats

  \begin{itemize}
  \tightlist
  \item
    Importance of adhering to field-specific and publisher guidelines

    \begin{itemize}
    \tightlist
    \item
      Consistency and professionalism
    \item
      Meeting submission requirements for journals, conferences, and
      other venues
    \end{itemize}
  \item
    Overview of common format requirements

    \begin{itemize}
    \tightlist
    \item
      Manuscript formatting: margins, line spacing, headings, etc.
    \item
      Citation and reference style
    \item
      Figure and table formatting
    \end{itemize}
  \item
    Using R and Quarto to apply formatting guidelines

    \begin{itemize}
    \tightlist
    \item
      Customizing document templates
    \item
      Applying and managing style files
    \end{itemize}
  \item
    Exporting research documents to various formats

    \begin{itemize}
    \tightlist
    \item
      Word: .docx files for collaboration and editing
    \item
      PDF: .pdf files for sharing and printing
    \item
      HTML: web-based documents for online publication
    \item
      ePub: e-book format for digital reading
    \end{itemize}
  \end{itemize}
\item
  Conclusion

  \begin{itemize}
  \tightlist
  \item
    Recap of the main topics covered in the chapter
  \item
    Importance of clear and effective communication in research
    reporting
  \item
    Encouragement to practice and refine reporting skills through
    various research projects
  \end{itemize}
\end{itemize}

\subsection{Structure}\label{structure}

\begin{example}[]\protect\hypertarget{exm-c-structure}{}\label{exm-c-structure}

Project structure

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{project/}
\ExtensionTok{├──}\NormalTok{ process/}
\ExtensionTok{│}\NormalTok{   ├── 1\_acquire\_data.qmd}
\ExtensionTok{│}\NormalTok{   ├── 2\_curate\_data.qmd}
\ExtensionTok{│}\NormalTok{   ├── 3\_transform\_data.qmd}
\ExtensionTok{│}\NormalTok{   └── 4\_analyze\_data.qmd}
\ExtensionTok{├──}\NormalTok{ data/}
\ExtensionTok{│}\NormalTok{   ├── analysis/}
\ExtensionTok{│}\NormalTok{   ├── derived/}
\ExtensionTok{│}\NormalTok{   │   ├── datasource\_dd.csv}
\ExtensionTok{│}\NormalTok{   │   └── datasource/}
\ExtensionTok{│}\NormalTok{   │       └── datasource{-}transformed.csv}
\ExtensionTok{│}\NormalTok{   └── original/}
\ExtensionTok{│}\NormalTok{       ├── datasource\_do.csv}
\ExtensionTok{│}\NormalTok{       └── datasource/}
\ExtensionTok{│}\NormalTok{           ├── datasource{-}set{-}1.csv}
\ExtensionTok{│}\NormalTok{           └── datasource{-}set{-}2.csv}
\ExtensionTok{├──}\NormalTok{ reports/}
\ExtensionTok{│}\NormalTok{   ├── figures/}
\ExtensionTok{│}\NormalTok{   ├── tables/}
\ExtensionTok{│}\NormalTok{   ├── manuscript/}
\ExtensionTok{│}\NormalTok{   └── presentation/}
\ExtensionTok{├──}\NormalTok{ DESCRIPTION}
\ExtensionTok{├──}\NormalTok{ Makefile}
\ExtensionTok{└──}\NormalTok{ README}
\end{Highlighting}
\end{Shaded}

\end{example}

Discuss \texttt{ggplot2::ggsave()} and
\texttt{kableExtra::save\_kable()} for saving figures and tables,
respectively.

\section{Reproducibility}\label{sec-contr-reproducibility}

Whether for other researchers or for your future self, creating research
that is well-documented and reproducible is a fundamental part of
conducting modern scientific inquiry. In this chapter we will emphasize
the importance of this endeavor and outline strategies for ensuring your
research project is reproducible. This will include directory and file
structure, key documentation files as well as how to effectively use
existing software resources and frameworks for publishing your research
(either for private use, journal requirements, or general public
consumption) on popular repositories such as GitHub and Open Science
Framework (OSF).

Reproducibility is the ability to recreate the results of a research
project. This is a fundamental part of the scientific process and is a
key component of the scientific method. Reproducibility is important for
several reasons. First, it allows other researchers to verify the
results of a study. Second, it allows other researchers to build on the
results of a study. Third, it allows researchers to revisit their own
work at a later date. Fourth, it allows researchers to share their work
with others. Finally, it allows researchers to use their work as a
template for future projects.

\begin{itemize}
\tightlist
\item
  Introduction
\item
  The importance of well-documented and reproducible research in modern
  scientific inquiry.

  \begin{itemize}
  \tightlist
  \item
    Ensuring accuracy and reliability of results.
  \item
    Facilitating knowledge sharing and collaboration.
  \end{itemize}
\item
  The role of collaboration in ensuring research reproducibility.

  \begin{itemize}
  \tightlist
  \item
    Combining expertise from different fields.
  \item
    Peer review and feedback.
  \end{itemize}
\item
  Developing a Comprehensive Research Plan
\item
  Organizing your project files for clarity and easy navigation.

  \begin{itemize}
  \tightlist
  \item
    Creating a logical folder structure.
  \item
    Naming conventions for files and folders.
  \end{itemize}
\item
  Writing essential documentation files (README, LICENSE, CONTRIBUTING,
  etc.).

  \begin{itemize}
  \tightlist
  \item
    README: project overview and setup instructions.
  \item
    LICENSE: specifying usage rights and restrictions.
  \item
    CONTRIBUTING: guidelines for collaborators to contribute to the
    project.
  \end{itemize}
\item
  Creating a detailed research protocol: its importance, components, and
  best practices.

  \begin{itemize}
  \tightlist
  \item
    Clear objectives and hypotheses.
  \item
    Detailed methodology, including materials, procedures, and analysis
    plans.
  \item
    Ethical considerations and approval, if applicable.
  \end{itemize}
\item
  Study pre-registration: concept, benefits, and steps for
  pre-registering a study.

  \begin{itemize}
  \tightlist
  \item
    Reducing publication bias and promoting transparency.
  \item
    Process of registering study design, hypotheses, and analysis plans.
  \item
    Platforms for pre-registration.
  \end{itemize}
\item
  Leveraging Open Resources and Tools
\item
  Utilizing open data and open source software: benefits, finding,
  using, and contributing to resources.

  \begin{itemize}
  \tightlist
  \item
    Cost-effectiveness and flexibility.
  \item
    Building upon existing work and contributing back to the community.
  \end{itemize}
\item
  An overview of popular software frameworks for managing research
  projects.

  \begin{itemize}
  \tightlist
  \item
    Project management tools like Trello, Asana, and Basecamp.
  \item
    Data analysis tools like R.
  \item
    Note-taking and reference management tools like Zotero.
  \end{itemize}
\item
  Version control in collaborative research projects: introduction to
  Git and other version control systems.

  \begin{itemize}
  \tightlist
  \item
    Tracking changes and maintaining a history of project files.

    \begin{itemize}
    \tightlist
    \item
      \href{https://git-lfs.com/}{Git large file storage (LFS)} for
      managing large files.
    \end{itemize}
  \item
    Collaborating efficiently with multiple researchers.
  \item
    Resolving conflicts in file versions.
  \end{itemize}
\item
  Publishing and Collaborating on Research Projects
\item
  Introduction to popular repositories like GitHub and Open Science
  Framework (OSF).

  \begin{itemize}
  \tightlist
  \item
    Hosting and sharing code, data, and documentation.
  \item
    Version control and collaboration features.
  \end{itemize}
\item
  Strategies for uploading and organizing your research project for
  private use, journal requirements, or public consumption.

  \begin{itemize}
  \tightlist
  \item
    Preparing files for upload: cleaning data, anonymizing sensitive
    information, and compressing large files.
  \item
    Creating a clear and informative repository structure.
  \end{itemize}
\item
  Collaboration best practices

  \begin{itemize}
  \tightlist
  \item
    Copying a repository

    \begin{itemize}
    \tightlist
    \item
      Forking a repository and setting up the local environment.
    \item
      Cloning, configuring Git, and adding upstream remote repository.
    \end{itemize}
  \item
    Editing, syncing, and committing changes

    \begin{itemize}
    \tightlist
    \item
      Creating branches, making changes, and committing locally.
    \item
      Fetching updates from the original repository and merging.
    \item
      Handling merge conflicts and syncing with the original repository.
    \end{itemize}
  \item
    Suggesting and reviewing changes

    \begin{itemize}
    \tightlist
    \item
      Pushing local changes to the forked repository.
    \item
      Creating, reviewing, discussing, and merging pull requests
    \item
      Participating in discussions, addressing feedback, and deleting
      branches after merging.
    \end{itemize}
  \end{itemize}
\item
  Conclusion
\item
  Recap of the key points discussed in the chapter.

  \begin{itemize}
  \tightlist
  \item
    Importance of well-documented and reproducible research.
  \item
    Developing a comprehensive research plan.
  \item
    Leveraging open resources and tools.
  \item
    Publishing and collaborating on research projects.
  \end{itemize}
\item
  Encouragement to prioritize collaboration, transparency, and
  reproducibility in future research endeavors.

  \begin{itemize}
  \tightlist
  \item
    The benefits of embracing these principles for individual
    researchers and the scientific community as a whole.
  \end{itemize}
\end{itemize}

Project structure (minimal) (reference to Approaching Analysis)

\begin{verbatim}
project/
├── data/
...
\end{verbatim}

Add description of each file/ folder:

\begin{itemize}
\tightlist
\item
  *\_main.R*

  \begin{itemize}
  \tightlist
  \item
    Use of \texttt{pacman} package to install and load packages
  \item
    Use of \texttt{here} package to set working directory
  \item
    Use of \texttt{source()} for R scripts or
    \texttt{Rscript\ -e\ \textless{}filename\textgreater{}} for R
    scripts
  \item
    Use of \texttt{knitr} package to compile Quarto documents
  \item
    Use of \texttt{sessioninfo} package to record session information
  \end{itemize}
\end{itemize}

Project structure (full) (reference to Approaching Analysis)

Functionality:

\begin{itemize}
\tightlist
\item
  Adds \emph{docs/} folder for documentation that may not figure in the
  main manuscript (e.g., data dictionaries, codebooks, etc.)
\item
  .renv/ folder for package management
\end{itemize}

\begin{verbatim}
project/
├── data/
├── docs/
...
\end{verbatim}

\section*{Activities}\label{activities-9}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}: \textbf{How}: \ldots{} \textbf{Why}: \ldots{}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, left=2mm, breakable, colframe=quarto-callout-color-frame, bottomrule=.15mm, leftrule=.75mm, opacityback=0, rightrule=.15mm, arc=.35mm, toprule=.15mm]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \textbf{How}: \ldots{} \textbf{Why}: \ldots{}

\end{tcolorbox}

\section*{Summary}\label{summary-10}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

\ldots{}

Further reading:

\begin{itemize}
\tightlist
\item
  Rodrigues (\citeproc{ref-Rodrigues2023}{2023})
\item
  Gandrud (\citeproc{ref-Gandrud2020}{2020})
\item
  J. D. Blischak, Carbonetto, and Stephens
  (\citeproc{ref-Blischak2019}{2019})
\item
  Gentleman and Temple Lang (\citeproc{ref-Gentleman2007}{2007})
\item
  Munafò et al. (\citeproc{ref-Munafo2017}{2017})
\end{itemize}

\bookmarksetup{startatroot}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-Ackoff1989}
Ackoff, Russell L. 1989. {``From Data to Wisdom.''} \emph{Journal of
Applied Systems Analysis} 16 (1): 3--9.

\bibitem[\citeproctext]{ref-Adel2020}
Ädel, Annelie. 2020. {``Corpus Compilation.''} In \emph{A Practical
Handbook of Corpus Linguistics}, edited by Magali Paquot and Stefan Th.
Gries, 3--24. Switzerland: Springer.

\bibitem[\citeproctext]{ref-Albert2015}
Albert, Saul, Laura E. de Ruiter, and J. P. de Ruiter. 2015. {``CABNC:
The Jeffersonian Transcription of the Spoken British National Corpus.''}
TalkBank.

\bibitem[\citeproctext]{ref-R-quarto}
Allaire, JJ. 2023. \emph{Quarto: R Interface to Quarto Markdown
Publishing System}. \url{https://github.com/quarto-dev/quarto-r}.

\bibitem[\citeproctext]{ref-Baayen2004}
Baayen, R. Harald. 2004. {``Statistics in Psycholinguistics: A Critique
of Some Current Gold Standards.''} \emph{Mental Lexicon Working Papers}
1 (1): 1--47.

\bibitem[\citeproctext]{ref-Baayen2008a}
---------. 2008. \emph{Analyzing Linguistic Data: A Practical
Introduction to Statistics Using r}. Cambridge Univ Pr.

\bibitem[\citeproctext]{ref-Baayen2011}
---------. 2011. {``Corpus Linguistics and Naive Discriminative
Learning.''} \emph{Revista Brasileira de
Lingu{\textbackslash{}}'{\textbackslash{}}istica Aplicada} 11 (2):
295--328.

\bibitem[\citeproctext]{ref-Baayen2006}
Baayen, R. Harald, L. B. Feldman, and R. Schreuder. 2006.
{``Morphological Influences on the Recognition of Monosyllabic
Monomorphemic Words.''} \emph{Journal of Memory and Language} 55:
290--313. \url{https://doi.org/10.1016/j.jml.2006.03.008}.

\bibitem[\citeproctext]{ref-R-languageR}
Baayen, R. H., and Elnaz Shafaei-Bajestan. 2019. \emph{languageR:
Analyzing Linguistic Data: A Practical Introduction to Statistics}.
\url{https://CRAN.R-project.org/package=languageR}.

\bibitem[\citeproctext]{ref-Baker2016}
Baker, Monya. 2016. {``1,500 Scientists Lift the Lid on
Reproducibility.''} \emph{Nature} 533 (7604): 452--54.
\url{https://doi.org/10.1038/533452a}.

\bibitem[\citeproctext]{ref-Bao2019}
Bao, Wang, Ning Lianju, and Kong Yue. 2019. {``Integration of
Unsupervised and Supervised Machine Learning Algorithms for Credit Risk
Assessment.''} \emph{Expert Systems with Applications} 128 (August):
301--15. \url{https://doi.org/10.1016/j.eswa.2019.02.033}.

\bibitem[\citeproctext]{ref-R-future}
Bengtsson, Henrik. 2023. \emph{Future: Unified Parallel and Distributed
Processing in r for Everyone}. \url{https://future.futureverse.org}.

\bibitem[\citeproctext]{ref-R-stopwords}
Benoit, Kenneth, David Muhr, and Kohei Watanabe. 2021. \emph{Stopwords:
Multilingual Stopword Lists}.
\url{https://github.com/quanteda/stopwords}.

\bibitem[\citeproctext]{ref-R-readtext}
Benoit, Kenneth, and Adam Obeng. 2023. \emph{Readtext: Import and
Handling for Plain and Formatted Text Files}.
\url{https://github.com/quanteda/readtext}.

\bibitem[\citeproctext]{ref-Blischak2019}
Blischak, John D., Peter Carbonetto, and Matthew Stephens. 2019.
{``Creating and Sharing Reproducible Research Code the Workflowr Way.''}
\emph{F1000Research} 8.

\bibitem[\citeproctext]{ref-R-workflowr}
Blischak, John, Peter Carbonetto, and Matthew Stephens. 2023.
\emph{Workflowr: A Framework for Reproducible and Collaborative Data
Science}. \url{https://github.com/workflowr/workflowr}.

\bibitem[\citeproctext]{ref-R-wordbankr}
Braginsky, Mika. 2023. \emph{Wordbankr: Accessing the Wordbank
Database}. \url{https://langcog.github.io/wordbankr/}.

\bibitem[\citeproctext]{ref-R-infer}
Bray, Andrew, Chester Ismay, Evgeni Chasnovski, Simon Couch, Ben Baumer,
and Mine Cetinkaya-Rundel. 2024. \emph{Infer: Tidy Statistical
Inference}. \url{https://github.com/tidymodels/infer}.

\bibitem[\citeproctext]{ref-Bresnan2007a}
Bresnan, Joan. 2007. {``A Few Lessons from Typology.''} \emph{Linguistic
Typology} 11 (1): 297--306.

\bibitem[\citeproctext]{ref-Bresnan2007}
Bresnan, Joan, Anna Cueni, Tatiana Nikitina, and R. Harald Baayen. 2007.
{``Predicting the Dative Alternation.''} In \emph{Cognitive Foundations
of Interpretation}, edited by G. Bouma, I. Kraemer, and Jan-Wouter C
Zwart, 1--33. Amsterdam: KNAW.

\bibitem[\citeproctext]{ref-Brown2005}
Brown, Keith. 2005. \emph{Encyclopedia of Language and Linguistics}.
Vol. 1. Elsevier.

\bibitem[\citeproctext]{ref-R-reprex}
Bryan, Jennifer, Jim Hester, David Robinson, Hadley Wickham, and
Christophe Dervieux. 2024. \emph{Reprex: Prepare Reproducible Example
Code via the Clipboard}. \url{https://reprex.tidyverse.org}.

\bibitem[\citeproctext]{ref-Buckheit1995}
Buckheit, Jonathan B., and David L. Donoho. 1995. {``Wavelab and
Reproducible Research.''} In \emph{Wavelets and Statistics}, 55--81.
Springer.

\bibitem[\citeproctext]{ref-Bychkovska2017}
Bychkovska, Tetyana, and Joseph J. Lee. 2017. {``At the Same Time:
Lexical Bundles in L1 and L2 University Student Argumentative
Writing.''} \emph{Journal of English for Academic Purposes} 30
(November): 38--52. \url{https://doi.org/10.1016/j.jeap.2017.10.008}.

\bibitem[\citeproctext]{ref-Campbell2001}
Campbell, Lyle. 2001. {``The History of Linguistics.''} In \emph{The
Handbook of Linguistics}, edited by Mark Aronoff and Janie Rees-Miller,
81--104. Blackwell Handbooks in Linguistics. Blackwell Publishers.

\bibitem[\citeproctext]{ref-Carmi2020}
Carmi, Elinor, Simeon J. Yates, Eleanor Lockley, and Alicja Pawluczuk.
2020. {``Data Citizenship: Rethinking Data Literacy in the Age of
Disinformation, Misinformation, and Malinformation.''} \emph{Internet
Policy Review} 9 (2).

\bibitem[\citeproctext]{ref-Chambers2020}
Chambers, John M. 2020. {``S, r, and Data Science.''} \emph{Proceedings
of the ACM on Programming Languages} 4 (HOPL): 1--17.
\url{https://doi.org/10.1145/3386334}.

\bibitem[\citeproctext]{ref-Chan2014}
Chan, Sin-wai. 2014. \emph{Routledge Encyclopedia of Translation
Technology}. Routledge.

\bibitem[\citeproctext]{ref-Conway2012}
Conway, Lucian Gideon, Laura Janelle Gornick, Chelsea Burfeind, Paul
Mandella, Andrea Kuenzli, Shannon C. Houck, and Deven Theresa Fullerton.
2012. {``Does Complex or Simple Rhetoric Win Elections? An Integrative
Complexity Analysis of u.s. Presidential Campaigns.''} \emph{Political
Psychology} 33 (5): 599--618.
\url{https://doi.org/10.1111/j.1467-9221.2012.00910.x}.

\bibitem[\citeproctext]{ref-Cross2006}
Cross, Nigel. 2006. {``Design as a Discipline.''} \emph{Designerly Ways
of Knowing}, 95--103.

\bibitem[\citeproctext]{ref-R-remotes}
Csárdi, Gábor, Jim Hester, Hadley Wickham, Winston Chang, Martin Morgan,
and Dan Tenenbaum. 2023. \emph{Remotes: R Package Installation from
Remote Repositories, Including GitHub}. \url{https://remotes.r-lib.org}.

\bibitem[\citeproctext]{ref-DataNeverSleeps08-2021}
{``Data Never Sleeps 7.0 Infographic.''} 2019.
https://www.domo.com/learn/infographic/data-never-sleeps-7.

\bibitem[\citeproctext]{ref-Deshors2016}
Deshors, Sandra C, and Stefan Th. Gries. 2016. {``Profiling Verb
Complementation Constructions Across New Englishes.''}
\emph{International Journal of Corpus Linguistics.} 21 (2): 192--218.

\bibitem[\citeproctext]{ref-Desjardins2019}
Desjardins, Jeff. 2019. {``How Much Data Is Generated Each Day?''}
\emph{Visual Capitalist}.

\bibitem[\citeproctext]{ref-Donoho2017}
Donoho, David. 2017. {``50 Years of Data Science.''} \emph{Journal of
Computational and Graphical Statistics} 26 (4): 745--66.
\url{https://doi.org/10.1080/10618600.2017.1384734}.

\bibitem[\citeproctext]{ref-Dubnjakovic2010}
Dubnjakovic, Ana, and Patrick Tomlin. 2010. \emph{A Practical Guide to
Electronic Resources in the Humanities}. Elsevier.

\bibitem[\citeproctext]{ref-Duran2004}
Duran, P. 2004. {``Developmental Trends in Lexical Diversity.''}
\emph{Applied Linguistics} 25 (2): 220--42.
\url{https://doi.org/10.1093/applin/25.2.220}.

\bibitem[\citeproctext]{ref-Eisenstein2012}
Eisenstein, Jacob, Brendan O'Connor, Noah A Smith, and Eric P Xing.
2012. {``Mapping the Geographical Diffusion of New Words.''}
\emph{Computation and Language}, 1--13.
\url{https://doi.org/10.1371/journal.pone.0113114}.

\bibitem[\citeproctext]{ref-Firth1957}
Firth, John R. 1957. \emph{Papers in Linguistics}. Oxford University
Press.

\bibitem[\citeproctext]{ref-Francom2022}
Francom, Jerid. 2022. {``Corpus Studies of Syntax.''} In \emph{The
Cambridge Handbook of Experimental Syntax}, edited by Grant Goodall,
687--713. Cambridge Handbooks in Language and Linguistics. Cambridge
University Press.

\bibitem[\citeproctext]{ref-R-qtalrkit}
---------. 2023. \emph{Qtalrkit: Quantitative Text Analysis for
Linguists Resource Kit}. \url{https://github.com/qtalr/qtalrkit}.

\bibitem[\citeproctext]{ref-Gandrud2015}
Gandrud, Christopher. 2015.
\emph{\href{https://www.ncbi.nlm.nih.gov/pubmed/17811671}{Reproducible
Research with r and r Studio}}. Second edition. CRC Press.

\bibitem[\citeproctext]{ref-Gandrud2020}
---------. 2020. \emph{Reproducible Research with r and RStudio}. Third
edition. The r Series. Boca Raton, FL: CRC Press.

\bibitem[\citeproctext]{ref-Garg2018}
Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018.
{``Word Embeddings Quantify 100 Years of Gender and Ethnic
Stereotypes.''} \emph{Proceedings of the National Academy of Sciences}
115 (16): E3635--44. \url{https://doi.org/10.1073/pnas.1720347115}.

\bibitem[\citeproctext]{ref-Gentleman2007}
Gentleman, Robert, and Duncan Temple Lang. 2007. {``Statistical Analyses
and Reproducible Research.''} \emph{Journal of Computational and
Graphical Statistics} 16 (1): 1--23.

\bibitem[\citeproctext]{ref-Gilquin2009}
Gilquin, Gaëtanelle, and Stefan Th Gries. 2009. {``Corpora and
Experimental Methods: A State-of-the-Art Review.''} \emph{Corpus
Linguistics and Linguistic Theory} 5 (1): 1--26.
\url{https://doi.org/10.1515/CLLT.2009.001}.

\bibitem[\citeproctext]{ref-Gomez-Uribe2015}
Gomez-Uribe, Carlos A., and Neil Hunt. 2015. {``The Netflix Recommender
System: Algorithms, Business Value, and Innovation.''} \emph{ACM
Transactions on Management Information Systems (TMIS)} 6 (4): 1--19.

\bibitem[\citeproctext]{ref-Gries2021a}
Gries, Stefan Th. 2021. \emph{Statistics for Linguistics with r}. De
Gruyter Mouton.

\bibitem[\citeproctext]{ref-Gries2023}
---------. 2023. {``Statistical Methods in Corpus Linguistics.''} In
\emph{Readings in Corpus Linguistics: A Teaching and Research Guide for
Scholars in Nigeria and Beyond,} 78--114.

\bibitem[\citeproctext]{ref-Gries2013a}
Gries, Stefan Th. 2013. \emph{Statistics for Linguistics with r. A
Practical Introduction}. 2nd revise.

\bibitem[\citeproctext]{ref-Gries2014}
Gries, Stefan Th., and Sandra C. Deshors. 2014. {``Using Regressions to
Explore Deviations Between Corpus Data and a Standard/Target: Two
Suggestions.''} \emph{Corpora} 9 (1): 109--36.
\url{https://doi.org/10.3366/cor.2014.0053}.

\bibitem[\citeproctext]{ref-Grieve2018}
Grieve, Jack, Andrea Nini, and Diansheng Guo. 2018. {``Mapping Lexical
Innovation on American Social Media.''} \emph{Journal of English
Linguistics} 46 (4): 293--319.

\bibitem[\citeproctext]{ref-Harris1954}
Harris, Zellig S. 1954. {``Distributional Structure.''} \emph{Word} 10
(2-3): 146--62. \url{https://doi.org/10.1080/00437956.1954.11659520}.

\bibitem[\citeproctext]{ref-Hay2002}
Hay, Jennifer. 2002. {``From Speech Perception to Morphology: Affix
Ordering Revisited.''} \emph{Language} 78 (3): 527--55.

\bibitem[\citeproctext]{ref-Head2015}
Head, Megan L., Luke Holman, Rob Lanfear, Andrew T. Kahn, and Michael D.
Jennions. 2015. {``The Extent and Consequences of p-Hacking in
Science.''} \emph{PLOS Biology} 13 (3): e1002106.
\url{https://doi.org/10.1371/journal.pbio.1002106}.

\bibitem[\citeproctext]{ref-R-fs}
Hester, Jim, Hadley Wickham, and Gábor Csárdi. 2023. \emph{Fs:
Cross-Platform File System Operations Based on Libuv}.
\url{https://fs.r-lib.org}.

\bibitem[\citeproctext]{ref-Hicks2019}
Hicks, Stephanie C., and Roger D. Peng. 2019. {``Elements and Principles
for Characterizing Variation Between Data Analyses.''} arXiv.
\url{https://doi.org/10.48550/arXiv.1903.07639}.

\bibitem[\citeproctext]{ref-R-textrecipes}
Hvitfeldt, Emil. 2023. \emph{Textrecipes: Extra Recipes for Text
Processing}. \url{https://github.com/tidymodels/textrecipes}.

\bibitem[\citeproctext]{ref-Ide2008}
Ide, Nancy, Collin Baker, Christiane Fellbaum, Charles Fillmore, and
Rebecca Passonneau. 2008. {``MASC: The Manually Annotated Sub-Corpus of
American English.''} In \emph{6th International Conference on Language
Resources and Evaluation, LREC 2008}, 2455--60. European Language
Resources Association (ELRA).

\bibitem[\citeproctext]{ref-Ignatow2017}
Ignatow, Gabe, and Rada Mihalcea. 2017. \emph{An Introduction to Text
Mining: Research Design, Data Collection, and Analysis}. Sage
Publications.

\bibitem[\citeproctext]{ref-Jaeger2007}
Jaeger, T Florian, and Neal Snider. 2007. {``Implicit Learning and
Syntactic Persistence: Surprisal and Cumulativity.''} \emph{University
of Rochester Working Papers in the Language Sciences} 3 (1).

\bibitem[\citeproctext]{ref-Johnson2008}
Johnson, K. 2008. \emph{Quantitative Methods in Linguistics}. Blackwell
Pub.

\bibitem[\citeproctext]{ref-R-gibasa}
Kato, Akiru, Shogo Ichinose, and Taku Kudo. 2023. \emph{Gibasa: An
Alternative Rcpp Wrapper of MeCab}.
\url{https://paithiov909.github.io/gibasa/}.

\bibitem[\citeproctext]{ref-Kaur2018}
Kaur, Jashanjot, and P. Kaur Buttar. 2018. {``A Systematic Review on
Stopword Removal Algorithms.''} \emph{International Journal on Future
Revolution in Computer Science \& Communication Engineering} 4 (4):
207--10.

\bibitem[\citeproctext]{ref-Kerr1998}
Kerr, Norbert L. 1998. {``HARKing: Hypothesizing After the Results Are
Known.''} \emph{Personality and Social Psychology Review} 2 (3):
196--217.

\bibitem[\citeproctext]{ref-Kloumann2012}
Kloumann, IM, CM Danforth, KD Harris, and CA Bliss. 2012. {``Positivity
of the English Language.''} \emph{PloS One}.

\bibitem[\citeproctext]{ref-Koehn2005}
Koehn, P. 2005. {``Europarl: A Parallel Corpus for Statistical Machine
Translation.''} \emph{MT Summit X}, 12--16.

\bibitem[\citeproctext]{ref-Kostic2003}
Kostić, Aleksandar, Tanja Marković, and Aleksandar Baucal. 2003.
{``Inflectional Morphology and Word Meaning: Orthogonal or
Co-Implicative Cognitive Domains?''} In \emph{Morphological Structure in
Language Processing}, edited by R. Harald Baayen and Robert Schreuder,
1--44. De Gruyter Mouton. \url{https://doi.org/10.1515/9783110910186.1}.

\bibitem[\citeproctext]{ref-R-TBDBr}
Kowalski, John, and Rob Cavanaugh. 2022. \emph{TBDBr: Easy Access to
TalkBankDB via r API}. \url{https://github.com/TalkBank/TalkBankDB-R}.

\bibitem[\citeproctext]{ref-Krathwohl2002}
Krathwohl, David R. 2002. {``A Revision of Bloom's Taxonomy: An
Overview.''} \emph{Theory into Practice} 41 (4): 212--18.

\bibitem[\citeproctext]{ref-R-swirl}
Kross, Sean, Nick Carchedi, Bill Bauer, and Gina Grdina. 2020.
\emph{Swirl: Learn r, in r}. \url{http://swirlstats.com}.

\bibitem[\citeproctext]{ref-Kucera1967}
Kucera, H, and W N Francis. 1967. \emph{Computational Analysis of
Present Day American English}. Brown University Press Providence.

\bibitem[\citeproctext]{ref-R-targets}
Landau, William Michael. 2024. \emph{Targets: Dynamic Function-Oriented
Make-Like Declarative Pipelines}.
\url{https://docs.ropensci.org/targets/}.

\bibitem[\citeproctext]{ref-Leech1992}
Leech, Geoffrey. 1992. {``100 Million Words of English: The British
National Corpus (BNC),''} no. 1991: 1--13.

\bibitem[\citeproctext]{ref-Lewis2004}
Lewis, Michael. 2004. \emph{Moneyball: The Art of Winning an Unfair
Game}. WW Norton \& Company.

\bibitem[\citeproctext]{ref-Liu2021}
Liu, Kanglong, and Muhammad Afzaal. 2021. {``Syntactic Complexity in
Translated and Non-Translated Texts: A Corpus-Based Study of
Simplification.''} Edited by Diego Raphael Amancio. \emph{PLOS ONE} 16
(6): e0253454. \url{https://doi.org/10.1371/journal.pone.0253454}.

\bibitem[\citeproctext]{ref-Lozano2009}
Lozano, Cristóbal. 2009. {``CEDEL2: Corpus Escrito Del Espa{ñ}ol L2.''}
\emph{Applied Linguistics Now: Understanding Language and Mind/La
Ling{ü}{í}stica Aplicada Hoy: Comprendiendo El Lenguaje y La Mente.
Almer{í}a: Universidad de Almer{í}a}, 197--212.

\bibitem[\citeproctext]{ref-Macwhinney2003}
Macwhinney, Brian. 2003. {``TalkBank.''} Repository. \emph{The TalkBank
System}. https://www.talkbank.org/.

\bibitem[\citeproctext]{ref-Manning2003}
Manning, Christopher. 2003. {``Probabilistic Syntax.''} In
\emph{Probabilistic Linguistics}, edited by Bod, Jennifer Hay, and
Jannedy, 289--341. Cambridge, MA: MIT Press.

\bibitem[\citeproctext]{ref-Marcus1993}
Marcus, Mitchell P, Beatrice Santorini, and Mary Ann Marcinkiewicz.
1993. {``Building a Large Annotated Corpus of English: The Penn
Treebank.''} \emph{Computational Linguistics} 19 (2): 313--30.

\bibitem[\citeproctext]{ref-Marwick2018}
Marwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. {``Packaging
Data Analytical Work Reproducibly Using r (and Friends).''} \emph{The
American Statistician} 72 (1): 80--88.

\bibitem[\citeproctext]{ref-Mikolov2013b}
Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. {``Distributed Representations of Words and Phrases and
Their Compositionality.''} In \emph{Advances in Neural Information
Processing Systems}, 3111--19.

\bibitem[\citeproctext]{ref-R-lingtypology}
Moroz, George. 2024. \emph{Lingtypology: Linguistic Typology and
Mapping}. \url{https://CRAN.R-project.org/package=lingtypology}.

\bibitem[\citeproctext]{ref-Morris2019}
Morris, Tim P., Ian R. White, and Michael J. Crowther. 2019. {``Using
Simulation Studies to Evaluate Statistical Methods.''} \emph{Statistics
in Medicine} 38 (11): 2074--2102.
\url{https://doi.org/10.1002/sim.8086}.

\bibitem[\citeproctext]{ref-Mosteller1963}
Mosteller, Frederick, and David L Wallace. 1963. {``Inference in an
Authorship Problem.''} \emph{Journal of the American Statistical
Association} 58 (302): 275--309.
\url{https://www.jstor.org/stable/2283270}.

\bibitem[\citeproctext]{ref-R-tokenizers}
Mullen, Lincoln. 2022. \emph{Tokenizers: Fast, Consistent Tokenization
of Natural Language Text}. \url{https://docs.ropensci.org/tokenizers/}.

\bibitem[\citeproctext]{ref-Munafo2017}
Munafò, Marcus R, Brian A Nosek, Dorothy V M Bishop, Katherine S Button,
Christopher D Chambers, Nathalie Percie, Uri Simonsohn, and Eric-jan
Wagenmakers. 2017. {``A Manifesto for Reproducible Science.''}
\emph{Nature} 1: 1--9. \url{https://doi.org/10.1038/s41562-016-0021}.

\bibitem[\citeproctext]{ref-Munoz2006}
Muñoz, Carmen, ed. 2006. \emph{Age and the Rate of Foreign Language
Learning}. 1st ed. Vol. 19. Second Language Acquisition Series.
Clevedon: Multilingual Matters.

\bibitem[\citeproctext]{ref-Nivre2020}
Nivre, Joakim, Marie-Catherine De Marneffe, Filip Ginter, Jan Hajič,
Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis
Tyers, and Daniel Zeman. 2020. {``Universal Dependencies V2: An
Evergrowing Multilingual Treebank Collection.''} \emph{arXiv Preprint
arXiv:2004.10643}. \url{https://arxiv.org/abs/2004.10643}.

\bibitem[\citeproctext]{ref-Olohan2008}
Olohan, Maeve. 2008. {``Leave It Out! Using a Comparable Corpus to
Investigate Aspects of Explicitation in Translation.''} \emph{Cadernos
de Tradu{ç}{ã}o}, 153--69.

\bibitem[\citeproctext]{ref-R-jsonlite}
Ooms, Jeroen. 2023. \emph{Jsonlite: A Simple and Robust JSON Parser and
Generator for r}.
\href{https://jeroen.r-universe.dev/jsonlite\%0Ahttps://arxiv.org/abs/1403.2805}{https://jeroen.r-universe.dev/jsonlite
https://arxiv.org/abs/1403.2805}.

\bibitem[\citeproctext]{ref-Paquot2020a}
Paquot, Magali, and Stefan Th. Gries, eds. 2020. \emph{A Practical
Handbook of Corpus Linguistics}. Switzerland: Springer.

\bibitem[\citeproctext]{ref-Petrenz2011}
Petrenz, Philipp, and Bonnie Webber. 2011. {``Stable Classification of
Text Genres.''} \emph{Computational Linguistics} 37 (2): 385--93.
\url{https://doi.org/10.1162/COLI_a_00052}.

\bibitem[\citeproctext]{ref-R-DBI}
R Special Interest Group on Databases (R-SIG-DB), Hadley Wickham, and
Kirill Müller. 2024. \emph{DBI: R Database Interface}.
\url{https://dbi.r-dbi.org}.

\bibitem[\citeproctext]{ref-Riehemann2001}
Riehemann, Susanne Z. 2001. {``A Constructional Approach to Idioms and
Word Formation.''} PhD thesis, Stanford.

\bibitem[\citeproctext]{ref-R-lexicon}
Rinker, Tyler. 2019. \emph{Lexicon: Lexicons for Text Analysis}.
\url{https://github.com/trinker/lexicon}.

\bibitem[\citeproctext]{ref-R-tidytext}
Robinson, David, and Julia Silge. 2023. \emph{Tidytext: Text Mining
Using Dplyr, Ggplot2, and Other Tidy Tools}.
\url{https://github.com/juliasilge/tidytext}.

\bibitem[\citeproctext]{ref-Rodrigues2023}
Rodrigues, Bruno. 2023. \emph{Building Reproducible Analytical Pipelines
with r}.

\bibitem[\citeproctext]{ref-Rossman2014a}
Rossman, Allan J., and Beth L. Chance. 2014. {``Using Simulation-Based
Inference for Learning Introductory Statistics.''} \emph{WIREs
Computational Statistics} 6 (4): 211--21.
\url{https://doi.org/10.1002/wics.1302}.

\bibitem[\citeproctext]{ref-Rowley2007}
Rowley, Jennifer. 2007. {``The Wisdom Hierarchy: Representations of the
DIKW Hierarchy.''} \emph{Journal of Information Science} 33 (2):
163--80. \url{https://doi.org/10.1177/0165551506070706}.

\bibitem[\citeproctext]{ref-Saxena2020}
Saxena, Shweta, and Manasi Gyanchandani. 2020. {``Machine Learning
Methods for Computer-Aided Breast Cancer Diagnosis Using Histopathology:
A Narrative Review.''} \emph{Journal of Medical Imaging and Radiation
Sciences} 51 (1): 182--93.

\bibitem[\citeproctext]{ref-Sedgwick2015}
Sedgwick, Philip. 2015. {``Units of Sampling, Observation, and
Analysis.''} \emph{BMJ (Online)} 351 (October): h5396.
\url{https://doi.org/10.1136/bmj.h5396}.

\bibitem[\citeproctext]{ref-Shriberg1994}
Shriberg, Elizabeth Ellen. 1994. {``Preliminaries to a Theory of Speech
Disfluencies.''} PhD thesis, Citeseer.

\bibitem[\citeproctext]{ref-R-janeaustenr}
Silge, Julia. 2022. \emph{Janeaustenr: Jane Austen's Complete Novels}.
\url{https://github.com/juliasilge/janeaustenr}.

\bibitem[\citeproctext]{ref-Szmrecsanyi2004}
Szmrecsanyi, Benedikt. 2004. {``On Operationalizing Syntactic
Complexity.''} In \emph{Le Poids Des Mots. Proceedings of the 7th
International Conference on Textual Data Statistical Analysis.
Louvain-La-Neuve}, 2:1032--39.

\bibitem[\citeproctext]{ref-Tottie2011}
Tottie, Gunnel. 2011. {``Uh and Um as Sociolinguistic Markers in British
English.''} \emph{International Journal of Corpus Linguistics} 16 (2):
173--97.

\bibitem[\citeproctext]{ref-Tottie2014}
---------. 2014. {``On the Use of Uh and Um in American English.''}
\emph{Functions of Language} 21 (1): 6--29.
\url{https://doi.org/10.1075/fol.21.1.02tot}.

\bibitem[\citeproctext]{ref-SWDA2008}
University of Colorado Boulder. 2008. {``Switchboard Dialog Act Corpus.
Web Download.''} Linguistic Data Consortium.

\bibitem[\citeproctext]{ref-R-washoku}
Uryu, Shinya. 2024. \emph{Washoku: Extra 'Recipes' for Japanese Text,
Date and Address Processing}.

\bibitem[\citeproctext]{ref-Voigt2017}
Voigt, Rob, Nicholas P. Camp, Vinodkumar Prabhakaran, William L.
Hamilton, Rebecca C. Hetey, Camilla M. Griffiths, David Jurgens, Dan
Jurafsky, and Jennifer L. Eberhardt. 2017. {``Language from Police Body
Camera Footage Shows Racial Disparities in Officer Respect.''}
\emph{Proceedings of the National Academy of Sciences} 114 (25):
6521--26.

\bibitem[\citeproctext]{ref-R-skimr}
Waring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia,
Hao Zhu, and Shannon Ellis. 2022. \emph{Skimr: Compact and Flexible
Summaries of Data}. \url{https://docs.ropensci.org/skimr/}.

\bibitem[\citeproctext]{ref-R-rsyntax}
Welbers, Kasper, and Wouter van Atteveldt. 2022. \emph{Rsyntax: Extract
Semantic Relations from Text by Querying and Reshaping Syntax}.

\bibitem[\citeproctext]{ref-R-jiebaR}
Wenfeng, Qin, and Wu Yanyi. 2019. \emph{jiebaR: Chinese Text
Segmentation}. \url{https://github.com/qinwf/jiebaR/}.

\bibitem[\citeproctext]{ref-R-ProjectTemplate}
White, John Myles. 2023. \emph{ProjectTemplate: Automates the Creation
of New Statistical Analysis Projects}. \url{http://projecttemplate.net}.

\bibitem[\citeproctext]{ref-Wickham2014a}
Wickham, Hadley. 2014. {``Tidy Data.''} \emph{Journal of Statistical
Software} 59 (10). \url{https://doi.org/10.18637/jss.v059.i10}.

\bibitem[\citeproctext]{ref-R-stringr}
---------. 2023a. \emph{Stringr: Simple, Consistent Wrappers for Common
String Operations}. \url{https://stringr.tidyverse.org}.

\bibitem[\citeproctext]{ref-R-tidyverse}
---------. 2023b. \emph{Tidyverse: Easily Install and Load the
Tidyverse}. \url{https://tidyverse.tidyverse.org}.

\bibitem[\citeproctext]{ref-R-rvest}
---------. 2024. \emph{Rvest: Easily Harvest (Scrape) Web Pages}.
\url{https://rvest.tidyverse.org/}.

\bibitem[\citeproctext]{ref-R-ggplot2}
Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen,
Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey
Dunnington. 2023. \emph{Ggplot2: Create Elegant Data Visualisations
Using the Grammar of Graphics}. \url{https://ggplot2.tidyverse.org}.

\bibitem[\citeproctext]{ref-R-dplyr}
Wickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis
Vaughan. 2023. \emph{Dplyr: A Grammar of Data Manipulation}.
\url{https://dplyr.tidyverse.org}.

\bibitem[\citeproctext]{ref-R-dbplyr}
Wickham, Hadley, Maximilian Girlich, and Edgar Ruiz. 2023. \emph{Dbplyr:
A Dplyr Back End for Databases}. \url{https://dbplyr.tidyverse.org/}.

\bibitem[\citeproctext]{ref-R-purrr}
Wickham, Hadley, and Lionel Henry. 2023. \emph{Purrr: Functional
Programming Tools}. \url{https://purrr.tidyverse.org/}.

\bibitem[\citeproctext]{ref-R-readr}
Wickham, Hadley, Jim Hester, and Jennifer Bryan. 2024. \emph{Readr: Read
Rectangular Text Data}. \url{https://readr.tidyverse.org}.

\bibitem[\citeproctext]{ref-R-haven}
Wickham, Hadley, Evan Miller, and Danny Smith. 2023. \emph{Haven: Import
and Export SPSS, Stata and SAS Files}.
\url{https://haven.tidyverse.org}.

\bibitem[\citeproctext]{ref-R-tidyr}
Wickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2024.
\emph{Tidyr: Tidy Messy Data}. \url{https://tidyr.tidyverse.org}.

\bibitem[\citeproctext]{ref-R-udpipe}
Wijffels, Jan. 2023. \emph{Udpipe: Tokenization, Parts of Speech
Tagging, Lemmatization and Dependency Parsing with the UDPipe 'NLP'
Toolkit}. \url{https://bnosac.github.io/udpipe/en/index.html}.

\bibitem[\citeproctext]{ref-Wulff2007}
Wulff, S, A Stefanowitsch, and Stefan Th. Gries. 2007. {``Brutal Brits
and Persuasive Americans.''} \emph{Aspects of Meaning}.

\bibitem[\citeproctext]{ref-R-bookdown}
Xie, Yihui. 2023a. \emph{Bookdown: Authoring Books and Technical
Documents with r Markdown}. \url{https://github.com/rstudio/bookdown}.

\bibitem[\citeproctext]{ref-R-tinytex}
---------. 2023b. \emph{Tinytex: Helper Functions to Install and
Maintain TeX Live, and Compile LaTeX Documents}.
\url{https://github.com/rstudio/tinytex}.

\bibitem[\citeproctext]{ref-Zipf1949}
Zipf, George Kingsley. 1949. \emph{Human Behavior and the Principle of
Least Effort}. Oxford, England: Addison-Wesley Press.

\end{CSLReferences}

\cleardoublepage
\phantomsection
\addcontentsline{toc}{part}{Appendices}
\appendix

\chapter{Data}\label{data}

\section{ANC}\label{sec-data-anc}

\begin{itemize}
\tightlist
\item
  ANC
\end{itemize}

\section{BNC}\label{sec-data-brown}

\begin{itemize}
\tightlist
\item
  Brown Corpus
\end{itemize}

\section{CABNC}\label{sec-data-cabnc}

\begin{itemize}
\tightlist
\item
  CABNC
\end{itemize}

\section{CEDEL2}\label{sec-data-cedel2}

\begin{itemize}
\tightlist
\item
  CEDEL2
\end{itemize}

\section{ENNTT}\label{sec-data-enntt}

\begin{itemize}
\tightlist
\item
  ENNTT
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  Europarl
\item
  Federalist Papers (LOC)
\item
  SOTU
\item
  SWDA
\item
  \ldots{}
\end{itemize}



\end{document}
