---
execute: 
  echo: true
---

# Curate datasets {#sec-curate-datasets}

```{r}
#| label: setup-options
#| child: "_common.qmd"
#| cache: false
```

::: {.callout-caution title="Caution"}
{{< fa route >}} In progress...
:::

<!-- 

Content:

- [ ] adding a description of the importance of the 'unit of analysis' in the structuring of the curated dataset
  - Note: this is touched upon in the 'Foundations' part, but it is important to see how this is implemented in the curation process. 
  - Also it should be mentioned that many times you want to keep the unit of observation and sometimes the unit of analysis somewhat flexible, so that you can explore different ways of framing the data (especially in the case of IDA approaches).
- [ ] the unit of analysis is tied with the research question --so consider how to frame the curation process for the various data/ datasets so that the unit of analysis
  - [ ] Note: the unit of observation is where we are going in the next chapter with "Transform datasets". So we need to make sure that we are clear about the distinction between the two.

Exercises:

- [ ] add concept questions to Activities
- [ ] add exercises to Activities
- [ ] add thought questions/ case studies to prose sections

Formatting:

- [ ] cite the packages used
- [ ] cite the europarl Corpus 
- [ ] correct the file names for Switchboard to reflect the dataset_curated.csv, and dataset_data_dictionary.csv format

-->

> The hardest bit of information to extract is the first piece.
>
> --- Robert Ferrigno

::: {.callout}
**{{< fa regular list-alt >}} Outcomes**

<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- [ ] change to outcomes -->

- what are some of the formats that data can take?
- what R programming strategies are used to read these formats into tabular, tidy dataset structures?
- what is the importance of maintaining modularity between data and data processing in a reproducible research project?
:::

```{r}
#| eval: false
#| label: curate-data-packages
#| message: false
#| echo: false

# [ ] confirm packages used in this chapter, that are not in the _common.qmd file

# Packages
pacman::p_load(readtext)
```

- [ ] Revise: to reflect the new structure/ content of the chapter

In this chapter we will now look at the next step in a text analysis project: data curation. That is, the process of converting the original data we acquire to a tidy dataset. Acquired data can come in a wide variety of formats. These formats tend to signal the richness of the metadata that is included in the file content. In this chapter we will consider three general types of content formats: (1) unstructured data, (2) structured data, and (3) semi-structured data. Regardless of the file type and the structure of the data, it will be necessary to consider how to curate a dataset that such that the structure reflects the basic the unit of analysis that we wish to investigate. The resulting dataset will form the base from which we will work to further transform the dataset such that it aligns with the unit(s) of observation required for the analysis method(s) that we will implement. Once the dataset is curated, we will create a data dictionary that describes the dataset and the variables that are included in the dataset for transparency and reproducibility.  

::: {.callout}
**{{< fa terminal >}} Swirl lesson**

<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- Learn how to use regular expressions to match patterns in text using the *stringr* package. Learn how to manipulate data using the `dplyr` package. -->
**What**: [Pattern Matching and Manipulate Datasets](https://github.com/qtalr/lessons)\
**How**: In the R Console pane load `swirl`, run `swirl()`, and follow prompts to select the lesson.\
**Why**: To familiarize yourself with the basics of using the pattern matching syntax Regular Expressions and the `dplyr` package to manipulate datasets.
:::

## Unstructured

The bulk of text ever created is of the unstructured variety. Unstructured data is data that has not been organized to make the information contained within machine-readable. Remember that text in itself is not information. Only when given explicit context does text become informative. The explicit contextual information that is included with data is called metadata. Metadata can be linguistic or non-linguistic in nature. So for unstructured data there is little to no metadata directly associated with the data. This information needs to be added or derived for the purposes of the research, either through manual inspection or (semi-)automatic processes. For now, however, our job is just to get the unstructured data into a structured format with a minimal set of metadata that we can derive from the resource.

### Orientation

As an example of an unstructured source of corpus data, let's take a look at the [Europarl Parallel Corpus](https://www.statmt.org/europarl/) [@Koehn2005]. This corpus contains parallel texts (source and translated documents) from the European Parliamentary proceedings between 1996-2011 for some 21 European languages. 

Let's assuming we selected this corpus data because we are interested in researching Spanish to English translations. After consulting the corpus website, downloading the compressed file, and inspecting the decompressed structure, we have the the file structure seen in @exm-cd-europarl-file-structure. 

::: {#exm-cd-europarl-file-structure}
```bash
project/
├── code/
│   ├── 1-acquire-data.qmd
│   ├── 2-curate-data.qmd
│   └── ...
├── data/
│   ├── analysis/
│   ├── derived/
│   └── original/
│       │── europarl_do.csv
│       └── europarl/
│           ├── europarl-v7.es-en.en
│           └── europarl-v7.es-en.es
├── output/
│   ├── figures/
│   ├── reports/
│   ├── results/
│   └── tables/
├── README.md
└── _main.R
```
:::

The *europarl_do.csv* file contains the data origin information documented as part of the acquisition process. The contents are seen in @tbl-cd-europarl-data-origin.

```{r}
#| label: tbl-cd-europarl-data-origin
#| tbl-cap: "Data origin: Europarl Corpus"
#| echo: false

# Read in the data origin file and display it as a table
read_csv("data/curate-datasets/cd-europarl_do.csv") |>
  kable() |>
  kable_styling()
```

```{r}
#| label: cd-acquire-europarl
#| eval: false
#| echo: false

get_compressed_data(
  url = "https://www.statmt.org/europarl/v7/es-en.tgz",
  target_dir = "../data/original/europarl",
  confirmed = TRUE
)
```

<!-- [x] Update directory structure and reference to the data -->

Now let's get familiar with the corpus directory structure and the files. In @exm-cd-europarl-file-structure, we see that there are two corpus files, *europarl-v7.es-en.es* and *europarl-v7.es-en.en*, that contain the source and target language texts, respectively. The file names indicate that the files contain Spanish-English parallel texts. The *.es* and *.en* extensions indicate the language of the text.

Looking at the beginning of the *.es* and *.en* files, in @exm-cd-europarl-sample-source and @exm-cd-europarl-sample-target, we see that the files contain a series of lines in either the source or target language. 

::: {layout="[50, -1, 50]" layout-valign="top" layout-align="left"}

::: {#exm-cd-europarl-sample-source}
*europarl-v7.es-en.es* sample
```xml
Reanudación del período de sesiones
Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.
Como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.
Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.
A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.
```
:::

::: {#exm-cd-europarl-sample-target}
*europarl-v7.es-en.en* sample
```xml
Resumption of the session
I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.
Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.
You have requested a debate on this subject in the course of the next few days, during this part-session.
In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.
```
:::
:::

We can clearly appreciate that the data is unstructured. That is, there is no explicit metadata associated with the data. The data is just a series of lines. The only information that we can surmise from structure of the data is that the texts are line-aligned and that the data in each file corresponds to source and target languages.

Now, before embarking on a data curation process, it is recommendable to define the structure of the data that we want to create. I call this the "idealized structure" of the data. For a curated dataset there are two considerations. First, we want to maintain the original structure of the data as much as possible. This provides a link to the original data and gives us a default dataset structure from which we can derive other structures that fit our analysis needs. Second, we want to add structure to the data that makes it easier to work with. This is the 'tidy' part of the data curation process. The aim is to embue the dataset with as much of the metadata the data resource makes available.

Given what we know about the data, we can define the idealized structure of the data as seen in @tbl-cd-europarl-structure-example.

```{r}
#| label: tbl-cd-europarl-structure-example
#| tbl-cap: "Idealized structure for the europarl Corpus dataset."
#| echo: false

# [ ] I need to review the use of kable() across the book, as I think I am using it in different ways
#    - kable()
#    - kable(booktabs = TRUE)
#    - kable(booktabs = TRUE) |> kable_styling()
#    - kable(booktabs = TRUE) |> kable_styling(latex_options = c("striped", "hold_position"))

tribble(
  ~doc_id, ~type, ~line_id, ~line,
  1, "Source", 1, "...line from source language",
  2, "Source", 2, "...",
  3, "Source", 3, "...",
  4, "Target", 1, "...line from target language",
  5, "Target", 2, "...",
  6, "Target", 3, "..."
) |>
  kable(booktabs = TRUE) |>
  kable_styling(latex_options = c("hold_position"))
```

The dataset structure in @tbl-cd-europarl-structure-example has four columns. The first column, `doc_id` is a unique identifier for each line in the corpus. `type`, indicates whether the line is from the source or target language. The third column, `line_id`, is a unique identifier for each line for each `type`. The last column, `line`, contains the text of the line, and maintains the structure of the original data. The observations are lines.

Our task now is to develop code that will read the original data and render the idealized structure as a curated dataset we will write to the *data/derived/* directory. The code we develop will be added to the *2-curate-data.qmd* file. And finally, the dataset will be documented with a data dictionary file. 

### Tidy the data

<!-- 
- [x] make clear reference to "idealized structure" table, for each of the sections (unstructured, structured, semi-structured)
-->

To create the idealized dataset structure in @tbl-cd-europarl-structure-example, lets's start by reading the files into R. There are many ways to read data, but for illustrative purposes we will use the `readtext()` function from the `readtext` package [@R-readtext]. `readtext()` is a versatile function that can read many different types of text files (*e.g.* *.txt*, *.csv*, *xml*, *etc.*). It can also read multiple files at once using wildcard matching. We will use it to read the *.es* and *.en* files by passing the path to the directory where the files are located, and then use the `*` wildcard to read all the files in the directory. 

```{r}
#| label: cd-europarl-readtext-show
#| eval: false

# Load package
library(readtext)

# Read Europarl files .es and .en
europarl_docs_tbl <-
  readtext("../data/original/europarl/*") |> # read in files
  tibble() # convert to tibble
```

::: {#exm-cd-europarl-readtext}
```{r}
#| label: cd-europarl-readtext-run
#| echo: false
#| message: false

# Load package
library(readtext)

readtext_options(verbosity = 0) # no messages from readtext

# Read Europarl files .es and .en
europarl_docs_tbl <- 
  readtext("data/curate-datasets/europarl/*") |> 
  tibble() 
```
:::

In @exm-cd-europarl-readtext, the `readtext()` function reads all the files in the *../data/curate-datasets/europarl/* directory and returns a tibble.

::: {.callout}
**{{< fa exclamation-triangle >}} Warning**

The `readtext()` function can read many different types of file formats, from structured to unstructured. However, it depends in large part on the extension of the file to recognize what algorithm to use when reading a file. In this particular case, the Europarl files do not have a typical extension (they have `.en` and `.es`). The `readtext()` function will treat them as plain text (`.txt`), but it will throw a warning message. To suppress the warning message you can add the `verbosity = 0` argument or set `readtext_options(verbosity = 0)`.
:::

Let's inspect the `europarl_docs_tbl` object with the `str()` function, in @exm-cd-europarl-readtext-str.

::: {#exm-cd-europarl-readtext-str}
```{r}
#| label: cd-europarl-readtext-str

# Preview data
str(europarl_docs_tbl) 
```
:::

We see that the output from @exm-cd-europarl-readtext-str is a tibble with two columns, `doc_id` and `text`. The `doc_id` column contains the name of the file from which the text was read. The `text` column contains the text of the file. There are two observations, one for each file. 

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

Note that the `str()` function from base R is similar to `glimpse()`. However, `glimpse()` will attempt to show you as much data as possible. In this case since our column `text` is a very long character vector it will take a long time to render. I've chosen the `str()` function as it will automatically truncate the data.
:::

<!-- [x] I need to standardize how I refer to packages `readtext` -->

The fact that we only have one row for each file means that all the text in each file is contained in one cell! We want to break these cells up into rows for each line, as they appear in the original data. You may be wondering why the `readtext()` function did not do this for us, after all the original data was already separated into lines. The reason is that the `readtext()` function chose to read the files as plain text, and plain text does not have any structure. The line breaks, however, are still there, they are just represented as a special character, `\n`. Comparing @exm-cd-europarl-text-lines and @exm-cd-europarl-text-lines-raw, we can see that the text is a single long character vector, and that the line breaks are represented as `\n`.

::: {#exm-cd-europarl-text-lines}
```{r}
#| label: cd-europarl-text-lines

# Preview first 50 characters
europarl_docs_tbl |> 
  pull(text) |> 
  str_trunc(50) |> 
  str_view() 
```
:::

::: {#exm-cd-europarl-text-lines-raw}
```{r}
#| label: cd-europarl-text-lines-raw

# Preview first 50 raw characters
europarl_docs_tbl |> 
  pull(text) |> 
  str_trunc(50) |> 
  str_view(use_escapes = TRUE)
```
:::

In @exm-cd-europarl-text-lines, we can see a truncated version of the text as it is in a printed format. The `str_trunc()` function from the `stringr` package [@R-stringr] truncates the text to the first 50 characters. The `str_view()` function from the same package allows us to see the text in a viewer pane. 

In @exm-cd-europarl-text-lines-raw, we can see the raw text, that is, the text as it is stored in the computer. The `use_escapes = TRUE` argument tells the `str_view()` function to show the special characters as they are stored in the computer. This includes the `\n` line-feed character, which represents a line break.

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

The `str_view()` with the argument `use_escapes = TRUE` also allows you to see other special characters, such as `\t` (tab) and `\r` (carriage return) as well as Unicode characters, such as `\u00f3n` (ó), `\u00ed` (í), *etc.*. These characters are not visible in the printed version of the text, but they are there in the raw text.
:::

We can see that the text is a single long character vector, and that the line breaks are represented as `\n`. So our goal is to split the text for each file into lines creating a new row for each line created. 

To do this we will use another function from the stringr package, `str_split()`, whose function is to split a character vector into smaller character vectors based on some splitting criteria. In @exm-cd-europarl-text-lines-tbl, we use the `str_split()` function to split the text into lines based on the `\n` character and assign it to the new column `lines` using `mutate()`. Since we will not need the `text` column anymore, we can use `select()` to drop it.

::: {#exm-cd-europarl-text-lines-tbl}
```{r}
#| label: cd-europarl-text-lines-tbl

# Split text into lines
europarl_lines_tbl <- 
  europarl_docs_tbl |> 
  mutate(lines = str_split(text, "\n")) |> 
  select(-text)

# Preview
europarl_lines_tbl
```
:::

Previewing the output in @exm-cd-europarl-text-lines-tbl, we can see that the `lines` column contains a list of character vectors. Why is this so? `str_split()` takes a character vector and splits it, as we know. It is a vectorized function, meaning that it can take a vector of character vectors and split each one into subsegments. Since any given input character vector can have a different number of subsegments, the number of subsegments in each file could be different. The ideal object for such data is a list. So `str_split()` returns a list of character vectors.

The list of character vectors, lines in our case, is still associated with the respective `doc_id` value. However, we want to create a new row for each line in the list. To do this, we will use the `unnest()` function from the `tidyr` package [@R-tidyr]. The `unnest()` function takes a list column and creates a new row for each element in the list. We use the `unnest()` function to create a new row for each line in the `lines` column. We can see the output from @exm-cd-europarl-text-lines-tbl-unnest.

::: {#exm-cd-europarl-text-lines-tbl-unnest}
```{r}
#| label: cd-europarl-text-lines-tbl-unnest

# Create a new row for each line
europarl_lines_tbl <- 
  europarl_lines_tbl |> 
  unnest(lines)

# Preview
europarl_lines_tbl |> 
  slice_head(n = 5)
```
:::

Remember that the data in the Europarl corpus is aligned, meaning that each line in the source file is aligned with a line in the target file. So we need to make sure that the number of lines in each file is the same. We can do this by grouping the data by `doc_id` and then counting the number of lines in each file. We can do this using the `group_by()` and `count()` functions from the `dplyr` package [@R-dplyr]. We can see the output in @exm-cd-europarl-text-lines-tbl-count.

::: {#exm-cd-europarl-text-lines-tbl-count}
```{r}
#| label: cd-europarl-text-lines-tbl-count

# Count the number of lines in each file
europarl_lines_tbl |> 
  group_by(doc_id) |> 
  count()
```
:::

The output of @exm-cd-europarl-text-lines-tbl-count shows that the number of lines in each file is the same. This is good. If the number of lines in each file was different, we would need to figure out why and fix it.

We now have our `lines` column and the associated observations for our idealized dataset, in @tbl-cd-europarl-structure-example. Let's now leverage the existing `doc_id` to create the `type` column. The goal is to assign the value of `type` according to the `doc_id` value. Specifically, when `doc_id` is 'europarl-v7.es-en.es' type should be 'Source' and when `doc_id` is 'europarl-v7.es-en.en' type should be 'Target'. 

We can do this using the `case_when()` function from the `dplyr` package [@R-dplyr]. The `case_when()` function takes a series of conditions and assigns a value based on the first condition that is met. In this case, we will use `mutate()` to create the `type` column and then use the `doc_id` column to create the conditions and the `type` column to assign the values from `case_when()`. We will store the output in a new object called `europarl_lines_type_tbl`, as seen in @exm-cd-europarl-text-lines-type-tbl.

::: {#exm-cd-europarl-text-lines-type-tbl}
```{r}
#| label: cd-europarl-text-lines-type-tbl

# Create `type` column
europarl_lines_type_tbl <- 
  europarl_lines_tbl |> 
  mutate(type = case_when(
    doc_id == "europarl-v7.es-en.es" ~ "Source",
    doc_id == "europarl-v7.es-en.en" ~ "Target"
  ))

# Preview dataset
glimpse(europarl_lines_type_tbl)
```
:::

The preview output from @exm-cd-europarl-text-lines-type-tbl shows us that we now have three columns, `doc_id`, `lines`, and `type`. The `type` column has been created and the values have been assigned according to the `doc_id` values. 

We can now overwrite the `doc_id` column with a unique identifier for each line. A numeric identifier makes sense. We can do this by using the `mutate()` function to assign `doc_id` a sequential number with `row_number()`, which increments by 1 for each row. We will store the output in a new object called `europarl_lines_type_id_tbl`, as seen in @exm-cd-europarl-text-lines-type-id-tbl.

::: {#exm-cd-europarl-text-lines-type-id-tbl}
```{r}
#| label: cd-europarl-text-lines-type-id-tbl

# Create new `doc_id` column
europarl_lines_type_id_tbl <- 
  europarl_lines_type_tbl |> 
  mutate(doc_id = row_number())

# Preview dataset
glimpse(europarl_lines_type_id_tbl)
```
:::

The preview output from @exm-cd-europarl-text-lines-type-id-tbl shows us that we now have the same three columns, `doc_id`, `lines`, and `type`. However, the values in the `doc_id` column now reflect a unique identifier for each line.

The last step to get to our envisioned dataset structure is to add the `line_id` column which will be calculated by grouping the data by `type` and then assigning a row number to each of the lines in each group. We use the `group_by()` function to perform the grouping as seen in @exm-cd-europarl-text-lines-type-id-line-id-tbl.

::: {#exm-cd-europarl-text-lines-type-id-line-id-tbl}
```{r}
#| label: cd-europarl-text-lines-type-id-line-id-tbl

# Create `line_id` column
europarl_lines_type_id_line_id_tbl <- 
  europarl_lines_type_id_tbl |> 
  group_by(type) |> 
  mutate(line_id = row_number()) |> 
  ungroup()

# Preview dataset
glimpse(europarl_lines_type_id_line_id_tbl)
```
:::

The preview output from @exm-cd-europarl-text-lines-type-id-line-id-tbl shows us that we now have the desired four columns, `doc_id`, `lines`, `type`, and `line_id`. 

Before we get to writing the dataset to disk, let's organized it in a way that the columns are in the order we want and the rows are sorted in a way that makes sense.

Reordering the columns involves using the `select()` function and list the order of the columns. To sort by columns, we will use `arrange()` and specify the columns to order by. We store the results in a more legible object, `europarl_curated_tbl`, as seen in @exm-cd-europarl-curated-tbl.

::: {#exm-cd-europarl-curated-tbl}
```{r}
#| label: cd-europarl-text-lines-type-id-line-id-tbl-reorder

# Reorder columns and sort rows
europarl_curated_tbl <- 
  europarl_lines_type_id_line_id_tbl |> 
  select(doc_id, type, line_id, lines) |> 
  arrange(line_id, type, doc_id)

# Preview dataset
glimpse(europarl_curated_tbl)
```
:::

The preview output from @exm-cd-europarl-curated-tbl shows us that we now have the desired four columns, `doc_id`, `type`, `line_id`, and `lines`. The rows are sorted by `line_id`, `type`, and `doc_id`. This gives us a dataset that reads left to right from document to line oriented attributes and top to bottom by source-target line pairs.

### Write dataset

At this point we have the curated dataset (`europarl_lines_type_id_line_id_tbl`) in a tidy format. This dataset, however, is only in the current R session. We will want to write this dataset to disk so that in the next step of the text analysis workflow (transformation) we will be able to start work on this dataset and make changes as needed to fit our analysis needs. 

We will leverage the project directory structure which has distinct directories for `original/` and `derived/` data(sets), seen in @exm-cd-europarl-write-directory.

::: {#exm-cd-europarl-write-directory}
```bash
data/
│── analysis/
├── derived/
└── original/
    │── europarl_do.csv
    └── europarl/
        ├── europarl-v7.es-en.es
        └── europarl-v7.es-en.en
```
:::

Since this is a tabular, tidy dataset we have various options for the file type to write. Many of these formats are software-specific, such as `*.xlsx` for Microsoft Excel, `*.sav` for SPSS, `*.dta` for Stata, and `*.rds` for R. We will use the `*.csv` format since it is a common format that can be read by many software packages. We will use the `write_csv()` function from the `readr` package to write the dataset to disk.

Now the question is where to save our CSV file. Since our `europarl_curated_tbl` dataset is derived by our work, we will added it to the `derived/` directory. I'll create a `europarl/` directory with `dir_create()` just to keep things organized. 

::: {#exm-cd-unstructured-write-europarl}
```{r}
#| label: cd-unstructured-write-europarl
#| eval: false

# Create the europarl/ directory
dir_create(path = "../data/derived/europarl/")

# Write the curated dataset to disk
write_csv(
  x = europarl_curated_tbl,
  file = "../data/derived/europarl/europarl_curated.csv"
) 
```
:::

After running the code in @exm-cd-unstructured-write-europarl, the directory structure under the `derived/` directory should look like @exm-cd-unstructured-write-europarl-directory.

::: {#exm-cd-unstructured-write-europarl-directory}
```bash
data/
│── analysis/
├── derived/
│   └── europarl/
│       └── europarl_curated.csv
└── original/
    └──europarl
        ├── europarl-v7.es-en.en
        └── europarl-v7.es-en.es
```
:::

### Summary

In this section we worked with unstructured data and looked at how to read the data into an R session and manipulate the data to form a tidy dataset with a few columns that we could derive based on the information we have about the corpus. 

In our discussion we worked step by step to curate the europarl Corpus, adding in intermediate steps for illustration purposes. However, in a more realistic case the code would most likely make more extensive use of piping (`|>`) to reduce the number of intermediate objects and make the code more legible. The final code base would be added to the *2-curate-data.qmd* file in the *code/* directory. 

The final step, as always, is to document the dataset. For datasets the documentation is a data dictionary, as discussed in @sec-ud-data-dictionaries. As with data origin files, you can use spreadsheet software to create and/ or edit the data dictionary. 

In the `qtalrkit` package we have a function, `create_data_dictionary()` that will generate the scaffolding for a data dictionary. The function takes two arguments, `data` and `file_path`. It reads the dataset columns and provides a template for the data dictionary. 

::: {.callout}
**{{< fa medal >}} Dive deeper**

The `create_data_dictionary()` function provides a rudimentary data dictionary template by default. However, you can take advantage of OpenAI's text generation models to generate a more detailed data dictionary for you to edit. To do this create [an OpenAI account](https://platform.openai.com/signup) and [an API key](https://platform.openai.com/account/api-keys) and add this key to your R environment (`Sys.setenv(OPENAI_API_KEY = "sk..."`). Then you can specify the model you would like to use in the function with the `model = ` argument. For example, `model = "gpt-3.5-turbo"` will use the GPT-3.5 Turbo model.
:::

::: {#exm-cd-unstructured-data-dictionary}
```{r}
#| label: cd-unstructured-data-dictionary
#| eval: false 

# Create the data dictionary
create_data_dictionary(
  data = europarl_curated_tbl,
  file_path <- "../data/derived/europarl/europarl_curated_dd.csv"
)
```
:::

An example of the data dictionary for the `europarl_curated_tbl` dataset is shown in @tbl-cd-unstructured-data-dictionary-example.

```{r}
#| label: tbl-cd-unstructured-data-dictionary-example
#| tbl-cap: "Data dictionary for the `europarl_curated_tbl` dataset."
#| echo: false

read_csv("data/curate-datasets/cd-europarl_curated_dd.csv") |> 
  kable() |> 
  kable_styling() 
```

## Structured

- [ ] revise this section: include R package (languageR::datives) and CSV file?
  - [ ] is is not clear that a truly structured dataset would need any curation. What could I say about structured data that would be useful? I could talk about verifying the data structure is as expected.

On the opposite side of the spectrum from unstructured data, structured data includes more metadata information --often much more. The association of metadata with the language to be analyzed means that the data has already be curated to some degree, therefore it is more apt to discuss structured data as a dataset. There are two questions, however, that need to be taken into account. One, logistical question, is what file format the dataset is in and how to we read it into R. And the second, more research-based, is whether the data is curated in a fashion that makes sense for the current research. Let's look at each of these questions briefly and then get to a practical example. 

Formats ... file types

- [ ] only databases, R datasets/ RDS files, and potentially .csv files are structured data. I need to make this clear.

- [ ] add example case(s):
  - [ ] R package datasets 
    - [ ] languageR::dative
    - [ ] janeaustenr::austen_books
    - [ ] ...
  - [ ] R package corpora (quanteda::corpus) + tidytext::tidy()
  - [ ] CABNC CSV files


CSV
There are file formats which are purposely designed for storing structured datasets. Some very common file types are .csv, .xml, .json, etc. The data within these files is explicitly organized. For example, in a .csv file, the dataset structure is represented by delimiting the columns and rows by commas. 

```text
column_1,column_2,column_3
row 1 value 1,row 1 value 2,row 1 value 3
row 2 value 1,row 2 value 2,row 2 value 3
```

When read into R, the .csv file format is converted to a data frame with the appropriate structure. 

```{r}
#| eval: false
#| label: tbl-cd-structured-example-table-csv
#| tbl-cap: "Example .csv file in R"
#| echo: false

csv_file_example <-
  read_csv(file = "data/curate-datasets/csv_file_example.csv")
csv_file_example |>
  knitr::kable(booktabs = TRUE)
```

With an understanding of how the information is encoded in various formats, we can now turn to considerations about how the original dataset is structured and how that structure is to be used for a given research project. The curation process that is reflected in a structured dataset may or may not initially align with the goals of our research either in terms of the type(s) of information or the unit of analysis of the structured dataset. The aim, then, is to take advantage of the information and curate it such that it does align. 

As an example case of curating structured datasets, we will look at ...

- [ ] R package datasets 
  - [ ] languageR::dative
  - [ ] janeaustenr::austen_books
  - [ ] ...
- [ ] R package corpora (quanteda::corpus) + tidytext::tidy()
- [ ] CABNC CSV files

### Orientation


### Read the dataset


### Explore the dataset

```{r}
#| eval: false
#| label: cd-structured-code-explore

# ...
```

### Tidy the datasets

- [ ] Add idealized structure for the dataset(s)

So our objectives are set, let's ...

### Write dataset

We now have a curated dataset that we can write to disk. 

### Summary

Again, to summarize, here is the code that will accomplish the steps we covered in this section on curating structured datasets. 

```{r}
#| eval: false
#| label: cd-structured-code-summary

# ...
```

### Tidy the datasets

- [ ] Add idealized structure for the dataset(s)

So our objectives are set, let's ...

### Write dataset

We now have a curated dataset that we can write to disk. 

### Summary

Again, to summarize, here is the code that will accomplish the steps we covered in this section on curating structured datasets. 

```{r}
#| eval: false
#| label: cd-structured-code-summary-unstructured

# ...
```

## Semi-structured

At this point we have discussed curating unstructured data and structured datasets. Between these two extremes falls semi-structured data. And as the name suggests, it is a hybrid between unstructured and structured data. This means that there will be important structured metadata included with unstructured elements. The file formats and approaches to encoding the structured aspects of the data vary widely from resource to resource and therefore often requires more detailed attention to the structure of the data and often includes more sophisticated programming strategies to curate the data to produce a tidy dataset. 



### Orientation

- [ ] this SWDA data has been mentioned, reference is all that is needed.

As an example we will work with the The Switchboard Dialog Act Corpus (SDAC) which extends the [Switchboard Corpus](https://catalog.ldc.upenn.edu/LDC97S62) with speech act annotation. **(ADD CITATION)**

The main directory structure of the SDAC data looks like this:

```bash
data/
├── derived/
└── original/
    └── sdac/
        ├── README
        ├── doc/
        ├── sw00utt/
        ├── sw01utt/
        ├── sw02utt/
        ├── sw03utt/
        ├── sw04utt/
        ├── sw05utt/
        ├── sw06utt/
        ├── sw07utt/
        ├── sw08utt/
        ├── sw09utt/
        ├── sw10utt/
        ├── sw11utt/
        ├── sw12utt/
        └── sw13utt/
```

The `README` file contains basic information about the resource, the `doc/` directory contains more detailed information about the dialog annotations, and each of the following directories prefixed with `sw...` contain individual conversation files. Here's a peek at internal structure of the first couple directories.

```bash
├── README
├── doc
│   └── manual.august1.html
├── sw00utt
│   ├── sw_0001_4325.utt
│   ├── sw_0002_4330.utt
│   ├── sw_0003_4103.utt
│   ├── sw_0004_4327.utt
│   ├── sw_0005_4646.utt
```

Let's take a look at the first conversation file (`sw_0001_4325.utt`) to see how it is structured.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-preview
#| echo: false
#| comment: ">"
#| linewidth: 0.9

readtext::readtext("data/understanding-data/formats_sdac_sample.txt", verbosity = 0) |> # sw_0001_4325.utt
  pull(text) |>
  cat(fill = TRUE)
```

There are few things to take note of here. First we see that the conversation files have a meta-data header offset from the conversation text by a line of `=` characters. Second the header contains meta-information of various types. Third, the text is interleaved with an annotation scheme. 

Some of the information may be readily understandable, such as the various pieces of meta-data in the header, but to get a better understanding of what information is encoded here let's take a look at the `README` file. In this file we get a birds eye view of what is going on. In short, the data includes 1155 telephone conversations between two people annotated with 42 'DAMSL' dialog act labels. The `README` file refers us to the `doc/manual.august1.html` file for more information on this scheme.

At this point we open the the `doc/manual.august1.html` file in a browser and do some investigation. We find out that 'DAMSL' stands for 'Discourse Annotation and Markup System of Labeling' and that the first characters of each line of the conversation text  correspond to one or a combination of labels for each utterance. So for our first utterances we have:

```plain
o = "Other"
qw = "Wh-Question"
qy^d = "Declarative Yes-No-Question"
+ = "Segment (multi-utterance)"
```

Each utterance is also labeled for speaker ('A' or 'B'), speaker turn ('1', '2', '3', etc.), and each utterance within that turn ('utt1', 'utt2', etc.). There is other annotation provided withing each utterance, but this should be enough to get us started on the conversations.

Now let's turn to the meta-data in the header. We see here that there is information about the creation of the file: 'FILENAME', 'TOPIC', 'DATE', etc. The `doc/manual.august1.html` file doesn't have much to say about this information so I returned to the [LDC Documentation](https://catalog.ldc.upenn.edu/docs/LDC97S62/) and found more information in the [Online Documentation](https://catalog.ldc.upenn.edu/docs/LDC97S62/) section. After some poking around in this documentation I discovered that that meta-data for each speaker in the corpus is found in the `caller_tab.csv` file. This tabular file does not contain column names, but the `caller_doc.txt` does. After inspecting these files manually and comparing them with the information in the conversation file I noticed that the 'FILENAME' information contained three pieces of useful information delimited by underscores `_`. 

```plain
*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*


FILENAME:	4325_1632_1519
TOPIC#:		323
DATE:		920323
TRANSCRIBER:	glp
```

The first information is the document id (`4325`), the second and third correspond to the speaker number: the first being speaker A (`1632`) and the second speaker B (`1519`).

In sum, we have 1155 conversation files. Each file has two parts, a header and text section, separated by a line of `=` characters. The header section contains a 'FILENAME' line which has the document id, and ids for speaker A and speaker B. The text section is annotated with DAMSL tags beginning each line, followed by speaker, turn number, utterance number, and the utterance text. With this knowledge in hand, let's set out to create a tidy dataset with the following column structure:

```{r}
#| label: tbl-cd-semi-sdac-idealized-dataset
#| tbl-cap: "Idealized structure for the SDAC dataset."
#| echo: false

# Sample data (created with `dput()`)

structure(list(doc_id = c("4325", "4325", "4325"), damsl_tag = c(
  "o ",
  "qw ", "qy^d "
), speaker = c("A", "A", "B"), turn_num = c(
  "1",
  "1", "2"
), utterance_num = c("1", "2", "1"), utterance_text = c(
  "Okay.  /",
  "{D So, }", "[ [ I guess, +"
), speaker_id = c(
  "1632", "1632",
  "1519"
)), .Names = c(
  "doc_id", "damsl_tag", "speaker", "turn_num",
  "utterance_num", "utterance_text", "speaker_id"
), row.names = c(
  NA,
  3L
), class = "data.frame") |>
  knitr::kable(booktabs = TRUE)
```

### Tidy the data

- [ ] Add idealized structure for the SWDA dataset

```{r}
#| eval: false
#| label: cd-semi-copy-data
#| include: false

# [ ] careful with this, it depends on the location of the project. It will be best to copy the data to the project ignored data/ directory for this chapter

# copy the original data from the `project_implementation` project
fs::dir_copy(path = "../project_implementation/data/original/sdac/", new_path = "data/curate-datasets/sdac/")
```

Let's begin by reading one of the conversation files into R as a character vector using the `read_lines()` function from the readr package.

```{r}
#| eval: false
#| label: cd-semi-sdac-read-example-file-show

# [ ] is it possible to use readtext, if not, then we need to explain why before jumping in to readr::read_lines()

doc <-
  read_lines(file = "../data/original/sdac/sw00utt/sw_0001_4325.utt") # read a single file as character vector
```

```{r}
#| eval: false
#| label: cd-semi-sdac-read-example-file-run
#| echo: false

doc <-
  read_lines(file = "data/curate-datasets/sdac/sw00utt/sw_0001_4325.utt") # read a single file as character vector
```

To isolate the vector element that contains the document and speaker ids, we use `str_detect()` from the stringr package. This function takes two arguments, a string and a pattern, and returns a logical value, `TRUE` if the pattern is matched or `FALSE` if not. We can use the output of this function, then, to subset the `doc` character vector and only return the vector element (line) that contains `digits_digits_digits` with a regular expression. The expression combines the digit matching operator `\\d` with the `+` operator to match 1 or more contiguous digits. We then separate three groups of `\\d+` with underscores `_`. The result is `\\d+_\\d+_\\d+`. 

```{r}
#| eval: false
#| label: cd-semi-sdac-doc-info-1

doc[str_detect(doc, pattern = "\\d+_\\d+_\\d+")] # isolate pattern
```

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

Regular Expressions are a powerful pattern matching syntax. They are used extensively in text manipulation and we will see them again and again.

To develop regular expressions, it is helpful to have a tool that allows you to interactively test your pattern matching. The stringr package has a handy function `str_view()` and `str_view_all()` which allow for interactive pattern matching. A good website to practice Regular Expressions is [RegEx101](https://regex101.com/). You can also install the regexplain package in R to get access to a useful [RStudio Addin](https://rstudio.github.io/rstudioaddins/). 
:::

The next step is to extract the three digit sequences that correspond to the `doc_id`, `speaker_a_id`, and `speaker_b_id`. First we extract the pattern that we have identified with `str_extract()` and then we can break up the single character vector into multiple parts based on the underscore `_`. The `str_split()` function takes a string and then a pattern to use to split a character vector. It will return a list of character vectors. 

```{r}
#| eval: false
#| label: cd-semi-sdac-doc-info-2

doc[str_detect(doc, "\\d+_\\d+_\\d+")] |> # isolate pattern
  str_extract(pattern = "\\d+_\\d+_\\d+") |> # extract the pattern
  str_split(pattern = "_") # split the character vector
```

- [] I will have introduced lists in earlier material (swirl Objects), at least.

A **list** is a special object type in R. It is an unordered collection of objects whose lengths can differ (contrast this with a data frame which is a collection of objects whose lengths are the same --hence the tabular format). In this case we have a list of length 1, whose sole element is a character vector of length 3 --one element per segment returned from our split. This is a desired result in most cases as if we were to pass multiple character vectors to our `str_split()` function we don't want the results to be conflated as a single character vector blurring the distinction between the individual character vectors. If we *would* like to conflate, or *flatten* a list, we can use the `unlist()` function.

```{r}
#| eval: false
#| label: cd-semi-sdac-doc-info-3

doc[str_detect(doc, "\\d+_\\d+_\\d+")] |> # isolate pattern
  str_extract(pattern = "\\d+_\\d+_\\d+") |> # extract the pattern
  str_split(pattern = "_") |> # split the character vector
  unlist() # flatten the list to a character vector
```

Let's flatten the list in this case, as we have a single character vector, and assign this result to `doc_speaker_info`.

```{r}
#| eval: false
#| label: cd-semi-sdac-doc-info-4

doc_speaker_info <-
  doc[str_detect(doc, "\\d+_\\d+_\\d+")] |> # isolate pattern
  str_extract(pattern = "\\d+_\\d+_\\d+") |> # extract the pattern
  str_split(pattern = "_") |> # split the character vector
  unlist() # flatten the list to a character vector
```

`doc_speaker_info` is now a character vector of length three. Let's subset each of the elements and assign them to meaningful variable names so we can conveniently use them later on in the tidying process.

```{r}
#| eval: false
#| label: cd-semi-sdac-doc-info-5

doc_id <- doc_speaker_info[1] # extract by index
speaker_a_id <- doc_speaker_info[2] # extract by index
speaker_b_id <- doc_speaker_info[3] # extract by index
```

The next step is to isolate the text section extracting it from rest of the document. As noted previously, a sequence of `=` separates the header section from the text section. What we need to do is to index the point in our character vector `doc` where that line occurs and then subset the `doc` from that point until the end of the character vector. Let's first find the point where the `=` sequence occurs. We will again use the `str_detect()` function to find the pattern we are looking for (a contiguous sequence of `=`), but then we will pass the logical result to the `which()` function which will return the element index number of this match. 

```{r}
#| eval: false
#| label: cd-semi-sdac-text

doc |>
  str_detect(pattern = "=+") |> # match 1 or more `=`
  which() # find vector index
```

So for this file `31` is the index in `doc` where the `=` sequence occurs. Now it is important to keep in mind that we are working with a single file from the `sdac/` data. We need to be cautious to not create a pattern that may be matched multiple times in another document in the corpus. As the `=+` pattern will match `=`, or `==`, or `===`, etc. it is not implausible to believe that there might be a `=` character on some other line in one of the other files. Let's update our regular expression to avoid this potential scenario by only matching sequences of three or more `=`. In this case we will make use of the curly bracket operators `{}`. 

```{r}
#| eval: false
#| label: cd-semi-sdac-text-2

doc |>
  str_detect(pattern = "={3,}") |> # match 3 or more `=`
  which() # find vector index
```

We will get the same result for this file, but will safeguard ourselves a bit as it is unlikely we will find multiple matches for `===`, `====`, etc.

`31` is the index for the `=` sequence, but we want the next line to be where we start reading the text section. To do this we increment the index by 1.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-3

text_start_index <-
  doc |>
  str_detect(pattern = "={3,}") |> # match 3 or more `=`
  which() # find vector index
text_start_index <- text_start_index + 1 # increment index by 1
```

The index for the end of the text is simply the length of the `doc` vector. We can use the `length()` function to get this index.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-4

text_end_index <- length(doc)
```

We now have the bookends, so to speak, for our text section. To extract the text we subset the `doc` vector by these indices.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-5

text <- doc[text_start_index:text_end_index] # extract text
head(text) # preview first lines of `text`
```

The text has some extra whitespace on some lines and there are blank lines as well. We should do some cleaning up before moving forward to organize the data. To get rid of the whitespace we use the `str_trim()` function which by default will remove leading and trailing whitespace from each line.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-6

text <- str_trim(text) # remove leading and trailing whitespace
head(text) # preview first lines of `text`
```

To remove blank lines we will use the a logical expression to subset the `text` vector. `text != ""` means return TRUE where lines are not blank, and FALSE where they are.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-7

text <- text[text != ""] # remove blank lines
head(text) # preview first lines of `text`
```

Our first step towards a tidy dataset is to now combine the `doc_id` and each element of `text` in a data frame. 

```{r}
#| eval: false
#| label: tbl-cd-semi-sdac-text-8
#| tbl-cap: "First 5 observations of prelim data curation of the SDAC data."

data <- data.frame(doc_id, text) # tidy format `doc_id` and `text`
slice_head(data, n = 5) |> # preview first lines of `text`
  knitr::kable(booktabs = TRUE)
```

With our data now in a data frame, its time to parse the `text` column and extract the damsl tags, speaker, speaker turn, utterance number, and the utterance text itself into separate columns. To do this we will make extensive use of regular expressions. Our aim is to find a consistent pattern that distinguishes each piece of information from other other text in a given row of `data$text` and extract it. 

The best way to learn regular expressions is to use them. To this end I've included a link to the interactive regular expression practice website [regex101](https://regex101.com).

Open this site and copy the text below into the 'TEST STRING' field.  

```plain
o          A.1 utt1: Okay.  /
qw          A.1 utt2: {D So, }
qy^d          B.2 utt1: [ [ I guess, +
+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /
+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /
qy          A.5 utt1: Does it say something? /
sd          B.6 utt1: I think it usually does.  /
ad          B.6 utt2: You might try, {F uh, }  /
h          B.6 utt3: I don't know,  /
ad          B.6 utt4: hold it down a little longer,  /
```

```{r}
#| eval: false
#| label: fig-cd-regex-101-image
#| echo: false
#| fig-cap: "RegEx101"

knitr::include_graphics("figures/curate-datasets/cd-regex-101.png")
```

Now manually type the following regular expressions into the 'REGULAR EXPRESSION' field one-by-one (each is on a separate line). Notice what is matched as you type and when you've finished typing. You can find out exactly what the component parts of each expression are doing by toggling the top right icon in the window or hovering your mouse over the relevant parts of the expression.

```plain
^.+?\s
[AB]\.\d+
utt\d+
:.+$
```

As you can now see, we have regular expressions that will match the damsl tags, speaker and speaker turn, utterance number, and the utterance text. To apply these expressions to our data and extract this information into separate columns we will make use of the `mutate()` and `str_extract()` functions. `mutate()` will take our data frame and create new columns with values we match and extract from each row in the data frame with `str_extract()`. Notice that `str_extract()` is different than `str_extract_all()`. When we work with `mutate()` each row will be evaluated in turn, therefore we only need to make one match per row in `data$text`. 

I've chained each of these steps in the code below, dropping the original `text` column with `select(-text)`, and overwriting `data` with the results.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-parse

# Extract column information from `text`
data <-
  data |> # current dataset
  mutate(damsl_tag = str_extract(string = text, pattern = "^.+?\\s")) |> # extract damsl tags
  mutate(speaker_turn = str_extract(string = text, pattern = "[AB]\\.\\d+")) |> # extract speaker_turn pairs
  mutate(utterance_num = str_extract(string = text, pattern = "utt\\d+")) |> # extract utterance number
  mutate(utterance_text = str_extract(string = text, pattern = ":.+$")) |> # extract utterance text
  select(-text) # drop the `text` column

glimpse(data) # preview the data set
```

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

One twist you will notice is that regular expressions in R require double backslashes (`\\\\`) where other programming environments use a single backslash (`\\`).
:::

There are a couple things left to do to the columns we extracted from the text before we move on to finishing up our tidy dataset. First, we need to separate the `speaker_turn` column into `speaker` and `turn_num` columns and second we need to remove unwanted characters from the `damsl_tag`, `utterance_num`, and `utterance_text` columns. 

To separate the values of a column into two columns we use the `separate()` function. It takes a column to separate and character vector of the names of the new columns to create. By default the values of the input column will be separated by non-alphanumeric characters. In our case this means the `.` will be our separator.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-organize

data <-
  data |> # current dataset
  separate(
    col = speaker_turn, # source column
    into = c("speaker", "turn_num")
  ) # separate speaker_turn into distinct columns: speaker and turn_num

glimpse(data) # preview the data set
```

To remove unwanted leading or trailing whitespace we apply the `str_trim()` function. For removing other characters we matching the character(s) and replace them with an empty string (`""`) with the `str_replace()` function. Again, I've chained these functions together and overwritten `data` with the results.

```{r}
#| eval: false
#| label: cd-semi-sdac-text-clean

# Clean up column information
data <-
  data |> # current dataset
  mutate(damsl_tag = str_trim(damsl_tag)) |> # remove leading/ trailing whitespace
  mutate(utterance_num = str_replace(string = utterance_num, pattern = "utt", replacement = "")) |> # remove 'utt'
  mutate(utterance_text = str_replace(string = utterance_text, pattern = ":\\s", replacement = "")) |> # remove ': '
  mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace

glimpse(data) # preview the data set
```

To round out our tidy dataset for this single conversation file we will connect the `speaker_a_id` and `speaker_b_id` with speaker A and B in our current dataset adding a new column `speaker_id`. The `case_when()` function does exactly this: allows us to map rows of `speaker` with the value "A" to `speaker_a_id` and rows with value "B" to `speaker_b_id`.

```{r}
#| eval: false
#| label: cd-semi-sdac-speaker-ids

# Link speaker with speaker_id
data <-
  data |> # current dataset
  mutate(speaker_id = case_when( # create speaker_id
    speaker == "A" ~ speaker_a_id, # speaker_a_id value when A
    speaker == "B" ~ speaker_b_id # speaker_b_id value when B
  ))

glimpse(data) # preview the data set
```

We now have the tidy dataset we set out to create. But this dataset only includes one conversation file! We want to apply this code to all 1155 conversation files in the `sdac/` corpus. The approach will be to create a custom function which groups the code we've done for this single file and then iterative send each file from the corpus through this function and combine the results into one data frame. 

Here's the custom function with some extra code to print a progress message for each file when it runs. 

```{r}
#| eval: false
#| label: cd-semi-extract-sdac-metadata-function

# [ ] rename to `extract_swda_data()`.
# [ ] add to `qtalrkit` package, note the convention of `extract_` prefix for curation functions. In combination with `get_compressed_data()` this corpus can be curated with few steps.

extract_sdac_metadata <- function(file) {
  # Function: to read a Switchboard Corpus Dialogue file and extract meta-data
  cat("Reading", basename(file), "...")
  
  # Read `file` by lines
  doc <- read_lines(file) 
  
  # Extract `doc_id`, `speaker_a_id`, and `speaker_b_id`
  doc_speaker_info <- 
    doc[str_detect(doc, "\\d+_\\d+_\\d+")] |> # isolate pattern
    str_extract("\\d+_\\d+_\\d+") |> # extract the pattern
    str_split(pattern = "_") |> # split the character vector
    unlist() # flatten the list to a character vector
  doc_id <- doc_speaker_info[1] # extract `doc_id`
  speaker_a_id <- doc_speaker_info[2] # extract `speaker_a_id`
  speaker_b_id <- doc_speaker_info[3] # extract `speaker_b_id`
  
  # Extract `text`
  text_start_index <- # find where header info stops
    doc |> 
    str_detect(pattern = "={3,}") |> # match 3 or more `=`
    which() # find vector index
  
  text_start_index <- text_start_index + 1 # increment index by 1
  text_end_index <- length(doc) # get the end of the text section
  
  text <- doc[text_start_index:text_end_index] # extract text
  text <- str_trim(text) # remove leading and trailing whitespace
  text <- text[text != ""] # remove blank lines
  
  data <- data.frame(doc_id, text) # tidy format `doc_id` and `text`
  
  # Extract column information from `text`
  data <- 
    data |> 
    mutate(damsl_tag = str_extract(string = text, pattern = "^.+?\\s")) |>  # extract damsl tags
    mutate(speaker_turn = str_extract(string = text, pattern = "[AB]\\.\\d+")) |> # extract speaker_turn pairs
    mutate(utterance_num = str_extract(string = text, pattern = "utt\\d+")) |> # extract utterance number
    mutate(utterance_text = str_extract(string = text, pattern = ":.+$")) |>  # extract utterance text
    select(-text)
  
  # Separate speaker_turn into distinct columns
  data <-
    data |> # current dataset
    separate(col = speaker_turn, # source column
             into = c("speaker", "turn_num")) # separate speaker_turn into distinct columns: speaker and turn_num
  
  # Clean up column information
  data <- 
    data |> 
    mutate(damsl_tag = str_trim(damsl_tag)) |> # remove leading/ trailing whitespace
    mutate(utterance_num = str_replace(string = utterance_num, pattern = "utt", replacement = "")) |> # remove 'utt'
    mutate(utterance_text = str_replace(string = utterance_text, pattern = ":\\s", replacement = "")) |> # remove ': '
    mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace
  
  # Link speaker with speaker_id
  data <- 
    data |> # current dataset
    mutate(speaker_id = case_when( # create speaker_id
      speaker == "A" ~ speaker_a_id, # speaker_a_id value when A
      speaker == "B" ~ speaker_b_id # speaker_b_id value when B
    ))
  cat(" done.\n")
  return(data) # return the data frame object
}
```

As a sanity check we will run the `extract_sdac_metadata()` function on a the conversation file we were just working on to make sure it works as expected.

```{r}
#| eval: false
#| label: cd-extract-sdac-metadata-test-show

extract_sdac_metadata(file = "../data/original/sdac/sw00utt/sw_0001_4325.utt") |>
  glimpse()
```

```{r}
#| eval: false
#| label: cd-extract-sdac-metadata-test-run
#| echo: false

extract_sdac_metadata(file = "data/curate-datasets/sdac/sw00utt/sw_0001_4325.utt") |>
  glimpse()
```

Looks good!

So now it's time to create a vector with the paths to all of the conversation files. `fs::dir_ls()` interfaces with our OS file system and will return the paths to the files in the specified directory. We also add a pattern to match conversation files (`regexp = \\.utt$`) so we don't accidentally include other files in the corpus. `recurse` set to `TRUE` means we will get the full path to each file.

```{r}
#| eval: false
#| label: cd-semi-sdac-list-files-show

sdac_files <-
  fs::dir_ls(
    path = "../data/original/sdac/", # source directory
    recurse = TRUE, # traverse all sub-directories
    type = "file", # only return files
    regexp = "\\.utt$"
  ) # only return files ending in .utt
head(sdac_files) # preview file paths
```

```{r}
#| eval: false
#| label: cd-semi-sdac-list-files-run
#| echo: false
#| results: hide

sdac_files <-
  fs::dir_ls(
    path = "data/curate-datasets/sdac/", # source directory
    recurse = TRUE, # traverse all sub-directories
    type = "file", # only return files
    regexp = "\\.utt$"
  ) # only return files ending in .utt
head(sdac_files)
```

```plain
../data/original/sdac/sw00utt/sw_0001_4325.utt
../data/original/sdac/sw00utt/sw_0002_4330.utt
../data/original/sdac/sw00utt/sw_0003_4103.utt
../data/original/sdac/sw00utt/sw_0004_4327.utt
../data/original/sdac/sw00utt/sw_0005_4646.utt
../data/original/sdac/sw00utt/sw_0006_4108.utt
```

o pass each conversation file in the vector of paths to our conversation files iteratively to the `extract_sdac_metadata()` function we use `map()`. This will apply the function to each conversation file and return a data frame for each. `bind_rows()` will then join the resulting data frames by rows to give us a single tidy dataset for all 1155 conversations. Note there is a lot of processing going on here we have to be patient.

```{r}
#| eval: false
#| label: cd-semi-sdac-tidy-all
#| results: hide

# Read files and return a tidy dataset
sdac <-
  sdac_files |> # pass file names
  map(extract_sdac_metadata) |> # read and tidy iteratively
  bind_rows() # bind the results into a single data frame
```

We now see that we have `nrow(sdac)` observations (individual utterances in this dataset). 

```{r}
#| eval: false
#| label: cd-semi-sdac-preview-complete

glimpse(sdac) # preview complete curated dataset
```

### Write datasets

Again as in the previous cases, we will write this dataset to disk to prepare for the next step in our text analysis project. 

```{r}
#| eval: false
#| label: cd-semi-sdac-write

fs::dir_create(path = "../data/derived/sdac/") # create sdac subdirectory
write_csv(sdac,
  file = "../data/derived/sdac/sdac_curated.csv"
) # write sdac to disk and label as the curated dataset
```

The directory structure now looks like this: 

```bash
data/
├── derived/
│   └── sdac/
│       └── sdac_curated.csv
└── original/
    └── sdac/
        ├── README
        ├── doc/
        ├── sw00utt/
        ├── sw01utt/
        ├── sw02utt/
        ├── sw03utt/
        ├── sw04utt/
        ├── sw05utt/
        ├── sw06utt/
        ├── sw07utt/
        ├── sw08utt/
        ├── sw09utt/
        ├── sw10utt/
        ├── sw11utt/
        ├── sw12utt/
        └── sw13utt/
```

### Summary

In this section we looked at semi-structured data. This type of data often requires the most work to organize into a tidy dataset. We continued to work with many of the R programming strategies introduced to this point in the coursebook. We also made more extensive use of regular expressions to pick out information from a semi-structured document format.

To round out this section I've provided a code summary of the steps involved to conduct the curation of the Switchboard Dialogue Act Corpus files. Note that I've added the `extract_sdac_metadata()` custom function to a file called `curate_functions.R` and sourced this file. This will make the code more succinct and legible here, as well in your own research projects. 

```{r}
#| eval: false
#| label: cd-semi-structured-code-summary

# Source the `extract_sdac_metadata()` function
source("../functions/curate_functions.R")

# Get list of the corpus files (.utt)
sdac_files <-
  fs::dir_ls(
    path = "../data/original/sdac/", # source directory
    recurse = TRUE, # traverse all sub-directories
    type = "file", # only return files
    regexp = "\\.utt$"
  ) # only return files ending in .utt

# Read files and return a tidy dataset
sdac <-
  sdac_files |> # pass file names
  map(extract_sdac_metadata) |> # read and tidy iteratively
  bind_rows() # bind the results into a single data frame

# Write curated dataset to disk
fs::dir_create(path = "../data/derived/sdac/") # create sdac subdirectory
write_csv(sdac,
  file = "../data/derived/sdac/sdac_curated.csv"
) # write sdac to disk and label as the curated dataset
```

## Documentation

- [ ] consider whether to leave this section, or integrate into the previous sections (most likely, the first section 'unstructured data')
  - [ ] note that the description of what a data dictionary is for and is in it, has already been covered in a previous chapter.
  - [ ] however, the function `data_dic_starter()` is new and should be introduced (somewhere), again, probably in the first section 'unstructured data'

At this stage we again want to ensure that the data that we have derived is well-documented. Where in the data acquisition process the documentation was focused on the sampling frame, curated datasets require documentation that describes the structure of the now rectangular dataset and its attributes. This documentation is known as a **data dictionary**. At the curation stage this documentation often contains the following information [@HowMakeData10-2021]: 

- names of the variables (as they appear in the dataset)
- human-readable names for the variables
- short prose descriptions of the variables, including units of measurement (where applicable)

A data dictionary will take the format of a table and can be stored in a tabular-oriented file format (such as .csv). It is often easier to work with a spreadsheet to create this documentation. I suggest creating a .csv file with the basic structure of the documentation. You can do this however you choose, but I suggest using something along these lines as seen in the following custom function, `data_dic_starter()`. 

```{r}
#| eval: false
#| label: cd-documentation-dic-starter-function"

data_dic_starter <- function(data, file_path) {
  # Function:
  # Creates a .csv file with the basic information
  # to document a curated dataset
  
  tibble(variable_name = names(data), # column with existing variable names 
       name = "", # column for human-readable names
       description = "") |> # column for prose description
  write_csv(file = file_path) # write to disk
}
```

Running this function in the R Console on the curated dataset (in this case the `sdac` dataset), will provide this structure. 

```{r}
#| eval: false
#| label: tbl-cd-documentation-dic-starter-structure
#| tbl-cap: "Data dictionary starter structure for the SDAC curated dataset."
#| echo: false

tibble(variable_name = names(sdac), name = "", description = "") |>
  knitr::kable(booktabs = TRUE)
```

The resulting .csv file can then be opened with spreadsheet software (such as MS Excel, Google Sheets, etc.) and edited.^[Note on RStudio Cloud you will need to download the .csv file and, after editing, upload the complete data dictionary file. Make sure to save the edited file as a .csv file.]

```{r}
#| eval: false
#| label: cd-documentation-starter-edit"
#| echo: false

knitr::include_graphics("figures/curate-datasets/cd-data-dictionary.png")
```

Save this file as a .csv file and replace the original starter file. Note that it is important to use a plain-text file format for the official documentation file and avoid proprietary formats to ensure open accessibility and future compatibility.^[Although based on spreadsheets, @Broman2018 outlines many of the best for good data organization regardless of the technology.]

Our `data/derived/` directory now looks like this.

```bash
data/
└── derived/
    └── sdac/
        ├── sdac_curated.csv
        └── data_dictionary_sdac.csv
```

## Summary {.unnumbered}

In this chapter we looked at the process of structuring data into a dataset. This included a discussion on three main types of data --unstructured, structured, and semi-structured. The level of structure of the original data(set) will vary from resource to resource and by the same token so will the file format used to support the level of meta-information included. The results from our data curation resulted in a curated dataset that is saved separate from the original data to maintain modularity between what the data(set) looked like before we intervene and afterwards. In addition to the code we use to derived the curated dataset's structure, we also include a data dictionary which documents the names of the variables and provides sufficient description of these variables so that it is clear what our dataset contains.

- [ ] emphasize the importance of ahereing to the unit of analysis for the research, and how this is reflected in the curated dataset. And how unit of observation derived from the transformational steps in the next chapter.

It is important to recognized that this curated dataset will form the base for the next step in our text analysis project and the last step in data preparation for analysis: dataset transformation. This last step in preparing data for analysis is to convert this curated dataset into a dataset that is directly aligned with the research aims (i.e. analysis method(s)) of the project. Since there can be multiple analysis approaches applied the original data in a research project, this curated dataset serves as the point of departure for each of the subsequent datasets derived from the transformational steps. 

## Activities {.unnumbered}

::: {.callout}
**{{< fa regular file-code >}} Recipe**

<!-- Understand, apply, and analyze verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- [ ] edit links and goals/ outcomes -->

**What**: [Organizing and documenting datasets](https://lin380.github.io/tadr/articles/recipe_7.html)\
**How**: Read Recipe 7 and participate in the Hypothes.is online social annotation.\
**Why**: To rehearse methods for deriving tidying datasets to use a the base for further project-specific purposes. We will explore how regular expressions are helpful in developing strategies for matching, extracting, and/ or replacing patterns in character sequences and how to change the dimensions of a dataset to either expand or collapse columns or rows.
:::

::: {.callout}
**{{< fa flask >}} Lab**

<!-- Analyze, evaluate, and create verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- [ ] edit links and goals/ outcomes -->

<!-- [ ] update name of lab to something more lively -->

**What**: [Pattern Matching and Manipulate Datasets](https://github.com/lin380/lab_7)\
**How**: Clone, fork, and complete the steps in Lab 7.\
**Why**: To gain experience working with coding strategies reshaping data using tidyverse functions and regular expressions, to practice reading/ writing data from/ to disk, and to
implement organizational strategies for organizing and documenting a dataset in reproducible fashion.
:::

## Questions {.unnumbered}

::: {.callout}
{{< fa wrench >}} **Conceptual questions**

1. ...
2. ...

:::

::: {.callout}
{{< fa wrench >}} **Technical questions**

1. ...
2. ...
:::
