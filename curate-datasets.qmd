---
execute: 
  echo: true
---

# Curate datasets {#sec-curate-datasets}

```{r}
#| label: setup-options
#| child: "_common.qmd"
#| cache: false
```

::: {.callout-caution title="Caution"}
{{< fa route >}} In progress...
:::

<!-- 

Content:

- [ ] adding a description of the importance of the 'unit of analysis' in the structuring of the curated dataset
  - Note: this is touched upon in the 'Foundations' part, but it is important to see how this is implemented in the curation process. 
  - Also it should be mentioned that many times you want to keep the unit of observation and sometimes the unit of analysis somewhat flexible, so that you can explore different ways of framing the data (especially in the case of IDA approaches).
- [ ] the unit of analysis is tied with the research question --so consider how to frame the curation process for the various data/ datasets so that the unit of analysis
  - [ ] Note: the unit of observation is where we are going in the next chapter with "Transform datasets". So we need to make sure that we are clear about the distinction between the two.

Exercises:

- [ ] add concept questions to Activities
- [ ] add exercises to Activities
- [ ] add thought questions/ case studies to prose sections

Formatting:

- [ ] cite the packages used
- [ ] cite the europarl Corpus 
- [ ] correct the file names for Switchboard to reflect the dataset_curated.csv, and dataset_data_dictionary.csv format

-->

> The hardest bit of information to extract is the first piece.
>
> --- Robert Ferrigno

::: {.callout}
**{{< fa regular list-alt >}} Outcomes**

<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- [ ] change to outcomes -->

- what are some of the formats that data can take?
- what R programming strategies are used to read these formats into tabular, tidy dataset structures?
- what is the importance of maintaining modularity between data and data processing in a reproducible research project?
:::

```{r}
#| eval: false
#| label: curate-data-packages
#| message: false
#| echo: false

# [ ] confirm packages used in this chapter, that are not in the _common.qmd file

# Packages
pacman::p_load(readtext)
```

- [ ] Revise: to reflect the new structure/ content of the chapter

In this chapter we will now look at the next step in a text analysis project: data curation. That is, the process of converting the original data we acquire to a tidy dataset. Acquired data can come in a wide variety of formats. These formats tend to signal the richness of the metadata that is included in the file content. In this chapter we will consider three general types of content formats: (1) unstructured data, (2) structured data, and (3) semi-structured data. Regardless of the file type and the structure of the data, it will be necessary to consider how to curate a dataset that such that the structure reflects the basic the unit of analysis that we wish to investigate. The resulting dataset will form the base from which we will work to further transform the dataset such that it aligns with the unit(s) of observation required for the analysis method(s) that we will implement. Once the dataset is curated, we will create a data dictionary that describes the dataset and the variables that are included in the dataset for transparency and reproducibility.  

::: {.callout}
**{{< fa terminal >}} Swirl lesson**

<!-- Remember and understand verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- Learn how to use regular expressions to match patterns in text using the *stringr* package. Learn how to manipulate data using the `dplyr` package. -->
**What**: [Pattern Matching and Manipulate Datasets](https://github.com/qtalr/lessons)\
**How**: In the R Console pane load `swirl`, run `swirl()`, and follow prompts to select the lesson.\
**Why**: To familiarize yourself with the basics of using the pattern matching syntax Regular Expressions and the `dplyr` package to manipulate datasets.
:::

## Unstructured

The bulk of text ever created is of the unstructured variety. Unstructured data is data that has not been organized to make the information contained within machine-readable. Remember that text in itself is not information. Only when given explicit context does text become informative. The explicit contextual information that is included with data is called metadata. Metadata can be linguistic or non-linguistic in nature. So for unstructured data there is little to no metadata directly associated with the data. This information needs to be added or derived for the purposes of the research, either through manual inspection or (semi-)automatic processes. For now, however, our job is just to get the unstructured data into a structured format with a minimal set of metadata that we can derive from the resource.

### Reading data

Let's consider some of the common file formats which contain unstructured data, such as TXT, PDF, and DOCX, and how to read these formats into an R session. 

It is very common for unstructured data resources to include data that is in TXT files. These are the simplest of file formats which contain no formatting or explicit metadata. Many times TXT files have the *.txt* extension, but it is not required. There are many ways to read TXT files into R and many packages that can be used to do so. For example, using the `readr` package, we can choose to read the entire file into a single vector of character strings with `read_file()` or read the file by lines with `read_lines()` in which each line is a character string in a vector. 

Less commonly used in prepared data resources, PDF and DOCX files are also formats for unstructured data. These formats are often more complex than TXT files as they contain formatting and embedded metadata. However, these attributes are primarily for visual presentation and not for machine-readability. Therefore, we need to use packages and functions that can extract the text content from these files and potentially some of the metadata. For example, using the `readtext` package [@R-readtext], we can read the text content from PDF and DOCX files into a single vector of character strings with `readtext()`.

Whether in TXT, PDF, or DOCX format, the resulting data structure will require further processing to convert the data into a tidy dataset. For example, we may need to split the data into multiple columns or rows, or extract metadata from the text content.

### Orientation

As an example of curating an unstructured source of corpus data, let's take a look at the [Europarl Parallel Corpus](https://www.statmt.org/europarl/) [@Koehn2005]. This corpus contains parallel texts (source and translated documents) from the European Parliamentary proceedings between 1996-2011 for some 21 European languages. 

Let's assuming we selected this corpus data because we are interested in researching Spanish to English translations. After consulting the corpus website, downloading the compressed file, and inspecting the decompressed structure, we have the the file structure seen in @exm-cd-europarl-file-structure. 

::: {#exm-cd-europarl-file-structure}
```bash
project/
├── code/
│   ├── 1-acquire-data.qmd
│   ├── 2-curate-data.qmd
│   └── ...
├── data/
│   ├── analysis/
│   ├── derived/
│   └── original/
│       │── europarl_do.csv
│       └── europarl/
│           ├── europarl-v7.es-en.en
│           └── europarl-v7.es-en.es
├── output/
│   ├── figures/
│   ├── reports/
│   ├── results/
│   └── tables/
├── README.md
└── _main.R
```
:::

The *europarl_do.csv* file contains the data origin information documented as part of the acquisition process. The contents are seen in @tbl-cd-europarl-data-origin.

```{r}
#| label: tbl-cd-europarl-data-origin
#| tbl-cap: "Data origin: Europarl Corpus"
#| echo: false

# Read in the data origin file and display it as a table
read_csv("data/curate-datasets/cd-europarl_do.csv") |>
  kable() |>
  kable_styling()
```

```{r}
#| label: cd-acquire-europarl
#| eval: false
#| echo: false

get_compressed_data(
  url = "https://www.statmt.org/europarl/v7/es-en.tgz",
  target_dir = "../data/original/europarl",
  confirmed = TRUE
)
```

<!-- [x] Update directory structure and reference to the data -->

Now let's get familiar with the corpus directory structure and the files. In @exm-cd-europarl-file-structure, we see that there are two corpus files, *europarl-v7.es-en.es* and *europarl-v7.es-en.en*, that contain the source and target language texts, respectively. The file names indicate that the files contain Spanish-English parallel texts. The *.es* and *.en* extensions indicate the language of the text.

Looking at the beginning of the *.es* and *.en* files, in @exm-cd-europarl-sample-source and @exm-cd-europarl-sample-target, we see that the files contain a series of lines in either the source or target language. 

::: {layout="[50, -1, 50]" layout-valign="top" layout-align="left"}

::: {#exm-cd-europarl-sample-source}
*europarl-v7.es-en.es* sample
```xml
Reanudación del período de sesiones
Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.
Como todos han podido comprobar, el gran "efecto del año 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.
Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.
A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.
```
:::

::: {#exm-cd-europarl-sample-target}
*europarl-v7.es-en.en* sample
```xml
Resumption of the session
I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.
Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.
You have requested a debate on this subject in the course of the next few days, during this part-session.
In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.
```
:::
:::

We can clearly appreciate that the data is unstructured. That is, there is no explicit metadata associated with the data. The data is just a series of lines. The only information that we can surmise from structure of the data is that the texts are line-aligned and that the data in each file corresponds to source and target languages.

Now, before embarking on a data curation process, it is recommendable to define the structure of the data that we want to create. I call this the "idealized structure" of the data. For a curated dataset there are two considerations. First, we want to maintain the original structure of the data as much as possible. This provides a link to the original data and gives us a default dataset structure from which we can derive other structures that fit our analysis needs. Second, we want to add structure to the data that makes it easier to work with. This is the 'tidy' part of the data curation process. The aim is to embue the dataset with as much of the metadata the data resource makes available.

Given what we know about the data, we can define the idealized structure of the data as seen in @tbl-cd-europarl-structure-example.

```{r}
#| label: tbl-cd-europarl-structure-example
#| tbl-cap: "Idealized structure for the europarl Corpus dataset."
#| echo: false

# [ ] I need to review the use of kable() across the book, as I think I am using it in different ways
#    - kable()
#    - kable(booktabs = TRUE)
#    - kable(booktabs = TRUE) |> kable_styling()
#    - kable(booktabs = TRUE) |> kable_styling(latex_options = c("striped", "hold_position"))

tribble(
  ~doc_id, ~type, ~line_id, ~line,
  1, "Source", 1, "...line from source language",
  2, "Source", 2, "...",
  3, "Source", 3, "...",
  4, "Target", 1, "...line from target language",
  5, "Target", 2, "...",
  6, "Target", 3, "..."
) |>
  kable(booktabs = TRUE) |>
  kable_styling(latex_options = c("hold_position"))
```

The dataset structure in @tbl-cd-europarl-structure-example has four columns. The first column, `doc_id` is a unique identifier for each line in the corpus. `type`, indicates whether the line is from the source or target language. The third column, `line_id`, is a unique identifier for each line for each `type`. The last column, `line`, contains the text of the line, and maintains the structure of the original data. The observations are lines.

Our task now is to develop code that will read the original data and render the idealized structure as a curated dataset we will write to the *data/derived/* directory. The code we develop will be added to the *2-curate-data.qmd* file. And finally, the dataset will be documented with a data dictionary file. 

### Tidy the data

<!-- 
- [x] make clear reference to "idealized structure" table, for each of the sections (unstructured, structured, semi-structured)
-->

To create the idealized dataset structure in @tbl-cd-europarl-structure-example, lets's start by reading the files into R. We will use the `readtext()` function from the `readtext` package. `readtext()` is a versatile function that can read many different types of text files (*e.g.* *.txt*, *.csv*, *xml*, *etc.*). It can also read multiple files at once using wildcard matching. We will use it to read the *.es* and *.en* files by passing the path to the directory where the files are located, and then use the `*` wildcard to read all the files in the directory. 

```{r}
#| label: cd-europarl-readtext-show
#| eval: false

# Load package
library(readtext)

# Read Europarl files .es and .en
europarl_docs_tbl <-
  readtext("../data/original/europarl/*") |> # read in files
  tibble() # convert to tibble
```

::: {#exm-cd-europarl-readtext}
```{r}
#| label: cd-europarl-readtext-run
#| echo: false
#| message: false

# Load package
library(readtext)

readtext_options(verbosity = 0) # no messages from readtext

# Read Europarl files .es and .en
europarl_docs_tbl <- 
  readtext("data/curate-datasets/europarl/*") |> 
  tibble() 
```
:::

In @exm-cd-europarl-readtext, the `readtext()` function reads all the files in the *../data/curate-datasets/europarl/* directory and returns a tibble.

::: {.callout}
**{{< fa exclamation-triangle >}} Warning**

The `readtext()` function can read many different types of file formats, from structured to unstructured. However, it depends in large part on the extension of the file to recognize what algorithm to use when reading a file. In this particular case, the Europarl files do not have a typical extension (they have `.en` and `.es`). The `readtext()` function will treat them as plain text (`.txt`), but it will throw a warning message. To suppress the warning message you can add the `verbosity = 0` argument or set `readtext_options(verbosity = 0)`.
:::

Let's inspect the `europarl_docs_tbl` object with the `str()` function, in @exm-cd-europarl-readtext-str.

::: {#exm-cd-europarl-readtext-str}
```{r}
#| label: cd-europarl-readtext-str

# Preview data
str(europarl_docs_tbl) 
```
:::

We see that the output from @exm-cd-europarl-readtext-str is a tibble with two columns, `doc_id` and `text`. The `doc_id` column contains the name of the file from which the text was read. The `text` column contains the text of the file. There are two observations, one for each file. 

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

Note that the `str()` function from base R is similar to `glimpse()`. However, `glimpse()` will attempt to show you as much data as possible. In this case since our column `text` is a very long character vector it will take a long time to render. I've chosen the `str()` function as it will automatically truncate the data.
:::

<!-- [x] I need to standardize how I refer to packages `readtext` -->

The fact that we only have one row for each file means that all the text in each file is contained in one cell! We want to break these cells up into rows for each line, as they appear in the original data. You may be wondering why the `readtext()` function did not do this for us, after all the original data was already separated into lines. The reason is that the `readtext()` function chose to read the files as plain text, and plain text does not have any structure. The line breaks, however, are still there, they are just represented as a special character, `\n`. Comparing @exm-cd-europarl-text-lines and @exm-cd-europarl-text-lines-raw, we can see that the text is a single long character vector, and that the line breaks are represented as `\n`.

::: {#exm-cd-europarl-text-lines}
```{r}
#| label: cd-europarl-text-lines

# Preview first 50 characters
europarl_docs_tbl |> 
  pull(text) |> 
  str_trunc(50) |> 
  str_view() 
```
:::

::: {#exm-cd-europarl-text-lines-raw}
```{r}
#| label: cd-europarl-text-lines-raw

# Preview first 50 raw characters
europarl_docs_tbl |> 
  pull(text) |> 
  str_trunc(50) |> 
  str_view(use_escapes = TRUE)
```
:::

In @exm-cd-europarl-text-lines, we can see a truncated version of the text as it is in a printed format. The `str_trunc()` function from the `stringr` package [@R-stringr] truncates the text to the first 50 characters. The `str_view()` function from the same package allows us to see the text in a viewer pane. 

In @exm-cd-europarl-text-lines-raw, we can see the raw text, that is, the text as it is stored in the computer. The `use_escapes = TRUE` argument tells the `str_view()` function to show the special characters as they are stored in the computer. This includes the `\n` line-feed character, which represents a line break.

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

The `str_view()` with the argument `use_escapes = TRUE` also allows you to see other special characters, such as `\t` (tab) and `\r` (carriage return) as well as Unicode characters, such as `\u00f3n` (ó), `\u00ed` (í), *etc.*. These characters are not visible in the printed version of the text, but they are there in the raw text.
:::

We can see that the text is a single long character vector, and that the line breaks are represented as `\n`. So our goal is to split the text for each file into lines creating a new row for each line created. 

To do this we will use another function from the `stringr` package, `str_split()`, whose function is to split a character vector into smaller character vectors based on some splitting criteria. In @exm-cd-europarl-text-lines-tbl, we use the `str_split()` function to split the text into lines based on the `\n` character and assign it to the new column `lines` using `mutate()`. Since we will not need the `text` column anymore, we can use `select()` to drop it.

::: {#exm-cd-europarl-text-lines-tbl}
```{r}
#| label: cd-europarl-text-lines-tbl

# Split text into lines
europarl_lines_tbl <- 
  europarl_docs_tbl |> 
  mutate(lines = str_split(text, "\n")) |> 
  select(-text)

# Preview
europarl_lines_tbl
```
:::

Previewing the output in @exm-cd-europarl-text-lines-tbl, we can see that the `lines` column contains a list of character vectors. Why is this so? `str_split()` takes a character vector and splits it, as we know. It is a vectorized function, meaning that it can take a vector of character vectors and split each one into subsegments. Since any given input character vector can have a different number of subsegments, the number of subsegments in each file could be different. The ideal object for such data is a list. So `str_split()` returns a list of character vectors.

The list of character vectors, lines in our case, is still associated with the respective `doc_id` value. However, we want to create a new row for each line in the list. To do this, we will use the `unnest()` function from the `tidyr` package [@R-tidyr]. The `unnest()` function takes a list column and creates a new row for each element in the list. We use the `unnest()` function to create a new row for each line in the `lines` column. We can see the output from @exm-cd-europarl-text-lines-tbl-unnest.

::: {#exm-cd-europarl-text-lines-tbl-unnest}
```{r}
#| label: cd-europarl-text-lines-tbl-unnest

# Create a new row for each line
europarl_lines_tbl <- 
  europarl_lines_tbl |> 
  unnest(lines)

# Preview
europarl_lines_tbl |> 
  slice_head(n = 5)
```
:::

Remember that the data in the Europarl corpus is aligned, meaning that each line in the source file is aligned with a line in the target file. So we need to make sure that the number of lines in each file is the same. We can do this by grouping the data by `doc_id` and then counting the number of lines in each file. We can do this using the `group_by()` and `count()` functions from the `dplyr` package [@R-dplyr]. We can see the output in @exm-cd-europarl-text-lines-tbl-count.

::: {#exm-cd-europarl-text-lines-tbl-count}
```{r}
#| label: cd-europarl-text-lines-tbl-count

# Count the number of lines in each file
europarl_lines_tbl |> 
  group_by(doc_id) |> 
  count()
```
:::

The output of @exm-cd-europarl-text-lines-tbl-count shows that the number of lines in each file is the same. This is good. If the number of lines in each file was different, we would need to figure out why and fix it.

We now have our `lines` column and the associated observations for our idealized dataset, in @tbl-cd-europarl-structure-example. Let's now leverage the existing `doc_id` to create the `type` column. The goal is to assign the value of `type` according to the `doc_id` value. Specifically, when `doc_id` is 'europarl-v7.es-en.es' type should be 'Source' and when `doc_id` is 'europarl-v7.es-en.en' type should be 'Target'. 

We can do this using the `case_when()` function from the `dplyr` package [@R-dplyr]. The `case_when()` function takes a series of conditions and assigns a value based on the first condition that is met. In this case, we will use `mutate()` to create the `type` column and then use the `doc_id` column to create the conditions and the `type` column to assign the values from `case_when()`. We will store the output in a new object called `europarl_lines_type_tbl`, as seen in @exm-cd-europarl-text-lines-type-tbl.

::: {#exm-cd-europarl-text-lines-type-tbl}
```{r}
#| label: cd-europarl-text-lines-type-tbl

# Create `type` column
europarl_lines_type_tbl <- 
  europarl_lines_tbl |> 
  mutate(type = case_when(
    doc_id == "europarl-v7.es-en.es" ~ "Source",
    doc_id == "europarl-v7.es-en.en" ~ "Target"
  ))

# Preview dataset
glimpse(europarl_lines_type_tbl)
```
:::

The preview output from @exm-cd-europarl-text-lines-type-tbl shows us that we now have three columns, `doc_id`, `lines`, and `type`. The `type` column has been created and the values have been assigned according to the `doc_id` values. 

We can now overwrite the `doc_id` column with a unique identifier for each line. A numeric identifier makes sense. We can do this by using the `mutate()` function to assign `doc_id` a sequential number with `row_number()`, which increments by 1 for each row. We will store the output in a new object called `europarl_lines_type_id_tbl`, as seen in @exm-cd-europarl-text-lines-type-id-tbl.

::: {#exm-cd-europarl-text-lines-type-id-tbl}
```{r}
#| label: cd-europarl-text-lines-type-id-tbl

# Create new `doc_id` column
europarl_lines_type_id_tbl <- 
  europarl_lines_type_tbl |> 
  mutate(doc_id = row_number())

# Preview dataset
glimpse(europarl_lines_type_id_tbl)
```
:::

The preview output from @exm-cd-europarl-text-lines-type-id-tbl shows us that we now have the same three columns, `doc_id`, `lines`, and `type`. However, the values in the `doc_id` column now reflect a unique identifier for each line.

The last step to get to our envisioned dataset structure is to add the `line_id` column which will be calculated by grouping the data by `type` and then assigning a row number to each of the lines in each group. We use the `group_by()` function to perform the grouping as seen in @exm-cd-europarl-text-lines-type-id-line-id-tbl.

::: {#exm-cd-europarl-text-lines-type-id-line-id-tbl}
```{r}
#| label: cd-europarl-text-lines-type-id-line-id-tbl

# Create `line_id` column
europarl_lines_type_id_line_id_tbl <- 
  europarl_lines_type_id_tbl |> 
  group_by(type) |> 
  mutate(line_id = row_number()) |> 
  ungroup()

# Preview dataset
glimpse(europarl_lines_type_id_line_id_tbl)
```
:::

The preview output from @exm-cd-europarl-text-lines-type-id-line-id-tbl shows us that we now have the desired four columns, `doc_id`, `lines`, `type`, and `line_id`. 

Before we get to writing the dataset to disk, let's organized it in a way that the columns are in the order we want and the rows are sorted in a way that makes sense.

Reordering the columns involves using the `select()` function and list the order of the columns. To sort by columns, we will use `arrange()` and specify the columns to order by. We store the results in a more legible object, `europarl_curated_tbl`, as seen in @exm-cd-europarl-curated-tbl.

::: {#exm-cd-europarl-curated-tbl}
```{r}
#| label: cd-europarl-text-lines-type-id-line-id-tbl-reorder

# Reorder columns and sort rows
europarl_curated_tbl <- 
  europarl_lines_type_id_line_id_tbl |> 
  select(doc_id, type, line_id, lines) |> 
  arrange(line_id, type, doc_id)

# Preview dataset
glimpse(europarl_curated_tbl)
```
:::

The preview output from @exm-cd-europarl-curated-tbl shows us that we now have the desired four columns, `doc_id`, `type`, `line_id`, and `lines`. The rows are sorted by `line_id`, `type`, and `doc_id`. This gives us a dataset that reads left to right from document to line oriented attributes and top to bottom by source-target line pairs.

### Write dataset

At this point we have the curated dataset (`europarl_curated_tbl`) in a tidy format. This dataset, however, is only in the current R session. We will want to write this dataset to disk so that in the next step of the text analysis workflow (transformation) we will be able to start work on this dataset and make changes as needed to fit our analysis needs. 

We will leverage the project directory structure which has distinct directories for `original/` and `derived/` data(sets), seen in @exm-cd-europarl-write-directory.

::: {#exm-cd-europarl-write-directory}
```bash
data/
│── analysis/
├── derived/
└── original/
    │── europarl_do.csv
    └── europarl/
        ├── europarl-v7.es-en.es
        └── europarl-v7.es-en.en
```
:::

Since this is a tabular, tidy dataset we have various options for the file type to write. Many of these formats are software-specific, such as `*.xlsx` for Microsoft Excel, `*.sav` for SPSS, `*.dta` for Stata, and `*.rds` for R. We will use the `*.csv` format since it is a common format that can be read by many software packages. We will use the `write_csv()` function from the `readr` package to write the dataset to disk.

Now the question is where to save our CSV file. Since our `europarl_curated_tbl` dataset is derived by our work, we will added it to the `derived/` directory. I'll create a `europarl/` directory with `dir_create()` just to keep things organized. 

::: {#exm-cd-unstructured-write-europarl}
```{r}
#| label: cd-unstructured-write-europarl
#| eval: false

# Create the europarl/ directory
dir_create(path = "../data/derived/europarl/")

# Write the curated dataset to disk
write_csv(
  x = europarl_curated_tbl,
  file = "../data/derived/europarl/europarl_curated.csv"
) 
```
:::

After running the code in @exm-cd-unstructured-write-europarl, the directory structure under the `derived/` directory should look like @exm-cd-unstructured-write-europarl-directory.

::: {#exm-cd-unstructured-write-europarl-directory}
```bash
data/
│── analysis/
├── derived/
│   └── europarl/
│       └── europarl_curated.csv
└── original/
    └──europarl
        ├── europarl-v7.es-en.en
        └── europarl-v7.es-en.es
```
:::

### Summary

In this section we worked with unstructured data and looked at how to read the data into an R session and manipulate the data to form a tidy dataset with a few columns that we could derive based on the information we have about the corpus. 

In our discussion we worked step by step to curate the europarl Corpus, adding in intermediate steps for illustration purposes. However, in a more realistic case the code would most likely make more extensive use of piping (`|>`) to reduce the number of intermediate objects and make the code more legible. The final code base is added to the *2-curate-data.qmd* file in the *code/* directory. 

The final step, as always, is to document the dataset. For datasets the documentation is a data dictionary, as discussed in @sec-ud-data-dictionaries. As with data origin files, you can use spreadsheet software to create and/ or edit the data dictionary. 

In the `qtalrkit` package we have a function, `create_data_dictionary()` that will generate the scaffolding for a data dictionary. The function takes two arguments, `data` and `file_path`. It reads the dataset columns and provides a template for the data dictionary. 

::: {.callout}
**{{< fa medal >}} Dive deeper**

The `create_data_dictionary()` function provides a rudimentary data dictionary template by default. However, you can take advantage of OpenAI's text generation models to generate a more detailed data dictionary for you to edit. To do this create [an OpenAI account](https://platform.openai.com/signup) and [an API key](https://platform.openai.com/account/api-keys) and add this key to your R environment (`Sys.setenv(OPENAI_API_KEY = "sk..."`). Then you can specify the model you would like to use in the function with the `model = ` argument. For example, `model = "gpt-3.5-turbo"` will use the GPT-3.5 Turbo model.
:::

::: {#exm-cd-unstructured-data-dictionary}
```{r}
#| label: cd-unstructured-data-dictionary
#| eval: false 

# Create the data dictionary
create_data_dictionary(
  data = europarl_curated_tbl,
  file_path <- "../data/derived/europarl/europarl_curated_dd.csv"
)
```
:::

An example of the data dictionary for the `europarl_curated_tbl` dataset is shown in @tbl-cd-unstructured-data-dictionary-example.

```{r}
#| label: tbl-cd-unstructured-data-dictionary-example
#| tbl-cap: "Data dictionary for the `europarl_curated_tbl` dataset."
#| echo: false

read_csv("data/curate-datasets/cd-europarl_curated_dd.csv") |> 
  kable() |> 
  kable_styling() 
```

## Structured

Structured data already reflects the physical and semantic structure of a tidy dataset. This means that the data is already in a tabular format and the relationships between columns and rows are already well-defined. Therefore the heavy lifting of curating the data is already done. There are two remaining questions, however, that need to be taken into account. One, logistical question, is what file format the dataset is in and how to read it into R. And the second, more research-based, is whether the data may benefit from some additional curation and documentation to make it more amenable to analysis and more understandable to others.

### Reading data

Let's consider some common formats for structured data and how to read them into R. First, we will consider R-native formats, such as package datasets and RDS files. Then will consider non-native formats, such as relational databases and datasets produced by other software. Finally, we will consider software agnostic formats, such as CSV.

R and some R packages provide structured datasets that are available for use directly within R. For example, the `languageR` package [@R-languageR] provides the `dative` dataset, which is a dataset containing the realization of the dative as NP or PP in the Switchboard corpus and the Treebank Wall Street Journal collection. The `janeaustenr` package [@R-janeaustenr] provides the `austen_books` dataset, which is a dataset of Jane Austen's novels. Package datasets are loaded into an R session using either the `data()` function, if the package is loaded, or the `::` operator, if the package is not loaded. For example, `data(dative)` or `languageR::dative`.

::: {.callout}
**{{< fa medal >}} Dive deeper**

To explore the available datasets in a package, you can use the `data(package = "package_name")` function. For example, `data(package = "languageR")` will list the datasets available in the `languageR` package. You can also explore all the datasets available in the loaded packages with the `data()` function using no arguments. For example, `data()`.
:::

R also provides a native file format for storing R objects, the RDS file. Any R object, including data frames, can be written from an R session to disk by using the `write_rds()` function from `readr`. The *.rds* files will be written to disk in a binary format that is not human-readable, which is not ideal for transparent data sharing. However, the files and the R objects can be read back into an R session using the `read_rds()` function with all the attributes intact, such as vector types, factor levels, *etc.*. 

R provides a suite of tools for importing data from non-native structured sources such as databases and datasets from software such as SPSS, SAS, and Stata. For instance, if you are working with data stored in a relational database such as MySQL, PostgreSQL, or SQLite, you can use the `DBI` package [@R-DBI] to connect to the database and the `dbplyr` package [@R-dbplyr] to query the database using the SQL language. Files from SPSS (*.sav*), SAS (*.sas7bdat*), and Stata (*.dta*) can be read into R using the `haven` package [@R-haven].

Software agnostic file formats include delimited files, such as CSV, TSV, *etc.*. These file formats lack the robust structural attributes of the other formats, but balance this shortcoming by storing structured data in more accessible, human-readable format. Delimited files are plain text files which use a delimiter, such as a comma (`,`), tab (`\t`), or pipe (`|`), to separate the columns and rows. For example, a CSV file is a delimited file where the columns and rows are separated by commas, as seen in @exm-cd-csv-example.

::: {#exm-cd-csv-example}
```xml
column_1,column_2,column_3
row 1 value 1,row 1 value 2,row 1 value 3
row 2 value 1,row 2 value 2,row 2 value 3
```
:::

Given the accessibility of delimited files, they are a common format for sharing structured data in reproducible research. It is not surprising, then, that this is the format which we have chosen for the derived datasets in this book.

### Orientation

<!-- [ ] rethink the opening sentence -->

With an understanding of the various structured formats, we can now turn to considerations about how the original dataset is structured and how that structure is to be used for a given research project. As an example, we will work with the CABNC datasets acquired in @sec-acquire-data. The structure of the original dataset is shown in @exm-cd-cabnc-structure.

::: {#exm-cd-cabnc-structure}
```bash
data/
├── analysis/
├── derived/
└── original/
    ├── cabnc_do.csv
    └── cabnc/
        ├── participants.csv
        ├── token_types.csv
        ├── tokens.csv
        ├── transcripts.csv
        └── utterances.csv
```
:::

In addition to other important information, the data origin file *cabnc_do.csv* shown in @tbl-cd-cabnc-do informs us the the datasets are related by a common variable. 

```{r}
#| label: tbl-cd-cabnc-do
#| tbl-cap: "Data origin: CABNC datasets"
#| echo: false

read_csv("data/curate-datasets/cd-cabnc_do.csv") |> 
  kable() |> 
  kable_styling()
```

The CABNC datasets are structured in a relational format, which means that the data is stored in multiple tables that are related to each other. The tables are related by a common column or set of columns, which are called a keys. A key is used to join the tables together to create a single dataset. There are two keys in the CABNC datasets, `filename` and `who`. Each variable corresponds to recording- and/ or participant-oriented datasets.

Now, let's envision a scenario in which we want to organize a dataset that can be used in a study that aims to investigate the relationship between speaker demographics and utterances. An ideal dataset would contain information about speakers and their utterances. In their original format, the CABNC datasets separate information about utterances and speakers in separate tables, `cabnc_utterances` and `cabnc_participants`, respectively. The idealized dataset, then, will combine the variables from each of these tables into a single dataset.

### Tidy the dataset

With our idealized dataset in mind, let's start the process of curation by reading the relevant datasets into an R session. Since we are working with CSV files will will use the `read_csv()` function, as seen in @exm-cd-cabnc-read.

::: {#exm-cd-cabnc-read}
```{r}
#| label: cd-cabnc-read

# Read the relevant datasets
cabnc_utterances <- 
  read_csv("data/curate-datasets/cabnc/utterances.csv")
cabnc_participants <- 
  read_csv("data/curate-datasets/cabnc/participants.csv")
```
:::

The next step is to inspect the structure of the datasets. We can use the `glimpse()` function for this task. 

::: {#exm-cd-cabnc-glimpse}
```{r}
#| label: cd-cabnc-glimpse

# Preview the structure of the datasets
glimpse(cabnc_utterances)
glimpse(cabnc_participants)
```
:::

From visual inspection of the output of @exm-cd-cabnc-glimpse we can see that there are common variables in both datasets. It is also possible to intersect the datasets to see which variables are common using the `intersect()` function to intersect the column names of each data frame with the `names()` function, as seen in @exm-cd-cabnc-intersect.

::: {#exm-cd-cabnc-intersect}
```{r}
#| label: cd-cabnc-intersect

# Find the common variables
common_vars <- 
  intersect(
    names(cabnc_utterances), 
    names(cabnc_participants)
  )

# Print the common variables
common_vars
```
:::

Using both visual inspection and column name intersection, we can make sure that the variable names and the values are consistent across the datasets. For example, if the variable `filename` in one dataset is called `file_name` in another dataset, then we will need to rename the variable in one of the datasets so that the variable names are consistent. Furthermore, if the values in the `filename` variable are not consistent across the datasets, then we will need to make the values consistent, if possible, before joining the datasets.

In this case, the variable names and values are consistent across the datasets. Therefore, we can join the datasets together using the `left_join()` function from the `dplyr` package, as seen in @exm-cd-cabnc-join. This function will take the dataset on the left (`x = `) and join it to the dataset on the right (`y = `). The `by` argument specifies the variables to join on. The choice of which dataset to put on the left usually depends on which dataset has the most detailed information. In this case, the `cabnc_utterances` dataset has more detailed information about the utterances so we will put this dataset on the left.

::: {#exm-cd-cabnc-join}
```{r}
#| label: cd-cabnc-join

# Join the datasets
cabnc_tbl <- 
  left_join(
    x = cabnc_utterances, 
    y = cabnc_participants, 
    by = common_vars
  )

# Preview the dimensions of the joined dataset
dim(cabnc_tbl)
```
:::

The result of @exm-cd-cabnc-join should produce a dataset with the same number of observations as the `cabnc_utterances` dataset, seen in @exm-cd-cabnc-glimpse, and a combined number of unique variables from both datasets, that is 23 minus the four common variables, seen in @exm-cd-cabnc-intersect. The output of `dim()` confirms this.

Now we will consider the variables that will be useful for future analysis. Since we are creating a curated dataset, the goal will be to retain as much information as possible from the original datasets. There are cases, however, in which there may be variables that are not informative and thus, will not prove useful for any analysis. These removable variables tend to be of one of two types: variables which show no variation across observations and variables where the information is redundant.

Let's get a high-level summary of the variables in the dataset. We can use the `skim()` function from the `skimr` package [@R-skimr] to get a summary of the variables in the dataset^[Note I've modified the output of `skim()` for display purposes.].

::: {#exm-cd-cabnc-skim}
```{r}
#| label: cd-cabnc-skim-show
#| eval: false

# Load package
library(skimr)

# Summarize the variables in the dataset
skim(cabnc_tbl)
```

```{r}
#| label: tbl-cd-cabnc-skim-run
#| tbl-cap: "Summary of variables in the CABNC dataset"
#| tbl-subcap: 
#|  - "Categorical variables"
#|  - "Logical variables"
#|  - "Numeric variables"
#| layout-nrow: 3
#| echo: false

# [ ] Issue with the ?caption showing up on the plots

library(skimr)

curate_skim <- skim_with(
  base = sfl(complete_rate = complete_rate),
  factor = sfl(n_unique = n_unique),
  character = sfl(n_unique = n_unique),
  numeric = sfl(mean = mean),
  logical = sfl(mean = mean),
  append = FALSE
  )

cabnc_tbl |> 
  curate_skim() |> 
  yank("character") |> 
  select(variable = skim_variable, everything()) |>
  tibble() |> 
  kable() |> 
  kable_styling(font_size = 8)

cabnc_tbl |>
  curate_skim() |>
  yank("logical") |>
  select(variable = skim_variable, everything()) |>
  tibble() |>
  kable() |>
  kable_styling(font_size = 8)

cabnc_tbl |>
  curate_skim() |>
  yank("numeric") |>
  select(variable = skim_variable, everything()) |>
  tibble() |>
  kable() |>
  kable_styling(font_size = 8)


# cabnc_skm <- cabnc_tbl |> skim() |> partition()
# cabnc_skm |> map_vec(kable)

# skim_types <- unique(skim(cabnc_tbl)$skim_type)
# purrr::walk(skim_types, ~ yank_tbl(cabnc_tbl, .x))
```
:::

We see from the `skim()` output in @tbl-cd-cabnc-skim-run-2, that the variables `postcodes` and `gems` have no values. Therefore we can remove these variables from the dataset. On the other hand, in @tbl-cd-cabnc-skim-run-1, the variables `role` and `language` have the same value for every observation as the output shows `n_unique` is `1`. We can also remove these variables from the dataset.

Another set of variables of potential interest are the `startTime` and `endTime` variables, in @tbl-cd-cabnc-skim-run-3. These each have a completion rate of less than 100%, specifically 84%. On first blush, it would seem that we would go ahead and remove them. However, we should pause and consider that these variables may still be of some use. At the curation stage, however, it is best to err on the side of caution and leave them in the dataset. We will decide later whether to keep or remove them as we explore the dataset further in the research. 

Another consideration is whether we have variables that are clearly redundant. For example, the variables `age` and `monthage` are both measures of age, stated in different measures, looking back at @exm-cd-cabnc-glimpse. As such, we can remove one of these variables from the dataset. In this case, we will remove the `age` variable as it is the least straightforward to interpret.

Another potentially redundant set of variables are `who` and `name` --both of which are speaker identifiers. The `who` variable is a unique identifier, but there may be some redundancy with the `name` variable, that is there may be two speakers with the same name. We can check this by looking at the number of unique values in the `who` and `name` variables from the `skim()` output in @tbl-cd-cabnc-skim-run-1. `who` has 568 unique values and `name` has 269 unique values. This suggests that there are multiple speakers with the same name.

Another way to explore this is to look at the number of unique values in the `who` variable for each unique value in the `name` variable. We can do this using the `group_by()` and `summarize()` functions from the `dplyr` package. For each value of `name`, we will count the number of unique values in `who` and then sort the results in descending order.

::: {#exm-cd-cabnc-who-name}
```{r}
#| label: cd-cabnc-who-name

cabnc_tbl |>
  group_by(name) |>
  summarize(n = unique(who) |> length()) |>
  arrange(desc(n))
```
:::

It is good that we performed the check in @exm-cd-cabnc-who-name beforehand. In addition to speakers with the same name, such as 'Chris' and 'David', we also have multiple speakers with generic codes, such as 'None' and 'Unknown_speaker'. It is clear that `name` is redundant and we can safely remove it from the dataset.

Another redundant variable is the `path` variable. This variable is not more informative than the `filename`, for our purposes. We will remove the `path` variable from the dataset too.

In all, we will remove the following variables from the dataset: `postcodes`, `gems`, `role`, `language`, `age`, `name`, and `path`. To drop variables from a data frame we can use the `select()` function in combination with the `-` operator. The `-` operator tells the `select()` function to drop the variables that follow it. 

::: {#exm-cd-cabnc-drop-vars}
```{r}
#| label: cd-cabnc-drop-vars

# Drop variables
cabnc_tbl <- 
  cabnc_tbl |> 
  select(-postcodes, -gems, -role, -language, -age, -name, -path)

# Preview the dataset
glimpse(cabnc_tbl)
```
:::


Now we have a dataset with 12 informative variables which describe the utterances for each recording. There are variables about the recordings, such as the `filename`, about the speakers, such as the `sex` and `who`, and about the utterances, such as the `utterance`, `utt_num`, *etc.*. Let's organize the columns to read left to right from most general to most specific. Again we turn to the `select()` function, this time including the variables in the order we want them to appear in the dataset. We will take this opportunity to rename some of the variable names so that they are more informative. 

::: {#exm-cd-cabnc-rename-vars}
```{r}
#| label: cd-cabnc-rename-vars

# Rename variables
cabnc_tbl <- 
  cabnc_tbl |> 
  select(
    doc_id = filename,
    utt_num,
    utt_start = startTime,
    utt_end = endTime,
    utterance,
    part_id = who,
    part_age = monthage,
    part_sex = sex,
    num_words = numwords,
    num_utts = numutts,
    avg_utt_len = avgutt,
    median_utt_len = medianutt
  )

# Preview the dataset
glimpse(cabnc_tbl)
```
:::

The variable order is organized after running @exm-cd-cabnc-rename-vars. Now let's sort the rows by `doc_id` and `utt_num` so that the utterances are in order. The `arrange()` function takes a data frame and a list of variables to sort by, in the order they are listed.

::: {#exm-cd-cabnc-sort-rows}
```{r}
#| label: cd-cabnc-sort-rows

# Sort rows
cabnc_tbl <-
  cabnc_tbl |>
  arrange(doc_id, utt_num)

# Preview the dataset
cabnc_tbl |> 
  slice_head(n = 10)
```
:::

Applying the sorting in @exm-cd-cabnc-sort-rows, we can see that the utterances are now our desired order. We have now completed the curation of the structured dataset aiming to provide a base for further analysis into the recorded utterances of speakers from the CABNC corpus.

### Write dataset

Let's now write this dataset to disk. Again, since this is a dataset we have created, we will store the dataset in the *data/derived/* directory. To keep the CABNC separate from any other datasets that we may need for our research project, I will create a subdirectory called *cabnc/* to store the dataset. Into this directory we can write a CSV file with our `cabnc_tbl` data frame with the `write_csv()` function, as seen in @exm-cd-cabnc-write-csv.

::: {#exm-cd-cabnc-write-csv}
```{r}
#| label: cd-cabnc-write-csv
#| eval: false

# Create cabnc directory
dir_create(path = "../data/derived/cabnc")

# Write dataset to disk
cabnc_tbl |> 
  write_csv(path = "../data/derived/cabnc/cabnc_curated.csv")
```
:::

### Summary

In this section we covered the steps to curate a structured dataset. We started by loading the raw data into R using the `read_csv()` function. We then performed some exploratory data analysis to get a sense of the data. We then removed redundant variables and renamed the remaining variables to be more informative. Finally, we sorted the rows by `doc_id` and `utt_num` and wrote the dataset to disk.

The final step is to create a data dictionary file for this dataset. We can do this using the `create_data_dictionary()` function, passing the data frame object name and a path to the directory where we want to store the data dictionary. The `create_data_dictionary()` function will create a CSV file with a data dictionary template in the specified directory. 

::: {#exm-cd-cabnc-create-data-dict}
```{r}
#| label: cd-cabnc-create-data-dict
#| eval: false

# Create data dictionary
create_data_dictionary(
  data = cabnc_tbl,
  file_path = "../data/derived/cabnc_curated_dd.csv"
)
```
:::

After running the code in @exm-cd-cabnc-create-data-dict, we can open the *cabnc_curated_dd.csv* file in the *data/derived/cabnc/* directory and fill in the data dictionary template. An example of an edited data dictionary is shown in @tbl-cd-cabnc-dd-example.

```{r}
#| label: tbl-cd-cabnc-dd-example
#| tbl-cap: "Data dictionary example for the curated CABNC dataset."
#| echo: false

read_csv("data/curate-datasets/cd-cabnc_curated_dd.csv") |> 
  kable() |> 
  kable_styling()
```

And the final directory structure for the *data/* directory is shown in @exm-cd-cabnc-final-dir.

::: {#exm-cd-cabnc-final-dir}
```bash
data/
├── analysis/
├── derived/
│   ├── cabnc_curated_dd.csv
│   └── cabnc/
│       └── cabnc_curated.csv
└── original/
    ├── cabnc_do.csv
    └── cabnc/
        ├── participants.csv
        ├── token_types.csv
        ├── tokens.csv
        ├── transcripts.csv
        └── utterances.csv
```
:::

## Semi-structured

At this point we have discussed curating unstructured data and structured datasets. Between these two extremes falls semi-structured data. And as the name suggests, it is a hybrid between unstructured and structured data. This means that there will be important structured metadata included with unstructured elements. The file formats and approaches to encoding the structured aspects of the data vary widely from resource to resource and therefore often requires more detailed attention to the structure of the data and often includes more sophisticated programming strategies to curate the data to produce a tidy dataset. 

### Reading data

### Tidy the data

### Write dataset

### Summary

In this section we looked at semi-structured data. This type of data often requires the most work to organize into a tidy dataset. We continued to work with many of the R programming strategies introduced to this point in the coursebook. We also made more extensive use of regular expressions to pick out information from a semi-structured document format.

To round out this section I've provided a code summary of the steps involved to conduct the curation of the Switchboard Dialogue Act Corpus files. Note that I've added the `extract_sdac_metadata()` custom function to a file called `curate_functions.R` and sourced this file. This will make the code more succinct and legible here, as well in your own research projects. 

```{r}
#| eval: false
#| label: cd-semi-structured-code-summary

# Source the `extract_sdac_metadata()` function
source("../functions/curate_functions.R")

# Get list of the corpus files (.utt)
sdac_files <-
  fs::dir_ls(
    path = "../data/original/sdac/", # source directory
    recurse = TRUE, # traverse all sub-directories
    type = "file", # only return files
    regexp = "\\.utt$"
  ) # only return files ending in .utt

# Read files and return a tidy dataset
sdac <-
  sdac_files |> # pass file names
  map(extract_sdac_metadata) |> # read and tidy iteratively
  bind_rows() # bind the results into a single data frame

# Write curated dataset to disk
fs::dir_create(path = "../data/derived/sdac/") # create sdac subdirectory
write_csv(sdac,
  file = "../data/derived/sdac/sdac_curated.csv"
) # write sdac to disk and label as the curated dataset
```

## Summary {.unnumbered}

In this chapter we looked at the process of structuring data into a dataset. This included a discussion on three main types of data --unstructured, structured, and semi-structured. The level of structure of the original data(set) will vary from resource to resource and by the same token so will the file format used to support the level of meta-information included. The results from our data curation resulted in a curated dataset that is saved separate from the original data to maintain modularity between what the data(set) looked like before we intervene and afterwards. In addition to the code we use to derived the curated dataset's structure, we also include a data dictionary which documents the names of the variables and provides sufficient description of these variables so that it is clear what our dataset contains.

- [ ] emphasize the importance of ahereing to the unit of analysis for the research, and how this is reflected in the curated dataset. And how unit of observation derived from the transformational steps in the next chapter.

It is important to recognized that this curated dataset will form the base for the next step in our text analysis project and the last step in data preparation for analysis: dataset transformation. This last step in preparing data for analysis is to convert this curated dataset into a dataset that is directly aligned with the research aims (i.e. analysis method(s)) of the project. Since there can be multiple analysis approaches applied the original data in a research project, this curated dataset serves as the point of departure for each of the subsequent datasets derived from the transformational steps. 

## Activities {.unnumbered}

::: {.callout}
**{{< fa regular file-code >}} Recipe**

<!-- Understand, apply, and analyze verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- [ ] edit links and goals/ outcomes -->

**What**: [Organizing and documenting datasets](https://lin380.github.io/tadr/articles/recipe_7.html)\
**How**: Read Recipe 7 and participate in the Hypothes.is online social annotation.\
**Why**: To rehearse methods for deriving tidying datasets to use a the base for further project-specific purposes. We will explore how regular expressions are helpful in developing strategies for matching, extracting, and/ or replacing patterns in character sequences and how to change the dimensions of a dataset to either expand or collapse columns or rows.
:::

::: {.callout}
**{{< fa flask >}} Lab**

<!-- Analyze, evaluate, and create verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->

<!-- [ ] edit links and goals/ outcomes -->

<!-- [ ] update name of lab to something more lively -->

**What**: [Pattern Matching and Manipulate Datasets](https://github.com/lin380/lab_7)\
**How**: Clone, fork, and complete the steps in Lab 7.\
**Why**: To gain experience working with coding strategies reshaping data using tidyverse functions and regular expressions, to practice reading/ writing data from/ to disk, and to
implement organizational strategies for organizing and documenting a dataset in reproducible fashion.
:::

## Questions {.unnumbered}

::: {.callout}
{{< fa wrench >}} **Conceptual questions**

1. ...
2. ...

:::

::: {.callout}
{{< fa wrench >}} **Technical questions**

1. ...
2. ...
:::
