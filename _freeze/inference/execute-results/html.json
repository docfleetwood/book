{
  "hash": "c428380f18d1b3fc64b93d0ae360200d",
  "result": {
    "markdown": "---\nexecute: \n  echo: true\n---\n\n\n# Inference {#sec-inference}\n\n\n\n\n\n\n\n<!-- TODO:\n\nContent:\n\nExercises:\n\nFormatting:\n\n- [ ] ...\n\n-->\n\n> People generally see what they look for, and hear what they listen for.\n>\n> --- Harper Lee, To Kill a Mockingbird\n\n::: callout-note\n## Keys\n\n- what are the three main types of inferential analysis approaches?\n- how does the informational value of the dependent variable relate to the statistical approach adopted?\n- how to descriptive, statistical, and evaluative steps work together to produce the reliable results?\n:::\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/inference-data-packages_85512f66a5386fad1c115b99c2404a59'}\n\n:::\n\n\nIn this chapter we consider approaches to deriving knowledge from information which can be generalized to the population from which the data is sampled. This process is known as statistical inference. The discussion here builds on concepts developed in [Chapter 3 \"Approaching analysis\"](#sec-approaching-analysis) and implements descriptive assessments, statistical tests, and evaluation procedures for a series of contexts which are common in the analysis of corpus-based data. The chapter is structured into three main sections which correspond to the number of variables included in the statistical procedure. Each of these sections includes a subsection dedicated to the informational value of the dependent variable; the variable whose variation is to be explained.\n\nFor this discussion two datasets will be used as the base to pose various questions to submit for interrogation. It is of note that the questions in the subsequent sections are posited to highlight various descriptive, statistic, and evaluation procedures and do not reflect the standard approach to hypothesis testing which assumes that the null and alternative hypotheses are developed at the outset of the research project.\n\nThe process for each inferential data analysis in this section will include three steps: (1) descriptive assessment, (2) statistical interrogation, and (3) evaluation of the results.\n\n<!-- TODO:\n- Add Bresnan citation for questions to interrogate `dative`?\n- Add Tottie citation for questions to interrogate `sdac_disfluencies`?\n\n-->\n\n::: callout-tip\n## Swirl\n\n**What**: [Significance testing](https://github.com/lin380/swirl)\\\n**How**: In the R Console pane load `swirl`, run `swirl()`, and follow prompts to select the lesson.\\\n**Why**: To learn how to implement various statistical tests in common significance testing scenarios.\n:::\n\n\n## Orientation {#ida-orientation}\n\nThe aim of this section is to provide an overview of inferential data analysis (IDA) and to introduce the three main types of inferential analysis approaches that are most common in text analysis research. \n\n### Research goals {#ida-research-goals}\n\nThe goal of IDA is to detect, explain, and generalize. The relationship to detect is predtermined by the hypothesis. In this situation the researcher will have identified an outcome variable (often known as a dependent variable) and often a predictor or set of predictor variables (independent variables) that are directly linked to the hypothesis in a way that the results of the statistical analysis allows to either confirm that null hypothesis or reject it. Explaining the results of the statistical analysis is key to summarizing the findings and how they relate to the hypothesis. To the extent that the text sample used is representative of the population from which the data is sampled, the results can be generalized to the population. IDA is not an exploratory endeavor and as such that anlysis is performed on the data in a much more conservative manner than is the case in exploratory data analysis (EDA) or predictive data analysis (PDA).\n\n### Approaches {#ida-approaches}\n\n- The dependent variable and predictor variables are fixed (tied to hypothesis)\n- Descriptive statistics and visualizations (plots or tables) are used to summarize the data and provide a preliminary assessment of the data\n- Inferential statistics are used to test the hypothesis\n\n#### Analysis types {#ida-analysis-types}\n\nThe type of analysis that is performed depends most heavily on the informational value of the dependent variable. The informational value of the dependent variable is determined by the type of data that is collected. Secondly, the number and informational types of the independent variables (predictor variables) also play a role in determining the type of analysis that is performed. \n\n- Categorical dependent variable\n    - Descriptive statistics\n        - Frequency\n        - Proportion\n        - Confidence intervals\n    - Inferential statistics\n        - Chi-square\n        - Logistic regression\n- Continuous dependent variable\n    - Descriptive statistics\n        - Mean\n        - Standard deviation\n        - Confidence intervals\n    - Inferential statistics\n        - Correlation\n        - Linear regression\n\n### Workflow {#ida-workflow}\n\nPrerequisites:\n- A testable hypothesis (covering the outcome space, i.e. null and alternative hypotheses).\n- A data set that aligns with the population targeted to generalize to. \n- A operationalized dependent and independent variable(s) that are tied to the hypothesis. \n\n#### Identify {#ida-identify}\n\n- Identify the informational value of the dependent and independent variable(s).\n- Assess the distribution of the independent and dependent variables with the appropriate descriptive statistics and visualizations.\n- Choose an appropriate statistical test based on the informational value and distribution of the dependent and independent variables.\n- Apply transformations to the data as needed to meet the assumptions of the statistical tests.\n\n#### Interrogate {#ida-interrogate}\n\n- Apply the appropriate statistical test to the data:\n  - Categorical dependent variable\n    - Chi-square (dependent and one independent variable)\n    - Logistic regression (dependent and one or more independent variables)\n  - Continuous dependent variable\n    - Correlation (dependent and one independent variable)\n    - Linear regression (dependent and one or more independent variables)\n- Assess the results of the statistical test (p-value, confidence intervals, effect size)\n\n#### Interpret {#ida-interpret}\n\nReview the results of the statistical test and interpret the results in the context of the hypothesis.\n\n## Analysis {#ida-analysis}\n\nRecap and introduction to the structure of the analysis subsection. \n\n- Categorical dependent variable\n  - Categorical/ continuous independent variable(s)\n    - Descriptive assessment\n      - 0-1 categorical independent variable: Tables summary statistics (contingency table)\n      - Continuous independent variable(s): Plots and summary statistics (boxplots, histograms, etc.)\n    - Statistical interrogation\n      - Chi-square (dependent and one independent variable)\n      - Logistic regression (dependent and one or more independent variables)\n    - Evaluation of results\n      - p-value\n      - Confidence intervals\n      - Effect size\n- Continuous dependent variable\n  - Continuous/ categorical independent variable(s)\n    - Descriptive assessment\n      - Tables, plots, and summary statistics\n    - Statistical interrogation\n      - Correlation\n      - Linear regression\n    - Evaluation of results\n      - p-value\n      - Confidence intervals\n      - Effect size\n\n### Categorical dependent variable {#ida-categorical-dependent-variable}\n\n#### Chi-square {#ida-chi-square}\n\nChi-square tests can be used for frequencies of categorical variables. The chi-square goodness of fit test is used to test whether the observed frequencies of a categorical variable match the expected frequencies of a single variable. The chi-square test of independence is used to test whether the observed frequencies of two categorical variables are independent of each other. For hypothesis tests that include more than two variables, the chi-square test is not appropriate. Instead, logistic regression is used.\n\n#### Logistic regression {#ida-logistic-regression}\n\nLogistic regression can handle more than one independent variable, and these variables need not be categorical. The dependent variable is a binary variable, and the independent variables can be continuous or categorical. The logistic regression model is used to predict the probability of the dependent variable being 1. The logistic regression model is used to test whether the independent variables are associated with the dependent variable. \n\nUsing the `infer` package, a logistic regression can be performed using the `specify()` and `generate()` functions. The `specify()` function is used to specify the dependent and independent variables. The `generate()` function is used to generate the model. The `calculate()` function is used to calculate the p-value and confidence intervals. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/ida-logistic-regression_e867415fc4e01c5a21ce25b9f2f47e3a'}\n\n```{.r .cell-code}\nlibrary(infer) # for inferential statistics\n\n# load the `dative` data from the `languageR` package\ndata(package = \"languageR\", data = \"dative\")\n\n# specify the dependent `RealizationOfRecipient` and independent variables `LengthOfTheme` and `AnimacyOfTheme` for a logistic regression model\nlog_reg <- \n  dative %>%\n  specify(RealizationOfRecipient ~ LengthOfTheme + AnimacyOfTheme) %>%\n  generate(response = \"logistic\", type = \"response\") %>%\n  calculate(stat = \"p.value\", order = \"descending\")\n```\n:::\n\n\n### Continuous dependent variable {#ida-continuous-dependent-variable}\n\n#### Correlation {#ida-correlation}\n\n#### Linear regression {#ida-linear-regression}\n\n## Reporting {#ida-reporting}\n\n## Summary \n\n## Activities {.unnumbered}\n\n::: callout-tip\n## Recipe\n\n**What**: [Statistical inference: prep, assess, interrogate, evaluate, and report](https://lin380.github.io/tadr/articles/recipe_9.html)\\\n**How**: Read Recipe 9 and participate in the Hypothes.is online social annotation.\\\n**Why**: To illustrate some common coding strategies for preparing datasets for inferential data analysis, as well as the steps conduct descriptive assessment, statistical interrogation, and evaluation and reporting of results.\n:::\n\n::: callout-tip\n## Lab\n\n**What**: [Statistical inference: prep, assess, interrogate, evaluate, and report](https://github.com/lin380/lab_9)\\\n**How**: Clone, fork, and complete the steps in Lab 9.\\\n**Why**: To gain experience working with coding strategies to prepare, assess, interrogate, evaluate, and report results from an inferential data analysis, practice transforming datasets and visualizing relationships, and implement organizational strategies for organizing and reporting results in a reproducible fashion.\n:::\n\n## Questions {.unnumbered}\n\n::: callout-note\n## Conceptual questions\n\n1. What is the difference between a descriptive and inferential analysis?\n2. What are the steps involved in conducting a descriptive analysis?\n3. What are the steps involved in conducting an inferential analysis?\n4. What are the steps involved in preparing data for inferential analysis?\n5. What are the steps involved in conducting a statistical interrogation?\n6. What are the steps involved in evaluating the results of an inferential analysis?\n7. What are the steps involved in reporting the results of an inferential analysis?\n8. Would word freqency differences between two groups of words be better assessed using a t-test or a chi-squared distribution?\n9. Would word lengths between two groups of words be better assessed using a t-test or a chi-squared distribution?\n10. What type of visualization would be best for exploring the relationship between two categorical variables?\n11. What type of visualization would be best for exploring the relationship between two non-categorical variables?\n12. What type of visualization would be best for exploring the relationship between a categorical and a non-categorical variable?\n13. What is the role of confidence intervals in inferential data analysis? How is this similar or differnt to the role of p-values?\n14. What is the role of effect size in inferential data analysis? How is this similar or differnt to the role of p-values?\n:::\n\n::: callout-note\n## Technical exercises\n\n1. Use the lm() function to create a linear model, assess the summary statistics, and evaluate the results.\n2. Use the glm() function to assess the relationship between two categorical variables and evaluate the results.\n3. Apply a chi-squared distribution to explore categorical dependent variables and evaluate the results.\n4. Calculate correlation coefficients between two non-categorical variables and evaluate the results.\n5. Read in a dataset and transform it to prepare it for inferential analysis.\n6. Decide which type of visualization is most appropriate for the dataset and then implement it using ggplot2.\n7. Use the effectsize() function to calculate effect size and confidence intervals.\n\n:::\n\n<!--\n\n- IDA (Inferential Data Analysis)\n- Overview:\n    - Research goals (detect, explain, generalize)\n    - Use of data\n        - Conservative: no reuse of data (outside bootstrapping)\n        - Outcome variable and predictor/ covariate variables \n        - Fixed outcome and fixed predictor variables/ features (tied to hypothesis)\n    - Methods\n        - Inferential statistics\n            - Descriptive statistics\n            - Inferential statistics\n    - Data types/ structures\n        - Data frames\n    - Interpreting results (quantitative)\n        - Visualizations\n        - Summary statistics\n- Inferential analysis\n    - Methods: \n        - Continuous outcome variable\n            - Descriptive statistics\n                - Mean\n                - Standard deviation\n                - Confidence intervals\n            - Inferential statistics\n                - Correlation\n                - Linear egression\n        - Categorical outcome variable\n            - Descriptive statistics\n                - Frequency\n                - Proportion\n                - Confidence intervals\n            - Inferential statistics\n                - Chi-square\n                - Logistic regression\n\nNote:\n\nIDA: Questions to consider:\n\n- Continuous outcome variable:\n    - The relationship between lexical retrieval (decisions, naming) and measures of frequency (observed frequency, adjusted frequency) and/ or Age of acquisition or Concreteness. (see @Gries2009a, p. 1-2)\n- Categorical outcome variable:\n    - ...\n-->\n\n## Preparation\n\nAt this point let's now get familiar with the datasets and prepare them for analysis. The first dataset to consider is the `dative` dataset. This dataset can be loaded from the languageR package [@R-languageR].  \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-dative-load_722530ba139cf0908c792d4dc104e358'}\n\n```{.r .cell-code}\ndative <- \n  languageR::dative |> # load the `dative` dataset  \n  as_tibble() # convert the data frame to a tibble object\n  \nglimpse(dative) # preview structure \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 3,263\n#> Columns: 15\n#> $ Speaker                <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n#> $ Modality               <fct> written, written, written, written, written, wr…\n#> $ Verb                   <fct> feed, give, give, give, offer, give, pay, bring…\n#> $ SemanticClass          <fct> t, a, a, a, c, a, t, a, a, a, a, a, t, a, c, a,…\n#> $ LengthOfRecipient      <int> 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1,…\n#> $ AnimacyOfRec           <fct> animate, animate, animate, animate, animate, an…\n#> $ DefinOfRec             <fct> definite, definite, definite, definite, definit…\n#> $ PronomOfRec            <fct> pronominal, nonpronominal, nonpronominal, prono…\n#> $ LengthOfTheme          <int> 14, 3, 13, 5, 3, 4, 4, 1, 11, 2, 3, 3, 5, 2, 8,…\n#> $ AnimacyOfTheme         <fct> inanimate, inanimate, inanimate, inanimate, ina…\n#> $ DefinOfTheme           <fct> indefinite, indefinite, definite, indefinite, d…\n#> $ PronomOfTheme          <fct> nonpronominal, nonpronominal, nonpronominal, no…\n#> $ RealizationOfRecipient <fct> NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP, NP,…\n#> $ AccessOfRec            <fct> given, given, given, given, given, given, given…\n#> $ AccessOfTheme          <fct> new, new, new, new, new, new, new, new, accessi…\n```\n:::\n:::\n\n\nFrom `glimpse()` we can see that this dataset contains 3,263 observations and 15 columns. \n\nThe R Documentation can be consulted using `?dative` in the R Console. The description states: \n\n> Data describing the realization of the dative as NP or PP in the Switchboard corpus and the Treebank Wall Street Journal collection.\n\nFor a bit more context, a dative is the phrase which reflects the entity that takes the recipient role in a ditransitive clause. In English, the recipient (dative) can be realized as either a noun phrase (NP) as seen in (1) or as a prepositional phrase (PP) as seen in (2) below. \n\n1. They give [you ~NP~] a drug test.\n2. They give a drug test [to you ~PP~].\n\nTogether these two syntactic options are known as the Dative Alternation.\n\nThe observational unit for this dataset is `RealizationOfRecipient` variable which is either 'NP' or 'PP'. For the purposes of this chapter I will select a subset of the key variables we will use in the upcoming analyses and drop the others. \n\n\n::: {#tbl-i-dative-simplify .cell layout-align=\"center\" tbl-cap='First 10 observations of simplified `dative` dataset.' hash='inference_cache/html/tbl-i-dative-simplify_03a7e56288d0d7c878f6bcbf94cd4f00'}\n\n```{.r .cell-code}\ndative <- \n  dative |> # dataset\n  select(RealizationOfRecipient, Modality, LengthOfRecipient, LengthOfTheme) |> # select key variables\n  janitor::clean_names() # normalize variable names\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-dative-preview_99acf3c30770d0ca1debd721bb174f76'}\n::: {.cell-output-display}\n|realization_of_recipient |modality | length_of_recipient| length_of_theme|\n|:------------------------|:--------|-------------------:|---------------:|\n|NP                       |written  |                   1|              14|\n|NP                       |written  |                   2|               3|\n|NP                       |written  |                   1|              13|\n|NP                       |written  |                   1|               5|\n|NP                       |written  |                   2|               3|\n|NP                       |written  |                   2|               4|\n|NP                       |written  |                   2|               4|\n|NP                       |written  |                   1|               1|\n|NP                       |written  |                   1|              11|\n|NP                       |written  |                   1|               2|\n:::\n:::\n\n\nIn @tbl-i-dative-dictionary I've created a data dictionary describing the variables in our new `dative` dataset based on the variable descriptions in the `languageR::dative` documentation.\n\n\n::: {#tbl-i-dative-dictionary .cell layout-align=\"center\" tbl-cap='Data dictionary for the `dative` dataset.' hash='inference_cache/html/tbl-i-dative-dictionary_bcbb70e5dc23de80b2bda7f514364ed2'}\n::: {.cell-output-display}\n|variable_name            |name                     |description                                                           |\n|:------------------------|:------------------------|:---------------------------------------------------------------------|\n|realization_of_recipient |Realization of Recipient |A factor with levels NP and PP coding the realization of the dative.  |\n|modality                 |Language Modality        |A factor with levels *spoken*, *written*.                             |\n|length_of_recipient      |Length of Recipient      |A numeric vector coding the number of words comprising the recipient. |\n|length_of_theme          |Length of Theme          |A numeric vector coding the number of words comprising the theme.     |\n:::\n:::\n\n\nThe second dataset that we will use in this chapter is the `sdac_disfluencies` dataset that we worked to derived in the previous chapter. Let's read in the dataset and preview the structure. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-sdac-disfluencies-read-show_5dfd6704a2034358547584fbba2f2a2b'}\n\n```{.r .cell-code}\nsdac_disfluencies <- \n  read_csv(file = \"../data/derived/sdac/sdac_disfluencies.csv\") # read transformed dataset\n\nglimpse(sdac_disfluencies) # preview structure\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-sdac-disfluencies-read-run_c758e8a500d543222d2521a351b592e0'}\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 447,212\n#> Columns: 9\n#> $ doc_id         <dbl> 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4…\n#> $ speaker_id     <dbl> 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519, 1…\n#> $ utterance_text <chr> \"Okay.  /\", \"Okay.  /\", \"{D So, }\", \"{D So, }\", \"[ [ I …\n#> $ filler         <chr> \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"…\n#> $ count          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0…\n#> $ sex            <chr> \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMA…\n#> $ birth_year     <dbl> 1962, 1962, 1962, 1962, 1971, 1971, 1962, 1962, 1971, 1…\n#> $ dialect_area   <chr> \"WESTERN\", \"WESTERN\", \"WESTERN\", \"WESTERN\", \"SOUTH MIDL…\n#> $ education      <dbl> 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1…\n```\n:::\n:::\n\n\nWe prepared a data dictionary that reflects this transformed dataset. Let's read that file and then view it in @tbl-i-sdac-disfluencies-dictionary. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-sdac-disfluencies-diciontary-read-show_0af4a493f2f380ec6a52f54e9f03072f'}\n\n```{.r .cell-code}\nsdac_disfluencies_dictionary <- read_csv(file = \"../data/derived/sdac/sdac_disfluencies_data_dictionary.csv\") # read data dictionary\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-sdac-disfluencies-diciontary-read-run_fca517171127caa8b6974891f3782d83'}\n\n:::\n\n::: {#tbl-i-sdac-disfluencies-dictionary .cell layout-align=\"center\" tbl-cap='Data dictionary for the `sdac_disfluencies` dataset.' hash='inference_cache/html/tbl-i-sdac-disfluencies-dictionary_6095f2d4b31b102fe974470ae259a716'}\n::: {.cell-output-display}\n|variable_name  |name           |description                                                                        |\n|:--------------|:--------------|:----------------------------------------------------------------------------------|\n|doc_id         |Document ID    |Unique identifier for each conversation file.                                      |\n|speaker_id     |Speaker ID     |Unique identifier for each speaker in the corpus.                                  |\n|utterance_text |Utterance Text |Transcribed utterances for each conversation. Includes disfluency annotation tags. |\n|filler         |Filler         |Filler type either uh or um.                                                       |\n|count          |Count          |Number of fillers for each utterance.                                              |\n|sex            |Sex            |Sex for each speaker either male or female.                                        |\n|birth_year     |Birth Year     |The year each speaker was born.                                                    |\n|dialect_area   |Dialect Area   |Region from the US where the speaker spent first 10 years.                         |\n|education      |Education      |Highest educational level attained: values 0, 1, 2, 3, and 9.                      |\n:::\n:::\n\n\nFor our analysis purposes we will reduce this dataset, as we did for the `dative` dataset, retaining only the variables of interest for the upcoming analyses. \n\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-sdac-disfluencies-simplify_67b8b99fb20add1e9338cf586e1d52f2'}\n\n```{.r .cell-code}\nsdac_disfluencies <- \n  sdac_disfluencies |> # dataset\n  select(speaker_id, filler, count, sex, birth_year, education) # select key variables\n```\n:::\n\n\nLet's preview this simplified `sdac_disfluencies` dataset. \n\n\n::: {#tbl-i-sdac-disfluencies-preview .cell layout-align=\"center\" tbl-cap='First 10 observations of simplified `sdac_disfluencies` dataset.' hash='inference_cache/html/tbl-i-sdac-disfluencies-preview_d91c09dec86d1ef21f55742f92f512d8'}\n::: {.cell-output-display}\n| speaker_id|filler | count|sex    | birth_year| education|\n|----------:|:------|-----:|:------|----------:|---------:|\n|       1632|uh     |     0|FEMALE |       1962|         2|\n|       1632|um     |     0|FEMALE |       1962|         2|\n|       1632|uh     |     0|FEMALE |       1962|         2|\n|       1632|um     |     0|FEMALE |       1962|         2|\n|       1519|uh     |     0|FEMALE |       1971|         1|\n|       1519|um     |     0|FEMALE |       1971|         1|\n|       1632|uh     |     0|FEMALE |       1962|         2|\n|       1632|um     |     0|FEMALE |       1962|         2|\n|       1519|uh     |     1|FEMALE |       1971|         1|\n|       1519|um     |     0|FEMALE |       1971|         1|\n:::\n:::\n\n\nNow the `sdac_disfluencies` dataset needs some extra transformation to better prepare it for statistical interrogation. On the one hand the variables `birth_year` and `education` are not maximally informative. First it would be more ideal if `birth_year` would reflect the age of the speaker at the time of the conversation(s) and furthermore the coded values of `education` are not explicit as far what the numeric values refer to.\n\nThe second issue has to do with preparing the `sdac_disfluencies` dataset for statistical analysis. This involves converting our column types to the correct vector types for statistical methods. Specifically we need to convert our categorical variables to the R type 'factor' (fct). This includes of our current variables which are character vectors, but also the `speaker_id` and `education` which appear as numeric but do not reflect a continuous variables; one  is merely a code which uniquely labels each speaker and the other is an ordinal list of educational levels.\n\nThis will be a three step process, first we will normalize the `birth_year` to reflect the age of the speaker, second we will convert all the relevant categorical variables to factors, and third we will convert the `education` variable to a factor adding meaningful labels for the levels of this factor.\n\nConsulting the [online manual for this corpus](https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1_manual.txt), we see that the recording date for these conversations took place in 1992, so we can simply subtract the `birth_year` from 1992 to get each participant's age. We'll rename this new column `age` and drop the `birth_year` column. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-sdac-disfluencies-age-transform_c857b0eafa46a2c5edc11723c87a51ac'}\n\n```{.r .cell-code}\nsdac_disfluencies <- \n  sdac_disfluencies |> # dataset\n  mutate(age = (1992 - birth_year)) |> # calculate age\n  select(-birth_year) # drop `birth_year` column\n```\n:::\n\n\nNow let's convert all the variables which are character vectors. We can do this using the the `factor()` function; first on `speaker_id` and then, with the help of `mutate_if()`, to all the other variables which are character vectors. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-sdac-disfluencies-to-factors_eeedef27384fe45a9c60e3dbadd5eb7c'}\n\n```{.r .cell-code}\nsdac_disfluencies <- \n  sdac_disfluencies |> # dataset\n  mutate(speaker_id = factor(speaker_id)) |> # convert numeric to factor\n  mutate_if(is.character, factor) # convert all character to factor\n```\n:::\n\n\nWe know from the data dictionary that the `education` column contains four values (0, 1, 2, 3, and 9). Again, consulting the corpus manual we can see what these values mean. \n\n```plain\nEDUCATION    COUNT\n--------------------\n\n0            14      less than high school\n1            39      less than college\n2            309     college\n3            176     more than college\n9            4       unknown\n```\n\nSo let's convert `education` to a factor adding these descriptions as factor level labels. The function `factor()` can take an argument `labels = ` which we can manually assign the label names for the factor levels in the order of the factor levels. Since the original values were numeric, the factor level ordering defaults to ascending order.\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-sdac-disfluencies-education-transform_ba495db7fabd4397c03dae0256bbd80d'}\n\n```{.r .cell-code}\nsdac_disfluencies <- \n  sdac_disfluencies |> # dataset\n  mutate(education = factor(education, \n                            labels = c(\"less than high school\", # value 0\n                                       \"less than college\", # value 1\n                                       \"college\", # value 2\n                                       \"more than college\", # value 3 \n                                       \"unknown\"))) # value 9\n```\n:::\n\n\nSo let's take a look at the `sdac_disfluencies` dataset we've prepared for analysis.\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-sdac-disfluencies-transformed-preview_002fee80f275e6e0f5cd3c6f59f75ff2'}\n\n```{.r .cell-code}\nglimpse(sdac_disfluencies)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 447,212\n#> Columns: 6\n#> $ speaker_id <fct> 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519, 1519,…\n#> $ filler     <fct> uh, um, uh, um, uh, um, uh, um, uh, um, uh, um, uh, um, uh,…\n#> $ count      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n#> $ sex        <fct> FEMALE, FEMALE, FEMALE, FEMALE, FEMALE, FEMALE, FEMALE, FEM…\n#> $ education  <fct> college, college, college, college, less than college, less…\n#> $ age        <dbl> 30, 30, 30, 30, 21, 21, 30, 30, 21, 21, 30, 30, 21, 21, 21,…\n```\n:::\n:::\n\n\nNow the datasets `dative` and `sdac_disfluencies` are ready to be statistically interrogated.\n\n## Univariate analysis\n\nIn what follows I will provide a description of inferential data analysis when only one variable is to be interrogated. This is known as a univariate analysis, or one-variable analysis. We will consider a case when the variable is categorical and the other continuous. \n\n### Categorical\n\nAs an example of a univariate analysis where the variable used in the analysis is categorical we will look at the `dative` dataset. In this analysis we may be interested in knowing whether the recipient role in a ditransitive construction is realized more as an 'NP' or 'PP'.\n\n**Descriptive assessment**\n\nThe `realization_of_recipient` variable contains the relevant information. Let's take a first look using the skimr package.\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-uni-cat-description-dative_accc8fb248402d7e37d175a12b7cfd4c'}\n\n```{.r .cell-code}\ndative |> # dataset\n  select(realization_of_recipient) |> # select the variable\n  skim() |> # get data summary\n  yank(\"factor\") # only show factor-oriented information\n```\n\n::: {.cell-output-display}\n**Variable type: factor**\n\n|skim_variable            | n_missing| complete_rate|ordered | n_unique|top_counts        |\n|:------------------------|---------:|-------------:|:-------|--------:|:-----------------|\n|realization_of_recipient |         0|             1|FALSE   |        2|NP: 2414, PP: 849 |\n:::\n:::\n\n\nThe output from `skim()` produces various pieces of information that can be helpful. On the one hand we get diagnostics that tell us if there are missing cases (`NA` values), what the proportion of complete cases is, if the the factor is ordered, how many distinct levels the factor has, as well as the level counts. \n\nLooking at the `top_counts` we can see that of the 3,263 observations, in 2,414 the dative is expressed as an 'NP' and 849 as 'PP'. Numerically we can see that there is a difference between the use of the alternation types. A visualization is often helpful for descriptive purposes in statistical analysis. In this particular case, however, we are considering a single categorical variable with only two levels (values) so a visualization is not likely to be more informative than the numeric values we have already obtained. But for demonstration purposes and to get more familiar with building plots, let's create a visualization.\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/figi-uni-cat-visual-dative_cb2d5895bbda7b4fe532d9e425b917b4'}\n\n```{.r .cell-code}\ndative |> # dataset\n  ggplot(aes(x = realization_of_recipient)) + # mapping\n  geom_bar() + # geometry\n  labs(x = \"Realization of recipient\", y = \"Count\") # labels\n```\n\n::: {.cell-output-display}\n![Barplot visualizing the realization of recipient](inference_files/figure-html/figi-uni-cat-visual-dative-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nThe question we want to address, however, is whether this numerical difference is in fact a statistical difference.\n\n**Statistical interrogation**\n\n<!-- TODO:\n- consider binom.test() vs. chisq.test() for two outcome univariate\n-->\n\nTo statistical assess the distribution for a categorical variable, we will turn to the Chi-squared test. This test aims to gauge whether the numerical differences between 'NP' and 'PP' counts observed in the data is greater than what would be expected by chance. Chance in the case where there are only two possible outcome levels is 50/50. For our particular data where there are 3,263 observations half would be 'NP' and the other half 'PP' --specifically 1631.5 for each. \n\nTo run this test we first will need to create a cross-tabulation of the variable. We will use the `xtabs()` function to create the table.\n\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-uni-cat-table-dative_42461cf8ad599a6e29f851b4a8da1b03'}\n\n```{.r .cell-code}\nror_table <- \n  xtabs(formula = ~ realization_of_recipient, # formula selecting the variable\n        data = dative) # dataset\n\nror_table # preview\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> realization_of_recipient\n#>   NP   PP \n#> 2414  849\n```\n:::\n:::\n\n\nNo new information here, but the format (i.e. an object of class 'table') is what is important for the input argument for the `chisq.test()` function we will use to run the test. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-uni-cat-chi-squared-dative_d425912f1e9d26e8542ce1de9d7c35d7'}\n\n```{.r .cell-code}\nc1 <- chisq.test(x = ror_table) # apply the chi-squared test to `ror_table`\n\nc1 # preview the test results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> \tChi-squared test for given probabilities\n#> \n#> data:  ror_table\n#> X-squared = 751, df = 1, p-value <2e-16\n```\n:::\n:::\n\n\nThe preview of the `c1` object reveals the main information of interest including the Chi-squared statistic, the degrees of freedom, and the $p$-value (albeit in scientific notation). However, the `c1` is an 'htest' object an includes a number of other pieces information about the test.\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-uni-cat-htest-dative_4d6a095b4fa7b46bad4a5ee3eca6b8b0'}\n\n```{.r .cell-code}\nnames(c1) # preview column names\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] \"statistic\" \"parameter\" \"p.value\"   \"method\"    \"data.name\" \"observed\" \n#> [7] \"expected\"  \"residuals\" \"stdres\"\n```\n:::\n:::\n\n\nFor our purposes let's simply confirm that the $p$-value is lower than the standard .05 threshold for statistical significance.\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-uni-cat-p-value-confirm_0c1e59789989c72a842dd6b19f80202c'}\n\n```{.r .cell-code}\nc1$p.value < .05 # confirm p-value below .05\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] TRUE\n```\n:::\n:::\n\n\nOther information can be organized in a more readable format using the broom package's `augment()` function. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-uni-cat-tidy-results_4b8e236753363de2810f225af3dd8587'}\n\n```{.r .cell-code}\nc1 |> # statistical result\n  augment() # view detailed statistical test information\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 2 × 6\n#>   realization_of_recipient .observed .prop .expected .resid .std.resid\n#>   <fct>                        <int> <dbl>     <dbl>  <dbl>      <dbl>\n#> 1 NP                            2414 0.740     1632.   19.4       27.4\n#> 2 PP                             849 0.260     1632.  -19.4      -27.4\n```\n:::\n:::\n\n\nHere we can see the observed and expected counts and the proportions for each level of `realization_of_recipient`. We also get additional information concerning residuals, but we will leave these aside. \n\n**Evaluation**\n\nAt this point we may think we are done. We have statistically interrogated the `realization_of_recipient` variable and found that the difference between 'NP' and 'PP' realization in the datives in this dataset is statistically significant. However, we need to evaluate the size ('effect size') and the reliability of the effect ('confidence interval'). The effectsize package provides a function `effectsize()` that can provide us both the effect size and confidence interval.\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-uni-cat-eval-dative_6dea090f6507e6dfd3beb819e2eb75e3'}\n\n```{.r .cell-code}\neffects <- \n  effectsize(c1) # evaluate effect size and generate a confidence interval (fei type given 2x1 contingency table)\n\neffects # preview effect size and confidence interval\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Fei  |       95% CI\n#> -------------------\n#> 0.48 | [0.45, 1.00]\n#> \n#> - Adjusted for uniform expected probabilities.\n#> - One-sided CIs: upper bound fixed at [1.00].\n```\n:::\n:::\n\n\n`effectsize()` recognizes the type of test results in `c1` and calculates the appropriate effect size measure and generates a confidence interval. Since the effect statistic (\"Fei\") falls between the 95\\% confidence interval this suggests the results are reliably interpreted (chances of Type I (false positive) or Type II (false negative) are low). \n\nNow, the remaining question is to evaluate whether the significant result here is a strong effect or not. To do this we can pass the effect size measure to the `interpret_r()` function. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-uni-cat-eval-dative-effect_6c2ddee8e8a736d478895ab7ab3c384d'}\n\n```{.r .cell-code}\ninterpret_r(effects$Fei) # interpret the effect size \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] \"very large\"\n#> (Rules: funder2019)\n```\n:::\n:::\n\n\nTurns out we have a strong effect; the realization of dative alternation heavily favors the 'NP' form in our data. The potential reasons why are not considered in this univariate analysis, but we will return to this question later as we add independent variables to the statistical analysis. \n\n### Continuous\n\nNow let's turn to a case when the variable we aim to interrogate is non-categorical. For this case we will turn to the `sdac_disfluencies` dataset. Specifically we will aim to test whether the use of fillers is normally distributed across speakers.\n\n::: callout-warning\n## Tip\nThis is an important step when working with numeric dependent variables as the type of distribution will dictate decisions about whether we will use parametric or non-parametric tests if we consider the extent to which an independent variable (or variables) can explain the variation of the dependent variable.\n:::\n\nSince the dataset is currently organized around fillers as the observational unit, I will first transform this dataset to sum the use of fillers for each speaker in the dataset. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-uni-cont-sdac-transform_7b54cfca5d8363e36dc88e9e466c0a36'}\n\n```{.r .cell-code}\nsdac_speaker_fillers <- \n  sdac_disfluencies |> # dataset\n  group_by(speaker_id) |> # group by each speaker\n  summarise(sum = sum(count)) |> # add up all fillers used\n  ungroup() # remove grouping parameter\n```\n:::\n\n::: {#tbl-i-uni-cont-sdac-transform-preview .cell layout-align=\"center\" tbl-cap='First 10 observations of `sdac_speaker_fillers` dataset.' hash='inference_cache/html/tbl-i-uni-cont-sdac-transform-preview_93979d474a4f5828a9124520d5268688'}\n::: {.cell-output-display}\n|speaker_id | sum|\n|:----------|---:|\n|155        |  28|\n|1000       |  45|\n|1001       | 264|\n|1002       |  54|\n|1004       |  45|\n|1005       | 129|\n|1007       |   0|\n|1008       |  27|\n|1010       |   2|\n|1011       |  54|\n:::\n:::\n\n\n**Descriptive assessment**\n\nLet's perform some descriptive assessement of the variable of interest `sum`. First let's apply the `skim()` function and retrieve just the relevant numeric descriptors with `yank()`. One twist here, however, is that I've customized the `skim()` function using the `skim_with()` to remove the default histogram and add the Interquartile Range (IQR) to the output. This new skim function `num_skim()` will take the place of `skim()`.\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-uni-cont-sdac-description_323a0fbe7fbcbe6296c625155290e2bd'}\n\n```{.r .cell-code}\nnum_skim <- \n  skim_with(numeric = sfl(hist = NULL, # remove hist skim\n                                   iqr = IQR)) # add IQR to skim\n\nsdac_speaker_fillers |> # dataset\n  select(sum) |> # variable of interest\n  num_skim() |> # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\n```\n\n::: {.cell-output-display}\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate| mean|  sd| p0| p25| p50| p75| p100| iqr|\n|:-------------|---------:|-------------:|----:|---:|--:|---:|---:|---:|----:|---:|\n|sum           |         0|             1| 87.1| 108|  0|  16|  45| 114|  668|  98|\n:::\n:::\n\n\nWe see here that the mean use of fillers is 87.1 across speakers. However, the standard deviation and IQR are large relative to this mean which indicates that the dispersion is quite large, in other words this suggests that there are large differences between speakers. Furthermore, since the median (p50) is smaller than the mean, the distribution is right skewed.\n\nLet's look a couple visualizations of this distribution to appreciate these descriptives. A histogram will provide us a view of the distribution using the counts of the values of `sum` and a density plot will provide a smooth curve which represents the scaled distribution of the observed data.\n\n<!-- TODO:\n- consider generating a normal distribution to overlay on the density plot\nhttps://homepage.divms.uiowa.edu/~luke/classes/STAT4580/histdens.html\n- consider log-transformed distribution?\n  - View `sum` as order of magnitudes\n-->\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/fig-i-uni-cont-sdac-visual_c8901f45fbb5f726cbaa076a759ff7a2'}\n\n```{.r .cell-code}\np1 <- \n  sdac_speaker_fillers |> # dataset\n  ggplot(aes(sum)) + # mapping\n  geom_histogram() +  # geometry\n  labs(x = \"Fillers\", y = \"Count\")\n\np2 <- \n  sdac_speaker_fillers |> # dataset\n  ggplot(aes(sum)) + # mapping\n  geom_density() + # geometry\n  geom_rug() +  # visualize individual observations\n  labs(x = \"Fillers\", y = \"Density\")\n\np1 + p2 + plot_annotation(\"Filler count distributions.\")\n```\n\n::: {.cell-output-display}\n![](inference_files/figure-html/fig-i-uni-cont-sdac-visual-1.png){#fig-i-uni-cont-sdac-visual fig-align='center' width=480}\n:::\n:::\n\n\nFrom the plots in @fig-i-uni-cont-sdac-visual we see that our initial intuitions about the distribution of `sum` are correct. There is large dispersion between speakers and the data distribution is right skewed. \n\n::: callout-warning\n## Tip\nNote that I've used the patchwork package for organizing the display of plots and including a plot annotation label.\n:::\n\n\nSince our aim is to test for normality, we can generate a Quantile-Quantile plots (QQ Plot). \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-uni-cont-sdac-qq-plot_d5db89e579034f1964f20d9303d4d32c'}\n\n```{.r .cell-code}\nsdac_speaker_fillers |> # dataset\n  ggplot(aes(sample = sum)) + # mapping\n  stat_qq() + # calculate expected quantile-quantile distribution\n  stat_qq_line() # plot the qq-line\n```\n\n::: {.cell-output-display}\n![](inference_files/figure-html/i-uni-cont-sdac-qq-plot-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nSince many points do not fall on the expected normal distribution line we have even more evidence to support the notion that the distribution of `sum` is non-normal.\n\n**Statistical interrogation**\n\nAlthough the descriptives and visualizations strongly suggest that we do not have normally distributed data let's run a normality test. For this we turn to the `shapiro.test()` function which performs the Shapiro-Wilk test of normality. We pass the `sum` variable to this function to run the test.\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-uni-cont-sdac-test_d9546104625214e35d11d1c29db47871'}\n\n```{.r .cell-code}\ns1 <- shapiro.test(sdac_speaker_fillers$sum) # apply the normality test to `sum`\n\ns1 # preview the test results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> \tShapiro-Wilk normality test\n#> \n#> data:  sdac_speaker_fillers$sum\n#> W = 0.8, p-value <2e-16\n```\n:::\n:::\n\n\nAs we saw with the results from the `chisq.test()` function, the `shapiro.test()` function produces an object with information about the test including the $p$-value. Let's run our logical test to see if the test is statistically significant. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-uni-cont-sdac-test-confirm_3f2c99708456e18c3f7bcaa781e3bea9'}\n\n```{.r .cell-code}\ns1$p.value < .05 # \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] TRUE\n```\n:::\n:::\n\n\n**Evaluation**\n\nThe results from the Shapiro-Wilk Normality Test tell us that the distribution of `sum` is statistically found to differ from the normal distribution. So in this case, statistical significance suggests that `sum` cannot be used as a parametric dependent variable. For our aims this is all the evaluation required. Effect size and confidence intervals are not applicable.\n\nIt is of note, however, that the expectation that the variable `sum` would conform to the normal distribution was low from the outset as we are working with count data. Count data, or frequencies, are in a strict sense not continuous, but rather discrete --meaning that they are real numbers (whole numbers which are always positive). This is a common informational type to encounter in text analysis. \n\n## Bivariate analysis\n\nA more common scenario in statistical analysis is the consideration of the relationship between two-variables, known as bivariate analysis. \n\n### Categorical\n\nLet's build on our univariate analysis of `realization_of_recipient` and include an explanatory, or independent variable which we will explore to test whether it can explain our earlier finding that 'NP' datives are more common that 'PP' datives. The question to test, then, is whether modality explains the distribution of the `realization_of_recipient`. \n\n**Descriptive assessment**\n\nBoth the `realization_of_recipient` and `modality` variables are categorical, specifically nominal as we can see by using `skim()`. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-bi-cat-description_347f0d7c9328c1d17b64cb99c31775a4'}\n\n```{.r .cell-code}\ndative |> \n  select(realization_of_recipient, modality) |> # select key variables\n  skim() |> # get custom data summary\n  yank(\"factor\") # only show factor-oriented information\n```\n\n::: {.cell-output-display}\n**Variable type: factor**\n\n|skim_variable            | n_missing| complete_rate|ordered | n_unique|top_counts          |\n|:------------------------|---------:|-------------:|:-------|--------:|:-------------------|\n|realization_of_recipient |         0|             1|FALSE   |        2|NP: 2414, PP: 849   |\n|modality                 |         0|             1|FALSE   |        2|spo: 2360, wri: 903 |\n:::\n:::\n\n\nFor this reason measures of central tendency are not applicable and we will turn to a contingency table to summarize the relationship. The janitor package has a set of functions, the primary function being `tabyl()`. Other functions used here are to adorn the contingency table with totals, percentages, and to format the output for readability. \n\n\n::: {#tbl-i-bi-cat-contingency-table .cell layout-align=\"center\" tbl-cap='Contingency table for `realization_of_recipient` and `modality`.' hash='inference_cache/html/tbl-i-bi-cat-contingency-table_467456ca128fb627a8fc9a61c729746c'}\n\n```{.r .cell-code}\ndative |> \n  tabyl(realization_of_recipient, modality) |> # cross-tabulate\n  adorn_totals(c(\"row\", \"col\")) |> # provide row and column totals\n  adorn_percentages(\"col\") |> # add percentages to the columns\n  adorn_pct_formatting(rounding = \"half up\", digits = 0) |> # round the digits\n  adorn_ns() |> # add observation number\n  adorn_title(\"combined\") |> # add a header title\n  kable(booktabs = TRUE) # pretty table)\n```\n\n::: {.cell-output-display}\n|realization_of_recipient/modality |spoken      |written    |Total       |\n|:---------------------------------|:-----------|:----------|:-----------|\n|NP                                |79% (1859)  |61% (555)  |74% (2414)  |\n|PP                                |21%  (501)  |39% (348)  |26%  (849)  |\n|Total                             |100% (2360) |100% (903) |100% (3263) |\n:::\n:::\n\n\nTo gain a better appreciation for this relationship let's generate a couple plots one which shows cross-tabulated counts and the other calculated proportions. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-bi-cat-visual_cb0a4db3cd01c7c38b4261dbe030b85c'}\n\n```{.r .cell-code}\np1 <- \n  dative |> # dataset\n  ggplot(aes(x = realization_of_recipient, fill = modality)) + # mappings\n  geom_bar() + # geometry\n  labs(y = \"Count\", x = \"Realization of recipient\") # labels\n\np2 <- \n  dative |> # dataset\n  ggplot(aes(x = realization_of_recipient, fill = modality)) + # mappings\n  geom_bar(position = \"fill\") + # geometry, with fill for proportion plot\n  labs(y = \"Proportion\", x = \"Realization of recipient\", fill = \"Modality\") # labels\n\np1 <- p1 + theme(legend.position = \"none\") # remove legend from left plot\n\np1 + p2 + plot_annotation(\"Relationship between Realization of recipient and Modality.\")\n```\n\n::: {.cell-output-display}\n![](inference_files/figure-html/i-bi-cat-visual-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nLooking at the count plot (in the left pane) we see that large difference between the realization of the dative as an 'NP' or 'PP' obscures to some degree our ability to see to what degree modality is related to the realization of the dative. So, a proportion plot (in the right pane) standardizes each level of `realization_of_recipient` to provide a more comparable view. From the proportion plot we see that there appears to be a trend towards more use of 'PP' than 'NP' in the written modality.\n\n**Statistical interrogation**\n\nAlthough the proportion plot is visually helpful, we use the raw counts to statistically analyze this relationship. Again, as we are working with categorical variables, now for a dependent and independent variable, we use the Chi-squared test. And as before we need to create the cross-tabulation table to pass to the `chisq.test()` to perform the test. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-bi-cat-test_6467d7c3ac4ef86f55d7264476da3452'}\n\n```{.r .cell-code}\nror_mod_table <- \n  xtabs(formula = ~ realization_of_recipient + modality, # formula \n        data = dative) # dataset\n\nc2 <- chisq.test(ror_mod_table) # apply the chi-squared test to `ror_mod_table`\n\nc2 # # preview the test results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> \tPearson's Chi-squared test with Yates' continuity correction\n#> \n#> data:  ror_mod_table\n#> X-squared = 101, df = 1, p-value <2e-16\n```\n:::\n\n```{.r .cell-code}\nc2$p.value < .05 # confirm p-value below .05\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] TRUE\n```\n:::\n:::\n\n\nWe can preview the result and provide a confirmation of the $p$-value. This evidence suggests that there is a difference between the distribution of dative realization according to modality. \n\nWe can also see more details about the test.\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-bi-cat-test-information_d9d1e4cfc56efc8eb17de5887d3b3f10'}\n\n```{.r .cell-code}\nc2 |> # statistical result\n  augment() # view detailed statistical test information\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 4 × 9\n#>   realization_of_…¹ modal…² .obse…³ .prop .row.…⁴ .col.…⁵ .expe…⁶ .resid .std.…⁷\n#>   <fct>             <fct>     <int> <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>\n#> 1 NP                spoken     1859 0.570   0.770   0.788   1746.   2.71    10.1\n#> 2 PP                spoken      501 0.154   0.590   0.212    614.  -4.56   -10.1\n#> 3 NP                written     555 0.170   0.230   0.615    668.  -4.37   -10.1\n#> 4 PP                written     348 0.107   0.410   0.385    235.   7.38    10.1\n#> # … with abbreviated variable names ¹​realization_of_recipient, ²​modality,\n#> #   ³​.observed, ⁴​.row.prop, ⁵​.col.prop, ⁶​.expected, ⁷​.std.resid\n```\n:::\n:::\n\n\n**Evaluation**\n\nNow we want to calculate the effect size and the confidence interval to provide measures of assurance that our finding is robust.\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-bi-cat-effect-ci_3556e6f565cec5e67be5d501f5e703b8'}\n\n```{.r .cell-code}\neffects <- effectsize(c2) # evaluate effect size and generate a confidence interval\n\neffects # preview effect size and confidence interval\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Cramer's V (adj.) |       95% CI\n#> --------------------------------\n#> 0.18              | [0.15, 1.00]\n#> \n#> - One-sided CIs: upper bound fixed at [1.00].\n```\n:::\n\n```{.r .cell-code}\ninterpret_r(effects$Cramers_v) # interpret the effect size\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] \"small\"\n#> (Rules: funder2019)\n```\n:::\n:::\n\n\nWe get effect size and confidence interval information. Note that the effect size, reflected by Cramer's V, for this relationship is weak. This points out an important aspect to evaluation of statistical tests. The fact that a test is significant does not mean that it is meaningful. A small effect size suggests that we should be cautious about the extent to which this significant finding is robust in the population from which the data is sampled.\n\n\n### Continuous\n\nFor a bivariate analysis in which the dependent variable is not categorical, we will turn to the `sdac_disfluencies` dataset. The question we will pose to test is whether the use of fillers is related to the type of filler ('uh' or 'um'). \n\n**Descriptive assessment**\n\nThe key variables to assess in this case are the variables `count` and `filler`. But before we start to explore this relationship we will need to transform the dataset such that each speaker's use of the levels of `filler` is summed. We will use `group_by()` to group `speaker_id` and `filler` combinations and then use `summarize()` to then sum the counts for each filler type for each speaker\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-bi-cont-sdac-fillers_950c6722a23eab582ff4df313200db14'}\n\n```{.r .cell-code}\nsdac_fillers <- \n  sdac_disfluencies |> # dataset\n  group_by(speaker_id, filler) |> # grouping parameters\n  summarize(sum = sum(count)) |> # summed counts for each speaker-filler combination\n  ungroup() # remove the grouping parameters\n```\n:::\n\n\nLet's preview this transformation.\n\n\n::: {#tbl-i-bi-cont-sdac-fillers-preview .cell layout-align=\"center\" tbl-cap='First 10 observations from `sdac_fillers` dataset.' hash='inference_cache/html/tbl-i-bi-cont-sdac-fillers-preview_9dc3db60e4cbf7150eb0cb0d4829b3af'}\n::: {.cell-output-display}\n|speaker_id |filler | sum|\n|:----------|:------|---:|\n|155        |uh     |  28|\n|155        |um     |   0|\n|1000       |uh     |  37|\n|1000       |um     |   8|\n|1001       |uh     | 262|\n|1001       |um     |   2|\n|1002       |uh     |  34|\n|1002       |um     |  20|\n|1004       |uh     |  30|\n|1004       |um     |  15|\n:::\n:::\n\n\nLet's take a look at them together by grouping the dataset by `filler` and then using the custom skim function `num_skim()` for the numeric variable`count`. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-bi-cont-description_acace7594dae249e111bb305f28d973a'}\n\n```{.r .cell-code}\nsdac_fillers |> # dataset\n  group_by(filler) |> # grouping parameter\n  num_skim() |> # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\n```\n\n::: {.cell-output-display}\n**Variable type: numeric**\n\n|skim_variable |filler | n_missing| complete_rate| mean|   sd| p0| p25| p50| p75| p100| iqr|\n|:-------------|:------|---------:|-------------:|----:|----:|--:|---:|---:|---:|----:|---:|\n|sum           |uh     |         0|             1| 71.4| 91.5|  0|  14|  39|  91|  661|  77|\n|sum           |um     |         0|             1| 15.7| 31.0|  0|   0|   4|  16|  265|  16|\n:::\n:::\n\n\nWe see here that the standard deviation and IQR for both 'uh' and 'um' are relatively large for the respective means (71.4 and 15.7) suggesting the distribution is quite dispersed. Let's take a look at a boxplot to visualize the counts in `sum` for each level of `filler`. \n\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-bi-cont-visual_44df833e7a33598fc97629afeec1b2a9'}\n\n```{.r .cell-code}\np1 <- \n  sdac_fillers |> # dataset\n  ggplot(aes(x = filler, y = sum)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  labs(x = \"Filler\", y = \"Counts\") # labels\n\np2 <- \n  sdac_fillers |> # dataset\n  ggplot(aes(x = filler, y = sum)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  ylim(0, 100) + # scale the y axis to trim outliers\n  labs(x = \"Filler\", y = \"\") # labels\n\np1 + p2\n```\n\n::: {.cell-output-display}\n![](inference_files/figure-html/i-bi-cont-visual-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nIn the plot in the left pane we see a couple things. First, it appears that there is in fact quite a bit of dispersion as there are quite a few outliers (dots) above the lines extending from the boxes. Recall that the boxes represent the first and third quantile, that is the IQR and that the notches represent the confidence interval. Second, when we compare the boxes and their notches we see that there is little overlap (looking horizontally). In the right pane I've zoomed in a bit trimming some outliers to get a better view of the relationship between the boxes. Since the overlap is minimal and in particular the notches do not overlap at all, this is a good indication that there is a significant trend.\n\nFrom the descriptive statistics and the visual summary it appears that the filler 'uh' is more common than 'um'. It's now time to submit this to statistical interrogation. \n\n**Statistical interrogation**\n\n<!-- TODO:\n- Investigate How to choose a family\n  - https://www.researchgate.net/post/How-to-determine-which-family-function-to-use-when-fitting-generalized-linear-model-glm-in-R\n-->\n\nIn a bivariate (and multivariate) analysis where the dependent variable is non-categorical we apply Linear Regression Modeling (LM). The default assumption of linear models, however, is that the dependent variable is normally distributed. As we have seen our variable `sum` does not conform to the normal distribution. We know this because of our tests in the univariate case, but as mentioned at the end of that section, we are working with count data which by nature is understood as discrete and not continuous in a strict technical sense. So instead of using the linear model for our regression analysis we will use the Generalized Linear Model (GLM) [@Baayen2008a; @Gries2013a]. \n\nThe function `glm()` implements generalized linear models. In addition to the formula (`sum ~ filler`) and the dataset to use, we also include an appropriate distribution family for the dependent variable. For count and frequency data the appropriate family is the \"Poisson\" distribution. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-bi-cont-test_c69b75ea30e4238d1e74ae536242e65a'}\n\n```{.r .cell-code}\nm1 <- \n  glm(formula = sum ~ filler, # formula\n      data = sdac_fillers, # dataset\n      family = \"poisson\") # distribution family\n\nsummary(m1) # preview the test results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Call:\n#> glm(formula = sum ~ filler, family = \"poisson\", data = sdac_fillers)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -11.95   -5.61   -3.94    0.80   41.99  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  4.26794    0.00564     757   <2e-16 ***\n#> fillerum    -1.51308    0.01327    -114   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 72049  on 881  degrees of freedom\n#> Residual deviance: 55071  on 880  degrees of freedom\n#> AIC: 58524\n#> \n#> Number of Fisher Scoring iterations: 6\n```\n:::\n:::\n\n\nLet's focus on the coefficients, specifically for the 'fillerum' line. Since our factor `filler` has two levels one level is used as the reference to contrast with the other level. In this case by default the first level is used as the reference. Therefore the coefficients we see in 'fillerum' are 'um' in contrast to 'uh'. Without digging into the details of the other parameter statistics, let's focus on the last column which contains the $p$-value. A convenient aspect of the `summary()` function when applied to regression model results is that it provides statistical significance codes. In this case we can see that the contrast between 'uh' and 'um' is signficant at $p < .001$ which of course is lower than our standard threshold of $.05$.  \n\nTherefore we can say with some confidence that the filler 'uh' is more frequent than 'um'. \n\n**Evaluation**\n\nGiven we have found a significant effect for `filler`, let's look at evaluating the effect size and the confidence interval. Again, we use the `effectsize()` function. We then can preview the `effects` object. Note that effect size of interest is in the second row of the coefficient (`Std_Coefficient`) so we subset this column to extract only the effect coefficient for the `filler` contrast.\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-bi-cont-eval-test_a5a82d0624c2697808ee56e9b2d5e782'}\n\n```{.r .cell-code}\neffects <- effectsize(m1) # evaluate effect size and generate a confidence interval\n\neffects # preview effect size and confidence interval\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # Standardization method: refit\n#> \n#> Parameter   | Std. Coef. |         95% CI\n#> -----------------------------------------\n#> (Intercept) |       4.27 | [ 4.26,  4.28]\n#> fillerum    |      -1.51 | [-1.54, -1.49]\n#> \n#> - Response is unstandardized.\n```\n:::\n\n```{.r .cell-code}\ninterpret_r(effects$Std_Coefficient[2]) # interpret the effect size\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] \"very large\"\n#> (Rules: funder2019)\n```\n:::\n:::\n\n\nThe coefficient statistic falls within the confidence interval and the effect size is strong so we can be confident that our findings are reliable given this data. \n\n## Multivariate analysis\n\nThe last case to consider is when we have more than one independent variable we want to use to assess their potential relationship to the dependent variable. Again we will consider a categorical and non-categorical dependent variable. But, in this case the implementation methods are quite similar, as we will see. \n\n### Categorical\n\nFor the categorical multivariate case we will again consider the `dative` dataset and build on the previous analyses. The question to be posed is whether modality in combination with the length of the recipient (`length_of_recipient`) together explain the distribution of the realization of the recipient (`realization_of_recipient`).\n\n**Descriptive assessment**\n\nNow that we have three variables, there is more to summarize to get our descriptive information. Luckily, however, the same process can be applied to three (or more) variables using the `group_by()` function and then passed to `skim()`. In this case we have two categorical variables and one numeric variable. So we will group by both the categorical variables and then pass the numeric variable to the custom `num_skim()` function --pulling out only the relevant descriptive information for numeric variables with `yank()`. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-multi-cat-description_12de15d0d2f8adb7db713e88cb797171'}\n\n```{.r .cell-code}\ndative |> # dataset\n  select(realization_of_recipient, modality, length_of_recipient) |> # select key variables\n  group_by(realization_of_recipient, modality) |> # grouping parameters\n  num_skim() |> # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\n```\n\n::: {.cell-output-display}\n**Variable type: numeric**\n\n|skim_variable       |realization_of_recipient |modality | n_missing| complete_rate| mean|   sd| p0| p25| p50| p75| p100| iqr|\n|:-------------------|:------------------------|:--------|---------:|-------------:|----:|----:|--:|---:|---:|---:|----:|---:|\n|length_of_recipient |NP                       |spoken   |         0|             1| 1.14| 0.60|  1|   1|   1|   1|   12|   0|\n|length_of_recipient |NP                       |written  |         0|             1| 1.95| 1.59|  1|   1|   2|   2|   17|   1|\n|length_of_recipient |PP                       |spoken   |         0|             1| 2.30| 2.04|  1|   1|   2|   3|   15|   2|\n|length_of_recipient |PP                       |written  |         0|             1| 4.75| 4.10|  1|   2|   4|   6|   31|   4|\n:::\n:::\n\n\nThere is much more information now that we are considering multiple independent variables, but if we look over the measures of dispersion we can see that the median and the IQR are relatively similar to their respective means suggesting that there are fewer outliers and relativley little skew. \n\nLet's take a look at a visualization of this information. Since we are working with a categorical dependent variable and there is one non-categorical variable we can use a boxplot. The addition here is to include a `color` mapping which will provide a distinct box for each level of modality ('written' and 'spoken'). \n\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-multi-cat-visual_5ed4d34df87b18ea87e6c6f99f83d4d5'}\n\n```{.r .cell-code}\np1 <- \n  dative |> # dataset\n  ggplot(aes(x = realization_of_recipient, y = length_of_recipient, color = modality)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  labs(x = \"Realization of recipient\", y = \"Length of recipient (in words)\", color = \"Modality\") # labels\n\np2 <- \n  dative |> # dataset\n  ggplot(aes(x = realization_of_recipient, y = length_of_recipient, color = modality)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  ylim(0, 15) + # scale the y axis to trim outliers\n  labs(x = \"Realization of recipient\", y = \"\", color = \"Modality\") # labels\n\np1 <- p1 + theme(legend.position = \"none\") # remove the legend from the left pane plot\n\np1 + p2\n```\n\n::: {.cell-output-display}\n![](inference_files/figure-html/i-multi-cat-visual-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nIn the left pane we see the entire visualization including all outliers. From this view it appears that there is a potential trend that the length of the recipient is larger when the realization of the recipient is 'PP'. There is also a potential trend for modality with written language showing longer recipient lengths overall. The pane on the right is scaled to get a better view of the boxes by scaling the y-axis down and as such trimming the outliers. This plot shows more clearly that the length of the recipient is longer when the recipient is realized as a 'PP'. Again, the contrast in modality is also a potential trend, but the boxes (of the same color), particularly for the spoken modality overlap to some degree. \n\nSo we have some trends in mind which will help us interpret the statistical interrogation so let's move there next.\n\n**Statistical interrogation**\n\nOnce we involve more than two variables, the choice of statistical method turns towards regression. In the case that the dependent variable is categorical, however, we will use Logistic Regression. The workhorse function `glm()` can be used for a series of regression models, including logistic regression. The requirement, however, is that we specify the family of the distribution. For logistic regression the family is \"binomial\". The formula includes the dependent variable as a function of our other two variables, each are separated by the `+` operator.  \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-multi-cat-test_e079219cb7cc431557c9fc4700d724d3'}\n\n```{.r .cell-code}\nm1 <- glm(formula = realization_of_recipient ~ modality + length_of_recipient, # formula\n          data = dative, # dataset\n          family = \"binomial\") # distribution family\n\nsummary(m1) # preview the test results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Call:\n#> glm(formula = realization_of_recipient ~ modality + length_of_recipient, \n#>     family = \"binomial\", data = dative)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -4.393  -0.598  -0.598   0.132   1.924  \n#> \n#> Coefficients:\n#>                     Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)          -2.3392     0.0797  -29.35   <2e-16 ***\n#> modalitywritten      -0.0483     0.1069   -0.45     0.65    \n#> length_of_recipient   0.7081     0.0420   16.86   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 3741.1  on 3262  degrees of freedom\n#> Residual deviance: 3104.7  on 3260  degrees of freedom\n#> AIC: 3111\n#> \n#> Number of Fisher Scoring iterations: 5\n```\n:::\n:::\n\n\nThe results from the model again provide a wealth of information. But the key information to focus on is the coefficients. In particular the coefficients for the independent variables `modality` and `length_of_recipient`. What we notice, is that the $p$-value for `length_of_recipient` is significant, but the contrast between 'written' and 'spoken' for `modality` is not. If you recall, we used this same dataset to explore `modality` as a single indpendent variable earlier --and it was found to be significant. So why now is it not? The answer is that when multiple variables are used to explain the distribution of a measure (dependent variable) each variable now adds more information to explain the dependent variable --each has it's own contribution. Since `length_of_recipient` is significant, this suggests that the explanatory power of `modality` is weak, especially when compared to `length_of_recipient`. This make sense as we saw in the earlier model the fact that the effect size for `modality` was not strong and that is now more evident that the `length_of_recipient` is included in the model.\n\n\n**Evaluation**\n\nNow let's move on and gauge the effect size and calculate the confidence interval for `length_of_recipient` in our model. We apply the `effectsize()` function to the model and then use `interpret_r()` on the coefficient of interest (which is in the fourth row of the `Std_Coefficients` column).\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-multi-cat-effects_3a5bfb1dd0b3d49cc5bb7ccd5ea35362'}\n\n```{.r .cell-code}\neffects <- effectsize(m1) # evaluate effect size and generate a confidence interval\n\neffects # preview effect size and confidence interval\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # Standardization method: refit\n#> \n#> Parameter           | Std. Coef. |         95% CI\n#> -------------------------------------------------\n#> (Intercept)         |      -1.03 | [-1.15, -0.92]\n#> modalitywritten     |      -0.05 | [-0.26,  0.16]\n#> length_of_recipient |       1.46 | [ 1.30,  1.64]\n#> \n#> - Response is unstandardized.\n```\n:::\n\n```{.r .cell-code}\ninterpret_r(effects$Std_Coefficient[4]) # interpret the effect size\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] NA\n#> (Rules: funder2019)\n```\n:::\n:::\n\n\nWe see we have a coefficient that falls within the confidence interval and the effect size is large. So we can saw with some confidence that the length of the recipient is a significant predictor of the use of 'PP' as the realization of the recipient in the dative alternation.\n\n### Continuous\n\nThe last case we will consider here is when the dependent variable is non-categorical and we have more than one independent variable. The question we will pose is whether the type of filler and the sex of the speaker can explain the use of fillers in conversational speech. \n\nWe will need to prepare the data before we get started as our current data frame `sdac_fillers` has filler and the sum count for each filler grouped by speaker --but it does not include the `sex` of each speaker. The `sdac_disfluencies` data frame does have the `sex` column, but it has not been grouped by speaker. So let's transform the `sdac_disfluencies` summarizing it to only get the `speaker_id` and `sex` combinations. This should result in a data frame with 441 observations, one observation for each speaker in the corpus.\n\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-multi-cont-transform-sdac_b99355c2c0864eb5e4570c52e9b53247'}\n\n```{.r .cell-code}\nsdac_speakers_sex <- \n  sdac_disfluencies |> # dataset\n  distinct(speaker_id, sex) # summarize for distinct `speaker_id` and `sex` values\n```\n:::\n\n\nLet's preview the first 10 observations form this transformation.\n\n\n::: {#tbl-i-multi-cont-transform-sdac-preview .cell layout-align=\"center\" tbl-cap='First 10 observations of the `sdac_speakers_sex` data frame.' hash='inference_cache/html/tbl-i-multi-cont-transform-sdac-preview_cd0c66fd828e8050fc6635f28f9489ff'}\n::: {.cell-output-display}\n|speaker_id |sex    |\n|:----------|:------|\n|155        |NA     |\n|1000       |FEMALE |\n|1001       |MALE   |\n|1002       |FEMALE |\n|1004       |FEMALE |\n|1005       |FEMALE |\n|1007       |FEMALE |\n|1008       |FEMALE |\n|1010       |MALE   |\n|1011       |FEMALE |\n:::\n:::\n\n\nGreat, now we have each `speaker_id` and `sex` for all 441 speakers. One thing to note, however, is that speaker '155' does not have a value for `sex` --this seems to be an error in the metadata that we will need to deal with before we proceed in our analysis. Let's move on to join our new `sdac_speakers_sex` data frame and the `sdac_fillers` data frame.\n\nNow that we have a complete dataset with `speaker_id` and `sex` we will now join this dataset with our `sdac_fillers` dataset effectively adding the column `sex`. We want to keep all the observations in `sdac_fillers` and add the column `sex` for observations that correspond between each data frame for the column `speaker_id` so we will use a left join with the function `left_join()` with the `sdac_fillers` dataset on the left. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-multi-cont-join-fillers-sex_15ef69d302e99810430166dab32164e9'}\n\n```{.r .cell-code}\nsdac_fillers_sex <- \n  left_join(sdac_fillers, sdac_speakers_sex) # join\n```\n:::\n\n\nNow let's preview the first observations in this new `sdac_fillers_sex` data frame.\n\n\n::: {#tbl-i-multi-cont-sdac-fillers-sex-preview .cell layout-align=\"center\" tbl-cap='First 10 observations of the `sdac_fillers_sex` data frame.' hash='inference_cache/html/tbl-i-multi-cont-sdac-fillers-sex-preview_48aa6ed584b6ca995a7c74900d71e870'}\n::: {.cell-output-display}\n|speaker_id |filler | sum|sex    |\n|:----------|:------|---:|:------|\n|155        |uh     |  28|NA     |\n|155        |um     |   0|NA     |\n|1000       |uh     |  37|FEMALE |\n|1000       |um     |   8|FEMALE |\n|1001       |uh     | 262|MALE   |\n|1001       |um     |   2|MALE   |\n|1002       |uh     |  34|FEMALE |\n|1002       |um     |  20|FEMALE |\n|1004       |uh     |  30|FEMALE |\n|1004       |um     |  15|FEMALE |\n:::\n:::\n\n\nAt this point let's drop this speaker from the `sdac_speakers_sex` data frame. \n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-multi-cont-drop-speaker_e3f96c835fb04caa63dc399171859e57'}\n\n```{.r .cell-code}\nsdac_fillers_sex <- \n  sdac_fillers_sex |> # dataset\n  filter(speaker_id != \"155\") # drop speaker_id 155\n```\n:::\n\n\nWe are now ready to proceed in our analysis.\n\n**Descriptive assessment**\n\nThe process by now should be quite routine for getting our descriptive statistics: select the key variables, group by the categorical variables, and finally pull the descriptives for the numeric variable.\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-multi-cont-description_1bdbe2c72412c1712e6a5d3f3b9cb34a'}\n\n```{.r .cell-code}\nsdac_fillers_sex |> # dataset\n  select(sum, filler, sex) |> # select key variables\n  group_by(filler, sex) |> # grouping parameters\n  num_skim() |> # get custom data summary\n  yank(\"numeric\") # only show numeric-oriented information\n```\n\n::: {.cell-output-display}\n**Variable type: numeric**\n\n|skim_variable |filler |sex    | n_missing| complete_rate|  mean|    sd| p0|  p25|  p50|   p75| p100|  iqr|\n|:-------------|:------|:------|---------:|-------------:|-----:|-----:|--:|----:|----:|-----:|----:|----:|\n|sum           |uh     |FEMALE |         0|             1| 63.22|  76.5|  0| 12.0| 39.0|  81.8|  509| 69.8|\n|sum           |uh     |MALE   |         0|             1| 78.74| 102.6|  0| 15.2| 37.5| 101.5|  661| 86.2|\n|sum           |um     |FEMALE |         0|             1| 22.38|  36.3|  0|  1.0|  9.0|  28.0|  265| 27.0|\n|sum           |um     |MALE   |         0|             1|  9.92|  24.2|  0|  0.0|  1.0|   8.0|  217|  8.0|\n:::\n:::\n\n\nLooking at these descriptives, it seems like there is quite a bit of variability for some combinations and not others. In short, it's a mixed bag. Let's try to make sense of these numbers with a boxplot.\n\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-mulit-cont-visual_8b69230fcf186fb8d78dda39600877dd'}\n\n```{.r .cell-code}\np1 <- \n  sdac_fillers_sex |> # dataset\n  ggplot(aes(x = filler, y = sum, color = sex)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  labs(x = \"Filler\", y = \"Counts\", color = \"Sex\") # labels\n\np2 <- \n  sdac_fillers_sex |> # dataset\n  ggplot(aes(x = filler, y = sum, color = sex)) + # mappings\n  geom_boxplot(notch = TRUE) + # geometry\n  ylim(0, 200) + # scale the y axis to trim outliers\n  labs(x = \"Filler\", y = \"\", color = \"Sex\") # labels\n\np1 <- p1 + theme(legend.position = \"none\") # drop the legend from the left pane plot\n\np1 + p2\n```\n\n::: {.cell-output-display}\n![](inference_files/figure-html/i-mulit-cont-visual-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nWe can see that 'uh' is used more than 'um' overall. But that whereas men and women use 'uh' in similar ways, women use more 'um' than men. This is known as an interaction. So we will approach our statistical analysis with this in mind.\n\n**Statistical interrogation**\n\nWe will again use a generalized linear model with the `glm()` function to conduct our test. The distribution family will be the same has we are again using the `sum` as our dependent variable which contains discrete count values. The formula we will use, however, is new. Instead of adding a new variable to our independent variables, we will test the possible interaction between `filler` and `sex` that we noted in the descriptive assessment. To encode an interaction the `*` operator is used. So our formula will take the form `sum ~ filler * sex`. Let's generate the model and view the summary of the test results as we have done before.\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-multi-cont-test_aefd462ccb5323bb1d366123cf1bc186'}\n\n```{.r .cell-code}\nm1 <- \n  glm(formula = sum ~ filler * sex, # formula\n      data = sdac_fillers_sex, # dataset\n      family = \"poisson\") # distribution family\n\nsummary(m1) # preview the test results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Call:\n#> glm(formula = sum ~ filler * sex, family = \"poisson\", data = sdac_fillers_sex)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -12.55   -6.21   -3.64    1.08   40.60  \n#> \n#> Coefficients:\n#>                  Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)       4.14660    0.00876   473.2   <2e-16 ***\n#> fillerum         -1.03827    0.01714   -60.6   <2e-16 ***\n#> sexMALE           0.21955    0.01145    19.2   <2e-16 ***\n#> fillerum:sexMALE -1.03344    0.02791   -37.0   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 71956  on 879  degrees of freedom\n#> Residual deviance: 53543  on 876  degrees of freedom\n#> AIC: 56994\n#> \n#> Number of Fisher Scoring iterations: 6\n```\n:::\n:::\n\n\nAgain looking at the coefficients we something new. First we see that there is a row for the `filler` contrast and the `sex` contrast but also the interaction between `filler` and `sex` ('fillerum:sexMALE'). All rows show significant effects. It is important to note that when an interaction is explored and it is found to be significant, the other simple effects, known as main effects ('fillerum' and 'sexMALE'), are ignored. Only the higer-order effect is considered significant. \n\nNow what does the 'fillerum:sexMALE' row mean. It means that there is an interaction between `filler` and `sex`. the directionality of that interaction should be interpreted using our descriptive assessment, in particular the visual boxplots we generated. In sum, women use more 'um' than men or stated another way men use 'um' less than women.\n\n**Evaluation**\n\nWe finalize our analysis by looking at the effect size and confidence intervals.\n\n\n::: {.cell layout-align=\"center\" hash='inference_cache/html/i-multi-cont-effects_4a5b11c01620adb1a8e020c0e3799005'}\n\n```{.r .cell-code}\neffects <- effectsize(m1) # evaluate effect size and generate a confidence interval\n\neffects # preview effect size and confidence interval\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # Standardization method: refit\n#> \n#> Parameter        | Std. Coef. |         95% CI\n#> ----------------------------------------------\n#> (Intercept)      |       4.15 | [ 4.13,  4.16]\n#> fillerum         |      -1.04 | [-1.07, -1.00]\n#> sexMALE          |       0.22 | [ 0.20,  0.24]\n#> fillerum:sexMALE |      -1.03 | [-1.09, -0.98]\n#> \n#> - Response is unstandardized.\n```\n:::\n\n```{.r .cell-code}\ninterpret_r(effects$Std_Coefficient[4]) # interpret the effect size\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] \"very large\"\n#> (Rules: funder2019)\n```\n:::\n:::\n\n\nWe can conclude, then, that there is a strong interaction effect for `filler` and `sex` and that women use more 'um' than men. \n\n\n## Summary\n\nIn this chapter we have discussed various approaches to conducting inferential data analysis. Each configuration, however, always includes a descriptive assessment, statistical interrogation, and an evaluation of the results. We considered univariate, bivariate, and multivariate analyses using both categorical and non-categorical dependent variables to explore the similarities and differences between these approaches. \n\n\n\n<!-- IDEAS:\n\n- Use Switchboard Dialogue Act Corpus\n  - Disfluencies (uh/ um)\n\n- languageR::dative\n  \n-------\n\n- Use BELC dataset from \"Approaching analysis\" chapter\n  - Question(s): \n    \n- Santa Barbara Corpus of Spoken American English\n  - analyzr::sbc\n\n- Love on the Spectrum/ Love is Blind\n  - Word frequency differences (from SUBTLEXus) as the dependent variable\n\n\n- Use other datasets from the \"Transform datasets\" chapter\n\nRESOURCES:\n\n- Example for Fillers: https://github.com/WFU-TLC/analyzr/blob/master/vignettes/analysis-an-inference-example.Rmd\n\n- \n\n\nThe chapter begins by defining statistical inference and discussing its importance for linguistics, and then covers different types of statistical inference and the steps involved in preparing data for statistical inference. The chapter also covers statistical tests for hypothesis testing and the interpretation of statistical results, and discusses various applications of statistical inference in the field of linguistics. Finally, the chapter concludes with a recap of the key points covered and a discussion of future directions for statistical inference in linguistics.\n\nI. Introduction to statistical inference\nA. Definition of statistical inference\ni. Statistical inference is the process of drawing conclusions about a population based on statistical analysis of a sample drawn from that population\nii. Statistical inference allows researchers to make generalizations about a population based on a smaller, more manageable sample\nB. Importance of statistical inference for linguistics\ni. Statistical inference is a powerful tool for linguists studying language and communication\nii. Statistical inference allows linguists to make informed conclusions about language patterns and trends based on statistical analysis of language data\n\nII. Types of statistical inference\nA. Descriptive inference\ni. Descriptive inference involves using statistical techniques to describe and summarize a dataset\nii. Examples of descriptive inference include calculating measures of central tendency and dispersion, such as the mean, median, and standard deviation\nB. Inferential inference\ni. Inferential inference involves using statistical techniques to draw conclusions about a population based on a sample\nii. Examples of inferential inference include hypothesis testing and estimating population parameters\n\nIII. Preparing data for statistical inference\nA. Data collection and sampling\ni. Data collection and sampling are crucial for statistical inference, as the quality and representativeness of the data can significantly impact the validity of the inferences drawn\nii. Linguists must carefully consider the sampling design and sampling method used to collect language data\nB. Data cleaning and preprocessing\ni. Data cleaning and preprocessing are important steps in preparing data for statistical inference\nii. This may involve tasks such as removing missing values, correcting errors, and scaling the data\n\nIV. Statistical tests for hypothesis testing\nA. Parametric tests\ni. Parametric tests are statistical tests that assume that the data follows a particular probability distribution, such as the normal distribution\nii. Examples of parametric tests include the t-test and ANOVA\nB. Nonparametric tests\ni. Nonparametric tests do not assume that the data follows a particular probability distribution\nii. Examples of nonparametric tests include the Mann-Whitney U test and the Wilcoxon signed-rank test\n\nV. Interpreting statistical results\nA. Effect size\ni. Effect size is a measure of the strength of the relationship between two variables\nii. Effect size can help researchers to interpret the practical significance of a statistical result\nB. Statistical significance\ni. Statistical significance is a measure of the probability that a statistical result is due to chance\nii. Statistical significance can help researchers to determine whether a result is likely to be true for the population as a whole\nC. Confidence intervals\ni. Confidence intervals are a range of values that are likely to include the true population parameter with a certain ...\nVI. Applications of statistical inference in linguistics\nA. Language evolution\ni. Statistical inference can be used to study the evolution of languages over time\nii. Linguists can use statistical techniques to identify trends and patterns in language change, and to make predictions about future language evolution\nB. Dialectology\ni. Statistical inference can be used to study the variation and change of dialects within a language\nii. Linguists can use statistical techniques to identify regional and social patterns in dialects, and to make inferences about the factors that influence dialectal variation\nC. Corpus linguistics\ni. Statistical inference can be used to study language use and variation in large text corpora\nii. Linguists can use statistical techniques to identify patterns and trends in language use, and to make inferences about the factors that influence language variation\n\nVII. Conclusion\nA. Recap of key points\ni. Statistical inference is the process of drawing conclusions about a population based on statistical analysis of a sample\nii. Statistical inference is an important tool for linguists studying language and communication\niii. Statistical tests can be used to test hypotheses and draw inferences about population parameters\nB. Future directions for statistical inference in linguistics\ni. Statistical inference is an active area of research in linguistics, with many potential areas for further exploration\nii. Future research may involve using advanced statistical techniques and tools, such as machine learning, to study language and communication\niii. As language data continues to grow and become more complex, linguists will likely develop new statistical methods and approaches to analyze this data and draw meaningful inferences.\n\n\n-->\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}