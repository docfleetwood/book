{
  "hash": "4413926533fb6582433aa846ca1c1b8a",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexecute:\n  echo: true\n---\n\n\n::: {.content-visible when-format=\"pdf\"}\n\n```{=latex}\n\\setDOI{10.4324/9781003393764.9}\n\\thispagestyle{chapterfirstpage}\n```\n\n:::\n\n# Predict {#sec-predict-chapter}\n\n\n\n\n\n\n::: {.callout}\n**{{< fa regular list-alt >}} Outcomes**\n\n- Identify the research goals of predictive data analysis\n- Describe the workflow for predictive data analysis\n- Recognize quantitative and qualitative methods for evaluating predictive models\n:::\n\n\n::: {.cell}\n\n:::\n\n\nIn this chapter, I introduce supervised learning as an approach to text analysis. Supervised learning aims to establish a relationship between a target (or outcome) variable and a set of feature variables derived from text data. By leveraging this relationship, statistical generalizations (models) can be created to accurately predict values of the target variable based on the values of the feature variables. Throughout the chapter, we explore practical tasks and theoretical applications of statistical learning in text analysis.\n\n::: {.callout}\n**{{< fa terminal >}} Lessons**\n\n**What**: Advanced Visualization\\\n**How**: In an R console, load {swirl}, run `swirl()`, and follow prompts to select the lesson.\\\n**Why**: To dive deeper into {ggplot2} to enhance visual summaries and provides an introduction to {factoextra} and {ggfortify} that extend {ggplot2} capabilities to model objects.\n:::\n\n## Orientation {#sec-predict-orientation}\n\nPredictive data analysis (PDA) is a powerful analysis method for making predictions about new or future data based on patterns in existing data. PDA is a type of supervised learning, which means that it involves training a model on a labeled dataset where the input data and desired output are both provided. The model is able to make predictions or classifications based on the input data by learning the relationships between the input and output data. Supervised machine learning is an important tool for linguists studying language and communication, as it allows us to analyze language data to identify patterns or trends in language use, assess hypotheses, and prescribe actions.\n\nThe approach to conducting predictive analysis shares some commonalities with exploratory data analysis (@sec-explore-orientation) (as well as inferential analysis @sec-infer-chapter), but there are also some key differences. Consider the workflow in @tbl-predict-workflow.\n\n<!-- Workflow -->\n\n::: {#tbl-predict-workflow tbl-colwidths=\"[5, 15, 80]\"}\n\n| Step | Name | Description |\n|------|-------------|-------------|\n| 1 | Identify | Consider the research question and aim and identify relevant variables |\n| 2 |  | Split the data into representative training and testing sets |\n| 3 |  | Apply variable selection and engineering procedures |\n| 4 | Inspect | Inspect the data to ensure that it is in the correct format and that the training and testing sets are representative of the data |\n| 5 | Interrogate | Train and evaluate the model on the training set, adjusting models or hyperparameters as needed, to produce a final model |\n| 6 | (Optional) Iterate | Repeat steps 3-5 to selecting new variables, models, hyperparameters |\n| 7 | Interpret | Interpret the results of the final model in light of the research question or hypothesis |\n\nWorkflow for predictive data analysis\n:::\n\nFocusing on the overlap with other analysis methods, we can see some fundamentals steps such as identifying relevant variables, inspecting the data, interrogating the data, and interpreting the results. And if our research aim is exploratory in nature, iteration may also be a part of the workflow.\n\nThere are two main differences, however, between the PDA and the EDA workflow we discussed in @sec-explore-chapter. The first, reflected the majority of the steps in the workflow, is that PDA requires partitioning the data into training and testing sets. The training set is used to develop the model, and the testing set is used to evaluate the model's performance. This strategy is used to ensure that the model is robust and generalizes well to new data. It is well known, and makes intuitive sense, that using the same data to develop and evaluate a model likely will not produce a model that generalizes well to new data. This is because the model will have potentially conflated the nuances of the data ('the noise') with any real trends ('the signal') and therefore will not be able to generalize well to new data. This is called **overfitting** and by holding out a portion of the data for testing, we can evaluate the model's performance on data that it has not seen before and therefore get a more accurate estimate of the generalizable trends in the data.\n\n::: {.callout .halfsize}\n**{{< fa medal >}} Dive deeper**\n\nPrediction modeling is a hot topic. Given the potential to make actionable predictions about future outcomes, it attracts a lot of attention from organizations which aim to leverage data to make informed decisions. It's use in research is also growing beyond the development of better models and using predictive models to address research questions and hypotheses.\n\nWe will apply predictive modeling in the context of language data as a semi-inductive method. However, it is also increasingly used in hypothesis testing scenarios, see @Gries2014, @Deshors2016, and @Baayen2011 for examples.\n:::\n\nAnother procedure to avoid the perils of overfitting, is to use resampling methods as part of the model evaluation on the training set. Resampling is the process of repeatedly drawing samples from the training set and evaluating the model on each sample. The two most common resampling methods are **bootstrapping** (resampling with replacement) and **cross-validation** (resampling without replacement). The performance of these multiple models are summarized and the error between them is assessed. The goal is to minimize the performance differences between the models while maximizing the overall performance. These measures go a long way to avoiding overfitting and therefore maximizing the chance that the training phase will produce a model which is robust at the testing phase.\n\nThe second difference, not reflected in the workflow but inherent in predictive analysis, is that PDA requires a fixed outcome variable. This means that the outcome variable must be defined from the outset and cannot be changed during the analysis. Furthermore, the informational nature of the outcome variable will dictate the what type of algorithm we choose to interrogate the data and how we will evaluate the model's performance.\n\nIf the outcome is categorical in nature, we will use a **classification algorithm** (*e.g.* logistic regression, naive bayes, *etc.*). Classification evaluation metrics include accuracy, precision, recall, and F1 score which can be derived from and visualized in a cross-tabulation of the predicted and actual outcome values.\n\nIf the outcome is numeric in nature, we will use a **regression algorithm** (*e.g.* linear regression, support vector regression, *etc.*). Since the difference between prediction and actual values is numeric, metrics that quantify numerical differences, such as root mean square error (RMSE) or $R^2$, are used to evaluate the model's performance.\n\nThe evaluation of the model is quantitative on the one hand, but it is also qualitative in that we need to consider the implications of the model's performance in light of the research question or hypothesis. Furthermore, depending on our research question we may be interested in exploring the features that are most important to the model's performance. This is called **feature importance** and can be derived from the model's coefficients or weights. Notably, however, some of the most powerful models in use today, such as deep neural networks, are not easily interpretable and therefore feature importance is not easily derived. This is something to keep in mind when considering the research question and the type of model that will be used to address it.\n\n## Analysis {#sec-predict-analysis}\n\n<!-- Goals of this section -->\n\nIn this section, we now turn to the practical application of predictive data analysis. The discussion will be separated into classification and regression tasks, as model selection and evaluation procedures differ between the two. For each task, we will frame a research goal and work through the process of building a predictive model to address that goal. Along the way we will cover concepts and methods that are common to both classification and regression tasks and specific to each.\n\n<!-- Research questions -->\n\nTo frame our analyses, we will posit research aimed at identifying language usage patterns in second language use, one for a classification task and one for a regression task. Our first research question will be to assess whether we Spanish language use can be used to predict natives and L1 English learners (categorical). Our second research question will be to gauge the extent to which the the L1 English learners' Spanish language placement test scores (numeric) can be predicted based on their language use.\n\nWe will use data from the CEDEL2 corpus [@Lozano2009]. We will include a subset of the variables from this data that are relevant to our research questions. The data dictionary for this dataset is seen in @tbl-predict-cedel2-data-dictionary.\n\n\n::: {.cell}\n\n:::\n\n::: {#tbl-predict-cedel2-data-dictionary .cell tbl-cap='Data dictionary for the CEDEL2 corpus' tbl-colwidths='[15,15,15,55]'}\n::: {.cell-output-display}\n\n+-------------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| variable    | name            | type        | description                                                                                                                                                 |\n+=============+=================+=============+=============================================================================================================================================================+\n| doc_id      | Document ID     | numeric     | Unique identifier for each document                                                                                                                         |\n+-------------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| subcorpus   | Subcorpus       | categorical | The subcorpus to which the document belongs ('Learner' or 'Native')                                                                                         |\n+-------------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| place_score | Placement Score | numeric     | The score obtained by the document author in a placement test. Null values indicate missing data (i.e. the document author did not take the placement test) |\n+-------------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| proficiency | Proficiency     | ordinal     | The level of language proficiency of the document author ('Upper intermediate', 'Lower advanced', 'Upper beginner', or 'Native')                            |\n+-------------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| text        | Text            | character   | The written text provided by the document author                                                                                                            |\n+-------------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------+\n:::\n:::\n\n\nLet's go ahead and read the transformed dataset and preview it in @exm-predict-cedel-read.\n\n::: {#exm-predict-cedel-read}\n```r\n# Read in the dataset\ncedel_tbl<-\n  read_csv(\"../data/cedel2/cedel2_transformed.csv\")\n\n# Preview\ncedel_tbl |> glimpse()\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 2,957\nColumns: 5\n$ doc_id      <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ subcorpus   <chr> \"Learner\", \"Learner\", \"Learner\", \"Learner\", \"Learner\", \"Le…\n$ place_score <dbl> 14.0, 16.3, 16.3, 18.6, 18.6, 18.6, 20.9, 20.9, 20.9, 20.9…\n$ proficiency <chr> \"Lower beginner\", \"Lower beginner\", \"Lower beginner\", \"Low…\n$ text        <chr> \"Yo vivo es Alanta, Georgia. Atlanta es muy grande ciudad.…\n```\n\n\n:::\n:::\n\n:::\n\nThe output of @exm-predict-cedel-read provides some structural information about the dataset, number of rows and columns as well as variable types.\n\n\n\n\n\nAfter I performed some diagnostics and made some adjustments, the dataset is in good order to proceed with the analysis. I updated the variables `subcorpus` and `proficiency` as factor variables and ordered them in a way that makes sense for the analysis. The `place_score` variable is distributed well across the proficiency levels. The `subcorpus` variable is less balanced, with around 65% of the texts being from learners. This is not a problem, but it is something to keep in mind when building and interpreting the predictive models.\n\nWe will be using the Tidymodels framework in R to perform these analyses. {tidymodels} is a metapackage, much like {tidyverse}, that provides a consistent interface for machine learning modeling. Some key packages unique to {tidymodels} are {recipes}, {parsnip}, {workflows}, and {tune}. {recipes} includes functions for preprocessing and engineering features. {parsnip} provides a consistent interface for specifying modeling algorithms. {worflows} allows us to combine recipes and models into a single pipeline. Finally, {tune} give us the ability to evaluate and tune hyperparameters of models.\n\nSince we are using text data, we will also be using {textrecipes} which makes various functions available for preprocessing text including extracting and engineering features.\n\nLet's go ahead and do the setup, loading the necessary packages, seen in @exm-predict-packages-data.\n\n<!-- [ ] tmp fmt -->\n\\pagebreak\n\n::: {#exm-predict-packages-data}\n```r\n# Load packages\nlibrary(tidymodels)   # modeling metapackage\nlibrary(textrecipes)  # text preprocessing\n\n# Prefer tidymodels functions\ntidymodels_prefer()\n```\n\n\n::: {.cell}\n\n:::\n\n:::\n\n### Text classification {#sec-predict-text-classification}\n\n<!-- Research question: outcome and features -->\n\nThe goal of this analysis is to classify texts as either native or learner based on the writing samples in the dataset. This is a binary classification problem. We will approach this problem from an exploratory perspective, and therefore our aim is to identify features from the text that best distinguish between the two classes and explore the features that are most important to the model's performance.\n\nLet's modify the data frame to include only the variables we need for this analysis, assigning it to `cls_tbl`. In the process, we will rename the `subcorpus` variable to `outcome` to reflect that it is the outcome variable. This is seen in @exm-predict-class-data.\n\n::: {#exm-predict-class-data}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Rename subcorpus to outcome\ncls_tbl <-\n  cedel_tbl |>\n  select(outcome = subcorpus, proficiency, text)\n```\n:::\n\n:::\n\n<!-- Step 1: identify features -->\n\nLet's begin the workflow from @tbl-predict-workflow by identifying the features that we will use to classify the texts. There may be many features that we could use. These could be features derived from raw text (*e.g.* characters, words, ngrams, *etc.*), feature vectors (*e.g.* word embeddings), or meta-linguistic features (*e.g.* part-of-speech tags, syntactic parses, or semantic features) that have been derived from these through manual or automatic annotation.\n\nIf as part of our research question the types of features is included, then we should proceed toward deriving those features. If not, a simple approach is to use words as the predictor features. This will serve as a baseline for more complex models, if necessary.\n\nThis provides us the linguistic unit we will use but we still need to decide how to operationalize what we mean by 'use' in our research statement. Do we use raw token counts? Do we use normalized frequencies? Do we use some type of weighting scheme? These are questions that we need to consider as we embark on this analysis. Since we are exploring, we can use trial-and-error or consider the implications of each approach and choose the one that best fits our research question --or both.\n\nLet's approach this with a bit more nuance as we already have some domain knowledge about word use. First, we know that in the frequency distribution of words is highly skewed, meaning that a few words occur very frequently and most words occur very infrequently. Second, we know that the most frequent words in a language are often function words (*e.g.* 'the', 'and', 'of', *etc.*) and that these words are tend not very informative for distinguishing between classes of texts. Third, we know that comparing raw counts across texts conflates the influence text class lengths.\n\nWith these considerations in mind, we will tokenize the text into words and then use the term frequency-inverse document frequency ($tf$-$idf$) weighting scheme to represent word use. This feature engineering will down weight words that are common across all documents and up weight words that are unique to a document. It also mitigates the varying lengths of the documents. This is a common approach in text classification and is a good starting point for our analysis.\n\n<!-- Step 2: Initial split -->\n\nWith our features and engineering approach identified, we can move on to step 2 of our workflow and split the data into training and testing sets. We make the splits to our data at this point to draw a line in the sand between the data we will use to train the model and the data we will use to test the model. A typical approach in supervised machine learning is to allocate around 75-80% of the data to the training set and the remaining 20-25% to the testing set, depending on the number of observations. We have 2957 observations in our data set, so we can allocate 80% of the data to the training set and 20% of the data to the testing set.\n\nIn @exm-predict-class-split, we will use the `initial_split()` function from {rsample} to split the data into training and testing sets. The `initial_split()` function takes a data frame and a proportion and returns a `split` object which contains the training and testing sets. We will use the `strata` argument to stratify the data by the `outcome` variable. This will ensure that the training and testing sets have the same proportion of native and learner texts.\n\n::: {#exm-predict-class-split}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(123)\n\n# Split the data into training and testing sets\ncls_split <-\n  initial_split(\n    data = cls_tbl,\n    prop = 0.8,\n    strata = outcome\n  )\n\n# Create training set\ncls_train <- training(cls_split)  # 80% of data\n\n# Create testing set\ncls_test <- testing(cls_split)    # 20% of data\n```\n:::\n\n:::\n\nA confirmation of the distribution of the data across the training and testing sets as well as a break down of the outcome variable, created by {janitor}'s `tabyl()` function, can be seen in @exm-predict-class-split-tabyl.\n\n::: {#exm-predict-class-split-tabyl}\n\n::: {.cell}\n\n```{.r .cell-code}\n# View the distribution\n# Training set\ncls_train |>\n  tabyl(outcome) |>\n  adorn_totals(\"row\") |>\n  adorn_pct_formatting(digits = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n outcome    n percent\n Learner 1524   64.5%\n  Native  840   35.5%\n   Total 2364  100.0%\n```\n\n\n:::\n\n```{.r .cell-code}\n# Testing set\ncls_test |>\n  tabyl(outcome) |>\n  adorn_totals(\"row\") |>\n  adorn_pct_formatting(digits = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n outcome   n percent\n Learner 382   64.4%\n  Native 211   35.6%\n   Total 593  100.0%\n```\n\n\n:::\n:::\n\n:::\n\nWe can see that the split was successful. The training and testing sets have very similiar proportion of native and learner texts.\n\n<!-- Step 3: Integrate: plan to select and engineer features -->\n\nWe are now ready to create a 'recipe', step 3 in our analysis. A recipe is tidymodels terminology for a set of instructions or blueprint which specify the outcome variable and the predictor variable and determines how to preprocess and engineer the feature variables.\n\nWe will use the `recipe()` function from {recipes} to create the recipe. The `recipe()` function minimally takes a formula and a data frame and returns a `recipe` object. **R formulas** provide a way to specify relationships between variables and are used extensively in R data modeling. Formulas specify the outcome variable ($y$) and the predictor variable(s) ($x_1 .. x_n$). For example, `y ~ x` can be read as \"y as a function of x\". In our particular case, we will use the formula `outcome ~ text` to specify that the outcome variable is the `outcome` variable and the predictor variable is the `text` variable. The code is seen in @exm-predict-class-recipe.\n\n::: {#exm-predict-class-recipe}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a recipe\nbase_rec <-\n  recipe(\n    formula = outcome ~ text, # formula\n    data = cls_train\n    )\n\n# Preview\nbase_rec\n```\n:::\n\n\n```\n── Recipe ─────────────────────────────────────────\n\n── Inputs\nNumber of variables by role\noutcome:   1\npredictor: 1\n```\n\n:::\n\nThe recipe object at this moment contains just one instruction, what the variables are and what their relationship is.\n\n::: {.callout .halfsize}\n**{{< fa regular hand-point-up >}} Tip**\n\nR formulas are a powerful way to specify relationships between variables and are used extensively in data modeling including exploratory, predictive, and inferential analysis. The basic formula syntax is `y ~ x` where `y` is the outcome variable and `x` is the feature variable. The formula syntax can be extended to include multiple feature variables, interactions, and transformations. For more information on R formulas, see [R for Data Science](https://r4ds.github.io/bookclub-tmwr/r-formula-syntax.html).\n:::\n\n{recipes} provides a wide range of `step_*()` functions which can be applied to the recipe to specify how to engineer the variables in our recipe call. These include functions to scale (*e.g* `step_center()`, `step_scale()`, *etc.*) and transform (*e.g.* `step_log()`, `step_pca()`, *etc.*) numeric variables, and functions to encode (*e.g.* `step_dummy()`, `step_labelencode()`, *etc.*) categorical variables.\n\nThese step functions are great when we have selected the variables we want to use in our model and we want to engineer them in a particular way. In our case, however, we need to derive features from the text in the `text` column of datasets before we engineer them.\n\nTo ease this process, {textrecipes} provides a number of step functions for preprocessing text data. These include functions to tokenize (*e.g.* `step_tokenize()`), remove stop words (*e.g.* `step_stopwords()`), and to derive meta-features (*e.g.* `step_lemma()`, `step_stem()`, *etc.*) [^1]. Furthermore, there are functions to engineer features in ways that are particularly relevant to text data, such as feature frequencies and weights (*e.g.* `step_tf()`, `step_tfidf()`, *etc.*) and token filtering (*e.g.* `step_tokenfilter()`).\n\n::: {.callout .halfsize}\n**{{< fa medal >}} Dive deeper**\nFor other tokenization strategies and feature engineering methods, see {textrecipes} documentation [@R-textrecipes]. There are, however, pacakges which provide integration with `textrecipes` for other languages, for example, {washoku} for Japanese text processing [@R-washoku].\n:::\n\n[^1]: Note that functions for meta-features require more sophisticated text analysis software to be installed on the computing environment (e.g. `spacyr` for `step_lemma()`, `step_pos()`, *etc.*). See {textrecipes} documentation for more information.\n\nSo let's build on our basic recipe `cls_rec` by adding steps relevant to our task. To extract our features, we will use the `step_tokenize()` function to tokenize the text into words. The default behavior of the `step_tokenize()` function is to tokenize the text into words, but other token units can be derived and various options can be added to the function call (as {tokenizers} is used under the hood). Adding the `step_tokenize()` function to our recipe is seen in @exm-predict-class-recipe-tokenize.\n\n::: {#exm-predict-class-recipe-tokenize}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add step to tokenize the text\ncls_rec <-\n  base_rec |>\n  step_tokenize(text) # tokenize\n\n# Preview\ncls_rec\n```\n:::\n\n\n<!-- [ ] tmp fmt -->\n\\pagebreak\n\n```\n── Recipe ─────────────────────────────────────────\n\n── Inputs\nNumber of variables by role\noutcome:   1\npredictor: 1\n\n── Operations\n• Tokenization for: text\n```\n:::\n\nThe recipe object `cls_rec` now contains two instructions, one for the outcome variable and one for the feature variable. The feature variable instruction specifies that the text should be tokenized into words.\n\nWe now need to consider how to engineer the word features. If we add `step_tf()` we will get a matrix of token counts by default, with the option to specifiy other weights. The step function `step_tfidf()` creates a matrix of term frequencies weighted by inverse document frequency.\n\nWe decided in step 1 that we will start with $tf$-$idf$, so we will add `step_tfidf()` to our recipe. This is seen in @exm-predict-class-recipe-tfidf.\n\n::: {#exm-predict-class-recipe-tfidf}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add step to tokenize the text\ncls_rec <-\n  cls_rec |>\n  step_tfidf(text, smooth_idf = FALSE)\n\n# Preview\ncls_rec\n```\n:::\n\n```\n── Recipe ─────────────────────────────────────────\nNumber of variables by role\noutcome:   1\npredictor: 1\n\n── Operations\n• Tokenization for: text\n• Term frequency-inverse document frequency with: text\n```\n:::\n\n<!-- Step 4: Inspect:  -->\n\n<!-- [ ] tmp fmt -->\n\\pagebreak\n\n::: {.callout .halfsize}\n**{{< fa regular hand-point-up >}} Tip**\n\nThe `step_tfidf()` function by default add a *smoothing term* to the inverse document frequency ($idf$) calculation. This setting has the effect of reducing the influence of the $idf$ calculation. Thus, terms that appear in many (or all) documents will not be downweighted as much as they would be if the smoothing term was not added. For our purposes, we want to downweight or eliminate the influence of the most frequent terms, so we will set `smooth_idf = FALSE`.\n:::\n\nTo make sure things are in order and that the recipe performs as expected, we can use the functions `prep()` and `bake()` to inspect the recipe. The `prep()` function takes a recipe object and a data frame and returns a `prep` object. The `prep` object contains the recipe and the data frame with the feature variables engineered according to the recipe. The `bake()` function takes a `prep` object and an optional new dataset to apply the recipe to. If we only want to see the application to the training set, we can use the `new_data = NULL` argument.\n\nIn @exm-predict-class-recipe-prep, we use the `prep()` and `bake()` functions to create a data frame with the feature variables. We can then inspect the data frame to see if the recipe performs as expected.\n\n::: {#exm-predict-class-recipe-prep}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prep and bake\ncls_bake <-\n  cls_rec |>\n  prep() |> # create a prep object\n  bake(new_data = NULL) # apply to training set\n\n# Preview\ncls_bake |> dim()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  2364 38115\n```\n\n\n:::\n:::\n\n:::\n\nThe resulting engineered features data frame has 2,364 observations and 38,115 variables. That is a lot of features! Given the the fact that for each writing sample, only a small subset of them will actually appear, most of our cells will be filled with zeros. This is what is known as a **sparse matrix**.\n\n::: {.callout .halfsize}\n**{{< fa regular hand-point-up >}} Tip**\n\nWhen applying tokenization and feature engineering steps to text data the result is often contained in a matrix object. Using {recipes} a data frame with a matrix-like structure is returned.\n\nFurthermore, the features are prefixed with the variable name and transformation step labels. In @exm-predict-class-recipe-prep we applied $tf$-$idf$ to the `text` variable. Therefore the features are prefixed with `tfidf_text_`.\n:::\n\nBut we should pause. This is an unweildy number of features, on for every single word, for a model and it is likely that many of these features are not useful for our classification task. Furthermore, the more features we have, the more chance these features will capture the nuances of these particular writing samples increasing the likelihood we overfit the model. All in all, we need to reduce the number of features.\n\nWe can filter out features by stopword list or by frequency of occurrence. Let's start by frequency of occurrence. We can set the maximum number of the top features with an arbitrary threshold to start. The `step_tokenfilter()` function can filters out features on a number of criteria. Let's use the `max_tokens` argument to set the maximum number of features to 100.\n\nThis particular step needs to be applied before the `step_tfidf()` step, so we will add it to our recipe before the `step_tfidf()` step. This is seen in @exm-predict-class-recipe-tokenfilter.\n\n::: {#exm-predict-class-recipe-tokenfilter}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Rebuild recipe with tokenfilter step\ncls_rec <-\n  base_rec |>\n  step_tokenize(text) |>\n  step_tokenfilter(text, max_tokens = 100) |>\n  step_tfidf(text, smooth_idf = FALSE)\n\n# Prep and bake\ncls_bake <-\n  cls_rec |>\n  prep() |>\n  bake(new_data = NULL)\n\n# Preview\ncls_bake |> dim()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2364  101\n```\n\n\n:::\n\n```{.r .cell-code}\ncls_bake[1:5, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 5\n  outcome tfidf_text_a tfidf_text_ahora tfidf_text_al tfidf_text_amigos\n  <fct>          <dbl>            <dbl>         <dbl>             <dbl>\n1 Learner      0                      0             0                 0\n2 Learner      0.00399                0             0                 0\n3 Learner      0.00615                0             0                 0\n4 Learner      0                      0             0                 0\n5 Learner      0.0111                 0             0                 0\n```\n\n\n:::\n:::\n\n:::\n\nWe now have a manageable set of features, and fewer of which will have a as many zeros. Only during the interrogation step will we know if they are useful.\n\n::: {.callout .halfsize}\n**{{< fa regular hand-point-up >}} Tip**\n\nThe `prep()` and `bake()` functions are useful for inspecting the recipe and the engineered features, but they are not required to build a recipe. When a recipe is added to a workflow, the `prep()` and `bake()` functions are called automatically as part of the process.\n:::\n\n<!-- Step 5: Interrogate -->\n\nWe are now ready to turn our attention to step 5 of our workflow, interrogating the data. In this step, we will first select a classification algorithm, then add this algorithm and our recipe to a workflow object. We will then use the workflow object to train and asess the resulting models, adjusting them until we believe we have a robust final model to apply on the testing set for our final evaluation.\n\n<!-- Select a classification algorithm -->\n\nThere are many classification algorithms to choose from with their own strengths and shortcomings. In @tbl-predict-class-algorithms, we list some of the most common classification algorithms and their characteristics.\n\n::: {#tbl-predict-class-algorithms tbl-colwidths=\"[15, 28, 28, 29]\"}\n\n| Algorithm | Strengths | Shortcomings | Tuning Recommendation |\n|-----------|-----------|--------------|-----------------------|\n| Logistic regression | Interpretable, fast, high-dimensional data | Linear relationship, not for complex tasks | Cross-validate regularization strength |\n| Naive Bayes | Interpretable, fast, high-dimensional data, multi-class | Assumes feature (naive) independence, poor with small data | None |\n| Decision trees | Nonlinear, interpretable, numerical/ categorical data | Overfitting, high variance | Cross-validate maximum tree depth |\n| Random forest | Nonlinear, numerical/ categorical data, less overfitting | Less interpretable, poor with high-dimensional data | Cross-validate number of trees |\n| Support vector machines | Nonlinear, high-dimensional data, numerical/ categorical | Requires parameter tuning, memory intensive | Cross-validate regularization parameter |\n| Neural networks | Nonlinear, large data, auto feature learning | Overfitting, difficult to interpret, expensive | Cross-validate learning rate |\n\nCommon classification algorithms\n:::\n\n<!-- Models: simple to complex  -->\n\nIn the process of selecting an algorithm, simple, computationally efficient, and interpretable models are preferred over complex, computationally expensive, and uninterpretable models, all things being equal. Only if the performance of the simple model is not good enough should we move on to a more complex model.\n\n::: {.callout .halfsize}\n**{{< fa regular hand-point-up >}} Tip**\n\n{parsnip} provides a consistent interface to many different models, 105 at the time of writing. You can peruse the list of models by running `parsnip::model_db`.\n\nYou can also retrieve the list of potential engines for a given model specification with the `show_engines()` function. For example, `show_engines(\"logistic_reg\")` will return a tibble with the engines available for the logistic regression model specification. Note, the engines represent R packages that need to be installed to use the engine.\n:::\n\nWith this end mind, we will start with a simple logistic regression model to see how well we can classify the texts in the training set with the features we have engineered. We will use the `logistic_reg()` function from {parsnip} to specify the logistic regression model. We then select the implementation engine (`glmnet` General Linear Model) and the mode of the model (`classification`). The implementation engine is the software that will be used to fit the model. The code to set up the model specification is seen in @exm-predict-class-model-spec.\n\n::: {#exm-predict-class-model-spec}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a model specification\ncls_spec <-\n  logistic_reg() |>\n  set_engine(\"glmnet\")\n\n# Preview\ncls_spec\n```\n:::\n\n```\nLogistic Regression Model Specification (classification)\n\nComputational engine: glmnet\n```\n:::\n\nNow, different algorithms will have different hyperparameters that can be tuned which can affect the performance of the model (see @tbl-predict-class-algorithms). The process of **hyperparameter tuning** involves fitting the model to the training set with different hyperparameters and evaluating the model's performance to determine the best hyperparameter values to use for the model.\n\n<!-- [ ] tmp fmt -->\n\\pagebreak\n\n::: {.callout .halfsize}\n**{{< fa regular hand-point-up >}} Tip**\n\nYou can find the hyperparameters for a model-engine by consulting the `parsnip::model_db` object and unnesting the `parameters` column. For example, `parsnip::model_db %>% filter(model == \"logistic_reg\") %>% unnest(parameters)` will return a tibble with the hyperparameters for the logistic regression model.\n\nTo learn more about the hyperparameters for a specific model, you can consult the documentation for `parsnip` model (*e.g.* `?logistic_reg`).\n:::\n\nFor example, the logistic regression model using `glmnet` can be tuned to prevent overfitting. The regularization typically applied is the LASSO (L1) penalty[^lasso]. The `logistic_reg()` function takes the arguments `penalty` and `mixture`. We set `mixture = 1`, but we now need to decide what value to use for the strength of the `penalty` argument. Values can range from 0 to 1, where 0 indicates no penalty and 1 indicates a maximum penalty.\n\n[^lasso]: The LASSO (least absolute shrinkage and selection operator) is a type of regularization that penalizes the absolute value of the coefficients. In essence, it smooths the coefficients by shrinking them towards zero to avoid coefficients picking up on particularities of the training data that will not generalize to new data.\n\nInstead of guessing, we will use {tune} to tune the hyperparameters of the model. The `tune()` functions serves as a placeholder for the hyperparameters we want to tune. We can add the `tune()` function to our model specification to specify the hyperparameters we want to tune. The code is seen in @exm-predict-class-model-spec-tune.\n\n::: {#exm-predict-class-model-spec-tune}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a model specification (with tune)\ncls_spec <-\n  logistic_reg(penalty = tune(), mixture = 1) |>\n  set_engine(\"glmnet\")\n\n# Preview\ncls_spec\n```\n:::\n\n```\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet\n```\n:::\n\nWe can see now that the `cls_spec` model specification now includes the `tune()` function as the value for the `penalty` argument.\n\n<!-- Combine recipe and model specification -->\n\nTo tune our model, we will need to combine our recipe and model specification into a workflow object which sequences our feature selection, engineering, and model selection. We will use the `workflow()` function from {workflows} to do this. The code is seen in @exm-predict-class-workflow.\n\n::: {#exm-predict-class-workflow}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a workflow\ncls_wf <-\n  workflow() |>\n  add_recipe(cls_rec) |>\n  add_model(cls_spec)\n\n# Preview\ncls_wf\n```\n:::\n\n```\n══ Workflow ═══════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ───────────────────────────────────\n3 Recipe Steps\n\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ──────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet\n```\n:::\n\nWe now have a workflow `cls_wf` that includes our recipe and model specification, including the `tune()` function as a placeholder for a range of values for the penalty hyperparameter. To tune the penalty hyperparameter, we use the `grid_regular()` function from {dials} to specify a grid of values to try. Let's choose a random set of 10 values, as seen in @exm-predict-class-model-spec-tune-grid-values.\n\n::: {#exm-predict-class-model-spec-tune-grid-values}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a grid of values for the penalty hyperparameter\ncls_grid <-\n  grid_regular(penalty(), levels = 10)\n\n# Preview\ncls_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 1\n         penalty\n           <dbl>\n 1 0.0000000001 \n 2 0.00000000129\n 3 0.0000000167 \n 4 0.000000215  \n 5 0.00000278   \n 6 0.0000359    \n 7 0.000464     \n 8 0.00599      \n 9 0.0774       \n10 1            \n```\n\n\n:::\n:::\n\n:::\n\n\nThe 10 values chosen to be in the grid range from nearly zero to 1, where 0 indicates no penalty and 1 indicates a strong penalty.\n\nNow to perform the tuning and arrive at an optimal value for `penalty` we need to create a tuning workflow. We do this by calling the `tune_grid()` function using our tuning model specification workflow, a resampling object, and our hyperparameter grid and returns a `tune_grid` object.\n\nResampling is a strategy that allows us to generate multiple training and testing sets from a single data set --in this case the training data we split at the outset. Each generated training-testing pairs is called a fold. Which is why this type of resampling is called **$k$-fold cross-validation**. The `vfold_cv()` function from {rsample} takes a data frame and a number of folds and returns a `vfold_cv` object. We will apply the `cls_wf` workflow to the 10 folds of the training set with `tune_grid()`. For each fold, each of the 10 values of the penalty hyperparameter will be tried and the model's performance will be evaluated. The code is seen in @exm-predict-class-model-spec-tune-grid-cv.\n\n::: {#exm-predict-class-model-spec-tune-grid-cv}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(123)\n\n# Create a resampling object\ncls_vfold <- vfold_cv(cls_train, v = 10)\n\n# Tune the model\ncls_tune <-\n  tune_grid(\n    cls_wf,\n    resamples = cls_vfold,\n    grid = cls_grid\n    )\n\n# Preview\ncls_tune\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits             id     .metrics          .notes          \n   <list>             <chr>  <list>            <list>          \n 1 <split [2127/237]> Fold01 <tibble [30 × 5]> <tibble [0 × 3]>\n 2 <split [2127/237]> Fold02 <tibble [30 × 5]> <tibble [0 × 3]>\n 3 <split [2127/237]> Fold03 <tibble [30 × 5]> <tibble [0 × 3]>\n 4 <split [2127/237]> Fold04 <tibble [30 × 5]> <tibble [0 × 3]>\n 5 <split [2128/236]> Fold05 <tibble [30 × 5]> <tibble [0 × 3]>\n 6 <split [2128/236]> Fold06 <tibble [30 × 5]> <tibble [0 × 3]>\n 7 <split [2128/236]> Fold07 <tibble [30 × 5]> <tibble [0 × 3]>\n 8 <split [2128/236]> Fold08 <tibble [30 × 5]> <tibble [0 × 3]>\n 9 <split [2128/236]> Fold09 <tibble [30 × 5]> <tibble [0 × 3]>\n10 <split [2128/236]> Fold10 <tibble [30 × 5]> <tibble [0 × 3]>\n```\n\n\n:::\n:::\n\n:::\n\nThe `cls_tune` object contains the results of the tuning for each fold. We can see the results of the tuning for each fold by calling the `collect_metrics()` function on the `cls_tune` object, as seen in @exm-predict-class-model-spec-tune-grid-collect.\n\n::: {#exm-predict-class-model-spec-tune-grid-collect}\n\n::: {.cell layout-nrow=\"2\"}\n\n```{.r .cell-code}\n# Collect the results of the tuning\ncls_tune_metrics <-\n  collect_metrics(cls_tune)\n\n# Visualize metrics\ncls_tune |> autoplot()\n```\n\n::: {.cell-output-display}\n![Metrics for each fold of the tuning process](9_predict_files/figure-html/fig-predict-class-model-spec-tune-grid-collect-1.png){#fig-predict-class-model-spec-tune-grid-collect fig-alt='Two line plots one showing accuracy and the other showing ROC-AUC for each fold of the tuning process. The y-axis is the metric value and the x-axis is the penalty hyperparameter value.' width=576}\n:::\n:::\n\n:::\n\nThe most common metrics for model performance in classification are accuracy and the area under the **receiver operating characteristic curve** (ROC-AUC). Accuracy is the proportion of correct predictions. The ROC-AUC is a measure of how well the model can distinguish between the two classes.\n\nIn the plot of the metrics, we can see that the many of the penalty values performed similarly, with a drop off in performance at the higher values. Conveniently, the `show_best()` function from {tune} takes a `tune_grid` object and returns the best performing hyperparameter values. The code is seen in @exm-predict-class-model-spec-tune-grid-collect-best.\n\n::: {#exm-predict-class-model-spec-tune-grid-collect-best}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Show the best performing hyperparameter value\ncls_tune |>\n  show_best(metric = \"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 7\n        penalty .metric .estimator  mean     n std_err .config              \n          <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1 0.000464      roc_auc binary     0.952    10 0.00487 Preprocessor1_Model07\n2 0.00599       roc_auc binary     0.952    10 0.00344 Preprocessor1_Model08\n3 0.0000000001  roc_auc binary     0.951    10 0.00502 Preprocessor1_Model01\n4 0.00000000129 roc_auc binary     0.951    10 0.00502 Preprocessor1_Model02\n5 0.0000000167  roc_auc binary     0.951    10 0.00502 Preprocessor1_Model03\n```\n\n\n:::\n:::\n\n:::\n\nWe can make this selection programmatically by using the `select_best()` function. This function needs a metric to select by. We will use the ROC-AUC and select the best value for the penalty hyperparameter. The code is seen in @exm-predict-class-model-spec-tune-grid-collect-select.\n\n::: {#exm-predict-class-model-spec-tune-grid-collect-select}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select the best performing hyperparameter value\ncls_best <-\n  select_best(cls_tune, metric = \"roc_auc\")\n\n# Preview\ncls_best\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n   penalty .config              \n     <dbl> <chr>                \n1 0.000464 Preprocessor1_Model07\n```\n\n\n:::\n:::\n\n:::\n\nAll of that to tune a hyperparameter!\n\nNow we can update the model specification and workflow with the best performing hyperparameter value using the previous `cls_wf_tune` workflow and the `finalize_workflow()` function. The `finalize_workflow()` function takes a workflow and the selected parameters and returns an updated `workflow` object, as seen in @exm-predict-class-tune-hyperparameters-update-workflow.\n\n::: {#exm-predict-class-tune-hyperparameters-update-workflow}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Update model specification\ncls_wf_lasso <-\n  cls_wf |>\n  finalize_workflow(cls_best)\n\n# Preview\ncls_wf_lasso\n```\n:::\n\n```\n══ Workflow ═══════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ───────────────────────────────────\n\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ──────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.000464158883361278\n  mixture = 1\n\nComputational engine: glmnet\n```\n:::\n\nOur model specification and the worflow are updated with the tuned hyperparameter.\n\nAs a reminder we are still working in step 5 of our workflow, interrogating the data. So far, we have selected and engineered the features, split the data into training and testing sets, and selected a classification algorithm. We have also tuned the hyperparameters of the model and updated the model specification and workflow with the best performing hyperparameter value.\n\nThe next step is to assess the performance of the model on the training set given the features we have engineered, the algorithm we have selected, and the hyperparameters we have tuned. Instead of evaluating the model on the training set directly, we will use cross-validation on the training set to gauge the variability of the model.\n\nThe reason for this is that the model's performance on the entire training set at once is not a reliable indicator of the model's performance on new data --just imagine if you were to take a test on the same material over and over again, you would get better and better at the test, but that doesn't mean you've learned the material any better. Cross-validation is a technique that allows us to estimate the model's performance on new data by simulating the process of training and testing the model on different subsets of the training data.\n\nSimilar to what we did to tune the hyperparameters, we can use cross-validation to gauge the variability of the model. The `fit_resamples()` function takes a workflow and a resampling object and returns metrics for each fold. The code is seen in @exm-predict-class-tune-hyperparameters-evaluate-workflow-cv.\n\n::: {#exm-predict-class-tune-hyperparameters-evaluate-workflow-cv}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cross-validate workflow\ncls_lasso_cv <-\n  cls_wf_lasso |>\n  fit_resamples(\n    resamples = cls_vfold,\n    # save predictions for confusion matrix\n    control = control_resamples(save_pred = TRUE)\n  )\n```\n:::\n\n:::\n\nWe want to aggregate the metrics across the folds to get a sense of the variability of the model. The `collect_metrics()` function takes the results of a cross-validation and returns a data frame with the metrics.\n\n::: {#exm-predict-class-tune-hyperparameters-evaluate-workflow-cv-collect}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Collect metrics\ncls_lasso_cv |> collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n  .metric     .estimator   mean     n std_err .config             \n  <chr>       <chr>       <dbl> <int>   <dbl> <chr>               \n1 accuracy    binary     0.884     10 0.00554 Preprocessor1_Model1\n2 brier_class binary     0.0823    10 0.00385 Preprocessor1_Model1\n3 roc_auc     binary     0.952     10 0.00487 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n:::\n\nFrom the accuracy and ROC-AUC metrics in @exm-predict-class-tune-hyperparameters-evaluate-workflow-cv-collect it appears we have a decent candidate model, but there is room for potential improvement. A good next step is to evaluate the model errors and see if there are any patterns that can be addressed before considering what approach to take to improve the model.\n\n::: {.callout .halfsize}\n**{{< fa regular hand-point-up >}} Tip**\n\nTo provide context in terms of what is a good model performance, it is useful to compare the model's performance to a null model. A **null model ** (or baseline model) is a simple model that is easy to implement and provides a benchmark for the model's performance. For classification tasks, a common null model is to predict the most frequent class. In modeling, this is the minimal benchmark we want to beat, if we are doing better than this, we are doing better than chance.\n:::\n\nFor classification tasks, a good place to start is to visualize a confusion matrix. A **confusion matrix** is a cross-tabulation of the predicted and actual outcomes. The `conf_mat_resampled()` function takes a `fit_resamples` object (with predictions saved) and returns a table (`tidy = FALSE`) with the confusion matrix for the aggregated folds. We can pass this to the `autoplot()` function to plot as in @exm-predict-class-tune-hyperparameters-evaluate-workflow-cv-confusion.\n\n\n\n\n\n::: {#exm-predict-class-tune-hyperparameters-evaluate-workflow-cv-confusion}\n\n```r\n# Plot confusion matrix\ncls_lasso_cv |>\n  conf_mat_resampled(tidy = FALSE) |>\n  autoplot(type = \"heatmap\")\n```\n:::\n\n::: {#fig-class-tune-hyperparameters-evaluate-workflow-cv-confusion}\n\n![](figures/fig-class-tune-hyperparameters-evaluate-workflow-cv-confusion.png){width=\"75%\" fig-alt=\"Heatmap of the confusion matrix for the aggregated folds of the cross-validation. Actual classes are on the y-axis and predicted classes are on the x-axis.\"}\n\nConfusion matrix for the aggregated folds of the cross-validation\n:::\n\nThe top left to bottom right diagonal contains the true positives and true negatives, these are the correct predictions. The top right to bottom left diagonal contains the false positives and false negatives --our errors. The convention is speak of one class being the positive class and the other class being the negative class. In our case, we will consider the positive class to be the 'learner' class and the negative class to be the 'natives' class.\n\nWe can see that there are more learners falsely predicted to be natives than the other way around. This may be due to the fact that there are simply more learners than natives in the data set or this could signal that there are some learners that are more similar to natives than other learners. Clearly this can't be the entire explanation as the model is not perfect, even some natives are classified falsely as learners! But may be an interesting avenue for further exploration. Maybe these are learners that are more advanced or have a particular style of writing that is more similar to natives.\n\n::: {.callout .halfsize}\n**{{< fa medal >}} Dive deeper**\n\nAnother perspective often applied to evaluate a model is the Reciever Operating Characteristic (ROC) curve. The ROC curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) for different classification thresholds. This metric, and visualization, can be useful to gauge the model's ability to distinguish between the two classes. {yardstick} provides the `roc_curve()` function to calculate the ROC curve on an `fit_resamples` object.\n:::\n\n<!-- Model improvement: tune max tokens -->\n\nTo improve supervised learning models, one should consider a number of possibilities:\n\n1. Engineer the features differently\n2. Select different (or additional) features\n3. Change the algorithm\n4. Tune the hyperparameters differently\n\nOf these options, adjusting the feature engineering features process is the option that diverges least from our current workflow `cls_wf_lasso`. Recall that in our recipe specification we set a token filter to limit the number of features to 100. We can adjust this number to see if it has an effect on the model's performance.\n\nTo avoid help select the optimimal number of tokens, we again can use the tuning process we explored for the hyperparameters. This time, however, the `tune()` placeholder will be included as the argument to the `max_tokens` argument in the `step_tokenfilter()` function.\n\nI repeat the recipe with the tuning placeholder in @exm-predict-class-tune-hyperparameters-tokenfilter.\n\n::: {#exm-predict-class-tune-hyperparameters-tokenfilter}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a recipe with a token filter step\ncls_rec <-\n  recipe(\n    formula = outcome ~ text,\n    data = cls_train\n    ) |>\n  step_tokenize(text) |>\n  step_tokenfilter(text, max_tokens = tune()) |>\n  step_tfidf(text)\n```\n:::\n\n:::\n\nWith the updated recipe, we can update the `cls_wf_lasso` and tune the `max_tokens` hyperparameter. The code is seen in @exm-predict-class-tune-hyperparameters-update-rec.\n\n::: {#exm-predict-class-tune-hyperparameters-update-rec}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Update workflow with token filter tuning\ncls_wf_lasso <-\n  cls_wf_lasso |>\n  update_recipe(cls_rec)\n```\n:::\n\n:::\n\nOne thing to note is that we will want to consider what values of `max_tokens` we want to use to tune the hyperparameter. So instead of only specifying the levels in the `grid_regular()` function, we are best off to provide a range of values that we think are reasonable. Let's add a range of values between our current value 100 and 2,000 to start. And let's tell the grid to select five values from this range.\n\nThe code is seen in @exm-predict-class-tune-hyperparameters-tokenfilter-grid.\n\n::: {#exm-predict-class-tune-hyperparameters-tokenfilter-grid}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a grid of values for the max tokens hyperparameter\ncls_grid <-\n  grid_regular(max_tokens(range = c(100, 2000)), levels = 5)\n\n# Preview\ncls_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 1\n  max_tokens\n       <int>\n1        100\n2        575\n3       1050\n4       1525\n5       2000\n```\n\n\n:::\n:::\n\n:::\n\nFrom here, the process is the same as before. We will use the `tune_grid()` function to tune the `max_tokens` hyperparameter, select the best value, and finalize the workflow, as seen from @exm-predict-class-model-spec-tune-grid-cv through @exm-predict-class-tune-hyperparameters-update-workflow.\n\n\n\n\n\nAfter tuning the `max_tokens` hyperparameter, the best performing value is 1,050. We now used the updated `cls_wf_lasso_tokens` workflow to cross-validate the model and collect the metrics. The code is seen in @exm-predict-class-tune-hyperparameters-tokenfilter-evaluate-workflow-cv-collect.\n\n::: {#exm-predict-class-tune-hyperparameters-tokenfilter-evaluate-workflow-cv-collect}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cross-validate workflow\ncls_lasso_tokens_cv <-\n  cls_wf_lasso_tokens |>\n  fit_resamples(\n    resamples = cls_vfold,\n    # save predictions for confusion matrix\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Collect metrics\ncls_lasso_tokens_cv |>\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n  .metric     .estimator   mean     n std_err .config             \n  <chr>       <chr>       <dbl> <int>   <dbl> <chr>               \n1 accuracy    binary     0.918     10 0.00555 Preprocessor1_Model1\n2 brier_class binary     0.0680    10 0.00418 Preprocessor1_Model1\n3 roc_auc     binary     0.968     10 0.00289 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n:::\n\nThe metrics from @exm-predict-class-tune-hyperparameters-tokenfilter-evaluate-workflow-cv-collect show that the model's performance has improved for both the accuracy and the ROC-AUC. The confusion matrix from @exm-predict-class-tune-hyperparameters-tokenfilter-evaluate-workflow-cv-confusion shows that the number of false positives and false negatives has decreased. This is a good sign that the model is more robust.\n\n\n\n\n\n::: {#exm-predict-class-tune-hyperparameters-tokenfilter-evaluate-workflow-cv-confusion}\n\n```r\n# Plot confusion matrix\ncls_lasso_tokens_cv |>\n  conf_mat_resampled(tidy = FALSE) |>\n  autoplot(type = \"heatmap\")\n```\n:::\n\n::: {#fig-predict-class-tune-hyperparameters-tokenfilter-evaluate-workflow-cv-confusion}\n\n![](figures/fig-predict-class-tune-hyperparameters-tokenfilter-evaluate-workflow-cv-confusion.png){width=\"75%\" fig-alt=\"Heatmap of the confusion matrix for the aggregated folds of the cross-validation. Actual classes are on the y-axis and predicted classes are on the x-axis.\"}\n\nConfusion matrix for the aggregated folds of the cross-validation\n:::\n\nIt appears that the model is more robust with the updated `max_tokens` hyperparameter. We could continue to explore other model improvement strategies, but for now we will move on to the next step in our workflow.\n\n<!-- Step 6: Evaluate -->\n\nWe are now ready to move on to step 7, evaluating the model on the test set. To do this we need to fit the tuned workflow to the training set, which is the actual training phase. We will use the `last_fit()` function from {workflows} to fit the workflow to the training set.\n\nThe `last_fit()` function takes a workflow and a split object and returns a `last_fit` object. The `last_fit` object contains the results of the model fit on the training set and the results of the model evaluation on the test set. The code is seen in @exm-predict-class-tune-hyperparameters-evaluate-test.\n\nWe will use the `last_fit()` function to train the final model and predict the outcome on the test set. The `collect_metrics()` function takes a data frame with the actual and predicted outcomes and returns a data frame with the metrics for the model. The code is seen in @exm-predict-class-tune-hyperparameters-evaluate-test.\n\n<!-- [ ] tmp fmt -->\n\\pagebreak\n\n::: {#exm-predict-class-tune-hyperparameters-evaluate-test}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the model to the training set and evaluate on the test set\ncls_final_fit <-\n  last_fit(\n    cls_wf_lasso_tokens,\n    split = cls_split\n  )\n\n# Evaluate model on testing set\ncls_final_fit |>\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  <chr>       <chr>          <dbl> <chr>               \n1 accuracy    binary        0.909  Preprocessor1_Model1\n2 roc_auc     binary        0.962  Preprocessor1_Model1\n3 brier_class binary        0.0758 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n:::\n\nThe performance metrics are very close to those we achieved on the training set in @exm-predict-class-tune-hyperparameters-tokenfilter-evaluate-workflow-cv-collect. This is a good sign that the model is robust as it performs well on both training and test sets. We can evaluate the confusion matrix on the test set as well. The code is seen in @exm-predict-class-tune-hyperparameters-evaluate-test-confusion.\n\n\n\n\n\n::: {#exm-predict-class-tune-hyperparameters-evaluate-test-confusion}\n\n```r\n# Plot confusion matrix\ncls_final_fit |>\n  collect_predictions() |>\n  conf_mat(truth = outcome, estimate = .pred_class) |>\n  autoplot(type = \"heatmap\")\n```\n:::\n\n::: {#fig-class-tune-hyperparameters-evaluate-workflow-cv-confusion}\n\n![](figures/fig-predict-class-tune-hyperparameters-evaluate-test-confusion.png){width=\"75%\" fig-alt=\"Heatmap of the confusion matrix for the test set. Actual classes are on the y-axis and predicted classes are on the x-axis.\"}\n\nConfusion matrix for the test set\n:::\n\nOn the test set the false instances are balanced, which is a good sign that the model is robust. Ideally, there would be no errors, but this is not realistic. The model is not perfect, but it is useful.\n\nNow a model that can predict the nativeness of a writer based on their writing sample is a useful tool in itself. You could imagine that this could be a preprocessing step for a language learning application, for example. But for a study that is more interested in learning about what features are most important for predicting the native versus non-native features of a writer, we still have some work to do. We can inspect the errors on the test set to gain some insight into what writing samples, and which proficiency levels of the writers, are most difficult to predict. We can also inspect the estimates for the features in the model to gain some insight into what features are most important for predicting the outcomes.\n\nLet's first approach this from a document-proficiency point of view. First, we will want to integrate the predictions with the test set to inspect the errors. We can use the `collect_predictions()` function to collect the predictions from the `last_fit` object and attach them with the test set `cls_test` with `bind_cols`. Note, we can drop the `outcome` variable from `cls_test` as we have this column in our fitted model. The code is seen in @exm-predict-class-tune-hyperparameters-integrate-test.\n\n::: {#exm-predict-class-tune-hyperparameters-integrate-test}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Collect predictions from the model\ncls_lasso_fit_preds_test <-\n  cls_final_fit |>\n  collect_predictions() |>\n  bind_cols(cls_test[, -1])\n\n# Preview\nglimpse(cls_lasso_fit_preds_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 593\nColumns: 9\n$ .pred_class   <fct> Learner, Learner, Learner, Native, Learner, Learner, Lea…\n$ .pred_Learner <dbl> 1.0000, 1.0000, 1.0000, 0.0996, 1.0000, 0.9928, 1.0000, …\n$ .pred_Native  <dbl> 9.59e-06, 1.13e-05, 3.16e-09, 9.00e-01, 2.83e-06, 7.17e-…\n$ id            <chr> \"train/test split\", \"train/test split\", \"train/test spli…\n$ .row          <int> 3, 7, 15, 21, 22, 25, 36, 43, 47, 50, 53, 57, 62, 66, 68…\n$ outcome       <fct> Learner, Learner, Learner, Learner, Learner, Learner, Le…\n$ .config       <chr> \"Preprocessor1_Model1\", \"Preprocessor1_Model1\", \"Preproc…\n$ proficiency   <fct> Lower beginner, Lower beginner, Lower beginner, Lower be…\n$ text          <chr> \"Sanaa Lathan es muy famosa persona. Ella es en de telev…\n```\n\n\n:::\n:::\n\n:::\n\nI will then select the columns with the actual outcome, the predicted outcome, the proficiency level, and the text and separate the predicted outcome to inspect them separately, as seen in @exm-predict-class-tune-hyperparameters-evaluate-test-errors.\n\n::: {#exm-predict-class-tune-hyperparameters-evaluate-test-errors}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Inspect errors\ncls_lasso_fit_preds_test |>\n  filter(outcome != .pred_class) |>\n  select(outcome, .pred_class, proficiency, text)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 54 × 4\n   outcome .pred_class proficiency        text                                  \n   <fct>   <fct>       <fct>              <chr>                                 \n 1 Learner Native      Lower beginner     \"Un día un pequeño nino fue dado una …\n 2 Learner Native      Upper beginner     \"Un dia, El niño estaba durmiendo cua…\n 3 Learner Native      Upper beginner     \"Yo vivo en la ciudad de Atlanta. En …\n 4 Learner Native      Upper beginner     \"Hola me llamo Jason.\\n Mis amigos es…\n 5 Learner Native      Lower intermediate \"Recientemente vi una película que es…\n 6 Learner Native      Upper intermediate \"Vivo en la ciudad de Richmond en Vir…\n 7 Learner Native      Upper intermediate \"A la semana pasada, yo vi la pelicul…\n 8 Learner Native      Upper intermediate \"Un día decidí llevarme a casa una ra…\n 9 Learner Native      Lower advanced     \"Bueno, el año pasado mi novia y yo v…\n10 Learner Native      Lower advanced     \"Un día Pablo, un niño de 6 años, enc…\n# ℹ 44 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n# Inspect learners falsely predicted to be natives\ncls_lasso_fit_preds_test |>\n  filter(outcome == \"Learner\", .pred_class == \"Native\") |>\n  select(outcome, .pred_class, proficiency, text) |>\n  count(proficiency, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  proficiency            n\n  <fct>              <int>\n1 Upper advanced        10\n2 Lower advanced         9\n3 Upper beginner         3\n4 Upper intermediate     3\n5 Lower beginner         1\n6 Lower intermediate     1\n```\n\n\n:::\n:::\n\n:::\n\nInterestingly, the majority of misclassified learners are advanced, which could be expected as they are more similar to natives. There are some beginners that are misclassified as natives, but this is not as common. Yes, it is still an open question as to why some natives are classified as learners.\n\nWe can inspect the estimates for the features in the model to gain some insight into what features are most important for predicting the outcomes. The `extract_fit_parsnip()` function takes a trained model specification `cls_final_fit` and returns a data frame with the estimated coefficients for each feature. The code is seen in @exm-predict-class-tune-hyperparameters-evaluate-test-estimates.\n\n::: {#exm-predict-class-tune-hyperparameters-evaluate-test-estimates}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract estimates\ncls_final_fit_features <-\n  cls_final_fit |>\n  extract_fit_parsnip() |>\n  tidy()\n```\n:::\n\n:::\n\nThe estimates are the log odds of the outcome. In a binary classification task, the log odds of the outcome is the log of the probability of the outcome divided by the probability of the other outcome. In our case, the reference outcome is \"Learner\", so negative log-odds indicate that the feature is associated with the \"Learner\" outcome and positive log-odds indicate that the feature is associated with the \"Native\" outcome.\n\nThe estimates are in log-odds, so we need to exponentiate them to get the odds. The odds are the probability of the outcome divided by the probability of the other outcome. The probability of the outcome is the odds divided by the odds plus one. The code is seen in @exm-predict-class-tune-hyperparameters-evaluate-test-estimates-probability.\n\n::: {#exm-predict-class-tune-hyperparameters-evaluate-test-estimates-probability}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate probability\ncls_final_fit_features |>\n  mutate(probability = exp(estimate) / (exp(estimate) + 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,051 × 4\n   term                  estimate  penalty probability\n   <chr>                    <dbl>    <dbl>       <dbl>\n 1 (Intercept)             -13.6  0.000464  0.00000129\n 2 tfidf_text_10             0    0.000464  0.5       \n 3 tfidf_text_2              0    0.000464  0.5       \n 4 tfidf_text_3              0    0.000464  0.5       \n 5 tfidf_text_4              0    0.000464  0.5       \n 6 tfidf_text_5              0    0.000464  0.5       \n 7 tfidf_text_a             64.9  0.000464  1         \n 8 tfidf_text_abandonado     7.02 0.000464  0.999     \n 9 tfidf_text_abuela        -8.64 0.000464  0.000176  \n10 tfidf_text_abuelos        2.14 0.000464  0.895     \n# ℹ 1,041 more rows\n```\n\n\n:::\n:::\n\n:::\n\nSo just looking at the snippet of the features returned from @exm-predict-class-tune-hyperparameters-evaluate-test-estimates-probability, we can see that the features 'a' and 'abandonado' are associated with the \"Native\" outcome, 'abuela' is associated with \"Learners\", and the other features are neutral (`probability` = 0.5).\n\nA quick way to extract the most important features for predicting the each outcome is to use the `vi()` function from {vip}. It takes a trained model specification and returns a data frame with the most important features. The code is seen in @exm-predict-class-tune-hyperparameters-evaluate-test-vip.\n\n::: {#exm-predict-class-tune-hyperparameters-evaluate-test-vip}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load package\nlibrary(vip)\n\n# Avoid conflicts for function names from other packages\nconflicted::conflicts_prefer(vip::vi)\n\n# Extract important features\nvar_importance_tbl <-\n  cls_final_fit |>\n  extract_fit_parsnip() |>\n  vi()\n\n# Preview\nvar_importance_tbl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,050 × 3\n   Variable           Importance Sign \n   <chr>                   <dbl> <chr>\n 1 tfidf_text_época         354. POS  \n 2 tfidf_text_mayoría       320. NEG  \n 3 tfidf_text_ésta          312. POS  \n 4 tfidf_text_ante          278. POS  \n 5 tfidf_text_proximo       274. NEG  \n 6 tfidf_text_esperar       245. NEG  \n 7 tfidf_text_mucha         244. NEG  \n 8 tfidf_text_seguir        242. POS  \n 9 tfidf_text_poder         241. POS  \n10 tfidf_text_ahí           235. POS  \n# ℹ 1,040 more rows\n```\n\n\n:::\n:::\n\n:::\n\nThe `Variable` column contains each feature (with the feature type and corresponding variable `tfidf_text_`), `Importance` provides the absolute log-odds value, and the `Sign` column indicates whether the feature is associated with the \"NEG\" (\"Learner\") or the \"POS\" (\"Native\") outcome. We can recode the `Variable` and `Sign` columns to make them more interpretable and then plot them using `ggplot()`, as in @exm-predict-class-tune-hyperparameters-evaluate-test-vip-plot.\n\n::: {#exm-predict-class-tune-hyperparameters-evaluate-test-vip-plot}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Recode variable and sign\nvar_importance_tbl <-\n  var_importance_tbl |>\n  mutate(\n    Feature = str_remove(Variable, \"tfidf_text_\"),\n    Outcome = case_when(\n      Sign == \"NEG\" ~ \"Learner\",\n      Sign == \"POS\" ~ \"Native\"),\n    ) |>\n  select(Outcome, Feature, Importance)\n\n# Plot\nvar_importance_tbl |>\n  slice_max(Importance, n = 50) |>\n  ggplot(aes(x = reorder(Feature, Importance), y = Importance)) +\n  geom_point() +\n  coord_flip() +\n  facet_wrap(~ Outcome, scales = \"free_y\") +\n  labs(x = NULL, y = \"Importance\", fill = NULL) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Most important features for predicting the outcome](9_predict_files/figure-html/fig-predict-class-tune-hyperparameters-evaluate-test-vip-plot-1.png){#fig-predict-class-tune-hyperparameters-evaluate-test-vip-plot fig-alt='Scatter plot of the most important features for predicting the outcome. The y-axis is the importance value and the x-axis is the feature. The features are faceted by the outcome.' width=576}\n:::\n:::\n\n:::\n\nWe can inspect @fig-predict-class-tune-hyperparameters-evaluate-test-vip-plot, and qualitatively assess what these features may be telling us about the differences between the learners and the natives.\n\nIn this section, we've build a text classifier using a regularized logistic regression model. We've tuned the hyperparameters to arrive at a robust model that performs well on both the training and test sets. We've also evaluated the model errors and inspected the most important features for predicting the outcome.\n\n### Text regression {#sec-predict-text-regression}\n\nWe will now turn our attention to the second task in this section, text regression. In this task, we will use the same original dataset as in the classification task, but we will predict the placement score based on the learner writing samples. I will make reference to but not repeat the steps we took in the classification task, as many of the steps are the same. This is one of the benefits of using tidymodels--the workflow is by-and-large the same for different tasks.\n\nLet's start by extracting the observations (only learners) and the relevant variables from the original data set. The code is seen in @exm-predict-reg-data.\n\n::: {#exm-predict-reg-data}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract observations and relevant variables\nreg_tbl <-\n  cedel_tbl |>\n  filter(subcorpus == \"Learner\") |>\n  select(outcome = place_score, proficiency, text)\n\n# Preview\nreg_tbl |> glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,906\nColumns: 3\n$ outcome     <dbl> 14.0, 16.3, 16.3, 18.6, 18.6, 18.6, 20.9, 20.9, 20.9, 20.9…\n$ proficiency <fct> Lower beginner, Lower beginner, Lower beginner, Lower begi…\n$ text        <chr> \"Yo vivo es Alanta, Georgia. Atlanta es muy grande ciudad.…\n```\n\n\n:::\n:::\n\n:::\n\n<!-- Step 1: Identify variables -->\n\nIn this task, our outcome variable is numeric and our predictor variable `text` is the same as before. It might be useful to engineer the features differently, but we will start with the same feature engineering process as before, namely the $tf$-$idf$ method for the top 1,050 words.\n\n<!-- Step 2: Initial split -->\n<!-- Step 3: Create recipe -->\n<!-- Step 4: Verify recipe -->\n\nI create a data split, `reg_split` and the training and testing sets, `reg_train` and `reg_test` and create a `reg_rec` recipe object which contains the starting recipe for the regression task. And, since we are using the same recipe as before, there is no need to validate the recipe. We can skip straight to the model building.\n\n\n\n\n\n<!-- Step 5: Interrogate -->\n\nAs before we will want to start with a simple model and then build up to more complex models. The list in @tbl-predict-class-algorithms, includes algorithms that are commonly used in classification tasks. Interestingly, many of these same algorithms can be applied to regression. One exception is that instead of logistic regression, linear regressions is used for numeric outcomes.\n\nAs with logistic regressionn, linear regression model is one of the simpler models. And just as with logistic regression, we will want to tune the regularlization hyperparameter of the linear regression model. Instead of detailing these steps again, let me summarize the process, in @tbl-workflow, and then we will discuss the results from the regularized linear regression model.\n\n::: {#tbl-workflow tbl-colwidths=\"[5, 95]\"}\n\n| Step | Description |\n| --- | --- |\n| 1 | Build a model specification with a placeholder to tune the model. |\n| 2 | Create a workflow with the recipe and the model specification. |\n| 3 | Create a grid of values for the regularization hyperparameter. |\n| 4 | Tune the model using cross-validation. |\n| 5 | Select the best performing hyperparameter value (based on RMSE). |\n| 6 | Update the model specification and workflow with the best performing hyperparameter value. |\n| 7 | Fit the model to the training set and evaluate the performance using cross-validation. |\n\nSteps to build and tune a model\n:::\n\n\n\n\n\nApplying the steps 1-7 we have cross-validated results for our model in the `reg_lasso_cv` object. We can collect the metrics and inspect the RMSE and $R^2$ values. The code is seen in @exm-predict-reg-metrics-lasso.\n\n::: {#exm-predict-reg-metrics-lasso}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Collect metrics\nreg_lasso_cv |> collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   14.1      10  0.269  Preprocessor1_Model1\n2 rsq     standard    0.621    10  0.0119 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\nNow, the **Root Mean Squared Error** (rmse) estimate is 14.1. RMSE is a measure of the difference between the predicted and the actual values expressed in the same units as the outcome variable. In this case, the outcome variable is the placement test score percent. So the RMSE is 14.1 percentage points. **R-Squared** $R^2$ (rsq) is a measure of the proportion of the variance in the outcome variable that is explained by the model. The $R^2$ estimate is 0.621. This means that the model explains 62% of the variance in the outcome variable. Taken together, this isn't the greatest model.\n\nBut how good or bad is it? This is where we can use the null model to compare the model to. The null model is a model that predicts the mean of the outcome variable for each of the outcomes. We can use the `null_model()` function to create a null model and submit it to cross-validation. The code is seen in @exm-predict-reg-model-spec-tune-fit-evaluate-null.\n\n::: {#exm-predict-reg-model-spec-tune-fit-evaluate-null}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create null model\nnull_model <-\n  null_model() |>\n  set_engine(\"parsnip\") |>\n  set_mode(\"regression\")\n\n# Cross-validate null model\nnull_cv <-\n  workflow() |>\n  add_recipe(reg_rec) |>\n  add_model(null_model) |>\n  fit_resamples(\n    resamples = vfold_cv(reg_train, v = 10),\n    metrics = metric_set(rmse)\n  )\n\n# Collect metrics\nnull_cv |> collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard    22.6    10   0.203 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n<!-- [ ] tmp fmt -->\n\\pagebreak\n\n::: {.callout .halfsize}\n**{{< fa exclamation-triangle >}} Warning**\n\nFor model specifications in which the model can be used in a classification or regression task, the model specification must be set to the correct mode before fitting the model. We have not set the mode for the `logistic_reg()` or `linear_reg()` model specifications, as the task is inferred. However, we have set the mode for the `null_model()`, and other model specifications that can be used in both classification and regression tasks.\n:::\n\n\n::: {.cell}\n\n:::\n\n:::\n\nOur regression model performs better than the null model (22.6) which means that it is picking up on some signal in the data.\n\nLet's visualize the distribution of the predictions and the errors from our model to see if there are any patterns of interest. We can use the `collect_predictions()` function to extract the predictions of the cross-validation and plot the true outcome against the predicted outcome using `ggplot()`, as in @exm-predict-reg-lr-eval-rmse.\n\n::: {#exm-predict-reg-lr-eval-rmse}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize predictions\nreg_lasso_cv |>\n  collect_predictions() |>\n  ggplot(aes(outcome, .pred, shape = id)) +\n  geom_point(alpha = 0.5, position = position_jitter(width = 0.5)) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.5) + # trend for each fold\n  labs(\n    x = \"Truth\",\n    y = \"Predicted score\",\n    shape = \"Fold\"\n  )\n```\n\n::: {.cell-output-display}\n![Distribution of the RMSE for the cross-validated linear regression model](9_predict_files/figure-html/fig-predict-reg-lr-eval-rmse-1.png){#fig-predict-reg-lr-eval-rmse fig-alt='Scatter plot of the true outcome against the predicted outcome for the cross-validated linear regression model. The x-axis is the true outcome and the y-axis is the predicted outcome. The shape of the points indicate the fold.' width=576}\n:::\n:::\n\n:::\n\nFrom @fig-predict-reg-lr-eval-rmse, we see data points for each predicted and truth value pair for each of the ten folds. There is a trend line for each fold which shows the linear relationship between the predicted and truth values for each fold. The trend lines are more similar than different, which is a good sign that the model is not wildly overfitting the training data. Looking closer, however, we can see the errors. Some are noticeably distant from the linear trend lines, *i.e.* outliers, in particular for test scores in the lower ranges.\n\nIf the $R^2$ value is in the ballpark, this means that somewhere around 40% of the variation is not explained by the frequency of the top 1,050 words. This is not surprising, as there are many other factors that contribute to the proficiency level of a text.\n\nWe have a model that is performing better than the null model, but it is not performing well enough to be very useful. We will need to update the model specification and/ or the features to try to improve the model fit. Let's start with the model. There are many different model specifications we could try, but we will likely need to use a more complex model specification to capture the complexity that we observe in the errors from the current linear regression model.\n\nLet's try a decision tree model. **Decision trees** are models that are able to model non-linear relationships and interactions between the features and the outcome and tend to be less influenced by outliers. Furthermore, decision trees are interpretable, which is a nice feature for an exploratory-oriented analysis. These are all desirable characteristics. Decision trees, however, can be prone to overfitting. For this reason, we will tune the maximum depth of the tree to minimize overfitting.\n\nTo implement a new model in `tidymodels`, we need to create a new model specification and a new workflow. We will use the `decision_tree()` function from {parsnip} to create the model specification. The `decision_tree()` function takes a `tree_depth` argument that we want to tune. We create the new model specification with the tuning placeholder in @exm-predict-reg-model-spec-decision-tree.\n\n::: {#exm-predict-reg-model-spec-decision-tree}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create model specification\nreg_spec <-\n  decision_tree(tree_depth = tune()) |>\n  set_engine(\"rpart\") |>\n  set_mode(\"regression\")\n\n# Preview\nreg_spec\n```\n:::\n\n```\nDecision Tree Model Specification (regression)\n\nMain Arguments:\n  tree_depth = tune()\n\nComputational engine: rpart\n```\n:::\n\nWith the model and tuning specification in place, we can now continue through the steps outlined in @tbl-workflow for this decision tree model. To create the grid of values for the tree depth hyperparameter, we will include the `grid_regular()` function with 10 levels, as seen in @exm-predict-reg-decision-tree-depth.\n\n\n::: {#exm-predict-reg-decision-tree-depth}\nTuning values for the tree depth hyperparameter\n\n```r\nreg_grid <-\n  grid_regular(tree_depth(), levels = 10)\n```\n\n:::\n\n\n\n\n\nWe can collect the metrics and inspect the RMSE and $R^2$ values. The code is seen in @exm-predict-reg-metrics-tree.\n\n::: {#exm-predict-reg-metrics-tree}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Collect metrics\nreg_tree_cv |> collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   15.9      10  0.256  Preprocessor1_Model1\n2 rsq     standard    0.510    10  0.0210 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\nThe performance for the decision tree is worse than the regularized linear regression model. The RSME is 15.9 and the $R^2$ is 0.51. And, if we compare the standard error between the two models, we can see that the decision tree model has a lower standard error. This means that the decision tree model is likely overfitting, despite our efforts to tune tree depth.\n\nGiven the sensitivity of the decision tree branching process and random initialization, it is possible that the decision tree model is capturing too much nuance, and not enough generalities. Re-running the model with a different seed may result in a different model. This is a limitation with decision tree models, but it is also a feature, if we consider combining multiple decision trees to make a prediction. This is the basis of **ensemble models**. An ensemble model is a model that combines multiple models with the goal to draw out the strengths of each model and minimize the weaknesses.\n\nA **random forest** is an ensemble model that combines multiple decision trees to make a prediction. In addition, random forests also perform random feature selection. This helps to reduce the correlation between the decision trees and thus works to reduce the overall variance of the model.\n\nLet's try a random forest model to address our text regression task. We will use the `rand_forest()` function from {parsnip} to create the model specification. The `rand_forest()` function also takes a hyperparameter for the number of trees to be used in the model. We will select the `ranger` engine. Additionally, we will add the `importance` argument to ensure that we can extract feature importance if this model proves to be useful. We create the new model specification in @exm-predict-reg-model-spec-random-forest.\n\n::: {#exm-predict-reg-model-spec-random-forest}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create model specification\nreg_spec <-\n  rand_forest(trees = tune()) |>\n  set_engine(\"ranger\", importance = \"impurity\") |>\n  set_mode(\"regression\")\n\n# Preview\nreg_spec\n```\n:::\n\n```\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = tune()\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger\n```\n:::\n\n::: {.callout .halfsize}\n**{{< fa regular lightbulb >}} Consider this**\n\nThe model building process is iterative and many of the steps are the same. This is a good indication that creating a custom function to build and tune the model would be a good idea.\n\nConsider the following: What would you include in the function? What would you leave out? What required and/ or optional arguments would you include? What would you hard code? What would you return?\n:::\n\nAgain, we apply the steps in @tbl-workflow to build and tune the random forest model. As part of this process, I will limit the range of the number of trees from 100 to 500 in five levels in the tuning grid, as seen in @exm-predict-reg-random-forest-trees.\n\n::: {#exm-predict-reg-random-forest-trees}\nTuning values for the number of trees hyperparameter\n\n```r\nreg_grid <-\n  grid_regular(trees(range = c(100, 500)), levels = 5)\n```\n\n:::\n\n\n\n\n\nLet's collect the metrics and inspect the RMSE and $R^2$ values. The code is seen in @exm-predict-reg-metrics-rf.\n\n::: {#exm-predict-reg-metrics-rf}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Collect metrics\nreg_rf_cv |> collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   12.9      10  0.320  Preprocessor1_Model1\n2 rsq     standard    0.697    10  0.0164 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\nThe random forest model performs better than the decision tree model and the regularized linear regression model. The RSME is 12.9 and the $R^2$ is 0.697. We also see that the standard error falls between the models we have tried so far.\n\nBefore we settle on this model, let's try one more model. In this case, we will introduce a neural network model. Neural networks are models that are able to model non-linear relationships and interactions between the features and the outcome. They are also able to model complex relationships between the features and the outcome. We will use the `mlp()` function from {parsnip} to create the model specification. We will choose the `brulee` engine which allows us to tune the learning rate. The **learning rate** is a hyperparameter that controls the size of the steps that the model takes to update the weights.\n\n We create the new model specification with the tuning placeholder in @exm-predict-reg-model-spec-mlp.\n\n::: {#exm-predict-reg-model-spec-mlp}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create model specification\nreg_spec <-\n  mlp(learn_rate = tune()) |>\n  set_engine(\"brulee\") |>\n  set_mode(\"regression\")\n\n# Preview\nreg_spec\n```\n:::\n\n```\nSingle Layer Neural Network Model Specification (regression)\n\nMain Arguments:\n  learn_rate = tune()\n\nComputational engine: brulee\n\nModel fit template:\nbrulee::brulee_mlp(x = missing_arg(), y = missing_arg(), learn_rate = tune())\n```\n:::\n\nAnd include the code in @exm-predict-reg-mlp-learning-rate to create a grid of values for the learning rate hyperparameter, as part of the model building workflow.\n\n::: {#exm-predict-reg-mlp-learning-rate}\nTuning values for the learning rate hyperparameter\n\n```r\nreg_grid <-\n  grid_regular(learn_rate(), levels = 10)\n```\n\n:::\n <!-- WARN: This needs to have `torch::install_torch()` run before it can be run. -->\n\n\n\n\nLet's collect the metrics and inspect the RMSE and $R^2$ values. The code is seen in @exm-predict-reg-metrics-mlp.\n\n::: {#exm-predict-reg-metrics-mlp}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Collect metrics\nreg_mlp_cv |> collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   15.6      10  1.54   Preprocessor1_Model1\n2 rsq     standard    0.562     9  0.0892 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n:::\n\nSo in summary, we've tried four different model specifications. The regularized linear regression model, the decision tree model, the random forest model, and the neural network model. The random forest model performed the best. For each of these models, however, we have only tried word features measured by $tf$-$idf$. We could imagine that the performance of these models could be improved by varying the features to include bigrams, for example. We could also explore different measures of word usage. Furthermore, for some of our models, we could try different engines and/ or hyperparameters (some have more than one!).\n\nWe could continue to try to explore these possible combinations, and you likely would in your research. But at this point we have a model that is performing better than the null model and is performing better than the other models we have tried. So we will consider this model to be good enough for our purposes.\n\nLet's take our Random Forest model, fit it to our training data, apply it to the testing data, and collect the metrics on the test set. The code is seen in @exm-predict-reg-model-spec-random-forest-workflow-tune-fit.\n\n::: {#exm-predict-reg-model-spec-random-forest-workflow-tune-fit}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the model to the training set and\n# evaluate on the test set\nreg_final_fit <-\n  last_fit(\n    reg_wf_rf,\n    split = reg_split\n  )\n\n# Evaluate model on testing set\nreg_final_fit |> collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      12.9   Preprocessor1_Model1\n2 rsq     standard       0.689 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n:::\n\nOk. The difference between the cross-validated metrics and the metrics for the test set differ --but only slightly. This suggests that the model is robust and that we have not overfit the data from the training set.\n\n<!-- Step 7: Interpret -->\n\nNow, our likely goal as an academic is to understanding something about the features that contribute to the performance of the model. So let's approach extracting feature importance from the Random Forest model we build with the `ranger` engine. Remember, we added an `importance` argument to the `set_engine()` function and set it to 'impurity'. We can now take advantage by using {vip} to extract the feature importance. The code is seen in @exm-predict-reg-final-fit-vip.\n\n::: {#exm-predict-reg-final-fit-vip}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract feature importance\nreg_vip <-\n  reg_final_fit |>\n  extract_fit_parsnip() |>\n  vi(scale = TRUE)\n\n# Preview\nreg_vip |>\n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 2\n   Variable        Importance\n   <chr>                <dbl>\n 1 tfidf_text_que       100  \n 2 tfidf_text_es         68.8\n 3 tfidf_text_una        57.4\n 4 tfidf_text_por        57.2\n 5 tfidf_text_pero       54.0\n 6 tfidf_text_del        48.5\n 7 tfidf_text_con        46.1\n 8 tfidf_text_se         44.9\n 9 tfidf_text_para       44.1\n10 tfidf_text_muy        43.6\n```\n\n\n:::\n:::\n\n:::\n\nWe can now visualize the feature importance of the model. The code is seen in @exm-predict-reg-final-fit-vip-visualize.\n\n::: {#exm-predict-reg-final-fit-vip-visualize}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract predictions\nreg_vip |>\n  mutate(Variable = str_replace(Variable, \"^tfidf_text_\", \"\")) |>\n  slice_max(Importance, n = 20) |>\n  # reorder variables by importance\n  ggplot(aes(reorder(Variable, Importance), Importance)) +\n  geom_point() +\n  coord_flip() +\n  labs(\n    x = \"Feature\",\n    y = \"Importance\"\n  )\n```\n\n::: {.cell-output-display}\n![Feature importance of the random forest model](9_predict_files/figure-html/fig-predict-reg-final-fit-vip-visualize-1.png){#fig-predict-reg-final-fit-vip-visualize fig-alt='A dot plot which shows the importance of the features in the random forest model. The y-axis is the feature (sorted by importance) and the x-axis is the importance value.' width=576}\n:::\n:::\n\n:::\n\nIn this section, we've built text regression models focusing on the ability to change algorithms and hyperparameters. We also seen some of the differences between evaluating model performance between classification and regression tasks. There are many more combinations of model specifications and feature selection and engineering that can be applied. In your research, you will find yourself using these tools to explore the best model for your data.\n\n## Activities {.unnumbered}\n\nIn the following activities, we will apply the concepts and techniques we have learned in this chapter. We will use the Tidymodels framework to build and evaluate supervised machine learning models for text classification and regression tasks.\n\n::: {.callout}\n**{{< fa regular file-code >}} Recipe**\n\n**What**: Building predictive models\\\n**How**: Read Recipe 9, complete comprehension check, and prepare for Lab 9.\\\n**Why**: To continue to build experience building predictive models with the Tidymodels framework.\n:::\n\n::: {.callout}\n**{{< fa flask >}} Lab**\n\n**What**: Text classification\\\n**How**: Clone, fork, and complete the steps in Lab 9.\\\n**Why**: To apply your knowledge of supervised machine learning to a text classification task.\n:::\n\n## Summary {.unnumbered}\n\nIn this chapter, we outlined the workflow for approach predictive modeling and the Tidymodels framework. We then applied the workflow to text classification and regression tasks. Gained experience identifying, selecting, and engineering features on the one hand, and building and tuning models on the other. To evaluate the models, we used cross-validation for performance and finalized our interpretation with techniques to extract feature importance.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}