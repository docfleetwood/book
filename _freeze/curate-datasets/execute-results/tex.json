{
  "hash": "f770e7c4faceaf595ad014a43e7be09a",
  "result": {
    "markdown": "---\nexecute: \n  echo: true\n  cache: false\n---\n\n\n# Curate data(sets) {#sec-curate-datasets}\n\n\n\n\n\n\n\n> The hardest bit of information to extract is the first piece.\n>\n> ―--Robert Ferrigno\n\n\n<!-- TODOs:\n\n- cite the packages used\n- cite the Europarle Corpus \n- correct the file names for Switchboard to reflect the dataset_curated.csv, and dataset_data_dictionary.csv format\n\n-->\n\n::: callout-note\n## Keys\n- what are some of the formats that data can take?\n- what R programming strategies are used to read these formats into tabular, tidy dataset structures?\n- what is the importance of maintaining modularity between data and data processing in a reproducible research project?\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nIn this chapter we will now look at the next step in a text analysis project: data curation. That is, the process of converting the original data we acquire to a tidy dataset. As Acquired data can come in a wide variety of formats that depend largely on the richness of the metadata that is included, but also can reflect individual preferences. In this chapter we will consider three general types of formats: (1) unstructured data, (2) structured data, and (3) semi-structured data. Regardless of the file type and the structure of the data, it will be necessary to consider how to curate a dataset that such that the structure reflects the basic the unit of analysis that we wish to investigate (see [Chapter 4, section 4.2](#sec-framing-research.html#research-question). The resulting dataset will be the base from which we will work to further transform the dataset such that it aligns with the analysis method(s) that we will implement. And as in previous implementation steps, we will discuss the important role of documentation.  \n\n## Unstructured\n\nThe bulk of text that is available in the wild is of the unstructured variety. Unstructured data is data that has not been organized to make the information contained within explicit. Explicit information that is included with data is called metadata. Metadata can be linguistic or non-linguistic in nature. So for unstructured data there is little to no metadata directly associated with the data. This information needs to be added or derived for the purposes of the research, either through manual inspection or (semi-)automatic processes. For now, however, our job is just to get the unstructured data into a structured format with a minimal set of metadata that we can derive from the resource. \n\nAs an example of an unstructured source of text data, let's take a look at the [Europarle Parallel Corpus](https://www.statmt.org/europarl/), as introduced in [Chapter 2 \"Understanding data\"](#sec-understanding-data). This data contains parallel texts (source and translated documents) from the European Parliamentary proceedings for some 21 European languages. Here we will focus in on the translation from Spanish to English (Spanish-English). \n\n### Orientation\n\nWith the data downloaded into the `data/original/europarle/` directory we see that there are two files. One corresponding to the source language (Spanish) and one for the target language (English). \n\n```bash\ndata/original/europarle/\n├── europarl-v7.es-en.en\n└── europarl-v7.es-en.es\n```\n\nLooking at the first 10 lines of the first file, we can see that this is running text. \n\n\n::: {.cell layout-align=\"center\" linewidth='80'}\n\n```{.r .cell-code}\nreadtext::readtext(\"data/understanding-data/formats_europarle-en_sample.txt\") |>\n  pull(text) |>\n  cat(fill = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n> Resumption of the session\n> I declare resumed the session of the European Parliament adjourned on Friday\n17 December 1999, and I would like once again to wish you a happy new year in\nthe hope that you enjoyed a pleasant festive period.\n> Although, as you will have seen, the dreaded 'millennium bug' failed to\nmaterialise, still the people in a number of countries suffered a series of\nnatural disasters that truly were dreadful.\n> You have requested a debate on this subject in the course of the next few\ndays, during this part-session.\n> In the meantime, I should like to observe a minute' s silence, as a number of\nMembers have requested, on behalf of all the victims concerned, particularly\nthose of the terrible storms, in the various countries of the European Union.\n> Please rise, then, for this minute' s silence.\n> (The House rose and observed a minute' s silence)\n> Madam President, on a point of order.\n> You will be aware from the press and television that there have been a number\nof bomb explosions and killings in Sri Lanka.\n> One of the people assassinated very recently in Sri Lanka was Mr Kumar\nPonnambalam, who had visited the European Parliament just a few months ago.\n```\n:::\n:::\n\n\nThe only meta information that we can surmise from these files is the fact that we know one is the source language and one is the target language and that each sentence is aligned (parallel) with the lines in the other file. \n\nSo with what we have we'd like to create a data frame that has the seen in @tbl-cd-unstructured-europarle-structure-example.  \n\n\n::: {#tbl-cd-unstructured-europarle-structure-example .cell layout-align=\"center\" tbl-cap='Idealized structure for the Europarle Corpus dataset.'}\n::: {.cell-output-display}\n\\begin{tabular}{lrl}\n\\toprule\ntype & sentence\\_id & sentence\\\\\n\\midrule\nSource & 1 & ...sentence from source language\\\\\nTarget & 1 & ...sentence from target language\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\n### Tidy the data\n\nTo create this dataset structure lets's read the files with the `readtext()` function from readtext package and assign them to a meaningful variable.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Read the Europarle files\neuroparle_en <-  # English target text\n  readtext::readtext(\"../data/original/europarle/europarl-v7.es-en.en\", # path to the data\n                     verbosity = 0) # don't show warnings\n\neuroparle_es <- # Spanish source text\n  readtext::readtext(\"../data/original/europarle/europarl-v7.es-en.es\", # path to the data\n                     verbosity = 0) # don't show warnings\n```\n:::\n\n\n::: callout-warning\n## Tip\nThe `readtext()` function can read many different types of file formats, from structured to unstructured. However, it depends in large part on the extension of the file to recognize what algorithm to use when reading a file. In this particular case the Europarle files do not have a typical extension (they have `.en` and `.es`). The `readtext()` function will treat them as plain text (`.txt`), but it will throw a warning message. To suppress the warning message you can add the `verbosity = 0` argument.\n:::\n\nNow there are a couple things to note about thbe `europarle_en` and `europarle_es` objects. If we inspect their structure, we will find that the dimensions of the data frame that is created is one row by two columns. \n\n\n::: {.cell layout-align=\"center\" linewidth='80'}\n\n```{.r .cell-code}\nstr(europarle_en) # inspect the structure of the object\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Classes 'readtext' and 'data.frame': 1 obs. of 2 variables:\n#> $ doc_id: chr \"europarl-v7.es-en.en\"\n#> $ text : chr \"Resumption of the session\\nI declare resumed the session of\nthe European Parliament adjourned on Friday 17 Dece\"| __truncated__\n```\n:::\n:::\n\n\n::: callout-warning\n## Tip\nNote that the `str()` function from base R is similar to `glimpse()`. However, `glimpse()` will attempt to show you as much data as possible. In this case since our column `text` is a very long character vector it will take a long time to render. I've chosen the `str()` function as it will automatically truncate the data.\n:::\n\nThe columns are `doc_id` and `text`. `doc_id` is created by readtext to index each file that is read in. The `text` column is where the text appears. The fact that we only have one row means that all the text in the entire file is contained in one cell! We will want to break this cell up into rows for each sentence, but for now let's work with getting the columns to line up with our idealized dataset structure. \n\nFirst let's change the type of data frame that we are working with to a tibble. This will make sure we don't accidentally print hundreds of lines to our R Markdown output and/ or the R Console. Then we will rename the `doc_id` column to `type` and change the value of that column to \"Target\" (for English) and \"Source\" (for Spanish). \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neuroparle_target <- \n  europarle_en %>% # readtext data frame\n  as_tibble() %>% # convert to tibble\n  rename(type = doc_id) %>% # rename doc_id to type\n  mutate(type = \"Target\") # change type value to 'Target'\n\neuroparle_source <- \n  europarle_es %>% # readtext data frame\n  as_tibble() %>% # convert to tibble\n  rename(type = doc_id) %>% # rename doc_id to type\n  mutate(type = \"Source\") # change type value to 'Source'\n```\n:::\n\n\nWe have two objects now, one corresponding to the 'Source' and the other the 'Target' parallel texts. Let's now join these two datasets, one on top of the other --that is, by rows. We wil use the `bind_rows()` function for this.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neuroparle <- \n  bind_rows(europarle_target, europarle_source)\n\nstr(europarle) # inspect the structure of the object\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> tibble [2 x 2] (S3: tbl_df/tbl/data.frame)\n#>  $ type: chr [1:2] \"Target\" \"Source\"\n#>  $ text: chr [1:2] \"Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Friday 17 Dece\"| __truncated__ \"Reanudación del período de sesiones\\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrump\"| __truncated__\n```\n:::\n:::\n\n\nThe `europarle` dataset now has 2 columns, as before, and 2 rows --each corresponding to the distinct language types (Source/ Target). \n\nRemember our goal is to create a dataset structure with three columns `type`, `sentence_id`, and `sentence`. At the moment we have `type` and `text` --where `text` has all of the sentences in for each type in a cell. So we are going to want to break up the `text` column into sentences, group the sentences that are created by `type`, and then number these sentences so that they are aligned between the distinct types. \n\nTo break up the text into sentences we are going to turn to the tidytext package. This package has a extremely useful function `unnest_tokens()` which provides an effective way to break text into various units (see `?tidytext::unnest_tokens` for a full list of token types). Since I know from looking at the raw text that each sentence is on its own line, the best strategy to break the text into sentence units is to find a way to break each line into a new row in our dataset. To do this we need to use the `token = \"regex\"` (for Regular Expression) and use the `pattern = \"\\\\n\"` which tells R to look for carriage returns to use as the breaking criterion.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neuroparle_sentences <- \n  europarle %>% \n  tidytext::unnest_tokens(output = sentence, # new column\n                          input = text, # column to find text\n                          token = \"regex\", # use a regular expression to break up the text\n                          pattern = \"\\\\n\", # break text by carriage returns (returns after lines)\n                          to_lower = FALSE) # do not lowercase the text\n\nglimpse(europarle_sentences) # preview the structure\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 3,926,375\n#> Columns: 2\n#> $ type     <chr> \"Target\", \"Target\", \"Target\", \"Target\", \"Target\", \"Target\", \"~\n#> $ sentence <chr> \"Resumption of the session\", \"I declare resumed the session o~\n```\n:::\n:::\n\n\n::: callout-warning\n## Tip\nRegular Expressions are a powerful pattern matching syntax. They are used extensively in text manipulation and we will see them again and again. A good website to practice Regular Expressions is [RegEx101](https://regex101.com/). You can also install the regexplain package in R to get access to a useful [RStudio Addin](https://rstudio.github.io/rstudioaddins/). \n:::\n\nOur new `europarle_sentences` object is a data frame with almost 4 million rows! The final step to get to our envisioned dataset structure is to add the `sentence_id` column which will be calculated by grouping the data by `type` and then assigning a row number to each of the sentences in each group. \n",
    "supporting": [
      "curate-datasets_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}