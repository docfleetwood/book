{
  "hash": "4fb97893f019f047681b028d7efdcfef",
  "result": {
    "markdown": "---\nexecute: \n  echo: true\n---\n\n\n# Transform datasets {#sec-transform-datasets}\n\n\n\n\n\n\n\n<!-- TODO:\n\n- [ ] Add a description of the importance of the unit of observation in the transformation process.\n    - Note: that some units of obsevation will require that the data have more textual context (e.g. syntactic parsing) than others (e.g. word frequencies). It is important that the presntation of the transformation steps are no necessarily in the order in which they are applied.\n- [ ] ...\n -->\n\n> Nothing is lost. Everything is transformed.\n>\n> --- Michael Ende, The Neverending Story\n\n::: {.callout-note}\n## Keys\n- What is the role of data transformation in a text analysis project?\n- What are the general processes for preparing datasets for analysis?\n- How do each of these general processes transform datasets?\n:::\n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/transform-data-packages_43d00c21651fca8419fb8fc2e19b0423'}\n\n:::\n\n\nIn this chapter we turn out attention to the process of moving a curated dataset one step closer to analysis. Where in the process of curating data into a dataset the goal was to derived a tidy dataset that contained the main relational characteristics of the data for our text analysis project, the transformation step refines and potential expands these characteristics such that they are more in line with our analysis aims. In this chapter I have grouped various transformation steps into four categories: normalization, recoding, generation, and merging. It is of note that the these categories have been ordered and are covered separately for descriptive reasons. In practice the ordering of which transformation to apply before another is highly idiosyncratic and requires that the researcher evaluate the characteristics of the dataset and the desired results.  \n\nFurthermore, since in any given project there may be more than one analysis that may be performed on the data, there may be distinct transformation steps which correspond to each analysis approach. Therefore it is possible that there are more than one transformed dataset created from the curated dataset. This is one of the reasons that we create a curated dataset instead of derived a transformed dataset from the original data. The curated dataset serves as a point of departure from which multiple transformational methods can derive distinct formats for distinct analyses. \n\nLet's now turn to demonstrations of some common transformational steps using datasets with which we are now familiar. \n\n::: {.callout-tip}\n## Swirl\n\n**What**: [Data manipulation](https://github.com/lin380/swirl)\\\n**How**: In the R Console pane load `swirl`, run `swirl()`, and follow prompts to select the lesson.\\\n**Why**: To learn more about data frame objects in R and how to manipulate them using the `dplyr` package.\n:::\n\n## Normalize\n\n<!-- Consider: \n\nI will need to think about how to balance what takes place at the curation stage and what happens in the transformation stage.\n\nI think that the curation stage should be focused on the creation of a tidy dataset that is ready for analysis. This may include things that I have added here such as tokenization and lemmatization. If the analysis unit is clear, say for a IDA approach, then it is fine to move the data curation process in that direction. \n\nWhat is text normalization? Text normalization includes the process of removing artifacts from the text that are not of interest to the analysis. This may include removing non-linguistic metadata, removing non-speech lines, removing punctuation, and removing stop words -- depending on the target unit(s) of analysis. \n\n> So I might to add text normalization to the curation stage and then add the other three categories to the transformation stage.\n\nWhat is data recoding for text analysis? Data recoding is the process of transforming the values of a variable or set of variables such that they are more in line with the analysis aims. This may include recoding values of a variable to a different value, recoding values of a variable to a different variable, and recoding values of a variable to a new variable.\n\nWhat is data merging for text analysis? Data merging is the process of combining two or more datasets into a single dataset. This may include merging datasets that are in the same format, merging datasets that are in different formats, and merging datasets that are in different formats but have the same structure. For example deriving new variables from a dictionary, deriving new variables from a related dataset (lexical decision latencies), deriving new variables from a lexicon (sentiment, concreteness, etc.), \n\nWhat is data generation for text analysis? Data generation is the process of deriving new variables that stems from the inclusion of other resources. This may inclde deriving new variables from a tool such as a sentiment analyzer or syntactic parser (part of speech, dependency, etc.).\n\n -->\n\n\nThe process of normalizing datasets in essence is to santize the values of variable or set of variables such that there are no artifacts that will contaminate subsequent processing. It may be the case that non-linguistic metadata may require normalization but more often than not linguistic information is the most common target for normalization as text often includes artifacts from the acquisition process which will not be desired in the analysis. \n\n**Europarle Corpus**\n\nConsider the curated Europarle Corpus dataset. I will read in the dataset. Since the dataset is quite large, I have also subsetted the dataset keeping only the first 1,000 observations for each of value of `type` for demonstration purposes.\n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-europarle-normalize-read-subset-show_1d497cacd55e08489536323f8e6b723f'}\n\n```{.r .cell-code}\neuroparle <- read_csv(file = \"../data/derived/europarle/europarle_curated.csv\") |>  # read curated dataset\n  filter(sentence_id < 1001) # keep first 1000 observations for each type\n\nglimpse(europarle)\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-europarle-normalize-read-subset-run_9fe2cb42bfc8134b33969c50faceae6f'}\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 2,000\n#> Columns: 3\n#> $ type        <chr> \"Source\", \"Target\", \"Source\", \"Target\", \"Source\", \"Target\"~\n#> $ sentence_id <dbl> 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, ~\n#> $ sentence    <chr> \"Reanudación del período de sesiones\", \"Resumption of the ~\n```\n:::\n:::\n\n\nSimply looking at the first 14 lines of this dataset, we can see that if our goal is to work with the transcribed ('Source') and translated ('Target') language, there are lines which do not appear to be of interest.\n\n\n::: {#tbl-td-europarle-preview-1 .cell layout-align=\"center\" tbl-cap='Europarle Corpus curated dataset preview.' hash='transform-datasets_cache/pdf/tbl-td-europarle-preview-1_ecdbdeaf5efac8a40e52f751ad6d0785'}\n::: {.cell-output-display}\n\\begin{tabular}{lrl}\n\\toprule\ntype & sentence\\_id & sentence\\\\\n\\midrule\nSource & 1 & Reanudación del período de sesiones\\\\\nTarget & 1 & Resumption of the session\\\\\nSource & 2 & Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\\\\\nTarget & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\\\\nSource & 3 & Como todos han podido comprobar, el gran \"efecto del año 2000\" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\\\\n\\addlinespace\nTarget & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\\\\nSource & 4 & Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\\\\nTarget & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\\\\nSource & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\\\\\nTarget & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\\\\n\\addlinespace\nSource & 6 & Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\\\\nTarget & 6 & Please rise, then, for this minute' s silence.\\\\\nSource & 7 & (El Parlamento, de pie, guarda un minuto de silencio)\\\\\nTarget & 7 & (The House rose and observed a minute' s silence)\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\n`sentence_id` 1 appears to be title and `sentence_id` 7 reflects description of the parliamentary session. Both of these are artifacts that we would like to remove from the dataset. \n\nTo remove these lines we can turn to the programming strategies we've previously worked with. Namely we will use `filter()` to filter observations in combination with `str_detect()` to detect matches for some pattern that is indicative of these lines that we want to remove and not of the other lines that we want to keep. \n\nBefore we remove any lines, let's try craft a search pattern to identify these lines, and exclude the lines we will want to keep. Condition one is lines which start with an opening parenthesis `(`. Condition two is lines that do not end in standard sentence punctuation (`.`, `!`, or `?`). I've added both conditions to one `filter()` using the logical *OR* operator (`|`)  to ensure that either condition is matched in the output. \n\n\n::: {#tbl-td-europarle-search-non-speech .cell layout-align=\"center\" tbl-cap='Non-speech lines in the Europarle dataset.' hash='transform-datasets_cache/pdf/tbl-td-europarle-search-non-speech_5346f077ae0e9cbec92a934f1bb688cc'}\n\n```{.r .cell-code}\n# Identify non-speech lines\neuroparle |> \n  filter(str_detect(sentence, \"^\\\\(\") | str_detect(sentence, \"[^.!?]$\")) |> # filter lines that detect a match for either condition 1 or 2\n  slice_sample(n = 10) |> # random sample of 10 observations\n  knitr::kable(booktabs = TRUE)\n```\n\n::: {.cell-output-display}\n\\begin{tabular}{lrl}\n\\toprule\ntype & sentence\\_id & sentence\\\\\n\\midrule\nSource & 110 & (El Parlamento rechaza la propuesta por 164 votos a favor, 166 votos en contra y 7 abstenciones)\\\\\nTarget & 93 & (Parliament rejected the request) President.\\\\\nSource & 85 & (Aplausos del grupo PSE)\\\\\nSource & 674 & A5-0087/1999 del Sr. Jonckheer, en nombre de la Comisión de Asuntos Económicos y Monetarios, sobre el séptimo informe sobre ayudas estatales a la industria y a otros sectores en la Unión Europea (COM(1999) 148- C5-0107/1999 - 1999/2110(COS));\\\\\nSource & 134 & Consejeros de seguridad para el transporte de mercancías peligrosas\\\\\n\\addlinespace\nSource & 221 & Transporte de mercancías peligrosas por carretera\\\\\nTarget & 109 & (Parliament rejected the request, with 164 votes for, 166 votes against and 7 abstentions)\\\\\nTarget & 638 & (The sitting was closed at 8.25 p.m.)\\\\\nTarget & 675 & A5-0087/1999 by Mr Jonckheer, on behalf of the Committee on Economic and Monetary Affairs, on the seventh survey on state aid in the European Union in the manufacturing and certain other sectors. [COM(1999) 148 - C5-0107/1999 - 1999/2110(COS)] (Report 1995-1997);\\\\\nTarget & 293 & Structural Funds - Cohesion Fund coordination\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nSince this search appears to match lines that we do not want to preserve, let's move now to eliminate these lines from the dataset. To do this we will use the same regular expression patterns, but now each condition will have it's own `filter()` call and the `str_detect()` will be negated with a prefixed `!`.\n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-europarle-filter-non-speech_0bffb71140184491cf6a65699cfc83f3'}\n\n```{.r .cell-code}\neuroparle <- \n  europarle |> # dataset\n  filter(!str_detect(sentence, pattern = \"^\\\\(\")) |> # remove lines starting with (\n  filter(!str_detect(sentence, pattern = \"[^.!?]$\")) # remove lines not ending in ., !, or ?\n```\n:::\n\n\nLet's look at the first 14 lines again, now that we have eliminated these artifacts. \n\n\n::: {#tbl-td-europarle-preview-2 .cell layout-align=\"center\" tbl-cap='Europarle Corpus non-speech lines removed.' hash='transform-datasets_cache/pdf/tbl-td-europarle-preview-2_d3c6ca3d9156c0aeb47dea8cb5fcad95'}\n::: {.cell-output-display}\n\\begin{tabular}{lrl}\n\\toprule\ntype & sentence\\_id & sentence\\\\\n\\midrule\nSource & 2 & Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\\\\\nTarget & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\\\\nSource & 3 & Como todos han podido comprobar, el gran \"efecto del año 2000\" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\\\\\nTarget & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\\\\nSource & 4 & Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\\\\\n\\addlinespace\nTarget & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\\\\nSource & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\\\\\nTarget & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\\\\nSource & 6 & Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\\\\nTarget & 6 & Please rise, then, for this minute' s silence.\\\\\n\\addlinespace\nSource & 8 & Señora Presidenta, una cuestión de procedimiento.\\\\\nTarget & 8 & Madam President, on a point of order.\\\\\nSource & 9 & Sabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en Sri Lanka.\\\\\nTarget & 9 & You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nOne further issue that we may want to resolve concerns the fact that there are whitespaces between possessive forms (i.e. \"minute’ s silence\"). In this case we can employ `str_replace_all()` inside the `mutate()` function to overwrite the `sentence` values that match an apostrophe `'` with whitespace (`\\\\s`) before `s`.\n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-europarle-remove-whitespace_8a5d717c62e735e878ddf5600c318dff'}\n\n```{.r .cell-code}\neuroparle <- \n  europarle |> # dataset\n  mutate(sentence = str_replace_all(string = sentence, \n                                    pattern = \"'\\\\ss\", \n                                    replacement = \"'s\")) # replace ' s with `s\n```\n:::\n\n\nNow we have normalized text in the `sentence` column in the Europarle dataset. \n\n**Last FM Lyrics**\n\n<!-- (stanza separation, other) -->\nLet's look at another dataset we have worked with during this coursebook: the Lastfm lyrics. Reading in the `lastfm_curated` dataset from the `data/derived/` directory we can see the structure for the curated structure. \n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-lastfm-read-show_349f115690a9dd44652001adb733c1cb'}\n\n```{.r .cell-code}\nlastfm <- read_csv(file = \"../data/derived/lastfm/lastfm_curated.csv\") # read in lastfm_curated dataset\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-lastfm-read-run_9aa4434c92db6f5e9ac4b2f26bc27c7b'}\n\n:::\n\n::: {#tbl-td-lastfm-read-preview .cell layout-align=\"center\" tbl-cap='Last fm lyrics dataset preview with one artist/ song per genre and the `lyrics` text truncated  at 200 characters for display purposes.' hash='transform-datasets_cache/pdf/tbl-td-lastfm-read-preview_df2a4706ceda1876a579f0375aa34fe8'}\n::: {.cell-output-display}\n\\begin{tabular}{llll}\n\\toprule\nartist & song & lyrics & genre\\\\\n\\midrule\nAlan Jackson & Little Bitty & Have a little love on a little honeymoonYou got a little dish and you got a little spoonA little bitty house and a little bitty yardA little bitty dog and a little bitty car Well, it's alright to b... & country\\\\\n50 Cent & In Da Club & Go, go, go, go, go, goGo, shortyIt's your birthdayWe gon' party like it's your birthdayWe gon' sip Bacardi like it's your birthdayAnd you know we don't give a fuck it's not your birthday You can fi... & hip-hop\\\\\nBlack Sabbath & Paranoid & Finished with my woman'Cause she couldn't help me with my mindPeople think I'm insaneBecause I am frowning all the time All day long, I think of thingsBut nothing seems to satisfyThink I'll lose my... & metal\\\\\na-ha & Take On Me & Talking awayI don't know whatWhat to sayI'll say it anywayToday is another day to find youShying awayOh, I'll be coming for your love, okay? Take On Me (Take On Me)Take me on (Take On Me)I'll be go... & pop\\\\\n3 Doors Down & Here Without You & A hundred days have made me olderSince the last time that I saw your pretty faceA thousand lies have made me colderAnd I don't think I can look at this the same But all the miles that separateDisap... & rock\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nThere are a few things that we might want to clean out of the `lyrics` column's values. First, there are lines from the original webscrape where the end of one stanza runs into the next without whitespace between them (i.e. \"honeymoonYou\"). These reflect contiguous end-new line segments where stanzas were joined in the curation process. Second, we see that there are what appear to be backing vocals which appear between parentheses (i.e. \"(Take On Me)\"). \n\nIn both cases we will use `mutate()`. With contiguous end-new line segments we will use `str_replace_all()` inside and for backing vocals in parentheses we will use `str_remove_all()`. \n\nThe pattern to match for end-new lines from the stanzas will use some regular expression magic. The base pattern includes finding a pair of lowercase-uppercase letters (i.e. \"nY\", in \"honeymoo**nY**ou\"). For this we can use the pattern `[a-z][A-Z]`. To replace this pattern using the lowercase letter then a space and then the uppercase letter we take advantage of the grouping syntax in regular expressions `(...)`. So we add parentheses around the two groups to capture like this `([a-z])([A-Z])`. In the replacement argument of the `str_replace_all()` function we then specify to use the captured groups in the order they appear `\\\\1` for the lowercase letter match and `\\\\2` for the uppercase letter match. \n\nNow, I've looked more extensively at the `lyrics` column and found that there are other combinations that are joined between stanzas. Namely that `'`, `!`, `,`, `)`, `?`, and `I` also may precede the uppercase letter. To make sure we capture these possibilities as well I've updated the regular expression to `([a-z'!,.)?I])([A-Z])`. \n\nNow to remove the backing vocals, the regex pattern is `\\\\(.+?\\\\)` --match the parentheses and everything within the parentheses. The added `?` after the `+` operator is what is known as a 'lazy' operator. This specifies that the `.+` will match the minimal string that is enclosed by the trailing `)`. If we did not include this then we would get matches that span from the first parenthesis `(` all the way to the last, which would match real lyrics, not just the backing vocals.\n\nPutting this to work let's clean the `lyrics` column.\n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-lastfm-clean-end-lines_816658f15e45f1debc864f56836c0cd8'}\n\n```{.r .cell-code}\nlastfm <- \n  lastfm |> # dataset\n  mutate(lyrics = \n           str_replace_all(string = lyrics, \n                           pattern = \"([a-z'!,.)?I])([A-Z])\", # find contiguous end/ new line segments\n                           replacement = \"\\\\1 \\\\2\")) |>  # replace with whitespace between\n  mutate(lyrics = str_remove_all(lyrics, \"\\\\(.+?\\\\)\")) # remove backing vocals (Take On Me)\n```\n:::\n\n::: {#tbl-td-lastfm-clean-end-lines-preview .cell layout-align=\"center\" tbl-cap='Last fm lyrics with cleaned lyrics...' hash='transform-datasets_cache/pdf/tbl-td-lastfm-clean-end-lines-preview_6a10cf2f21d6a44564ceb3de6a342bbc'}\n::: {.cell-output-display}\n\\begin{tabular}{llll}\n\\toprule\nartist & song & lyrics & genre\\\\\n\\midrule\nAlan Jackson & Little Bitty & Have a little love on a little honeymoon You got a little dish and you got a little spoon A little bitty house and a little bitty yard A little bitty dog and a little bitty car Well, it's alright t... & country\\\\\n50 Cent & In Da Club & Go, go, go, go, go, go Go, shorty It's your birthday We gon' party like it's your birthday We gon' sip Bacardi like it's your birthday And you know we don't give a fuck it's not your birthday You c... & hip-hop\\\\\nBlack Sabbath & Paranoid & Finished with my woman' Cause she couldn't help me with my mind People think I'm insane Because I am frowning all the time All day long, I think of things But nothing seems to satisfy Think I'll lo... & metal\\\\\na-ha & Take On Me & Talking away I don't know what What to say I'll say it anyway Today is another day to find you Shying away Oh, I'll be coming for your love, okay? Take On Me  Take me on  I'll be gone In a day or t... & pop\\\\\n3 Doors Down & Here Without You & A hundred days have made me older Since the last time that I saw your pretty face A thousand lies have made me colder And I don't think I can look at this the same But all the miles that separate D... & rock\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nNow given the fact that songs are poems, there are many lines that are not complete sentences so there is no practical way to try to segment these into grammatical sentence units. So in this case, this seems like a good stopping point for normalizing the lastfm dataset.\n\n## Recode {#td-recode}\n\n<!-- Consider:\n\nThe process of recoding aims to _recast_ the values of a variable or set of variables to a new variable or set of variables to enable more direct access. This may include extracting values from a variable, stemming or lemmatization of words, tokenization of linguistic forms (words, ngrams, sentences, etc.), calculating the lengths of linguistic units, calculating type-token ratios, syntactic complexity, and/ or removing variables that will not be used in the analysis, etc.\n-->\n\nNormalizing text can be seen as an extension of dataset curation to some extent in that the structure of the dataset is maintained. In both the Europarle and Lastfm cases we saw this to be true. In the case of recoding, and other transformational steps, the aim will be to modify the dataset structure either by rows, columns, or both. Recoding processes can be characterized by the creation of structural changes which are derived from values in variables effectively recasting values as new variables to enable more direct access in our analyses.\n\n**Switchboard Dialogue Act Corpus**\n\nThe Switchboard Dialogue Act Corpus dataset that was curated in the previous chapter contains a number of variables describing conversations between speakers of American English. \n\nLet's read in this dataset and take a closer look.\n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-sdac-read-show_c3a8be342d9ac00780acdbcbc4edc20a'}\n\n```{.r .cell-code}\nsdac <- read_csv(file = \"../data/derived/sdac/sdac_curated.csv\") # read curated dataset\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-sdac-read-run_a2664455dbaa84bef297377dda34afe7'}\n\n:::\n\n\nAmong a number of metadata variables, curated dataset includes the `utterance_text` column which contains dialogue from the conversations interleaved with a [disfluency annotation scheme](https://staff.fnwi.uva.nl/r.fernandezrovira/teaching/DM-materials/DFL-book.pdf). \n\n\n::: {#tbl-td-sdac-preview-curated-dataset .cell layout-align=\"center\" tbl-cap='20 randomly sampled lines of the SDAC curated dataset.' hash='transform-datasets_cache/pdf/tbl-td-sdac-preview-curated-dataset_834a038365f628499195fd20c8bf174a'}\n::: {.cell-output-display}\n\\begin{tabular}{rllrrlr}\n\\toprule\ndoc\\_id & damsl\\_tag & speaker & turn\\_num & utterance\\_num & utterance\\_text & speaker\\_id\\\\\n\\midrule\n3697 & sd & B & 74 & 2 & {}[ I'm, + I'm ] planning on it.  / & 1424\\\\\n3813 & qh & A & 29 & 3 & all I can think of is if you don't keep [ a real, + a real ] tight budget, how do you control expenses,  / & 1461\\\\\n3214 & sv & B & 48 & 2 & \\{C and \\} you never read it again -- & 1352\\\\\n2967 & b & B & 46 & 1 & Yeah.  / & 1072\\\\\n4733 & sv & A & 73 & 1 & \\{D You know, \\} I just feel they do better that way. / & 1437\\\\\n\\addlinespace\n3052 & sd(\\textasciicircum{}q) & A & 44 & 2 & \\{C and \\} the wife said \"these are not  his slacks\"  / & 1285\\\\\n2537 & sv & B & 62 & 6 & {}[ [ that, +  the, ] +  the ] absolute refusal to accept the possibility [ of, +  of ] mistakes on the testing, & 1142\\\\\n3121 & \\% & B & 116 & 2 & \\{C and, \\} -/ & 1318\\\\\n2324 & sd & B & 22 & 5 & \\{C and \\} it'll convert every <noise> measurement on there -- & 1138\\\\\n3750 & sd & B & 35 & 2 & \\{C and, \\} \\{F uh, \\}  in the afternoon, \\{D you know, \\} it is still four o'clock.  / & 1051\\\\\n\\addlinespace\n4723 & b & B & 54 & 1 & Right. / & 1611\\\\\n2602 & aa & B & 2 & 1 & Okay,  / & 1122\\\\\n2441 & + & A & 141 & 1 & -- \\{D you know, \\}  / & 1151\\\\\n2372 & b & B & 106 & 1 & Huh-uh. / & 1135\\\\\n3080 & bh & B & 100 & 1 & \\{F Oh, \\} is that right. / & 1095\\\\\n\\addlinespace\n3777 & sv & A & 101 & 3 & \\{D well, \\} a couple of the, \\{F uh, \\} ones Disney's doing aren't too bad  / & 1477\\\\\n2064 & b & A & 69 & 1 & Yeah. / & 1148\\\\\n2515 & b & B & 48 & 1 & Okay. / & 1035\\\\\n4032 & b & A & 55 & 1 & Uh-huh. / & 1514\\\\\n2691 & + & A & 99 & 1 & \\# and \\# had their house on the market down there,  / & 1233\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nLet's drop a few variables from our dataset to rein in our focus. I will keep the `doc_id`, `speaker_id`, and `utterance_text`. \n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-sdac-simplified_2e65eecede7bbb9076811d7b9e2ce539'}\n\n```{.r .cell-code}\nsdac_simplified <- \n  sdac |> # dataset\n  select(doc_id, speaker_id, utterance_text) # columns to retain\n```\n:::\n\n::: {#tbl-td-sdac-simple-preview .cell layout-align=\"center\" tbl-cap='First 10 lines of the simplified SDAC curated dataset.' hash='transform-datasets_cache/pdf/tbl-td-sdac-simple-preview_9cf7cd3311b9c6f3adf06f596a95f7fe'}\n::: {.cell-output-display}\n\\begin{tabular}{rrl}\n\\toprule\ndoc\\_id & speaker\\_id & utterance\\_text\\\\\n\\midrule\n4325 & 1632 & Okay.  /\\\\\n4325 & 1632 & \\{D So, \\}\\\\\n4325 & 1519 & {}[ [ I guess, +\\\\\n4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? /\\\\\n4325 & 1519 & I think, ] + \\{F uh, \\} I wonder ] if that worked. /\\\\\n\\addlinespace\n4325 & 1632 & Does it say something? /\\\\\n4325 & 1519 & I think it usually does.  /\\\\\n4325 & 1519 & You might try, \\{F uh, \\}  /\\\\\n4325 & 1519 & I don't know,  /\\\\\n4325 & 1519 & hold it down a little longer,  /\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nIn this disfluency annotation system, there are various conventions used for non-sentence elements. If say, for example, a researcher were to be interested in understanding the use of filled pauses ('uh' or 'uh'), the aim would be to identify those lines where the `{F ...}` annotation is used around the utterances 'uh' and 'um'.  \n\nTo do this we turn to the `str_count()` function. This function will count the number of matches found for a pattern. We can use a regular expression to identify the pattern of interest which is all the instances of `{F` followed by either `uh` or `um`. Since the disfluencies may start an utterance, and therefore be capitalized we need to formulate a regular expression which allows for either `U` or `u` for each disfluency type. The result from each disfluency match will be added to a new column. To create a new column we will wrap each `str_count()` with `mutate()` and give the new column a meaningful name. In this case I've opted for `uh` and `um`. \n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-sdac-count-disfluencies_c472963f6b827478e6ea185c8f9a8ddb'}\n\n```{.r .cell-code}\nsdac_disfluencies <- \n  sdac_simplified |> # dataset\n  mutate(uh = str_count(utterance_text, \"\\\\{F [Uu]h\")) |> # match {F Uh or {F uh}\n  mutate(um = str_count(utterance_text, \"\\\\{F [Uu]m\")) # match {F Um or {F um}\n```\n:::\n\n::: {#tbl-td-sdac-count-disfluencies-show .cell layout-align=\"center\" tbl-cap='First 20 lines of SDAC dataset with counts for the disfluencies \\'uh\\' and \\'um\\'.' hash='transform-datasets_cache/pdf/tbl-td-sdac-count-disfluencies-show_f2497a918f6aa0ec0c86f9c7e99377e2'}\n::: {.cell-output-display}\n\\begin{tabular}{rrlrr}\n\\toprule\ndoc\\_id & speaker\\_id & utterance\\_text & uh & um\\\\\n\\midrule\n4325 & 1632 & Okay.  / & 0 & 0\\\\\n4325 & 1632 & \\{D So, \\} & 0 & 0\\\\\n4325 & 1519 & {}[ [ I guess, + & 0 & 0\\\\\n4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? / & 0 & 0\\\\\n4325 & 1519 & I think, ] + \\{F uh, \\} I wonder ] if that worked. / & 1 & 0\\\\\n\\addlinespace\n4325 & 1632 & Does it say something? / & 0 & 0\\\\\n4325 & 1519 & I think it usually does.  / & 0 & 0\\\\\n4325 & 1519 & You might try, \\{F uh, \\}  / & 1 & 0\\\\\n4325 & 1519 & I don't know,  / & 0 & 0\\\\\n4325 & 1519 & hold it down a little longer,  / & 0 & 0\\\\\n\\addlinespace\n4325 & 1519 & \\{C and \\} see if it, \\{F uh, \\} -/ & 1 & 0\\\\\n4325 & 1632 & Okay <beep>.  / & 0 & 0\\\\\n4325 & 1632 & <<long pause>> \\{D Well, \\} & 0 & 0\\\\\n4325 & 1519 & Okay  / & 0 & 0\\\\\n4325 & 1519 & {}[ I, + & 0 & 0\\\\\n\\addlinespace\n4325 & 1632 & Does it usually make a recording or s-, / & 0 & 0\\\\\n4325 & 1519 & \\{D Well, \\} I ] don't remember.  / & 0 & 0\\\\\n4325 & 1519 & It seemed like it did,  / & 0 & 0\\\\\n4325 & 1519 & \\{C but \\} <laughter> it might not.  / & 0 & 0\\\\\n4325 & 1519 & {}[ I guess + -- & 0 & 0\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nNow we have two new columns, `uh` and `um` which indicate how many times the relevant pattern was matched for a given utterance. By choosing to focus on disfluencies, however, we have made a decision to change the unit of observation from the utterance to the use of filled pauses (`uh` and `um`). This means that as the dataset stands, it is not in tidy format --where each observation corresponds to the observational unit. When datasets are misaligned in this particular way, there are in what is known as 'wide' format. What we want to do, then, is to restructure our dataset such that each row corresponds to the unit of observation --in this case each filled pause type. \n\nTo convert our current (wide) dataset to one where each filler type is listed and the counts are measured for each utterance we turn to the `pivot_longer()` function. This function creates two new columns, one in which the column names are listed and one for the values for each of the column names.  \n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-sdac-disfluencies-longer_83c8d692183af21aac0900d2e9194aa3'}\n\n```{.r .cell-code}\nsdac_disfluencies <- \n  sdac_disfluencies |> # dataset\n  pivot_longer(cols = c(\"uh\", \"um\"), # columns to convert\n               names_to = \"filler\", # column for the column names (i.e. filler types)\n               values_to = \"count\") # column for the column values (i.e. counts)\n```\n:::\n\n::: {#tbl-td-sdac-count-disfluencies-longer-show .cell layout-align=\"center\" tbl-cap='First 20 lines of SDAC dataset with tidy format for `fillers` as the unit of observation.' hash='transform-datasets_cache/pdf/tbl-td-sdac-count-disfluencies-longer-show_69a52177adfce4824eb66539d0255c0d'}\n::: {.cell-output-display}\n\\begin{tabular}{rrllr}\n\\toprule\ndoc\\_id & speaker\\_id & utterance\\_text & filler & count\\\\\n\\midrule\n4325 & 1632 & Okay.  / & uh & 0\\\\\n4325 & 1632 & Okay.  / & um & 0\\\\\n4325 & 1632 & \\{D So, \\} & uh & 0\\\\\n4325 & 1632 & \\{D So, \\} & um & 0\\\\\n4325 & 1519 & {}[ [ I guess, + & uh & 0\\\\\n\\addlinespace\n4325 & 1519 & {}[ [ I guess, + & um & 0\\\\\n4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? / & uh & 0\\\\\n4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? / & um & 0\\\\\n4325 & 1519 & I think, ] + \\{F uh, \\} I wonder ] if that worked. / & uh & 1\\\\\n4325 & 1519 & I think, ] + \\{F uh, \\} I wonder ] if that worked. / & um & 0\\\\\n\\addlinespace\n4325 & 1632 & Does it say something? / & uh & 0\\\\\n4325 & 1632 & Does it say something? / & um & 0\\\\\n4325 & 1519 & I think it usually does.  / & uh & 0\\\\\n4325 & 1519 & I think it usually does.  / & um & 0\\\\\n4325 & 1519 & You might try, \\{F uh, \\}  / & uh & 1\\\\\n\\addlinespace\n4325 & 1519 & You might try, \\{F uh, \\}  / & um & 0\\\\\n4325 & 1519 & I don't know,  / & uh & 0\\\\\n4325 & 1519 & I don't know,  / & um & 0\\\\\n4325 & 1519 & hold it down a little longer,  / & uh & 0\\\\\n4325 & 1519 & hold it down a little longer,  / & um & 0\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\n**Last fm**\n\n<!-- tokenization (`unnest_tokens`, words/ n-grams) -->\n\nIn the previous example, we used a matching approach to extract information embedded in one column of the dataset and recoded the dataset to maintain the fidelity between the particular unit of observation and the other metadata.\n\nAnother common approach for recoding datasets in text analysis projects involves recoding linguistic units as smaller units; a process known as tokenization. \n\nLet's return to the `lastfm` object we normalized earlier in the chapter to see the various ways one can choose to tokenize linguistic information. \n\n\n::: {#tbl-td-lastfm-clean-end-lines-preview-2 .cell layout-align=\"center\" tbl-cap='Last fm dataset with normalized lyrics.' hash='transform-datasets_cache/pdf/tbl-td-lastfm-clean-end-lines-preview-2_c21ca9bd93e1e72798cc71c57f509446'}\n::: {.cell-output-display}\n\\begin{tabular}{llll}\n\\toprule\nartist & song & lyrics & genre\\\\\n\\midrule\nAlan Jackson & Little Bitty & Have a little love on a little honeymoon You got a little dish and you got a little spoon A little bitty house and a little bitty yard A little bitty dog and a little bitty car Well, it's alright t... & country\\\\\n50 Cent & In Da Club & Go, go, go, go, go, go Go, shorty It's your birthday We gon' party like it's your birthday We gon' sip Bacardi like it's your birthday And you know we don't give a fuck it's not your birthday You c... & hip-hop\\\\\nBlack Sabbath & Paranoid & Finished with my woman' Cause she couldn't help me with my mind People think I'm insane Because I am frowning all the time All day long, I think of things But nothing seems to satisfy Think I'll lo... & metal\\\\\na-ha & Take On Me & Talking away I don't know what What to say I'll say it anyway Today is another day to find you Shying away Oh, I'll be coming for your love, okay? Take On Me  Take me on  I'll be gone In a day or t... & pop\\\\\n3 Doors Down & Here Without You & A hundred days have made me older Since the last time that I saw your pretty face A thousand lies have made me colder And I don't think I can look at this the same But all the miles that separate D... & rock\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nIn the current `lastfm` dataset, the unit of observation is the lyrics for the entire artist, song, and genre combination. If, however, we would like to change the unit to say words, we would like each word used to appear on its own row, while still maintaining the other relevant attributes associated with each word. \n\nThe tidytext package includes a very useful function `unnest_tokens()` which allows us to tokenize some textual input into smaller linguistic units. The 'unnest' part of the the function name refers to the process of extracting the unit of interest while maintaining the other relevant attributes. Let's see this in action. \n\n\n::: {#tbl-td-lastfm-tokenize-words .cell layout-align=\"center\" tbl-cap='First 10 observations for lastfm dataset tokenized by words.' hash='transform-datasets_cache/pdf/tbl-td-lastfm-tokenize-words_6156098b99b55ed5a0712a76e04cb481'}\n\n```{.r .cell-code}\nlastfm |> # dataset\n  unnest_tokens(output = word, # column for tokenized output\n                input = lyrics, # input column\n                token = \"words\") |> # tokenize unit type\n  slice_head(n = 10) |>  # preview first 10 lines\n  kable(booktabs = TRUE)\n```\n\n::: {.cell-output-display}\n\\begin{tabular}{llll}\n\\toprule\nartist & song & genre & word\\\\\n\\midrule\nAlan Jackson & Little Bitty & country & have\\\\\nAlan Jackson & Little Bitty & country & a\\\\\nAlan Jackson & Little Bitty & country & little\\\\\nAlan Jackson & Little Bitty & country & love\\\\\nAlan Jackson & Little Bitty & country & on\\\\\n\\addlinespace\nAlan Jackson & Little Bitty & country & a\\\\\nAlan Jackson & Little Bitty & country & little\\\\\nAlan Jackson & Little Bitty & country & honeymoon\\\\\nAlan Jackson & Little Bitty & country & you\\\\\nAlan Jackson & Little Bitty & country & got\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nWe can see from the output, each word appears on a separate line in the order of appearance in the input text (`lyrics`). Furthermore, the output is in tidy format as each of the words is still associated with the relevant attribute values (`artist`, `song`, and `genre`). By default the tokenized text output is lowercased and the original text input column is dropped. These can be overridden, however, if desired.\n\nIn addition to 'words', the `unnest_tokens()` function provides easy access to a number of common tokenized units including 'characters', 'sentences', and 'paragraphs'. \n\n\n::: {#tbl-td-lastfm-tokenize-characters .cell layout-align=\"center\" tbl-cap='First 10 observations for lastfm dataset tokenized by characters.' hash='transform-datasets_cache/pdf/tbl-td-lastfm-tokenize-characters_e0e6f2bb7fbde1637de893996eca1311'}\n\n```{.r .cell-code}\nlastfm |> # dataset\n  unnest_tokens(output = character, # column for tokenized output\n                input = lyrics, # input column\n                token = \"characters\") |> # tokenize unit type\n  slice_head(n = 10) |>  # preview first 10 lines\n  kable(booktabs = TRUE)\n```\n\n::: {.cell-output-display}\n\\begin{tabular}{llll}\n\\toprule\nartist & song & genre & character\\\\\n\\midrule\nAlan Jackson & Little Bitty & country & h\\\\\nAlan Jackson & Little Bitty & country & a\\\\\nAlan Jackson & Little Bitty & country & v\\\\\nAlan Jackson & Little Bitty & country & e\\\\\nAlan Jackson & Little Bitty & country & a\\\\\n\\addlinespace\nAlan Jackson & Little Bitty & country & l\\\\\nAlan Jackson & Little Bitty & country & i\\\\\nAlan Jackson & Little Bitty & country & t\\\\\nAlan Jackson & Little Bitty & country & t\\\\\nAlan Jackson & Little Bitty & country & l\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nThe other two built-in options 'sentences' and 'paragraphs' depend on punctuation and/ or line breaks to function, so in this particular dataset, these options will not work given the particular characteristics of the `lyrics` variable. \n\nThere are even other options which allow for the creation of sequences of linguistic units. Say we want to tokenize our lyrics into two-word sequences, we can specify the `token` as 'ngrams' and then add the argument `n = 2` to reflect we want two-word sequences. \n\n\n::: {#tbl-td-lastfm-tokenize-bigrams .cell layout-align=\"center\" tbl-cap='First 10 observations for lastfm dataset tokenized by bigrams' hash='transform-datasets_cache/pdf/tbl-td-lastfm-tokenize-bigrams_9af737b7fed64b81e12d049c21bf2893'}\n\n```{.r .cell-code}\nlastfm |> \n  unnest_tokens(output = bigram, # column for tokenized output\n                input = lyrics, # input column\n                token = \"ngrams\", # tokenize unit type\n                n = 2) |>  # size of word sequences \n  slice_head(n = 10) |>  # preview first 10 lines\n  kable(booktabs = TRUE)\n```\n\n::: {.cell-output-display}\n\\begin{tabular}{llll}\n\\toprule\nartist & song & genre & bigram\\\\\n\\midrule\nAlan Jackson & Little Bitty & country & have a\\\\\nAlan Jackson & Little Bitty & country & a little\\\\\nAlan Jackson & Little Bitty & country & little love\\\\\nAlan Jackson & Little Bitty & country & love on\\\\\nAlan Jackson & Little Bitty & country & on a\\\\\n\\addlinespace\nAlan Jackson & Little Bitty & country & a little\\\\\nAlan Jackson & Little Bitty & country & little honeymoon\\\\\nAlan Jackson & Little Bitty & country & honeymoon you\\\\\nAlan Jackson & Little Bitty & country & you got\\\\\nAlan Jackson & Little Bitty & country & got a\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nThe 'n' in 'ngram' refers to the number of word-sequence units we want to tokenize. Two-word sequences are known as 'bigrams', three-word sequences 'trigrams', and so on.\n\n## Generate\n\nIn the process of recoding a dataset the transformation of the dataset works with information that is already explicit. The process of generation, however, aims to make implicit information explicit. The most common type of operation involved in the generation process is the addition of linguistic annotation. This process can be accomplished manually by a researcher or research team or automatically through the use of pre-trained linguistic resources and/ or software. Ideally the annotation of linguistic information can be conducted automatically. \n\nThere are important considerations, however, that need to be taken into account when choosing whether linguistic annotation can be conducted automatically. First and foremost has to do with the type of annotation desired. Information such as part of speech (grammatical category) and morpho-syntactic information are the the most common types of linguistic annotation that can be conducted automatically. Second the degree to which the resource that will be used to annotate the linguistic information is aligned with the language variety and/or register is also a key consideration. As noted, automatic linguistic annotation methods are contingent on pre-trained resources. The language and language variety used to develop these resources may not be available for the language under investigation, or if it does, the language variety and/ or register may not align. The degree to which a resource does not align with the linguistic information targeted for annotation is directly related to the quality of the final annotations. To be clear, no annotation method, whether manual or automatic is guaranteed to be perfectly accurate. \n\nLet's take a look at annotation some of the language from the Europarle dataset we normalized. \n\n\n::: {#tbl-td-europarle-en-preview .cell layout-align=\"center\" tbl-cap='First 10 lines in English from the normalized SDAC dataset.' hash='transform-datasets_cache/pdf/tbl-td-europarle-en-preview_61b9225cd986973d94de6200382fd17e'}\n\n```{.r .cell-code}\neuroparle |> \n  filter(type == \"Target\") |> \n  slice_head(n = 10) |> \n  kable(booktabs = TRUE)\n```\n\n::: {.cell-output-display}\n\\begin{tabular}{lrl}\n\\toprule\ntype & sentence\\_id & sentence\\\\\n\\midrule\nTarget & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\\\\nTarget & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\\\\nTarget & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\\\\nTarget & 5 & In the meantime, I should like to observe a minute's silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\\\\nTarget & 6 & Please rise, then, for this minute's silence.\\\\\n\\addlinespace\nTarget & 8 & Madam President, on a point of order.\\\\\nTarget & 9 & You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\\\\\nTarget & 10 & One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\\\\\nTarget & 11 & Would it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament's regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\\\\\nTarget & 12 & Yes, Mr Evans, I feel an initiative of the type you have just suggested would be entirely appropriate.\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nWe will use the cleanNLP package to do our linguistic annotation. The annotation process depends on the pre-trained language models. There is [a list of the models available to access](https://github.com/bnosac/udpipe#pre-trained-models). The `load_model_udpipe()` custom function below downloads the specified language model and initialized the `udpipe` engine (`cnlp_init_udpipe()`) for conducting annotations. \n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-functions-load-udpipe-model_25981e0ecdbca62bbc76406932118f41'}\n\n```{.r .cell-code}\nload_model_udpipe <- function(model_lang) {\n  # Function\n  # Download and load the specified udpipe language model\n  \n  cnlp_init_udpipe(model_lang) # to download the model, if not downloaded\nbase_path <- system.file(\"extdata\", package = \"cleanNLP\") # get the base path\n  model_name <- # extract the model_name\n    base_path |> # extract the base path\n    dir() |> # get the directory\n    stringr::str_subset(pattern = paste0(\"^\", model_lang)) # extract the name of the model\n  \n  model_path <- udpipe::udpipe_load_model(file = file.path(base_path, model_name, fsep = \"/\")) # create the path to the downloaded model stored on disk\n    return(model_path)\n}\n```\n:::\n\n\n\nIn a test case, let's load the 'english' model to annotate a sentence line from the Europarle dataset to illustrate the basic workflow. \n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-generation-europarle-en-example_c8354c9204785d5091de6b63d0a65fde'}\n\n```{.r .cell-code}\neng_model <- load_model_udpipe(\"english\") # load and initialize the language model, 'english' in this case.\n\neng_annotation <- \n  europarle |> # dataset \n  filter(type == \"Target\" & sentence_id == 6) |> # select English and sentence_id 6\n  cnlp_annotate(text_name = \"sentence\", # input text (sentence)\n                doc_name = \"sentence_id\") # specify the grouping column (sentence_id)\n\nglimpse(eng_annotation) # preview structure\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> List of 2\n#>  $ token   : tibble [11 x 11] (S3: tbl_df/tbl/data.frame)\n#>   ..$ doc_id       : num [1:11] 6 6 6 6 6 6 6 6 6 6 ...\n#>   ..$ sid          : int [1:11] 1 1 1 1 1 1 1 1 1 1 ...\n#>   ..$ tid          : chr [1:11] \"1\" \"2\" \"3\" \"4\" ...\n#>   ..$ token        : chr [1:11] \"Please\" \"rise\" \",\" \"then\" ...\n#>   ..$ token_with_ws: chr [1:11] \"Please \" \"rise\" \", \" \"then\" ...\n#>   ..$ lemma        : chr [1:11] \"please\" \"rise\" \",\" \"then\" ...\n#>   ..$ upos         : chr [1:11] \"INTJ\" \"VERB\" \"PUNCT\" \"ADV\" ...\n#>   ..$ xpos         : chr [1:11] \"UH\" \"VB\" \",\" \"RB\" ...\n#>   ..$ feats        : chr [1:11] NA \"Mood=Imp|VerbForm=Fin\" NA \"PronType=Dem\" ...\n#>   ..$ tid_source   : chr [1:11] \"2\" \"0\" \"2\" \"10\" ...\n#>   ..$ relation     : chr [1:11] \"discourse\" \"root\" \"punct\" \"advmod\" ...\n#>  $ document: tibble [1 x 2] (S3: tbl_df/tbl/data.frame)\n#>   ..$ type  : chr \"Target\"\n#>   ..$ doc_id: num 6\n#>  - attr(*, \"class\")= chr [1:2] \"cnlp_annotation\" \"list\"\n```\n:::\n:::\n\n\nWe see that the structure returned by the `cnlp_annotate()` function is a list. This list contains two data frames (tibbles). One for the tokens (and there annotation information) and the document (the metadata information). We can inspect the annotation characteristics for this one sentence by targetting the `$tokens` data frame. Let's take a look at the linguistic annotation information returned. \n\n\n::: {#tbl-td-generation-test-annotation-english .cell layout-align=\"center\" tbl-cap='Annotation information for a single English sentence from the Europarle dataset.' hash='transform-datasets_cache/pdf/tbl-td-generation-test-annotation-english_64d954ec6967f479badc9633cb7041b8'}\n::: {.cell-output-display}\n\\begin{tabular}{rrlllllllll}\n\\toprule\ndoc\\_id & sid & tid & token & token\\_with\\_ws & lemma & upos & xpos & feats & tid\\_source & relation\\\\\n\\midrule\n6 & 1 & 1 & Please & Please & please & INTJ & UH & NA & 2 & discourse\\\\\n6 & 1 & 2 & rise & rise & rise & VERB & VB & Mood=Imp|VerbForm=Fin & 0 & root\\\\\n6 & 1 & 3 & , & , & , & PUNCT & , & NA & 2 & punct\\\\\n6 & 1 & 4 & then & then & then & ADV & RB & PronType=Dem & 10 & advmod\\\\\n6 & 1 & 5 & , & , & , & PUNCT & , & NA & 10 & punct\\\\\n\\addlinespace\n6 & 1 & 6 & for & for & for & ADP & IN & NA & 10 & case\\\\\n6 & 1 & 7 & this & this & this & DET & DT & Number=Sing|PronType=Dem & 8 & det\\\\\n6 & 1 & 8 & minute & minute & minute & NOUN & NN & Number=Sing & 10 & nmod:poss\\\\\n6 & 1 & 9 & 's & 's & 's & PART & POS & NA & 8 & case\\\\\n6 & 1 & 10 & silence & silence & silence & NOUN & NN & Number=Sing & 2 & conj\\\\\n\\addlinespace\n6 & 1 & 11 & . & . & . & PUNCT & . & NA & 2 & punct\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nThere is quite a bit of information which is returned from `cnlp_annotate()`. First note that the input sentence has been tokenized by word. Each token includes the token, lemma, part of speech (`upos` and `xpos`), morphological features (`feats`), and syntactic relationships (`tid_source` and `relation`). It is also key to note that the `doc_id`, `sid` and `tid` maintain the relational attributes from the original dataset --and therefore maintains our annotated dataset in tidy format.\n\nLet's now annotate the same sentence from the Europarle corpus for the Source ('Spanish') and note the similarities and differences.\n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-generation-europarle-es-example_abc6c42d71e53c4fab15121921df29ed'}\n\n```{.r .cell-code}\nspa_model <- load_model_udpipe(\"spanish\") # load and initialize the language model, 'spanish' in this case.\n\nspa_annotation <- \n  europarle |> # dataset \n  filter(type == \"Source\" & sentence_id == 6) |> # select Spanish and sentence_id 6\n  cnlp_annotate(text_name = \"sentence\", # input text (sentence)\n                doc_name = \"sentence_id\") # specify the grouping column (sentence_id)\n```\n:::\n\n::: {#tbl-td-generation-test-annotation-spanish .cell layout-align=\"center\" tbl-cap='Annotation information for a single Spanish sentence from the Europarle dataset.' hash='transform-datasets_cache/pdf/tbl-td-generation-test-annotation-spanish_f0b71d220175b43aa86959eee8c5bbe4'}\n::: {.cell-output-display}\n\\begin{tabular}{rrlllllllll}\n\\toprule\ndoc\\_id & sid & tid & token & token\\_with\\_ws & lemma & upos & xpos & feats & tid\\_source & relation\\\\\n\\midrule\n6 & 1 & 1 & Invito & Invito & Invito & VERB & NA & Gender=Masc|Number=Sing|VerbForm=Fin & 0 & root\\\\\n6 & 1 & 2 & a & a & a & ADP & NA & NA & 3 & case\\\\\n6 & 1 & 3 & todos & todos & todo & PRON & NA & Gender=Masc|Number=Plur|PronType=Tot & 1 & obj\\\\\n6 & 1 & 4 & a & a & a & ADP & NA & NA & 7 & mark\\\\\n6 & 1 & 5 & que & que & que & SCONJ & NA & NA & 4 & fixed\\\\\n\\addlinespace\n6 & 1 & 6 & nos & nos & yo & PRON & NA & Case=Acc,Dat|Number=Plur|Person=1|PrepCase=Npr|PronType=Prs|Reflex=Yes & 7 & iobj\\\\\n6 & 1 & 7 & pongamos & pongamos & pongar & VERB & NA & Mood=Ind|Number=Plur|Person=1|Tense=Pres|VerbForm=Fin & 1 & advcl\\\\\n6 & 1 & 8 & de & de & de & ADP & NA & NA & 9 & case\\\\\n6 & 1 & 9 & pie & pie & pie & NOUN & NA & Gender=Masc|Number=Sing & 7 & obl\\\\\n6 & 1 & 10 & para & para & para & ADP & NA & NA & 11 & mark\\\\\n\\addlinespace\n6 & 1 & 11 & guardar & guardar & guardar & VERB & NA & VerbForm=Inf & 1 & advcl\\\\\n6 & 1 & 12 & un & un & uno & DET & NA & Definite=Ind|Gender=Masc|Number=Sing|PronType=Art & 13 & det\\\\\n6 & 1 & 13 & minuto & minuto & minuto & NOUN & NA & Gender=Masc|Number=Sing & 11 & obj\\\\\n6 & 1 & 14 & de & de & de & ADP & NA & NA & 15 & case\\\\\n6 & 1 & 15 & silencio & silencio & silencio & NOUN & NA & Gender=Masc|Number=Sing & 13 & nmod\\\\\n\\addlinespace\n6 & 1 & 16 & . & . & . & PUNCT & NA & NA & 1 & punct\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nFor the Spanish version of this sentence, we see the same variables. However, the `feats` variable has morphological information which is specific to Spanish --notably gender and mood. \n\n::: callout-warning\n## Tip\nThe rsyntax package [@R-rsyntax] can be used to recode and extract patterns from the output from automatic linguistic annotations using cleanNLP. [See the documentation for more information](https://github.com/vanatteveldt/rsyntax).\n:::\n\n<!-- Consider:\n\n\n- Europarle Corpus, syntactic annotation\n\n- Syntactic parsing for last.fm music lyrics, phrasal verbs (compound:prt) a la [@Akbary2018], with automated approach\n- SOTU Corpus, frequency weighting and scaling for clustering algorithm?\n- ...\n\n\nThe process of generation aims to _augment_ a variable or set of variables. In essence this aims to make implicit attributes explicit to that they are directly accessible. This often targeted at the automatic generation of linguistic annotations such as grammatical category (part-of-speech) or syntactic structure. \n\n\n-->\n\n## Merge\n\n<!--\nConsider: \n\n- MRC word information to merge\n- English Lexicon project RT times: https://elexicon.wustl.edu/index.html\n\n- Change dataset: \n  - SOTU\n  - Written (Brown? Other better matched for register/ time-period) vs. Spoken (Switchboard)\n-->\n\nOne final class of transformations that can be applied to curated datasets to enhance their informativeness for a research project is the process of merging two or more datasets. To merge datasets it is required that the datasets share one or more attributes. With a common attribute two datasets can be joined to coordinate the attributes of one dataset with the other effectively adding attributes and one dataset with extended information. Another approach is to join datasets with the goal of filtering one of the datasets given the matching attribute. \n\nLet's see this in practice. Take the `lastfm` dataset. Let's tokenize the dataset into words, using `unnest_tokens()` such that our unit of observation is words. \n\n\n::: {#tbl-td-lastfm-tokens .cell layout-align=\"center\" tbl-cap='First 10 observations for `lastfm_words` dataset.' hash='transform-datasets_cache/pdf/tbl-td-lastfm-tokens_2803c7e530bac7ae9608292ac1a18c39'}\n\n```{.r .cell-code}\nlastfm_words <- \n  lastfm |> # dataset\n  unnest_tokens(output = \"word\", # output column\n                input = \"lyrics\", # input column\n                token = \"words\") # tokenized unit (words)\n\nlastfm_words |> # dataset\n  slice_head(n = 10) |> # first 10 observations\n  kable(booktabs = TRUE)\n```\n\n::: {.cell-output-display}\n\\begin{tabular}{llll}\n\\toprule\nartist & song & genre & word\\\\\n\\midrule\nAlan Jackson & Little Bitty & country & have\\\\\nAlan Jackson & Little Bitty & country & a\\\\\nAlan Jackson & Little Bitty & country & little\\\\\nAlan Jackson & Little Bitty & country & love\\\\\nAlan Jackson & Little Bitty & country & on\\\\\n\\addlinespace\nAlan Jackson & Little Bitty & country & a\\\\\nAlan Jackson & Little Bitty & country & little\\\\\nAlan Jackson & Little Bitty & country & honeymoon\\\\\nAlan Jackson & Little Bitty & country & you\\\\\nAlan Jackson & Little Bitty & country & got\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nConsider the `get_sentiments()` function which returns words which have been classified as 'positive'- or 'negative'-biased, if the lexicon is set to 'bing' [@Hu2004].\n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-get-sentiment-lexicon_0287b567a49e613f2a0f9ffc19536600'}\n\n```{.r .cell-code}\nsentiments_bing <- \n  tidytext::get_sentiments(lexicon = \"bing\") # get 'bing' lexicon from get_sentiments\n\nsentiments_bing |> \n  slice_head(n = 10) # preview first 10 observations\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 10 x 2\n#>    word        sentiment\n#>    <chr>       <chr>    \n#>  1 2-faces     negative \n#>  2 abnormal    negative \n#>  3 abolish     negative \n#>  4 abominable  negative \n#>  5 abominably  negative \n#>  6 abominate   negative \n#>  7 abomination negative \n#>  8 abort       negative \n#>  9 aborted     negative \n#> 10 aborts      negative\n```\n:::\n:::\n\n\nSince the `sentiments_bing` dataset and the `lastfm_words` dataset both share a column `word` (which has the same type of values) we can join these two datasets. The `sentiments_bing` dataset has 6786 unique words. Let's check how many distinct words our `lastfm_words` dataset has. \n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-view-distinct-words_6d07bcf89c068f5d13c9bd51f969d40a'}\n\n```{.r .cell-code}\nlastfm_words |> # dataset\n  distinct(word) |> # find unique words\n  nrow() # count distinct rows/ words\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 4614\n```\n:::\n:::\n\n\nOne thing to note is that the `sentiments_bing` dataset does not include function words, that is words that are associated with closed-class categories (pronouns, determiners, prepositions, etc.) as these words do not have semantic content along the lines of positive and negative. So many of the words that appear in the `lastfm_words` will not be matched. Other thing to note is that the `sentiments_bing` lexicon will undoubtly have words that do not appear in the `lastfm_words` and vice versa. \n\nIf we want to keep all the words in the `lastfm_words` and add the sentiment information for those words that do match in both datasets, we can use the `left_join()` function. `lastfm_words` will be the dataset on the 'left' and therefore all rows in this dataset will be retained.\n\n\n::: {#tbl-td-lastfm-words-bing-left-joing .cell layout-align=\"center\" tbl-cap='First 10 observations for the `lastfm_words` sentiments_bing` left join.' hash='transform-datasets_cache/pdf/tbl-td-lastfm-words-bing-left-joing_8c92cab131ed2a44974d80e761e37a25'}\n\n```{.r .cell-code}\nleft_join(lastfm_words, sentiments_bing) |> \n  slice_head(n = 10) |> # first 10 observations\n  kable(booktabs = TRUE)\n```\n\n::: {.cell-output-display}\n\\begin{tabular}{lllll}\n\\toprule\nartist & song & genre & word & sentiment\\\\\n\\midrule\nAlan Jackson & Little Bitty & country & have & NA\\\\\nAlan Jackson & Little Bitty & country & a & NA\\\\\nAlan Jackson & Little Bitty & country & little & NA\\\\\nAlan Jackson & Little Bitty & country & love & positive\\\\\nAlan Jackson & Little Bitty & country & on & NA\\\\\n\\addlinespace\nAlan Jackson & Little Bitty & country & a & NA\\\\\nAlan Jackson & Little Bitty & country & little & NA\\\\\nAlan Jackson & Little Bitty & country & honeymoon & NA\\\\\nAlan Jackson & Little Bitty & country & you & NA\\\\\nAlan Jackson & Little Bitty & country & got & NA\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nSo we see that quite a few of the words from `lastfm_words` are not matched. To focus in on those words in `lastfm_words` that do match, we'll run the same join operation and filter for rows where `sentiment` is not empty (i.e. that there is a match in the `sentiments_bing` lexicon). \n\n\n::: {.cell layout-align=\"center\" tbl-cap='First 10 observations for the `lastfm_words` sentiments_bing` left join.' hash='transform-datasets_cache/pdf/tbltd-lastfm-words-bing-left-joing-filter_d1c753243e617ef34c1782770620e710'}\n\n```{.r .cell-code}\nleft_join(lastfm_words, sentiments_bing) |>\n  filter(sentiment != \"\") |> # return matched sentiments\n  slice_head(n = 10) |> # first 10 observations\n  kable(booktabs = TRUE)\n```\n\n::: {.cell-output-display}\n\\begin{tabular}{lllll}\n\\toprule\nartist & song & genre & word & sentiment\\\\\n\\midrule\nAlan Jackson & Little Bitty & country & love & positive\\\\\nAlan Jackson & Little Bitty & country & well & positive\\\\\nAlan Jackson & Little Bitty & country & well & positive\\\\\nAlan Jackson & Little Bitty & country & well & positive\\\\\nAlan Jackson & Little Bitty & country & smile & positive\\\\\n\\addlinespace\nAlan Jackson & Little Bitty & country & well & positive\\\\\nAlan Jackson & Little Bitty & country & well & positive\\\\\nAlan Jackson & Little Bitty & country & well & positive\\\\\nAlan Jackson & Little Bitty & country & smile & positive\\\\\nAlan Jackson & Little Bitty & country & good & positive\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nLet's turn to another type of join: an anti-join. The purpose of an anti-join is to eliminate matches. This makes sense for a quick and dirty approach to removing function words (i.e. those grammatical words with little semantic content). In this case we use the `get_stopwords()` function to get the dataset. We'll specify English as the language and we'll use the default lexicon ('Snowball'). \n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-get-stopwords-english_d21e2dcc15bdd8e15bc11986c04540db'}\n\n```{.r .cell-code}\nenglish_stopwords <- \n  get_stopwords(language = \"en\") # get English stopwords from the Snowball lexicon\n\nenglish_stopwords |> \n  slice_head(n = 10) # preview first 10 observations\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 10 x 2\n#>    word      lexicon \n#>    <chr>     <chr>   \n#>  1 i         snowball\n#>  2 me        snowball\n#>  3 my        snowball\n#>  4 myself    snowball\n#>  5 we        snowball\n#>  6 our       snowball\n#>  7 ours      snowball\n#>  8 ourselves snowball\n#>  9 you       snowball\n#> 10 your      snowball\n```\n:::\n:::\n\n\nNow if we want to eliminate stopwords from our `lastfm_words` dataset we use `anti_join()`. All the observations in the `lastfm_words` where there is not a match in `english_stopwords` will be returned. \n\n\n::: {#tbl-td-lastfm-words-stopwords-anti-join .cell layout-align=\"center\" tbl-cap='First 10 observations in `lastfm_words` after filtering for English stopwords.' hash='transform-datasets_cache/pdf/tbl-td-lastfm-words-stopwords-anti-join_3ef1a1bf7b393e14703bfc964a18548b'}\n\n```{.r .cell-code}\nanti_join(lastfm_words, english_stopwords) |> \n  slice_head(n = 10) |> \n  kable(booktabs = TRUE)\n```\n\n::: {.cell-output-display}\n\\begin{tabular}{llll}\n\\toprule\nartist & song & genre & word\\\\\n\\midrule\nAlan Jackson & Little Bitty & country & little\\\\\nAlan Jackson & Little Bitty & country & love\\\\\nAlan Jackson & Little Bitty & country & little\\\\\nAlan Jackson & Little Bitty & country & honeymoon\\\\\nAlan Jackson & Little Bitty & country & got\\\\\n\\addlinespace\nAlan Jackson & Little Bitty & country & little\\\\\nAlan Jackson & Little Bitty & country & dish\\\\\nAlan Jackson & Little Bitty & country & got\\\\\nAlan Jackson & Little Bitty & country & little\\\\\nAlan Jackson & Little Bitty & country & spoon\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\nWe can also merge datasets that we generate in our analysis or that we import from other sources. This can be useful when there are cases in which a corpus has associated metadata that is contained in files separate from the corpus itself. This is the case for the Switchboard Dialogue Act Corpus. \n\nOur existing, disfluency recoded, version includes the following variables. \n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-sdac-disfluencies_c357d5ef04b03085f55f062502036136'}\n\n```{.r .cell-code}\nsdac_disfluencies |> # dataset\n  slice_head(n = 10) # preview first 10 observations\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 10 x 5\n#>    doc_id speaker_id utterance_text                                 filler count\n#>     <dbl>      <dbl> <chr>                                          <chr>  <int>\n#>  1   4325       1632 Okay.  /                                       uh         0\n#>  2   4325       1632 Okay.  /                                       um         0\n#>  3   4325       1632 {D So, }                                       uh         0\n#>  4   4325       1632 {D So, }                                       um         0\n#>  5   4325       1519 [ [ I guess, +                                 uh         0\n#>  6   4325       1519 [ [ I guess, +                                 um         0\n#>  7   4325       1632 What kind of experience [ do you, + do you ] ~ uh         0\n#>  8   4325       1632 What kind of experience [ do you, + do you ] ~ um         0\n#>  9   4325       1519 I think, ] + {F uh, } I wonder ] if that work~ uh         1\n#> 10   4325       1519 I think, ] + {F uh, } I wonder ] if that work~ um         0\n```\n:::\n:::\n\n\nThe [online documentation page](https://catalog.ldc.upenn.edu/docs/LDC97S62/) provides a key file `caller_tab.csv` which contains speaker metadata information. Included in this `.csv` file is a column `caller_no` which contains the `speaker_id` we currently have in the `sdac_disfluencies` dataset. Let's read this file into our R session renaming `caller_no` to `speaker_id` to prepare to join these datasets. \n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-sdac-meta-read_9b9bd42211b91c097e69893a5ab459ab'}\n\n```{.r .cell-code}\nsdac_speaker_meta <- \n  read_csv(file = \"https://catalog.ldc.upenn.edu/docs/LDC97S62/caller_tab.csv\", \n           col_names = c(\"speaker_id\", # changed from `caller_no`\n                         \"pin\",\n                         \"target\",\n                         \"sex\",\n                         \"birth_year\",\n                         \"dialect_area\",\n                         \"education\",\n                         \"ti\",\n                         \"payment_type\",\n                         \"amt_pd\",\n                         \"con\",\n                         \"remarks\",\n                         \"calls_deleted\",\n                         \"speaker_partition\"))\n\nglimpse(sdac_speaker_meta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 543\n#> Columns: 14\n#> $ speaker_id        <dbl> 1000, 1001, 1002, 1003, 1004, 1005, 1007, 1008, 1010~\n#> $ pin               <dbl> 32, 102, 104, 5656, 123, 166, 274, 322, 445, 461, 57~\n#> $ target            <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y~\n#> $ sex               <chr> \"FEMALE\", \"MALE\", \"FEMALE\", \"MALE\", \"FEMALE\", \"FEMAL~\n#> $ birth_year        <dbl> 1954, 1940, 1963, 1947, 1958, 1956, 1965, 1939, 1932~\n#> $ dialect_area      <chr> \"SOUTH MIDLAND\", \"WESTERN\", \"SOUTHERN\", \"NORTH MIDLA~\n#> $ education         <dbl> 1, 3, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 3, 3, 2, 3~\n#> $ ti                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\n#> $ payment_type      <chr> \"CASH\", \"GIFT\", \"GIFT\", \"NONE\", \"GIFT\", \"GIFT\", \"CAS~\n#> $ amt_pd            <dbl> 15, 10, 11, 7, 11, 22, 20, 3, 11, 9, 25, 9, 1, 16, 1~\n#> $ con               <chr> \"N\", \"N\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"N\", \"N~\n#> $ remarks           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~\n#> $ calls_deleted     <dbl> 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0~\n#> $ speaker_partition <chr> \"DN2\", \"XP\", \"XP\", \"DN2\", \"XP\", \"ET\", \"DN1\", \"DN1\", ~\n```\n:::\n:::\n\n\nNow to join the `sdac_disfluencies` and `sdac_speaker_meta`. Let's turn to `left_join()` again as we want to retain all the observations (rows) from `sdac_disfluencies` and add the columns for `sdac_speaker_meta` where the `speaker_id` column values match. \n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-sdac-join-metadata_4c0ee8bba97ea791c4c9280620a5efa7'}\n\n```{.r .cell-code}\nsdac_disfluencies <- \n  left_join(sdac_disfluencies, sdac_speaker_meta) # join by ``speaker_id`\n\nglimpse(sdac_disfluencies)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 447,212\n#> Columns: 18\n#> $ doc_id            <dbl> 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325, 4325~\n#> $ speaker_id        <dbl> 1632, 1632, 1632, 1632, 1519, 1519, 1632, 1632, 1519~\n#> $ utterance_text    <chr> \"Okay.  /\", \"Okay.  /\", \"{D So, }\", \"{D So, }\", \"[ [~\n#> $ filler            <chr> \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\", \"um\", \"uh\"~\n#> $ count             <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0~\n#> $ pin               <dbl> 7713, 7713, 7713, 7713, 775, 775, 7713, 7713, 775, 7~\n#> $ target            <chr> \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N~\n#> $ sex               <chr> \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"FEMALE\", \"F~\n#> $ birth_year        <dbl> 1962, 1962, 1962, 1962, 1971, 1971, 1962, 1962, 1971~\n#> $ dialect_area      <chr> \"WESTERN\", \"WESTERN\", \"WESTERN\", \"WESTERN\", \"SOUTH M~\n#> $ education         <dbl> 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1~\n#> $ ti                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\n#> $ payment_type      <chr> \"CASH\", \"CASH\", \"CASH\", \"CASH\", \"CASH\", \"CASH\", \"CAS~\n#> $ amt_pd            <dbl> 10, 10, 10, 10, 4, 4, 10, 10, 4, 4, 10, 10, 4, 4, 4,~\n#> $ con               <chr> \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y~\n#> $ remarks           <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~\n#> $ calls_deleted     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~\n#> $ speaker_partition <chr> \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UNC\", \"UN~\n```\n:::\n:::\n\n\nNow there are some metadata columns we may want to keep and others we may want to drop as they may not be of importance for our analysis. I'm going to assume that we want to keep `sex`, `birth_year`, `dialect_area`, and `education` and drop the rest. \n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-sdac-disfluencies-subset-cols_0f087cd2e7b67aae208102b574c73530'}\n\n```{.r .cell-code}\nsdac_disfluencies <- \n  sdac_disfluencies |> # dataset\n  select(doc_id:count, sex:education) # subset key columns\n```\n:::\n\n::: {#tbl-td-sdac-disfluencies-meta-preview .cell layout-align=\"center\" tbl-cap='First 10 observations for the `sdac_disfluencies` dataset with speaker metadata.' hash='transform-datasets_cache/pdf/tbl-td-sdac-disfluencies-meta-preview_9917f3dfa95204e57eeeb24f4a87867c'}\n::: {.cell-output-display}\n\\begin{tabular}{rrllrlrlr}\n\\toprule\ndoc\\_id & speaker\\_id & utterance\\_text & filler & count & sex & birth\\_year & dialect\\_area & education\\\\\n\\midrule\n4325 & 1632 & Okay.  / & uh & 0 & FEMALE & 1962 & WESTERN & 2\\\\\n4325 & 1632 & Okay.  / & um & 0 & FEMALE & 1962 & WESTERN & 2\\\\\n4325 & 1632 & \\{D So, \\} & uh & 0 & FEMALE & 1962 & WESTERN & 2\\\\\n4325 & 1632 & \\{D So, \\} & um & 0 & FEMALE & 1962 & WESTERN & 2\\\\\n4325 & 1519 & {}[ [ I guess, + & uh & 0 & FEMALE & 1971 & SOUTH MIDLAND & 1\\\\\n\\addlinespace\n4325 & 1519 & {}[ [ I guess, + & um & 0 & FEMALE & 1971 & SOUTH MIDLAND & 1\\\\\n4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? / & uh & 0 & FEMALE & 1962 & WESTERN & 2\\\\\n4325 & 1632 & What kind of experience [ do you, + do you ] have, then with child care? / & um & 0 & FEMALE & 1962 & WESTERN & 2\\\\\n4325 & 1519 & I think, ] + \\{F uh, \\} I wonder ] if that worked. / & uh & 1 & FEMALE & 1971 & SOUTH MIDLAND & 1\\\\\n4325 & 1519 & I think, ] + \\{F uh, \\} I wonder ] if that worked. / & um & 0 & FEMALE & 1971 & SOUTH MIDLAND & 1\\\\\n\\bottomrule\n\\end{tabular}\n:::\n:::\n\n\n\n\n## Documentation\n\nDocumentation of the transformed dataset is just as important as the curated dataset. Therefore we use the same process as covered in the previous chapter. First we write the transformed dataset to disk and then we work to provide a data dictionary for this dataset. I've included the `data_dic_starter()` custom function to apply to our dataset(s). \n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-documentation-dic-starter-function_334b3298e7cc5be3bfc20124710c8f88'}\n\n```{.r .cell-code}\ndata_dic_starter <- function(data, file_path) {\n  # Function:\n  # Creates a .csv file with the basic information\n  # to document a curated dataset\n  \n  tibble(variable_name = names(data), # column with existing variable names \n       name = \"\", # column for human-readable names\n       description = \"\") |> # column for prose description\n  write_csv(file = file_path) # write to disk\n}\n```\n:::\n\n\nLet's apply our function to the `sdac_disfluencies` dataset using the R console (not part of our project script to avoid overwriting our documentation!).\n\n\n::: {.cell layout-align=\"center\" hash='transform-datasets_cache/pdf/td-sdac-disfluencies-data-dictionary-starter_f71af7e6a574952dcbabc936425f4948'}\n\n```{.r .cell-code}\ndata_dic_starter(data = sdac_disfluencies, file_path = \"../data/derived/sdac/sdac_disfluencies_data_dictionary.csv\")\n```\n:::\n\n\n```bash\ndata/derived/\n└── sdac/\n    ├── data_dictionary_sdac.csv\n    ├── sdac_curated.csv\n    ├── sdac_disfluencies.csv\n    └── sdac_disfluencies_data_dictionary.csv\n```\n\nOpen the `data_dictionary_sdac_disfluencies.csv` file in spreadsheet software and add the relevant description of the dataset. \n\n## Summary {-}\n\nIn this chapter we covered the process of transforming datasets. The goal is to manipulate the curated dataset to make it align better for analysis. \nThere are four general types of transformation steps: normalization, recoding, generation, and merging. In any given research project some or all of these steps will be employed --but not necessarily in the order presented in this chapter. Furthermore there may also be various datasets generated in at this stage each with a distinct analysis focus in mind. In any case it is important to write these datasets to disk and to document them according to the principles that we have established in the previous chapter. \n\nThis chapter concludes the section on data/ dataset preparation. The next section we turn to analyzing datasets. This is the stage where we interrogate the datasets to derive knowledge and insight either through inference, prediction, and/ or exploratory methods.\n\n## Activities {.unnumbered}\n\n::: callout-tip\n## Recipe\n\n**What**: [Dataset manipulation: tokenization and joining datasets](https://lin380.github.io/tadr/articles/recipe_8.html)\\\n**How**: Read Recipe 8 and participate in the Hypothes.is online social annotation.\\\n**Why**: To work with to primary types of transformations, tokenization and joins. Tokenization is the process of recasting textual units as smaller textual units. The process of joining datasets aims to incorporate other datasets to augment or filter the dataset of interest.\n:::\n\n::: callout-tip\n## Lab\n\n**What**: [Dataset manipulation: tokenization and joining datasets](https://github.com/lin380/lab_8)\\\n**How**: Clone, fork, and complete the steps in Lab 8.\\\n**Why**: To gain experience working with coding strategies for transforming datasets using tidyverse functions and regular expressions, practice reading/ writing data from/ to disk, and implement organizational strategies for organizing and documenting a dataset in reproducible fashion.\n:::\n\n## Questions {.unnumbered}\n\n::: callout-note\n## Conceptual questions\n\n1. ...\n2. ...\n\n:::\n\n::: callout-note\n## Technical exercises\n\n1. ...\n2. ...\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}