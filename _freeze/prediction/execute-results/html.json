{
  "hash": "7d541bcc09910b52ca8eca79be1fb57a",
  "result": {
    "markdown": "---\nexecute: \n  echo: true\n  cache: true\n---\n\n\n# Prediction {#sec-prediction}\n\n\n\n\n\n\n\n... Quote ...\n\n::: callout-note\n## Keys\n\n- ...\n:::\n\n\n::: {.cell layout-align=\"center\" hash='prediction_cache/html/prediction-data-packages_a156a1b5694ff60178b36e6bc07e81d0'}\n\n:::\n\n\nIn this chapter I present an introduction to approaches to data analysis known as machine learning, specifically supervised learning. In a nutshell, the aim of supervised learning is to leverage a potential relationship between a target or outcome variable and a set of other variables (features) derived from text to create a statistical generalization (model) that can accurately predict the values of the target variable using the values of the feature variables. We consider practical tasks as well as theoretical applications of the statistical learning in text analysis highlighting the standard workflow for building predictive models, testing and evaluating models, working to improve model performance, and how to interpret and report findings.\n\n::: callout-tip\n## Swirl\n\n**What**: [Supervised Learning](https://github.com/lin380/swirl)\\\n**How**: In the R Console pane load `swirl`, run `swirl()`, and follow prompts to select the lesson.\\\n**Why**: ...\n:::\n\n## Orientation {#pda-orientation}\n\nThe aim of this section is to introduce the reader to the concept of supervised learning and to provide a brief overview of the workflow for building predictive models for text analysis. \n\n### Research goals {#pda-research-goals}\n\nSupervised learning is a type of machine learning that involves training a model on a labeled dataset where the input data and desired output are both provided. The model is able to make predictions or classifications based on the input data by learning the relationships between the input and output data. Supervised machine learning is an important tool for linguists studying language and communication, as it allows them to analyze language data to identify patterns or trends in language use, verify hypotheses, and prescribe actions. Supervised machine learning is an active area of research in linguistics, with many potential applications and areas for further exploration.\n\nPredictive analyses are more inductive than exploratory analyses, which are more deductive. This means that we are more interested in the relationship between a particular outcome variable and a set of predictor variables than we are in the relationship between the predictor variables themselves, as we would be in an exploratory analysis. In this sense, we have a particular outcome in mind from the outset. On the other hand, the input variables are mutable, meaning that they can be changed to see how they affect the outcome --which points to the exploratory aspect of predictive analyses.\n\n### Approaches {#pda-approaches}\n\n- Outcome variable and any number of predictor variables\n- Predictor variables are features derived from text and are mutable. \n\n#### Analysis types {#pda-analysis-types}\n\nThere are two main types of supervised machine learning algorithms: classification, which is used to predict a categorical outcome such as the genre of a text, and regression, which is used to predict a continuous outcome such as the sentiment of a text.\n\n- Supervised learning\n    - Classification\n        - Categorical outcome variable\n    - Regression\n        - Continuous outcome variable\n\n### Workflow {#pda-workflow}\n\nPrerequisites: \n- A working research question or hypothesis\n- A dataset which aligns with the research question or hypothesis in terms of its sampling frame and the variables it contains or can be derived from the text and a target variable to be predicted. \n- A set of preliminary features to be derived from the text that are used to predict the target variable\n\n#### Identify {#pda-identify}\n\nData cleaning and feature extraction are the first steps in the process of preparing data for supervised machine learning.\n\nIn order to use supervised machine learning, linguists must first identify measurable properties of the text use use as the input variables or features that are most likely to produce a model that performs well (i.e. that when used make accurate predictions). Once the feature types are identified, the data is processed to clean any extraneous elements and format the structure of the dataset given the requirements of the algorithm that will be used in subsequent steps. \n\n#### Interrogate {#pda-interrogate}\n\nModel training is the next step towards building a predictive model. \n\nIn this step, the data is split into training and testing sets. The training set is used to train the model, and the testing set is used to evaluate the model's performance. The testing set is reserved and not used to train the model, so that the model's performance can be evaluated on data that it has not seen before.\n\n\nThe model is then trained on the training data and evaluated on the testing data. The results are then evaluated and the hyperparameters of the model may be adjusted to optimize its performance.\n\nand the hyperparameters of the model may be adjusted to optimize its performance. \n\nHyperparameters are variables that are set prior to running a machine learning algorithm whose values influence the final result. In supervised machine learning, hyperparameters are typically used to control the learning process such as the learning rate, momentum, and batch size.\n\n\nSome applications of supervised machine learning in linguistics include text classification, part-of-speech tagging, and language identification. Supervised machine learning is an active area of research in linguistics, with many potential applications and areas for further exploration.\n\n#### Interpret {#pda-interpret}\n\nTo either evaluate the training or testing set, the model is used to make predictions on the data in the set. The predictions are then compared to the actual values of the target variable in the set to evaluate the model's performance. So how is the model's performance evaluated? \n\nFor classification, there are a number of metrics that can be used to evaluate the performance of a model, including accuracy, precision, recall, and F1 score. To understand these measures it is helpful to consider a confusion matrix, which is a table that describes the performance of a classification model on data for which the true values are known. The confusion matrix is a two-by-two matrix that shows the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN), as seen in @tbl-pda-confusion-matrix.\n\n\n::: {#tbl-pda-confusion-matrix .cell layout-align=\"center\" tbl-cap='A labeled confusion matrix' hash='prediction_cache/html/tbl-pda-confusion-matrix_94bc0fa89694bde591bd2642decd0164'}\n::: {.cell-output-display}\n|                |Predicted positive |Predicted negative |\n|:---------------|:------------------|:------------------|\n|Actual positive |TP                 |FP                 |\n|Actual negative |FN                 |TN                 |\n:::\n:::\n\n\nNow let's fill this confusion matrix with hypothetical values, as seen in @tbl-pda-confusion-matrix-example to see how the metrics are calculated.\n\n\n::: {#tbl-pda-confusion-matrix-example .cell layout-align=\"center\" tbl-cap='Confusion matrix for a hypothetical model\\'s performance on a test set' hash='prediction_cache/html/tbl-pda-confusion-matrix-example_18fa0e0f68397ab0bc54ddb157bcd634'}\n::: {.cell-output-display}\n|                | Predicted positive| Predicted negative|\n|:---------------|------------------:|------------------:|\n|Actual positive |                100|                 10|\n|Actual negative |                 20|                 50|\n:::\n:::\n\n\n- Accuracy is defined as the proportion of correct predictions made by the model. \n\n$$\n\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n$${#eq-pda-accuracy}\n\nThe number of correct predictions is the sum of true positives and true negatives. So in our case this is 100 + 50 = 150. The total number of predictions is the sum of all four cells in the confusion matrix, so in our case this is 100 + 10 + 20 + 50 = 180. So the accuracy of our hypothetical model is 150/180 = 0.833.\n\n- Precision is defined as the proportion of positive predictions that are correct. \n\n$$\n\\text{Precision} = \\frac{\\text{Number of true positives}}{\\text{Number of true positives + false positives}}\n$${#eq-pda-precision}\n\n\n\n- Recall is defined as the proportion of actual positives that are correctly identified. \n\n$$\n\\text{Recall} = \\frac{\\text{Number of true positives}}{\\text{Number of true positives + false negatives}}\n$${#eq-pda-recall}\n\n- The F1 score is the harmonic mean of precision and recall.\n\n$$\n\\text{F1 score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n$${#eq-pda-f1}\n\nArea under the curve (AUC) is the area under the ROC (Receiver Operating Characteristic) curve, which is a graph of true positives (TPR) and false positives (FPR). The AUC is a measure of the model's performance across all possible classification thresholds. The AUC is a number between 0 and 1, where 0.5 represents a model that is no better than random guessing, and 1 represents a perfect model.\n\n\nFor regression, the most common metric is the root mean squared error (RMSE). The RMSE is the square root of the mean of the squared differences between the predicted values and the actual values. The lower the RMSE, the better the model fits the data. \n\nSupervised machine learning algorithms for regression are typically evaluated using measures of error such as mean squared error (MSE), root mean squared error (RMSE), and mean absolute error (MAE). MSE is used to measure the average of the squares of the errors, MAE is the average of the absolute differences between the prediction and the actual data, and RMSE is the square root of the mean squared error. For each of these statistics, the lower the value, the better the model fits the data. The differences between these statistics are shown in @tbl-pda-error-metrics.\n\n\n::: {#tbl-pda-error-metrics .cell layout-align=\"center\" tbl-cap='A table showing the differences between mean squared error, root mean squared error, and mean absolute error' hash='prediction_cache/html/tbl-pda-error-metrics_9690c1b73593788cae17bbb1e3410e64'}\n::: {.cell-output-display}\n|error |formula                                                    |description                                                                        |\n|:-----|:----------------------------------------------------------|:----------------------------------------------------------------------------------|\n|MSE   |$$\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$         |The average of the squared differences between the prediction and the actual data  |\n|RMSE  |$$\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$  |The square root of the mean squared error                                          |\n|MAE   |$$\\frac{1}{n} \\sum_{i=1}^{n} \\vert y_i - \\hat{y}_i \\vert$$ |The average of the absolute differences between the prediction and the actual data |\n:::\n:::\n\n\nThe main advantages of using MSE, RMSE, and MAE are that they are all on the same scale as the dependent variable, and they are all differentiable, which makes them useful for optimization algorithms. MSE is the most commonly used metric for regression, but RMSE and MAE are also used. MSE is more sensitive to outliers than RMSE and MAE, so it is more useful when the data has outliers. RMSE and MAE are more useful when the data does not have outliers.\n\n<!-- See https://smltar.com/mlregression.html for more info on evaluation (and other aspects) of supervised machine learning regression models.  -->\n\n\n::: {.cell layout-align=\"center\" hash='prediction_cache/html/pda-regression_a4352c73ddf0a52d2c99a1f5bb7e6a1c'}\n\n:::\n\n\nPlot the actual and predicted values to see how well the model fits the data.\n\n\n::: {.cell layout-align=\"center\" hash='prediction_cache/html/pda-regression-plot_7870f2ae5f777e7714bd56c39023a1cb'}\n::: {.cell-output-display}\n![A plot of the actual and predicted values for a regression model](prediction_files/figure-html/pda-regression-plot-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nWe can now apply our error metrics to the `results` data to see how well the model fits the data.\n\n\n::: {.cell layout-align=\"center\" hash='prediction_cache/html/pda-regression-error_b2da16389a0451407913014f6159c04c'}\n\n```{.r .cell-code}\n# Calculate the error metrics for the `results` data\nresults |> \n    mutate(error = actual - predicted) |> # calculate the error\n    summarise(mse = mean(error^2),  # calculate the MSE\n              rmse = sqrt(mse),  # calculate the RMSE\n              mae = mean(abs(error)),  # calculate the MAE\n              n = n()) |>  # calculate the number of observations\n    mutate(mse = mse * 1/n,  # multiply by 1/n to get the MSE for n observations\n           rmse = rmse * 1/n,  # multiply by 1/n to get the RMSE for n observations\n           mae = mae * 1/n)  # multiply by 1/n to get the MAE for n observations\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 1 × 4\n#>      mse   rmse     mae     n\n#>    <dbl>  <dbl>   <dbl> <int>\n#> 1 0.0113 0.0106 0.00947   100\n```\n:::\n:::\n\n\n\n\nFormula for calculating the MSE: \n\n$$\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$${#eq-pda-mse}\n\nSo we can implement this in R subtracting the actual values from the predicted values, squaring the differences, then taking the mean of the all the squared differences, and finally multiplying by $1/n$ to get the MSE for $n$ observations.\n\n\n::: {.cell layout-align=\"center\" hash='prediction_cache/html/pda-regression-mse_6777e990aa9f8b264445a279a80af5c2'}\n\n```{.r .cell-code}\nresults |> # use the `results` data\n    mutate(error = actual - predicted) |> # calculate the error\n    summarise(mse = mean(error^2),  # calculate the MSE\n              n = n()) |>  # calculate the number of observations\n    mutate(mse = mse * 1/n) |>  # multiply by 1/n to get the MSE for n observations\n    pull(mse) # pull the MSE value out of the tibble\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.01126773\n```\n:::\n:::\n\n\nFormula for calculating the RMSE:\n\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n$${#eq-pda-rmse}\n\n## Analysis {#pda-analysis}\n\nRecap and introduction to the structure of the analysis subsection. \n\n- Introduce an algorithm\n- Build a model\n    - Preprocessing (tokenization, lemmatization, etc.)\n    - Feature extraction (TF-IDF, word embeddings, etc.)\n    - Model selection (logistic regression, SVM, etc.)\n    - Model training\n- Evaluate (and adjust) the model on the training data\n    - Cross-validation\n    - Evaluation metrics\n    - Compare to null and/ or other models\n    - Adjust the model (hyperparameters, regularization, etc.) as necessary\n- Evaluate the model on the test data\n    - Evaluation metrics\n    - Evaluate feature importance\n    - Evaluate the features of correct and incorrect predictions\n\n### Classification {#pda-classification}\n\nWe will first start with classification which is by far the most common text analysis approach in supervised machine learning. Again, classification is the task of predicting a categorical variable from a set of features. The features we use will be derived from the text but can take many forms. For example, we can use the raw text, the word counts, the TF-IDF values, or the word embeddings. We also will take into account the number of features we use. There is a trade-off, however, to the number of features: a) the more features we use, the more complex the model will be, and the more likely it will overfit the training data and b) the less features we use, the less complex the model will be, and the more likely it will underfit the training data. To find the optimal number of features we can use a technique called cross-validation.\n\n\nThe most common text classification algorithms are logistic regression, k-nearest neighbors, Naive Bayes, and support vector machines. We will start with logistic regression and k-nearest neighbors as they are the simplest to understand and implement. We will then move on to Naive Bayes and support vector machines as they are more complex and require more explanation.\n\nBuilding a null model for classification we simply predict the most common class in the training data. This makes sense as we have seen earlier, with categorical data the central tendency is estimated by the mode --i.e. the most common value. \n\n#### K-nearest neighbors {#pda-k-nearest-neighbors}\n\nK-nearest neighbors is a simple supervised machine learning method for classification. It is a non-parametric method, which means that it does not make any assumptions about the underlying distribution of the data. It is a lazy learning method, which means that it does not learn a discriminant function from the training data but instead stores the training data. It is a distance-based method, which means that it uses a distance metric to find the $k$ nearest neighbors to a new observation. It is a simple method, which means that it is easy to understand and implement.\n\n#### Logistic regression {#pda-logistic-regression}\n\nLogistic regression is a supervised machine learning method for classification. It is a parametric method, which means that it makes assumptions about the underlying distribution of the data. It is an iterative method, which means that it uses an iterative algorithm to find the optimal parameters. To avoid overfitting it uses regularization such as ridge regression or lasso regression. These regularization methods penalize the model for having too many parameters. However, how does one know what the optimal number of parameters is? This is where cross-validation comes in. \n\n#### Naive Bayes {#pda-naive-bayes}\n\nNaive Bayes is a supervised machine learning method for classification. It is a parametric method, which means that it makes assumptions about the underlying distribution of the data. It is a probabilistic method, which means that it uses Bayes' theorem to calculate the probability of a class given the predictor variables. It is a generative method, which means that it learns the joint probability distribution of the predictor variables and the outcome variable. It is a simple method, which means that it makes the assumption that the predictor variables are independent of each other. This assumption is called the naive assumption. Now this assumption does not theoretically hold for language data as words are not independent of each other. However, in practice, Naive Bayes' models still perform well on many text classification tasks.\n\n#### Decision trees {#pda-decision-trees}\n\nDecision trees for text classification are a supervised machine learning method for classification. They are a non-parametric method, which means that they do not make any assumptions about the underlying distribution of the data. They are a greedy method, which means that they use a greedy algorithm to find the optimal split of the predictor variables. They are a simple method, which means that they are easy to understand and implement. \n\nIn practical terms using decision trees for text classification can be very useful as they are easy to interpret. For example, we can see which words are most important for the classification of a text. However, they are prone to overfitting the training data. To avoid this we can use a technique called bagging. Bagging is a technique that uses multiple decision trees to make a prediction. The prediction is then the mode of the predictions of the individual decision trees. This is called a random forest.\n\n### Regression {#pda-regression}\n\nIn supervised machine learning regression tasks contrast to classification tasks as the outcome variable is continuous. A typical example outside of language would be to predict the price of a house given the number of bedrooms, the number of bathrooms, the size of the house, etc. For language this means that the labled outcome variable is a number, not a class. For example, we can predict the number of words in a text given the number of characters in the text, the number of sentences in the text, etc. Other applications of regression in text analysis are sentiment analysis (where the outcome is a scalar value) and topic modeling (where the outcome is a probability distribution over topics).\n\n#### Linear regression {#pda-linear-regression}\n\nLinear regression can be used to predict a continuous outcome variable from a set of features. It is a parametric method, which means that it makes assumptions about the underlying distribution of the data. It is an iterative method, which means that it uses an iterative algorithm to find the optimal parameters. To avoid overfitting it uses regularization such as ridge regression or lasso regression. These regularization methods penalize the model for having too many parameters. However, how does one know what the optimal number of parameters is? This is where cross-validation comes in.\n\n#### Decision trees (regression) {#pda-decision-trees-regression}\n\nDecision trees for regression are a supervised machine learning method for regression. They are a non-parametric method, which means that they do not make any assumptions about the underlying distribution of the data. They are a greedy method, which means that they use a greedy algorithm to find the optimal split of the predictor variables. They are a simple method, which means that they are easy to understand and implement.\n\n#### Neural networks {#pda-neural-networks}\n\nNeural networks are a supervised machine learning method for regression and classification. They are a non-parametric method, which means that they do not make any assumptions about the underlying distribution of the data. They are an iterative method, which means that they use an iterative algorithm to find the optimal parameters. They are a complex method, which means that they are difficult to understand and implement. However, they are very powerful and can be used to solve a wide range of problems. However, they are expensive to train and require a lot of data. It is often the case that a simpler method will perform just as well as a neural network in certain contexts. \n\n## Summary {#pda-summary}\n\nIn this chapter we have learned about supervised machine learning. We have learned about the different types of supervised machine learning methods and how they can be used to predict and classify. We have also learned about the different types of data structures that are used in supervised machine learning. Finally, we have learned about the different types of evaluation metrics that are used to evaluate the performance of supervised machine learning models.\n\n## Activities {.unnumbered}\n\n::: callout-tip\n## Recipe\n\n**What**: [Predictive models: prep, train, test, and evaluate](https://lin380.github.io/tadr/articles/recipe_10.html)\\\n**How**: Read Recipe 10 and participate in the Hypothes.is online social annotation.\\\n**Why**: To illustrate some common coding strategies for preparing datasets for inferential data analysis, as well as the steps conduct descriptive assessment, statistical interrogation, and evaluation and reporting of results.\n:::\n\n::: callout-tip\n## Lab\n\n**What**: [Predictive Data Analysis](https://github.com/lin380/lab_10)\\\n**How**: Clone, fork, and complete the steps in Lab 10.\\\n**Why**: To gain experience working with coding strategies to prepare, feature engineer, train and test a predictive model, and evaluate results from a predictive data analysis, practice transforming datasets into new object formats and visualizing relationships, and implement organizational strategies for organizing and reporting results in a reproducible fashion.\n:::\n\n## Questions {.unnumbered}\n\n::: callout-note\n## Conceptual questions\n\n1. What is the difference between a continuous and a categorical variable?\n2. What is the difference between a regression and a classification model?\n3. What is the difference between a training set and a testing set?\n4. What is the difference between a hyperparameter and a parameter?\n5. What is the difference between a supervised and an unsupervised machine learning model?\n6. What advantages and disadvantages do supervised machine learning models have over traditional methods of text analysis?\n7. What are some potential applications of supervised machine learning in linguistics?\n\n:::\n\n::: callout-note\n## Technical exercises\n\n1. Write a program to build a classification model which uses a set of collected text features to predict a target variable.\n2. Use the classification model to classify a series of documents and assess the accuracy of the model.\n3. Develop a regression model which uses text features to predict a continuous target variable.\n4. Create a text mining application to analyze a large body of text and discover correlations between variables.\n5. Use a clustering algorithm to discover clusters in a large dataset, and create a visualization to present the identified clusters.\n6. Analyze the structure of a text corpus and identify patterns in word usage and feature distributions.\n7. Build a predictive model using text as an input and binary or categorical outcomes as the target.\n8. Develop a natural language processing application which classifies text into predefined categories using a supervised learning algorithm.\n9. Use a supervised learning algorithm to build a predictive model which classifies a set of unseen texts into predefined categories.\n10. Develop a web application which allows users to easily explore a set of text documents, visualize the content of the documents, and generate predictive models from the text.\n\n:::\n\n\n<!---\n\n- Overview:\n    - Research goals (predict, prescribe, verify)\n    - Use of data\n        - Partitioned: training (reusable) and test (reserved) sets\n        - Outcome variable and predictor/ covariate variables\n        - Fixed outcome and mutable predictor variables/ features\n    - Methods\n        - Supervised machine learning methods\n            - Regression\n            - Classification\n    - Data types/ structures\n        - Matrices\n    - Interpreting results (quantitative)\n        - Summary statistics: \n            - Accuracy\n            - Precision\n            - Recall\n            - F1\n            - Confusion matrix\n- Supervised machine learning analysis\n    - Methods: \n        - Regression (continuous outcome variable)\n            - Linear regression\n            - Decision trees\n            - Neural networks\n        - Classification (categorical outcome variable)\n            - Logistic regression\n            - Decision trees\n            - Neural networks\n\n\n\n\n\nSupervised machine learning is a type of artificial intelligence that involves training a model on a labeled dataset where the input data and desired output are both provided. The model is able to make predictions or classifications based on the input data by learning the relationships between the input and output data. Supervised machine learning is an important tool for linguists studying language and communication, as it allows them to analyze language data and identify patterns or trends in language use. There are two main types of supervised machine learning algorithms: classification, which is used to predict a categorical outcome such as the genre of a text, and regression, which is used to predict a continuous outcome such as the sentiment of a text. In order to use supervised machine learning, linguists must first prepare the data by cleaning and preprocessing it, and then split the data into training and testing sets. The model is then trained on the training data and evaluated on the testing data, and the hyperparameters of the model may be adjusted to optimize its performance. Some applications of supervised machine learning in linguistics include text classification, part-of-speech tagging, and language identification. Supervised machine learning is an active area of research in linguistics, with many potential applications and areas for further exploration.\n\n\nI. Introduction to supervised machine learning\nA. Definition of supervised machine learning\ni. Supervised machine learning involves training a model on a labeled dataset where the input data and desired output are both provided\nii. The model is able to make predictions or classifications based on the input data by learning the relationships between the input and output data\nB. Importance of supervised machine learning for linguistics\ni. Supervised machine learning can be used to analyze language data and identify patterns or trends in language use\nii. Supervised machine learning can help linguists classify texts, predict language evolution, and identify language-specific features\n\nII. Types of supervised machine learning algorithms\nA. Classification\ni. Classification algorithms are used to predict a categorical outcome, such as whether a text belongs to a particular genre\nii. Examples of classification algorithms include decision trees, logistic regression, and support vector machines\nB. Regression\ni. Regression algorithms are used to predict a continuous outcome, such as the sentiment of a text\nii. Examples of regression algorithms include linear regression and multiple linear regression\n\nIII. Preparing data for supervised machine learning\nA. Data preprocessing\ni. Data preprocessing involves cleaning and preparing the data for analysis\nii. This may include tasks such as removing missing values, scaling the data, and encoding categorical variables\nB. Data splitting\ni. Data splitting involves dividing the data into training and testing sets\nii. The training set is used to train the model, while the testing set is used to evaluate the model's performance\n\nIV. Training and evaluating a supervised machine learning model\nA. Training a model\ni. Training a model involves providing the model with the training data and adjusting the model's parameters to minimize the error between the model's predictions and the true output\nB. Evaluating a model\ni. Evaluating a model involves using the testing data to measure the model's performance\nii. Common evaluation metrics for supervised machine learning include accuracy, precision, and recall\nC. Tuning model hyperparameters\ni. Hyperparameters are the parameters of the machine learning model that are set before training begins\nii. Tuning hyperparameters involves adjusting the values of the hyperparameters to optimize the model's performance\n\nV. Applications of supervised machine learning in linguistics\nA. Text classification\ni. Text classification involves categorizing texts into predefined categories, such as genre or topic\nii. Supervised machine learning can be used to classify texts based on their content or linguistic features\nB. Part-of-speech tagging\ni. Part-of-speech tagging involves assigning a part of speech to each word in a text\nii. Supervised machine learning can be used to identify the part of speech of a word based on its context and other linguistic features\nC. Language identification\ni. Language identification involves determining the language of a text\nii. Supervised machine learning can be used to identify the language of a text based on its content or linguistic features\n\nVI. Conclusion\nA. Recap of key points\ni. Supervised machine learning is a type of machine learning that involves training a model on a labeled dataset\nii. Supervised machine learning can be used to classify texts, predict language evolution, and identify language-specific features\nB. Future directions for supervised machine learning in linguistics\ni. Supervised machine learning is an active area of research in linguistics, with many potential applications and areas for further exploration\nii. Future research may focus on developing more sophisticated\n\n\n\nResearch\n\nStrengths:\n1. Neural networks are able to capture complex relationships between words and phrases in text.\n2. Neural networks can learn from large amounts of data and can be trained to classify text with high accuracy.\n3. Neural networks can be used to classify text into multiple categories.\n4. Neural networks can be used to identify patterns in text that are not easily detected by traditional methods.\n\nWeaknesses:\n1. Neural networks require a large amount of data to train and can be computationally expensive.\n2. Neural networks can be prone to overfitting if the data is not properly preprocessed.\n3. Neural networks can be difficult to interpret and explain due to their complexity.\n4. Neural networks can be sensitive to noise and outliers in the data.\n\n\nBuilding and evaluating a predictive model for text classification requires multiple steps. Here are the key ones:\n\n1. Data collection - Collect a set of labeled documents to be used in the model.\n\n2. Data preprocessing - Clean the data and prepare it for the model by tokenizing, stemming, and removing stop words.\n\n3. Feature extraction - Extract features from the data using methods such as bag of words, TF-IDF, word embeddings, and feature selection.\n\n4. Model training - Train the model on the extracted features using methods such as Logistic Regression, Support Vector Machines, Naive Bayes, and Neural Networks.\n\n5. Model evaluation - Evaluate the model using metrics such as accuracy, precision, recall, and F1-score.\n\n6. Model tuning - Optimize the model by adjusting its parameters to obtain better results.\n\n\n-->\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}