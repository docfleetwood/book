{
  "hash": "f49eaebd0049468217a5b581a5a6f66e",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexecute:\n  echo: true\n---\n\n\n# Contribute {#sec-contribute-chapter}\n\n\n\n\n\n\n> The reproducibility of studies and the ability to follow up on the work of others is key for innovation in science and engineering.\n>\n> --- Leland Wilkinson\n\n::: {.callout}\n**{{< fa regular list-alt >}} Outcomes**\n\n- Identify the aims of public-facing and peer-facing communication\n- Understand the overlapping and distinct elements and goals of research presentations and articles\n- Comprehend the importance of and pinpoint the aspects of well-documented and reproducible research\n:::\n\nWe have discussed the design and implementation of research that is purposive, inquisitive, informed, and methodical. Now, we turn to the task of sharing research results in a manner that is communicable. There are two primary ways to communicate the results of research: public-facing and peer-facing. Public-facing research communication includes presentations, articles, and other forms of dissemination that are intended for audiences to become familiar with the research. Peer-facing communication, on the other hand, targets other researchers, often working in same field, and focuses on the more technical aspects such as data, code, and documentation that enable other researchers to reproduce and/ or build upon the research. In this chapter, we will cover both aspects of research communication, providing guidelines for effective research reporting and strategies to ensure the reproducibility of your research project.\n\n::: {.callout}\n**{{< fa terminal >}} Lessons**\n\n**What**: Computing environment\\\n**How**: In an R console, load `swirl`, run `swirl()`, and follow prompts to select the lesson.\\\n**Why**: To interact with the R package `renv` and learn how to manage package versions in a research project.\n:::\n\n## Public-facing {#sec-contr-public-facing}\n\nDissemination of research findings is a critical part of the research process. Whether it is through presentations, articles, blog posts, or social media, the ability to effectively communicate the results of research is essential for the impact and dissemination of research. Furthermore, the act of composing a research document can help to develop and refine ideas and conclusions.\n\nThe two most common forms of research dissemination in academics are presentations and articles. Both share a common goal: to effectively communicate the research to an audience. However, they also have distinct purposes and require different approaches to achieve their goals. These purposes complement each other, with presentations often serving as a means to engage and elicit feedback from an audience, and articles serving as a more comprehensive and permanent record of the research.\n\n### Structure {#sec-contr-ao-structure}\n\nFirst, let's focus on the structural elements that appear in both research presentations and articles. The components in @tbl-ao-common-components reflect the typical structure for presenting research in the social sciences [@Gries2016; @Gries2020; @Sternberg2010]. Their combined purpose is to trace the research narrative from the rationale and goals to connecting the findings with the research questions and aims.\n\n| Component | Purpose |\n|-----------|---------|\n| Introduction | Provide context and rationale based on previous research |\n| Methods | Describe the research design and procedures |\n| Results | Present the findings including key statistics and table and/ or visual summaries |\n| Discussion | Interpret the findings and discuss implications |\n| Conclusion | Summarize the research and suggest future work |\n\n: Common components of research presentations and articles {#tbl-ao-common-components tbl-colwidths=\"[20, 80]\"}\n\nWhen research is connected to a well-designed plan, as described in @sec-research-chapter, key elements in this structure will have already begun to take shape. The steps taken to identify an area and a problem or gap in the literature find themselves in the introduction. This section builds the case for the research and provides the context for the research question(s) and aim(s). The methods section describes the research design and procedures, including the data collection and analysis steps that are key to contextualize the findings. In the results section, the findings are presented in the appropriate manner given the research aim and the analysis performed.\n\nThe discussion and conclusions sections, however, are where the research narrative is brought together. Crafting these sections can be seen as an extension of the research process itself. Instead of elaborating on the planning steps and their implementation, the discussion focuses on the interpretation of the findings in the light of the research questions and previous literature. At this stage, the act of articulating the implications of the findings is where deeper insights are developed and refined. The conclusion, for its part, puts a finer point on the research goal and main findings, but also is an opportunity to extend suggestions to where subsequent research might go.\n\n### Purpose {#sec-contr-ao-purpose}\n\nUnderstanding the roles the structural elements play in contributing to the overall narrative is essential for effective research communication. Yet, presentations and articles are not the same. They have distinct goals which are reflected in the emphasis that each communication channel places on particular narrative elements and the level of detail and nuance that is included in the narrative.\n\nIt is likely not a surprise that articles are more detailed and nuanced than presentations. But what is sometimes overlooked is that presentations should emphasize storytelling and relatability, over nuance and detail. A 'less is more' approach can help maintain connection with the take home message and reduce information overload. To be sure, the research should be accurate and reliable, but the focus is on engaging the audience and connecting the research to some broader themes. Even if your audience is familiar with the research area, maintaining a connection with 'why this matters' is important.\n\nTabular and visual summaries are key to convey complex findings, regardless of the mode of communication. However, in presentations, the use of visual aids is especially effective for engaging the audience as the visual modality does not compete with the spoken word for attention. Along these lines, limiting the amount of text on slides and increasing natural discourse with the audience is a good practice. Your presentation will be more engaging leading to more questions and feedback that you can use to refine your current or to seed future research.\n\nThe purpose of an article is to provide a comprehensive record of the research. In this record, the methods and results sections are particularly significant. The methods section should provide the reader with the necessary information to understand the research design and procedures and to evaluate the findings, as it should in presentations, but, in contrast to presentations, it should also speak to researchers providing the details required to reproduce the research. These details summarize and, ideally, point to the data and code that are used to produce the findings in your reproducible research project (see @sec-contr-peer-facing).\n\nThe results section, for its part, should present the findings in a manner that is clear and concise, but also comprehensive. The research aim and the analysis performed will determine the appropriate measures and/ or summaries to use. @tbl-c-results-summaries outlines the statistical results, tables, and visualizations that often figure in the results section for exploratory, predictive, and inferential analyses.\n\n| Research aim | Statistical results | Summaries |\n|---------------|-------------------------|--------|\n| Exploratory | Descriptive statistics | Extensive use of tables and/ or visualizations |\n| Predictive | Descriptive statistics, model performance metrics | Tables for model performance comparisons and/ or visualizations for feature importance measures |\n| Inferential | Descriptive statistics, hypothesis testing confidence metrics | Tables for hypothesis testing results and/ or visualizations to visualize trends |\n\n: Key statistical results, tables, and visualizations for research results {#tbl-c-results-summaries tbl-colwidths=\"[17, 35, 48]\"}\n\nBy and large, the results section should be a descriptive and visual summary of the findings as they are, without interpretation. The discussion section is where the interpretation of the findings and their implications are presented. This general distinction between the results and discussion may be less pronounced in exploratory research, as the interpretation of the findings may be more intertwined with the presentation of the findings given the nature of the research.\n\n### Strategies {#sec-contr-ao-strategies}\n\nStrong research write-ups begin with well-framed and well-documented research plans. The steps outlined in @sec-fr-plan are the foundation for much of the research narrative. Furthermore, you can further prepare for the research write-up by leaving yourself a breadcrumb trail during the research process. This includes documenting the literature that you consulted, the data, processing steps, and analysis choices that you made, and saving the key statistical results, tables, and visualizations that you generated in your process script for the analysis. This will make it easier to connect the research narrative to the research process.\n\nThe introduction includes the rationale, goals, and research questions and aims. These components are directly connected to the primary literature that you consulted. For this reason, it is a good practice to keep a record of the literature that you consulted and the notes that you took. This record will help you to trace the development of your ideas and to provide the necessary context for your research. A reference manager, such as Zotero, Mendeley, or EndNote, is a good tool for this purpose. These tools allow you to manage your ideas and keep notes, organize your references and resources, and integrate your references and resources with your writing in Quarto through BibTeX entry citation keys.\n\nSimilarly, if you are following best practices, you will have documented your data, processing steps, and analysis choices while conducting your research. The methods section stems directly from these resources. Data origin files provide the necessary context for the data that you used in your research. Data dictionary files clarify variables and values in your datasets. Literate programming, as implemented in Quarto, can further provide process and analysis documentation.\n\nThe results section can also benefit from some preparation. The key statistical results, tables, and visualizations generated in your process script for the analysis should be saved as outputs. This provides a more convenient way to include these results in your research document(s).\n\nIf you are using a project structure similiar to the one outlined in @sec-fr-scaffold, you can write statistical results as R objects using `saveRDS()`, and write tables and visualizations as files using `kableExtra::save_kable()` and `ggplot2::ggsave()`, respectively, to the corresponding *outputs/* directory. This will allow you to easily access and include these results in your research document(s) to avoid having to recreate the analysis steps from a dataset or manually copy and paste results from the console, which can be error-prone and is not reproducible.\n\n::: {.callout .halfsize}\n**{{< fa regular hand-point-up >}} Tip**\n\nThe `qtkit` package provides three functions for writing R objects, ggplot2 objects, and kable objects to a given directory. These functions are `write_obj()`, `write_gg()`, and `write_kbl()`, respectively. These functions also provide functionality to automatically name the output files based on the label of the code block in which they are called to make it easier to connect the output to the code that generated it. For more information, see the [qtkit documentation](https://qtalr.github.io/qtkit/).\n:::\n\nAt this point we have our ducks in a row, so to speak. We have a well-documented research plan, a record of the literature that we consulted, and a record of the data, processing steps, and analysis choices that we made. We have also saved the key statistical results, tables, and visualizations that we generated in our process script for the analysis. Now, we can begin to write our research document(s).\n\nAlthough there are many tools and platforms for creating and sharing research presentations and articles, I advocate for using Quarto to create and share both. First, Quarto documents fit squarely into a reproducible workflow, as we have seen throughout this textbook. Secondly, using Quarto for both presentations and articles allows for a seamless transition between the two. This is particularly useful when you are presenting research that you have written about in an article, or vice versa. The statistical results, tables, and visualizations that you saved as outputs can be easily included in both presentations and articles seamlessly. Third, changes in your research process will naturally be reflected in your write-ups. This helps maintain the fidelity of your research across the various stages of the research process, including write-ups. Finally, Quarto provides a variety of output formats, including PDF, HTML, and Word, which are suitable for sharing research presentations and articles. These documents can be shared on various platforms, including GitHub pages, and can be easily converted to other formats. Quarto also provides a styles for citations and bibliographies and a variety of extensions for journal-specific formatting, which can be useful for publishing articles in specific venues.\n\n## Peer-facing {#sec-contr-peer-facing}\n\nWhether for other researchers or for your future self, creating research that is well-documented and reproducible is a fundamental part of conducting modern scientific inquiry. Reproducible research projects do not replace the need to document methods and results in write-ups, but they do provide a more comprehensive and transparent record of the research that elevates transparency, encourages collaboration, and enhances the visibility and impact of research.\n\n### Structure {#sec-contr-ro-structure}\n\nReproducible research consists of two main components: a research compendium and a computing environment. These components are interleaved and when shared, work together to ensure that the research project is transparent, well-documented, and reproducible.\n\nResearch compendium is a term used to describe a collection of files and documentation that organize and document a research project. This includes the data, code, and documentation files. To ensure that the project is legible and easy to navigate, the research compendium content and the project scaffolding should be predictable and consistent, following best practices outlined in @sec-research-chapter ([-@sec-fr-scaffold]) and found in more detail in @Wilson2017.\n\nIn short, there should be a separation between input, output, and the code that interacts with the input to produce the output. Furthermore, documentation for data, code, and the project as a whole should be clear and comprehensive. This includes a README file, a data origin file, and a data dictionary file, among others. Finally, a main script should be used to execute and coordinate the processing of the project steps.\n\nAll computational projects require a computing environment. This includes the software and hardware that are used to execute the code and process the data. For a text analysis project using R, this will include R and R packages. Regardless of the language, however, there are system-level dependencies, an operating system, and hardware resources that the project relies on.\n\n@fig-contr-ro-structure visualizes the relationship between the computing environment and the research compendium as layers of a research environment. The research compendium is the top layer, each of the subsequent layers represents elements of the computing environment.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Layers and components of a computational research environment](figures/nested-project.drawio.png){#fig-contr-ro-structure width=1198}\n:::\n:::\n\n\nThe research compendium is the most visible layer, as it is the primary means of interacting with the research project. The software layer includes R, R packages, and system-level dependencies. System-level dependencies serve to support the software layer. Software itself, these dependencies are not directly interacted with, but they are necessary for the more 'visible' software to function. Most people are familiar with operating systems, such as Windows, macOS, and perhaps Linux, but there are many different versions of these operating systems. Furthermore, hardware resources also vary. One of the most important aspects of hardware to consider for reproducibility is the architecture of the processor (the CPU).\n\nWe will consider how to create a reproducible environment which addresses each of these layers later in this chapter.\n\n### Purpose {#sec-contr-ro-purpose}\n\nWhile the goal is the same, to ensure that the computational research project is transparent, well-documented, and reproducible, the purpose of the research compendium and the computing environment are distinct.\n\nThe research compendium is in large part a guide book to the research process. Efforts here increase research transparency, facilitate collaboration and peer review, and enhance the visibility and impact of research. It is also the case that keeping tabs on the process in this way helps to ensure that the research is accurate and reliable by encouraging you to be more mindful of the choices that you make and the steps that you take. Any research project is bound to have its share of false starts, dead ends, or logical lapses, but leaving a breadcrumb trail during the research process can help to make these more visible and help you (and others) learn from them.\n\nThe computing environment is a means to an end. It is the infrastructure that is used to execute the code, process the data, and return (and report) the results. The purpose of the computing environment is to ensure that the research can be executed and processed in the same way, producing the same results, regardless of the time or place. While a research compendium has value on its own, the ability to add a level of 'future-proofing' to the project only adds to that value. This is both true for other researchers who might want to build upon your research and for yourself, as returning to a project after some time away can highlight how much computing environments can change when errors litter the screen!\n\n### Strategies {#sec-contr-ro-strategies}\n\nThe strategies for creating a reproducible research project are many and varied, although that gap is closing as the tools and resources for reproducible research continue to grow. In this section, I will present an opinionated set of strategies to address each of the layers of a computational research project seen in @fig-contr-ro-structure, in a way that better positions research to be accessible to more people and to be more resilient to the changes that are inevitable in the computing environment.\n\n<!-- Research compendium -->\n\nA key component to research compendiums which integrate into a reproducible workflow is the use of a project structure that modularizes the research project into predictable and consistent components. This will will primarily consist of input, output, and the code that excutes and documents the processing steps. But it also consist of a coordinating script, that is used to orchestrate each module in the project step sequence.\n\nA particularly effective framework for implementing a research compendium with these features is the Quarto website. Quarto documents, as literal programming is in general, provides rich support for integrating source content, computations, and visualizations in a single document. In addition, Quarto documents are designed to be modular --each is run in a separate R session making no assumptions about inputs or previous computing states. When tied to logical processing steps, this can help to ensure that each step says what it does, and does what it says, enhancing the transparency and reproducibility of the research project.\n\nThe Quarto website treats each document as part of a set of documents that are coordinated by a `_quarto.yml` configuration file. Rendering a Quarto website will execute and compile the Quarto documents as determined in the configuration settings. In this way, the goal of easy execution of the project is satified in a way that is consistent and predictable and coopts a framework with wide support in the R community.\n\nCreating the scaffolding for a research compendium in Quarto is a matter of creating a new Quarto website through RStudio, the R Console, or the command-line interface (CLI) and adding the necessary files, directories, and documentation.\n\nIn @lst-quarto-site a Quarto site structure augmented to reflect the project structure is shown. @lst-quarto-yml shows a glimpse of the `_quarto.yml` configuration file for a Quarto project website is shown. This file is used to coordinate the Quarto documents in the project and to specify the output format for the project as a whole and for individual documents.\n\n::: {#exm-quarto-sites layout-ncol=2}\n```{#lst-quarto-site .bash lst-cap=\"Quarto website structure\"}\nproject/\n  ├── input/\n  │   └── ...\n  ├── code/\n  │   └── ...\n  ├── output/\n  │   └── ...\n  ├── _quarto.yml\n  ├── DESCRIPTION\n  ├── index.qmd\n  └── README.md\n```\n\n```{#lst-quarto-yml .yaml lst-cap=\"Quarto configuration file\"}\nproject:\n  title: \"Project title\"\n  type: website\n\nwebsite:\n  sidebar:\n    contents:\n      - index.qmd\n      - section: \"Code\"\n        contents: code/*\n      - section: \"Reports\"\n        contents: output/*\n\nformat:\n  html: default\n```\n\n:::\n\nWhile the Quarto website as a whole will be rendered to HTML, individual documents can be rendered to other formats. This can be leveraged to create PDF versions of write-ups, for example, or use `revealjs` for Quarto to create presentations that are rendered and easily shared on the web. For ways to extend the Quarto website, visit the [Quarto documentation](https://quarto.org/docs/).\n\n<!-- Computing environment -->\n\n<!-- Software layer -->\n\nLet's now turn to layers of the computing environment, starting with the portion of the software layer which includes R and R packages. R and R packages are updated, new packages are introduced, and some packages are removed from circulation. These changes are good overall, but it means that code we write today may not work in the future. It sure would be nice if we could keep the same versions of packages that worked for a project.\n\n`renv` is a package that helps manage R package installation by versions [@R-renv]. It does this by creating a separate environment for each R project where `renv` is initialized. This environment allows us to keep snapshots of the state of the project's R environment in a lockfile --a file that contains the list of packages used in the project and their versions. This can be helpful for developing a project in a consistent environment and controlling what packages and package versions you use and update. More importantly, however, if the lockfile is shared with the project, it can be used to restore the project's R environment to the state it was in when the lockfile was created, yet on a different machine or at a different time.\n\nAdding a lockfile to a project is as simple as initializing `renv` in the project directory with `renv::init()` and running `renv::snapshot()`. Added to the project, in @exm-quarto-sites, we see the addition of the `renv.lock` file and the `renv/` directory, in @exm-quarto-renv-site.\n\n::: {#exm-quarto-renv-site}\n\n```{#lst-quarto-renv .bash lst-cap=\"Quarto website structure with renv\"}\nproject/\n  ├── input/\n  │   └── ...\n  ├── code/\n  │   └── ...\n  ├── output/\n  │   └── ...\n  ├── renv/\n  │   └── ...\n  ├── _quarto.yml\n  ├── index.qmd\n  ├── README.md\n  └── renv.lock\n```\n:::\n\nThe *renv.lock* file serves as to document the computing environment and packages used to conduct the analysis. It therefore replaces the need for a *DESCRIPTION* file. The `renv/` directory contains the R environment for the project. This includes a separate library for the project's packages, and a cache for the packages that are installed. This directory is not shared with the project, as we will see, as the lockfile is sufficient to restore the project's R environment.\n\nAs R packages change over time, so too do other resources including R, system dependencies, and the operating system --maybe less frequently, however. These change will inevitably affect our ability to reliably execute the project ourselves over time, but it is surely more pronounced when we expect our project to run on a different machine! To address these elements of the computing environment, we need another, more computing-comprehensive approach.\n\nA powerful and popular approach to reproducible software and operating system, as well as hardware environments, is to use Docker. Docker is software that provides a way to create and manage entire computing environments. These environments are called containers, and they are portable, consistent, and almost entiely isolated from the host system. This means that a container can be run on any machine that has Docker installed, and it will run the same way regardless of the host system. The technology behind Docker is called containerization, and is a type of virtualization. Containers are widely used as they are quick to develop, easy to share, and allow for the execution of code safely separate from the host system.\n\nTo prepare a computational project for containerization, we need to create a Dockerfile. A Dockerfile is a text file that contains a set of instructions for creating our reproducible computing environment. A Dockerfile is a sequence of layers each pertaining to a different aspect of the computing environment, such as the operating system, system dependencies, and software as well as any other resources that are necessary for the project --including the research compendium.\n\nLet's look at a sample Dockerfile in @lst-dockerfile-example.\n\n```{#lst-dockerfile-example .yaml lst-cap=\"Dockerfile\"}\n# Layer 1: operating system ---------\nFROM ubuntu:latest\n\n# Layer 2: system dependencies ------\nRUN apt-get -y update\n\n# Layer 3: software ------------------\nRUN apt-get -y install r-base\n\n# Layer 4: project structure ---------\nCOPY . /project\n```\n\n@lst-dockerfile-example, when executed at the CLI with `docker build` in project directory, will create an image file. A Docker image is a blueprint for creating a container or any number of containers which contain the computing environment specified in the Dockerfile. In our example, the first layer adds the operating system, the second layer updates the system dependencies, the third layer installs R, and the fourth layer adds the research compendium to the container. Docker images can be shared and run on any machine that has Docker installed, effectively sharing a complete reproducible computing environment.\n\nNotice that the operating system is `ubuntu:latest`. `ubuntu` refers to a version of the Ubuntu operating system, which is based on Linux. In line with our goal to use open source software, Ubuntu is a popular choice for Docker images. The `:latest` tag refers to the most recent version of Ubuntu. This can be set to a fixed version, such as `ubuntu:20.04` to ensure that the container is built with a specific version of Ubuntu. Other resources can be versioned in a similar way.\n\nThanks to helpful R community members, there are also Docker images built specifically for the R community and distributed as part of the Rocker Project. These images include a variety of R versions and R environment setups (*e.g.* RStudio Server, Shiny Server, *etc.*). These images can be found in image/ container registries such as Docker Hub or GitHub Container Registry. The Rocker Project's images are widely used and well-maintained, and are a good choice for creating a reproducible computing environment for an R project.\n\nLet's update the Dockerfile to use the Rocker Project's `rstudio` image with R version 4.3.3.\n\n```{#lst-dockerfile-rstudio .yaml lst-cap=\"Dockerfile with Rocker Project image\"}\n# Layer 1/2/3: operating system, system dependencies, and R\nFROM rocker/rstudio:4.3.3\n\n# Layer 4: project structure\nCOPY . /project\n```\n\n@lst-dockerfile-rstudio is a much simpler and efficient Dockerfile. In addition, the `rstudio` image includes RStudio Server, which can be accessed through a web browser. This can be useful for sharing the research project with others, as it allows for the execution of and interaction with the project in a web-based environment.\n\nThere are two more elements to address in our computational environment, however, the R packages in the `renv` lockfile and potential hardware differences across machines. The R packages can added to the image by adding the lines in @lst-dockerfile-renv to the Dockerfile.\n\n```{#lst-dockerfile-renv .yaml lst-cap=\"Dockerfile with renv\"}\n# Install `renv` and restore the project's R environment\nRUN R -e 'install.packages(\"renv\")'\nRUN R -e 'renv::restore()'\n```\n\nCalling `renv::restore()` in the Dockerfile will install the packages as part of the image. This will increase the size of the image, but it ensures that there is no post-build step to restore the project's R environment. The trade-off, however, is the size of the image, the time it takes to build the image, and the frequency with which the project's R environment changes can be factors to consider.\n\n::: {.callout .halfsize}\n**{{< fa medal >}} Dive deeper**\n\nAnother approach to making the R computing environment accessible to others as part of your reproducible project, is to include a user-side step after the container is built. In this way, a simpler Docker image is created and steps are included on how to complete the computing environment setup.\n\nOne approach is to clone the project from a remote repository (such as GitHub) and then restore the project's R environment. There are potentially two advantages to this approach: (1) size, computing time, and environment changes are addressed more naturally, and (2) a simpler Docker image is also more flexible and can potentially be used as the base to set up multiple projects.\n:::\n\nThe hardware considerations concern whether the container will run on the same architecture as the host system. The most widely used architecture is known as AMD64. The AMD64 architecture is used by the wide majority of computer manufacturers in their machines. Recently, however, ARM64 has been gaining popularity with the introduction of Apple's M chip series. AMD64 and ARM64 are not compatible, so a container built for one architecture will not run on the other. By default, Docker will build a container for the architecture of the host system.\n\nIf you plan to make the image available to others, it is a good idea to build the image for multiple architectures. To address this, Docker provides a way to build images to create containers for different architectures using `docker buildx`. We will not cover this in detail here.\n\n::: {.callout .halfsize}\n**{{< fa medal >}} Dive deeper**\n\nOne of the more effective ways to address the hardware considerations is to use a continuous integration (CI) service to build and publish the Docker image to a container registry. GitHub provides a CI service called GitHub Actions. GitHub Actions can be used to build and publish the Docker image to GitHub Container Registry. Once the image is published, it can be pulled and run on any machine that has Docker installed. This is a more advanced approach, but it provides a way to automate the building and publishing of the image. For more information, see the [GitHub Actions documentation](https://docs.github.com/en/actions).\n:::\n\n<!-- Publishing repositories -->\n\nOnce the research compendium and computing environment are prepared, the project can be published. If you are using Git to manage your project, you will likely want to publish the project to a remote repository. This makes your project accessible to others and provides a means to collaborate with other researchers. GitHub is a popular platform for publishing coding projects and it provides a number of services that are useful for research projects, including version control, issue tracking, website hosting, and continuous integration (CI). Continuous integration is a practice of automatically building, testing, and/ or deploying code changes when they are added to a repository.\n\nI want to stress that adding your project to a publically visable code repository is a form of publication. And when we work with data and datasets we need to consider the ethical implications of sharing data. As part of our project preparation we will have considered the data sources we used and the data we collected, including the licensing and privacy considerations. The steps outlined in @sec-acquire-chapter to [-@sec-transform-chapter] will either gather data from other sources or modify these sources which we add to our *data/* directory. If we do not have permissions to share the data included in this directory, or sub-directories, we should not share it on our remote repository. To avoid sharing sensitive data, we can use a *.gitignore* file to exclude the data from the repository. This file is a text file that contains a list of files and directories that should be ignored by Git. This file can be added to the project directory and committed to the repository.\n\nSince we have explicitly built in mechanisms in our project structure to ensure that the processing code is modular and that it does not depend on input or previous states, a researcher can easily recreate this data by executing our project. In this way, we do not share the data, but rather we share the means to recreate the data. This is a good practice for sharing data and is a form of reproducibility.\n\nWith your project published to a remote repository, you can connect it to other venues that list research projects, such as Open Science Framework, Zenodo, and Figshare. These platforms enhance the visibility of your project and provide a means to collaborate with other researchers. A Digital Object Identifier (DOI) will be assigned to the proejct which can be used to cite the project in articles and other research outputs.\n\nWebsite hosting can also be enable with GitHub through GitHub Pages. GitHub Pages is a static site hosting service that takes HTML, CSS, and JavaScript files from a GitHub repository on a given branch and publishes a website. This can be useful for sharing the research project with others, as it provides a means to interact with the project in a web-based environment.\n\n::: {.callout}\n**{{< fa medal >}} Dive deeper**\n\nThere are a few ways to publish a Quarto website on GitHub. One way is to modify the `_quarto.yml` configuration file to include an output directory for the rendered site and then modify the GitHub repository configuration under the Pages to publish the site based on this directory. When you push your rendered site to the repository, it will be published to the web.\n\nAnother way, is to set up a separate branch in GitHub `gh-pages` to use to store and serve your website. The `quarto` command line interface provides a command to render the site and publish it to the web. `quarto publish gh-pages` will render the site and push it to the `gh-pages` branch. In this scenario, you will not need to modify the `_quarto.yml` configuration file but you will have to manually call `quarto publish gh-pages` to render and publish the site.\n\nAnother way is to use GitHub Actions to render the site and publish it to the web. This is a more advanced approach, but it provides a way to automate the rendering and publishing of the site. For more information, see the [Quarto documentation](https://quarto.org/docs/).\n:::\n\nTo make our project more accessible to others, we can also publish the Docker image to a container registry. This can be done on your local machine, or through GitHub's CI service GitHub Actions. GitHub Actions can be used to build and publish the Docker image to GitHub Container Registry (see [Publishing Docker images](https://docs.github.com/en/actions/publishing-packages/publishing-docker-images)). One primary advantage is the fact that building images for mulitple architectures is supported. Once the image is published, it can be pulled and run on any machine that has Docker installed.\n\nLet's now summarize the steps outlined here to publish a computational research project. First, we prepare the research compendium and computing environment. This includes creating a project structure that modularizes the research project into predictable and consistent components, and creating a Dockerfile to create a reproducible computing environment. We then publish the project to a remote repository, and connect it to other venues that list research projects. We also enable website hosting with GitHub Pages, and publish the Docker image to a container registry. This provides a means to interact with the project in a web-based environment, and a means to run the project on any machine that has Docker installed.\n\nA project structure with these elements represented is shown in @lst-quarto-repo-proj.\n\n```{#lst-quarto-repo-proj .bash lst-cap=\"Quarto website structure with Dockerfile and GitHub Actions\"}\nproject/\n  ├── .github/\n  │   └── workflows/\n  ├── input/\n  │   └── ...\n  ├── code/\n  │   └── ...\n  ├── output/\n  │   └── ...\n  ├── renv/\n  │   └── ...\n  ├── _quarto.yml\n  ├── .gitignore\n  ├── Dockerfile\n  ├── index.qmd\n  ├── README.md\n  └── renv.lock\n```\n\nThe project structure, computing environment, and publication strategies outlined here are opinionated, but they are also flexible and can be adapted to suit the needs of the research project. The goal is to ensure that the computational research project is transparent, well-documented, and reproducible, and that it is accessible to others.\n\n----\n\nNow, as we wrap up this chapter, and the book, it is an opportune moment to consider the big picture of a reproducible research project. In @fig-big-picture, we see the relationship between each stage of the research process, from planning to publication, and their interconnectivity. These efforts reflect our goal to generate insight from data.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Big picture of a reproducible research project](figures/qtal-big-picture.png){#fig-big-picture width=702}\n:::\n:::\n\n\nI represent five main stages in reproducible research: frame, prepare, analyze, and communicate. Each of these stages, and substages are represented as parts and chapters in this book. In @tbl-research-stages-substages, I summarize the stages and substages of a reproducible research project, including the purpose of each stage and substage, the code that is used to execute the stage and substage, and the input and output of each stage and substage.\n\n| Stage | Substage | Purpose | Code | Input | Output |\n|-------|----------|---------|------|-------|--------|\n| Frame | Plan | Develop a research plan | - | Primary literature | Prospectus |\n| Prepare | Acquire | Gather data | Collects data | - | Original data, data origin file |\n| Prepare | Curate | Tidy data | Create rectangular dataset | Original data | Curated dataset, data dictionary file |\n| Prepare | Transform | Augment and adjust dataset | Prepare and/ or enrich dataset | Curated dataset | Research-aligned data, data dictionary file |\n| Analyze | Explore, predict, or infer | Analyze data | Apply statistical methods | Transformed dataset | Key statistical results, tables, visualizations |\n| Communicate | Public- and/ or Peer-facing | Share research | Write-up, publish | Analyzed data artifacts | Research document(s), computing environment, website |\n\n: Stages and substages of a reproducible research project {#tbl-research-stages-substages}\n\nIn conclusion, the goal of research is to develop and refine ideas and hypotheses, sharing them with others, and to build on the work of others. The research process outlined here aims to improve the transparency, reliability, and accessibility of research, and to enhance the visibility and impact of research. These goals are not exclusive to text analysis, nor linguistics, nor any other field for that matter, but are fundamental to conducting modern scientific inquiry. I hope that the strategies and tools outlined in this book will help you to achieve these goals in your research projects.\n\nImportantly, however, if you do not find yourself performing research in the future, I hope that the strategies and tools outlined in this book will help you to critically evaluate the research that you encounter. We are surrounded by research, and it is important to be able to evaluate the quality and reliability of the research that we encounter. This is a fundamental part of being an informed citizen and a critical thinker.\n\n## Activities {.unnumbered}\n\n::: {.callout}\n**{{< fa regular file-code >}} Recipe**\n\n<!-- Understand, apply, and analyze verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->\n\n**What**: Manage project and computing environments\\\n**How**: Read Recipe 11, complete comprehension check, and prepare for Lab 11.\\\n**Why**: To follow the steps for managing a research project and computing environment for effective communication and reproducibility.\n:::\n\n::: {.callout}\n**{{< fa flask >}} Lab**\n\n<!-- Analyze, evaluate, and create verbs: https://tips.uark.edu/blooms-taxonomy-verb-chart/ -->\n\n**What**: Future-proofing research\\\n**How**: Clone, fork, and complete the steps in Lab 11.\\\n**Why**: To apply the strategies for ensuring that your research project is reproducible.\n:::\n\n## Summary {.unnumbered}\n\nIn this chapter, we have discussed the importance of clear and effective communication in research reporting, and the strategies for ensuring that your research project is reproducible. We have discussed the role of public-facing research including presentations and articles. We also emphasized the importance of well-documented and reproducible research in modern scientific inquiry and outlined strategies for ensuring your research project is reproducible. As modern research practice continue to evolve, the details may change, but the principles of transparency, reliability, and accessibility will remain fundamental to conducting modern scientific inquiry.\n\n<!--\n\n\nLesson:\n\n- Using `renv` for package management in a research project\n  - Motivation/ problem\n    - R package versions change over time\n      - Introduces reproducibility issues\n    - It would be nice to keep the same versions of packages that worked for a project.\n    - CRAN keeps historical snapshots of packages by their version\n    - `renv` is a package that helps manage package installation by versions\n  - `renv` basics\n    - Concepts: lockfile, snapshot, restore\n  - `renv` in a research project\n    - Initializing `renv` in a research project\n    - Adding and removing packages\n    - Snapshotting and restoring the environment\n\n\n-->\n\n\n\n\n\n<!--\n\nFurther reading:\n\n- @Rodrigues2023\n- @Gandrud2020\n- @Blischak2019\n- @Gentleman2007\n- @Munafo2017\n- Gries and Paquot (2020) \"Writing up a corpus linguistic paper\" DOI: 10.1007/987-3-030-46216-1\n\n-->\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}