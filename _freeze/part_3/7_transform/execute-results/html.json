{
  "hash": "02e865edcebadbefadb2b3a7d2a60e72",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexecute:\n  echo: true\n---\n\n\n# Transform {#sec-transform-chapter}\n\n\n\n\n\n\n> Nothing is lost. Everything is transformed.\n>\n> --- Michael Ende, The Neverending Story\n\n::: {.callout}\n**{{< fa regular list-alt >}} Outcomes**\n\n- Understand the role of data transformation in a text analysis project.\n- Identify the main types of transformations used to prepare datasets for analysis.\n- Recognize the importance of planning and documenting the transformation process.\n:::\n\n\n::: {.cell}\n\n:::\n\n\nIn this chapter, we will focus on transforming curated datasets to refine and possibly expand their relational characteristics to align with our research. I will approach the transformation process by breaking it down into two sub-processes: preparation and enrichment. The preparation process involves normalizing and tokenizing text. The enrichment process involves generating, recoding, and integrating variables. These processes are not sequential but may occur in any order based on the researcher's evaluation of the dataset characteristics and the desired outcome.\n\n::: {.callout}\n**{{< fa terminal >}} Lessons**\n\n**What**: Reshape Datasets by Rows, Reshape datasets by Columns\\\n**How**: In an R console, load `swirl`, run `swirl()`, and follow prompts to select the lesson.\\\n**Why**: Explore data preprocessing skills to manipulate rows and columns using powerful packages like `dplyr` and `tidytext` to normalization, tokenize, and integrate datasets equipping you with the essential techniques to structure datasets for analysis.\n:::\n\n## Preparation {#sec-td-preparation}\n\nIn this section we will cover the processes of normalization and tokenization. These processes are particularly relevant for text analysis, as text conventions can introduce unwanted variability in the data and the unit of observation may need to be adjusted to align with the research question.\n\nTo illustrate these processes, we will use a curated version of the Europarl Corpus [@Koehn2005]. This dataset contains transcribed source language (Spanish) and translated target language (English) from the proceedings of the European Parliament. The unit of observation is the `lines` variable whose values are lines of dialog. We will use this dataset to explore the normalization and tokenization processes.\n\nThe contents of the data dictionary for this dataset appears in @tbl-td-europarl-dd.\n\n\n::: {#tbl-td-europarl-dd .cell tbl-cap='Data dictionary for the curated Europarl Corpus.'}\n::: {.cell-output-display}\n\n+----------+---------------+-------------+-------------------------------------------------------------------+\n| variable | name          | type        | description                                                       |\n+==========+===============+=============+===================================================================+\n| doc_id   | Document ID   | numeric     | Unique identification number for each document                    |\n+----------+---------------+-------------+-------------------------------------------------------------------+\n| type     | Document Type | categorical | Type of document; either 'Source' (Spanish) or 'Target' (English) |\n+----------+---------------+-------------+-------------------------------------------------------------------+\n| line_id  | Line ID       | numeric     | Unique identification number for each line in each document type  |\n+----------+---------------+-------------+-------------------------------------------------------------------+\n| lines    | Lines         | categorical | Content of the lines in the document                              |\n+----------+---------------+-------------+-------------------------------------------------------------------+\n:::\n:::\n\n\nLet's read in the dataset CSV file with `read_csv()` and inspect the first lines of the dataset with `slice_head()` in @exm-tb-europarl-preview.\n\n::: {#exm-tb-europarl-preview}\n```r\n# Read in the dataset\neuroparl_tbl <-\n  read_csv(file = \"../data/derived/europarl_curated.csv\")\n\n# Preview the first 10 lines\neuroparl_tbl |>\n  slice_head(n = 10)\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 4\n    doc_id type   line_id lines                                                 \n     <dbl> <chr>    <dbl> <chr>                                                 \n 1 1965735 Source       1 \"Reanudación del período de sesiones\"                 \n 2       1 Target       1 \"Resumption of the session\"                           \n 3 1965736 Source       2 \"Declaro reanudado el período de sesiones del Parlame…\n 4       2 Target       2 \"I declare resumed the session of the European Parlia…\n 5 1965737 Source       3 \"Como todos han podido comprobar, el gran \\\"efecto de…\n 6       3 Target       3 \"Although, as you will have seen, the dreaded 'millen…\n 7 1965738 Source       4 \"Sus Señorías han solicitado un debate sobre el tema …\n 8       4 Target       4 \"You have requested a debate on this subject in the c…\n 9 1965739 Source       5 \"A la espera de que se produzca, de acuerdo con mucho…\n10       5 Target       5 \"In the meantime, I should like to observe a minute' …\n```\n\n\n:::\n:::\n\n:::\n\nThis dataset includes 3,931,468 observations and four variables. The key variable for our purposes is the `lines` variable. This variable contains the text we will be working with. The other variables are metadata that may be of interest for our analyses.\n\n### Normalization {#sec-td-normalization}\n\nThe process of normalizing datasets in essence is to sanitize the values of variable or set of variables such that there are no artifacts that will contaminate subsequent processing. It may be the case that non-linguistic metadata may require normalization but more often than not linguistic information is the most common target for normalization as text often includes artifacts from the acquisition process which will not be desired in the analysis.\n\nSimply looking at the first 10 lines of the dataset from @exm-tb-europarl-preview gives us a clearer sense of the dataset structure, but, in terms of normalization procedures we might apply, it is likely not sufficient. We want to get a sense of any potential inconsistencies in the dataset, in particular in the `lines` variable. Since this is a large dataset with 3,931,468 observations, we will need to explore the dataset in more detail using procedures for summarizing and filtering data.\n\nAfter exploring variations in the `lines` variable, I identified a number of artifacts in this dataset that we will want to consider addressing. These are included in @tbl-td-europarl-normalization.\n\n| Description | Examples |\n|:---|:---|\n| Non-speech annotations | `(Abucheos)`, `(A4-0247/98)`, `(The sitting was opened at 09:00)` |\n| Inconsistent whitespace | `5 % ,`, <code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code>, `Palacio' s` |\n| Non-sentence punctuation | ` - ` |\n| Abbreviations | `Mr.`, `Sr.`, `Mme.`, `Mr`, `Sr`, `Mme`, `Mister`, `Señor`, `Madam` |\n| Text case | `The`, `the`, `White`, `white` |\n\n: Characteristics of the Europarl Corpus dataset that may require normalization. {#tbl-td-europarl-normalization tbl-colwidths=\"[30, 70]\" .striped}\n\nThese artifacts either may not be of interest or may introduce unwanted variability in the that could prove problematic for subsequent processing (*e.g* tokenization, calculating frequencies, *etc.*).\n\nThe majority of text normalization procedures incorporate the `stringr` package [@R-stringr]. This package provides a number of functions for manipulating text strings. The workhorse functions we will use for our tasks are the `str_remove()` and `str_replace()` functions. As the these functions give us the ability to remove or replace text based on literal strings and Regular Expressions.\n\nOur first step, however, is to identify the patterns we want to remove or replace. For demonstration purposes, let's focus on removing non-speech annotations from the `lines` variable. Do develop a search pattern to identify these annotations, there are various possibilities, `str_view()`, `str_detect()` inside a `filter()` call, or `str_extract()` inside a `mutate()` call. No matter which approach we choose, we need to be sure that our search pattern does not over- or under-generalize the text we want to remove or replace. If we are too general, we may end up removing or replacing text that we want to keep. If we are too specific, we may not remove or replace all the text we want to remove or replace.\n\nIn @exm-td-europarl-search-non-speech, I've used `str_detect()` which detects a pattern in a character vector and returns a logical vector, `TRUE` if the pattern is detected and `FALSE` if it is not. In combination with `filter()` we can identify a variable with rows that match a pattern. I've added the `slice_sample()` function at then end to return a small, random sample of the dataset to get a better sense how well our pattern works across the dataset.\n\nNow, how about our search pattern? From the examples above, we can see that these instances are wrapped with parentheses `(` and `)`. The text within the parentheses can vary, so we need a Regular Expression to do the heavy lifting. To start out we can match any one or multiple characters with `.+`. But it is important to recognize the `+` (and also the `*`) operators are 'greedy', meaning that if there are multiple matches (in this case multiple sets of parentheses) in a single line of text, the longest match will be returned. That is, the match will extend as far as possible. This is not what we want in this case. We want to match the shortest match. To do this we can append the `?` operator to make the `+` operator 'lazy'. This will match the shortest match.\n\n\n::: {.cell}\n\n:::\n\n\n::: {#exm-td-europarl-search-non-speech}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load package\nlibrary(stringr)\n\n# Identify non-speech lines\neuroparl_tbl |>\n  filter(str_detect(lines, \"\\\\(.+?\\\\)\")) |>\n  slice_sample(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 4\n    doc_id type   line_id lines                                                 \n     <dbl> <chr>    <dbl> <chr>                                                 \n 1 3225772 Source 1260038 (PT) Señor Presidente, quisiera plantear dos pregunta…\n 2 3715842 Source 1750108 (El Parlamento decide la devolución a la Comisión)    \n 3 1961715 Target 1961715 (Parliament adopted the resolution)                   \n 4 1429470 Target 1429470 27, originally Greens/EFA amendment in FEMM); binding…\n 5   51632 Target   51632 Question No 8 by (H-0376/00):                         \n 6 2482671 Source  516937 La Comisión propone proporcionar a las Agencias nacio…\n 7 1059628 Target 1059628 (The President cut off the speaker)                   \n 8 1507254 Target 1507254 in writing. - (LT) I welcomed this document, because …\n 9 2765325 Source  799591 (Aplausos)                                            \n10 2668536 Source  702802    Las preguntas que, por falta de tiempo, no han rec…\n```\n\n\n:::\n:::\n\n:::\n\nThe results from @exm-td-europarl-search-non-speech show that we have identified the lines that contain at least one of the parliamentary session description annotations. A more targeted search to identify specific instances of the parliamentary session descriptions can be accomplished adding the `str_extract_all()` function as seen in @exm-td-europarl-search-non-speech-2.\n\n\n::: {.cell}\n\n:::\n\n\n::: {#exm-td-europarl-search-non-speech-2}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract non-speech fragments\neuroparl_tbl |>\n  filter(str_detect(lines, \"\\\\(.+?\\\\)\")) |>\n  mutate(non_speech = str_extract_all(lines, \"\\\\(.+?\\\\)\")) |>\n  slice_sample(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 5\n    doc_id type   line_id lines                                       non_speech\n     <dbl> <chr>    <dbl> <chr>                                       <list>    \n 1 3225772 Source 1260038 (PT) Señor Presidente, quisiera plantear d… <chr [1]> \n 2 3715842 Source 1750108 (El Parlamento decide la devolución a la C… <chr [1]> \n 3 1961715 Target 1961715 (Parliament adopted the resolution)         <chr [1]> \n 4 1429470 Target 1429470 27, originally Greens/EFA amendment in FEM… <chr [1]> \n 5   51632 Target   51632 Question No 8 by (H-0376/00):               <chr [1]> \n 6 2482671 Source  516937 La Comisión propone proporcionar a las Age… <chr [2]> \n 7 1059628 Target 1059628 (The President cut off the speaker)         <chr [1]> \n 8 1507254 Target 1507254 in writing. - (LT) I welcomed this documen… <chr [1]> \n 9 2765325 Source  799591 (Aplausos)                                  <chr [1]> \n10 2668536 Source  702802    Las preguntas que, por falta de tiempo,… <chr [1]> \n```\n\n\n:::\n:::\n\n:::\n\nOK, that might not be what you expected. The `str_extract_all()` function returns a list of character vectors. This is because for any given line in `lines` there may be a different number of matches. To maintain the data frame as rectangular, a list is returned for each value of `non_speech`. We could expand the list into a data frame with the `unnest()` function if our goal were to work with these matches. But that is not our aim. Rather, we want to know if we have multiple matches per line. Note that the information provided for the `non_speech` column by the tibble object tells use that we have some lines with muliple matches, as we can see in line 6 of our small sample. So good thing we checked!\n\nLet's now remove these non-speech annotations from each line in the `lines` column. We turn to `str_remove_all()`, a variant of `str_remove()`, that, as you expect, will remove multiple matches in a single line. We will use the `mutate()` function to overwrite the `lines` column with the modified text. The code is seen in @exm-td-europarl-remove-non-speech.\n\n::: {#exm-td-europarl-remove-non-speech}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove non-speech fragments\neuroparl_tbl <-\n  europarl_tbl |>\n  mutate(lines = str_remove_all(lines, \"\\\\(.+?\\\\)\"))\n```\n:::\n\n:::\n\nI recommend spot checking the results of this normalization step by running the code in @exm-td-europarl-search-non-speech again, if nothing appears we've done our job.\n\nWhen you are content with the results, drop the observations that have no text in the `lines` column. These were rows where the entire line was non-speech annotation. This can be done with the `is.na()` function and the `filter()` function as seen in @exm-td-europarl-drop-empty-lines.\n\n::: {#exm-td-europarl-drop-empty-lines}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Drop empty lines\neuroparl_tbl <-\n  europarl_tbl |>\n  filter(!is.na(lines))\n```\n:::\n\n:::\n\nNormalization goals will vary from dataset to dataset but the procedures often follow a similar line of attack to those outlined in this section.\n\n### Tokenization {#sec-td-tokenization}\n\nTokenization is the process of segmenting units of language into components relevant for the research question. This includes breaking text in curated datasets into smaller units, such as words, $n$-grams, sentences, *etc.* or combining smaller units into larger units.\n\nThe process of tokenization is fundamentally row-wise. Changing the unit of observation changes the number of rows. It is important both for the research and the text processing to operationalize our language units beforhand. For example, while it may appear obvious to you what 'word' or 'sentence' means, a computer, and your reproducible research, needs a working definition. This can prove tricker than it seems. For example, in English, we can segment text into words by splitting on whitespace. This works fairly well but there are some cases where this is not ideal. For example, in the case of contractions, such as `don't`, `won't`, `can't`, *etc.* the apostrophe is not a whitespace character. If we want to consider these contractions as separate words, then perhaps we need to entertain a different tokenization strategy.\n\n::: {.callout}\n**{{< fa regular lightbulb >}} Consider this**\n\nConsider the following paragraph:\n\n> \"As the sun dipped below the horizon, the sky was set ablaze with shades of orange-red, illuminating the landscape. It's a sight Mr. Johnson, a long-time observer, never tired of. On the lakeside, he'd watch with friends, enjoying the ever-changing hues—especially those around 6:30 p.m.—and reflecting on nature's grand display. Even in the half-light, the water's glimmer, coupled with the echo of distant laughter, created a timeless scene. The so-called 'magic hour' was indeed magical, yet fleeting, like a well-crafted poem; it was the essence of life itself.\"\n\nWhat text conventions would pose issues for word tokenization based on a whitespace critieron?\n:::\n\nFurthermore, tokenization strategies can vary between languages. For German words are often compounded together, meaning many 'words' will not be captured by the whitespace convention. Whitespace may not even be relevant for word tokenization in logographic writing systems, such as Chinese. The take home message is there is no one-size-fits-all tokenization strategy.\n\n::: {.callout}\n**{{< fa medal >}} Dive deeper**\n\nFor processing Chinese text, including tokenization, see the `jiebaR` package [@R-jiebaR] and the `gibasa` package [@R-gibasa].\n:::\n\nLet's continue to work with the Europarl Corpus dataset to demonstrate tokenization. We will start by tokenizing the text into words. If we envision what this should look like, we might imagine something like @tbl-td-europarl-tokenization-words-example.\n\n::: {#tbl-td-europarl-tokenization-words-example}\n\n|  doc_id   | type | line_id | token |\n|----------|--------|----------|------|\n| 1 | Target | 2 | I |\n| 1 | Target | 2 | declare |\n| 1 | Target | 2 | resumed |\n| 1 | Target | 2 | the |\n| 1 | Target | 2 | session |\n\nExample of tokenizing the `lines` variable into word tokens.\n:::\n\nComparing @tbl-td-europarl-tokenization-words-example to the fourth row of the output of @exm-tb-europarl-preview, we can see that we want to segment the words in `lines` and then have each segment appear as a separate observation, retaining the relevant metadata variables.\n\nTokenization that maintains the tidy dataset structure very common strategy in text analysis using R. So common, in fact, that the `tidytext` package [@R-tidytext] includes a function, `unnest_tokens()` that tokenizes text in just such a way. Various tokenization types can be specified including 'characters', 'words', 'ngrams', 'sentences' among others. We will use the 'word' tokenization type to recreate the structure we envisioned in @tbl-td-europarl-tokenization-words-example.\n\nIn @exm-td-europarl-tokenization-words-tidytext, we see set our output variable to `token` and our input variable to `lines`.\n\n::: {#exm-td-europarl-tokenization-words-tidytext}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load package\nlibrary(tidytext)\n\n# Tokenize the lines into words\neuroparl_unigrams_tbl <-\n  europarl_tbl |>\n  unnest_tokens(\n    output = token,\n    input = lines,\n    token = \"words\"\n  )\n```\n:::\n\n:::\n\nLet's preview the very same lines we modeled in @tbl-td-europarl-tokenization-words-example to see the results of our tokenization.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Preview\neuroparl_unigrams_tbl |>\n  filter(type == \"Target\", line_id == 2) |>\n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 4\n   doc_id type   line_id token     \n    <dbl> <chr>    <dbl> <chr>     \n 1      2 Target       2 i         \n 2      2 Target       2 declare   \n 3      2 Target       2 resumed   \n 4      2 Target       2 the       \n 5      2 Target       2 session   \n 6      2 Target       2 of        \n 7      2 Target       2 the       \n 8      2 Target       2 european  \n 9      2 Target       2 parliament\n10      2 Target       2 adjourned \n```\n\n\n:::\n:::\n\n\nThe `token` column now contains our word tokens. One thing to note, however, is that text is lowercased and punctuation is stripped by default. If we want to retain the original case or punctuation, keep the original variable, or change the tokenization strategy, we can update the `to_lower`, `strip_punct`, `drop`, or `token` parameters, respectively.\n\nAs we derive datasets to explore, let's also create bigram tokens. We can do this by changing the `token` parameter to `\"ngrams\"` and specifying the value for $n$ with the `n` parameter. I will assign the result to `europarl_bigrams_tbl` as we will have two-word tokens, as seen in @exm-td-europarl-tokenization-bigrams-tidytext.\n\n::: {#exm-td-europarl-tokenization-bigrams-tidytext}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tokenize the lines into bigrams\neuroparl_bigrams_tbl <-\n  europarl_tbl |>\n  unnest_tokens(\n    output = token,\n    input = lines,\n    token = \"ngrams\",\n    n = 2\n  )\n# Preview\neuroparl_bigrams_tbl |>\n  filter(type == \"Target\", line_id == 2) |>\n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 4\n   doc_id type   line_id token               \n    <dbl> <chr>    <dbl> <chr>               \n 1      2 Target       2 i declare           \n 2      2 Target       2 declare resumed     \n 3      2 Target       2 resumed the         \n 4      2 Target       2 the session         \n 5      2 Target       2 session of          \n 6      2 Target       2 of the              \n 7      2 Target       2 the european        \n 8      2 Target       2 european parliament \n 9      2 Target       2 parliament adjourned\n10      2 Target       2 adjourned on        \n```\n\n\n:::\n:::\n\n:::\n\nThe two-word token sequences for lines appear as observations in the `europarl_bigrams_tbl` dataset.\n\n::: {.callout}\n**{{< fa medal >}} Dive deeper**\n\nThe `tidytext` package is one of a number of packages that provide tokenization functions. Some other notable packages include `tokenizers` [@R-tokenizers] and `textrecipes` [@R-textrecipes]. In fact, the functions from the `tokenizers` package are used under the hood in the `tidytext` package. The `textrecipes` package is part of the `tidymodels` framework and is designed to work with the `tidymodels` suite of packages. It is particularly useful for integrating tokenization with other preprocessing steps and machine learning models, as we will see in @sec-predict-chapter.\n:::\n\nThe most common tokenization strategy is to segment text into smaller units, often words. However, there are times when we may want text segements to be larger than the existing token unit, effectively collapsing over rows. Let's say that we are working with a dataset like the one we created in `europarl_unigrams_tbl` and we want to group the words into sentences. We can again turn to the `unnest_tokens()` function to accomplish this. In @exm-td-europarl-tokenization-sentences, we use the `token = \"sentences\"` and `collapse = c(\"type\", \"line_id\")` parameters to group the words into sentences by the `type` and `line_id` variables.\n\n::: {#exm-td-europarl-tokenization-sentences}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tokenize the lines into sentences\neuroparl_sentences_tbl <-\n  europarl_unigrams_tbl |>\n  unnest_tokens(\n    output = token,\n    input = token,\n    token = \"sentences\",\n    collapse = c(\"type\", \"line_id\")\n  )\n\n# Preview\neuroparl_sentences_tbl |>\n  slice_head(n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 3\n  type   line_id token                                                          \n  <chr>    <dbl> <chr>                                                          \n1 Source       1 reanudación del período de sesiones                            \n2 Target       1 resumption of the session                                      \n3 Source       2 declaro reanudado el período de sesiones del parlamento europe…\n4 Target       2 i declare resumed the session of the european parliament adjou…\n5 Source       3 como todos han podido comprobar el gran efecto del año 2000 no…\n```\n\n\n:::\n:::\n\n:::\n\nIn this example, we have collapsed the word tokens into sentences. But note, the `token` column contained no punctuation so all the tokens grouped by `type` and `line_id` were concatenated together. This works for our test dataset as lines are sentences. However, in other scenarios, we would need punctuation to ensure that the sentences are properly segmented --if, in fact, punctuation is the cue for sentence boundaries.\n\n## Enrichment {#sec-td-enrichment}\n\nWhere preparation steps are focused on sanitizing and segmenting the text, enrichment steps are aimed towards augmenting the dataset either through recoding, generating, or integrating variables. These processes can prove invaluable for aligning the dataset with the research question and facilitating the analysis.\n\nAs a pratical example of these types of transformations, we'll posit that we are conducting translation research. Specifically, we will set up an investigation into the effect of translation on the syntactic simplification of text. The basic notion is that when translators translate text from one language to another, they subconsciously simplify the text, relative to native texts [@Liu2021].\n\nTo address this research question, we will use the ENNTT corpus, introduced in @sec-cd-semi-structured-orientation. This data contains European Parliament proceedings and the type of text (native, non-native, or translation) from which the text was extracted. There is one curated dataset for each of the text types.\n\nThe data dictionary for the curated native dataset appears in @tbl-td-enntt-native-dd.\n\n\n::: {#tbl-td-enntt-native-dd .cell tbl-cap='Data dictionary for the curated native ENNTT dataset.'}\n::: {.cell-output-display}\n\n+-------------+------------------+-------------+-----------------------------------------------+\n| variable    | name             | type        | description                                   |\n+=============+==================+=============+===============================================+\n| session_id  | Session ID       | categorical | Unique identifier for each session            |\n+-------------+------------------+-------------+-----------------------------------------------+\n| speaker_id  | Speaker ID       | categorical | Unique identifier for each speaker            |\n+-------------+------------------+-------------+-----------------------------------------------+\n| state       | State            | categorical | The country or region the speaker is from     |\n+-------------+------------------+-------------+-----------------------------------------------+\n| session_seq | Session Sequence | ordinal     | The order in which the session occurred       |\n+-------------+------------------+-------------+-----------------------------------------------+\n| text        | Text             | categorical | The spoken text during the session            |\n+-------------+------------------+-------------+-----------------------------------------------+\n| type        | Type             | categorical | The type of speaker. Natives in this dataset. |\n+-------------+------------------+-------------+-----------------------------------------------+\n:::\n:::\n\n\nAll three curated datasets have the same variables. The unit of observation for each dataset is the `text` variable.\n\nBefore we get started, let's consider what the transformed dataset might look like and what its variables mean. First, we will need to operationalize what we mean by syntactic simplification There are many measures of syntactic complexity [@Szmrecsanyi2004]. For our purposes, we will focus on two measures of syntactic complexity: number of T-units and sentence length (in words). A T-unit is a main clause and all of its subordinate clauses. To calculate the number of T-units, we will need to identify the main clauses and their subordinate clauses. The sentence length is straightforward to calculate after word tokenization.\n\nAn idealized transformed dataset dictionary for this investigation should look something like @tbl-td-generation-idealized.\n\n\n::: {#tbl-td-generation-idealized .cell tbl-cap='Idealized transformed dataset for the syntactic simplification investigation.' tbl-colwidths='[10,17,19,54]'}\n::: {.cell-output-display}\n\n+----------+-------------+-----------+--------------------------------------+\n| variable | name        | type      | description                          |\n+==========+=============+===========+======================================+\n| doc_id   | Document ID | integer   | Unique identifier for each document. |\n+----------+-------------+-----------+--------------------------------------+\n| type     | Type        | character | Type of text (native or translated). |\n+----------+-------------+-----------+--------------------------------------+\n| t_units  | T-units     | integer   | Number of T-units in the text.       |\n+----------+-------------+-----------+--------------------------------------+\n| word_len | Word Length | integer   | Number of words in the text.         |\n+----------+-------------+-----------+--------------------------------------+\n:::\n:::\n\n\nWe will be using the the native and translated datasets for our purposes so let's go ahead and read in these datasets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read in curated natives\nenntt_natives_tbl <-\n  read_csv(\"data/enntt_natives_curated.csv\")\n\n# Read in curated translations\nenntt_translations_tbl <-\n  read_csv(\"data/enntt_translations_curated.csv\")\n```\n:::\n\n\n### Generation {#sec-td-generation}\n\nThe process of generation involves the addition of information to a dataset. This differs from other transformation procedures in that instead of manipulating, classifying, and/ or deriving information based on characteristics explicit in a dataset, generation involves deriving new information based on characteristics implicit in a dataset.\n\nThe most common type of operation involved in the generation process is the addition of linguistic annotation. This process can be accomplished manually by a researcher or research team or automatically through the use of pre-trained linguistic resources and/ or software. Ideally, the annotation of linguistic information can be conducted automatically.\n\nTo identify the main clauses and their subordinate clauses in our datasets, we will need to derive syntactic annotation information from the ENNTT `text` variable.\n\nAs fun as it would be to hand-annotate the ENNTT corpus, we will instead turn to automatic linguistic annotation. Specifically, we will use the `udpipe` package [@R-udpipe] which provides an interface for annotating text using pre-trained models from the [Universal Dependencies](https://universaldependencies.org/) (UD) project [@Nivre2020]. The UD project is an effort to develop cross-linguistically consistent treebank annotation for a variety of languages.\n\nOur first step, then, is to peruse the available pre-trained models for the languages we are interested in and selected the most register-aligned models. The models, model names, and licensing information are documented in the `udpipe` package and can be accessed by running `?udpipe::udpipe_download_model()` in the R console. For illustrative purposes, the `english` treebank model from the *https://github.com/bnosac/udpipe.models.ud* repository which is released under the [CC-BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/). This model is trained on various sources including news, Wikipedia, and web data of various genres.\n\nLet's set the stage by providing an overview of the annotation process.\n\n1. Load the `udpipe` package.\n2. Select the pre-trained model to use and the directory where the model will be stored in your local environment.\n3. Prepare the dataset to be annotated (if necessary). This includes ensuring that the dataset has a column of text to be annotated and a grouping column. By default, the names of these columns are expected to be `text` and `doc_id`, respectively. The `text` column needs to be a character vector and the `doc_id` column needs to be a unique index for each text to be annotated.\n4. Annotate the dataset. The result returns a data frame.\n\nSteps 3 and 4 are repeated for the `enntt_natives_tbl` and the `enntt_translations_tbl` datasets. For brevity, I will only show the code for the dataset for the natives. Additionally, I will subset the dataset to 10,000 randomly selected lines for both datasets for the natives. Syntactic annotation is a computationally expensive operation and the natives and translations datasets contain 116,341 and 738,597 observations, respectively.\n\n::: {.callout}\n**{{< fa medal >}} Dive deeper**\n\nIn your own research computationally expensive cannot be avoided, but it can be managed. One strategy is to work with a subset of the data until your code is working as expected. Once you are confident that your code is working as expected, then you can scale up to the full dataset.\n\nIf you are using Quarto, you can use the `cache: true` metadata field in your code blocks to cache the results of computationally expensive code blocks. This will allow you to run your code once and then use the cached results for subsequent runs.\n\nParallel processing is another strategy for managing computationally expensive code. Some packages, such as `udpipe`, have built-in support for parallel processing. Other packages, such as `tidytext`, do not. In these cases, you can use the `future` package [@R-future] to parallelize your code.\n:::\n\n\n\n\n\n\n\n\n\n\n\nWith the subsetted `enntt_natives_tbl` object, let's execute steps 1-4, as seen in @exm-td-generation-udpipe-natives.\n\n::: {#exm-td-generation-udpipe-natives}\n```r\n# Load package\nlibrary(udpipe)\n\n# Model and directory\nmodel <- \"english\"\nmodel_dir <- \"../data/\"\n\n# Prepare the dataset to be annotated\nenntt_natives_prepped_tbl <-\n  enntt_natives_tbl |>\n  mutate(doc_id = row_number()) |>\n  select(doc_id, text)\n\n# Annotate the dataset\nenntt_natives_ann_tbl <-\n  udpipe(\n    x = enntt_natives_prepped_tbl,\n    object = model,\n    model_dir = model_dir\n  ) |>\n  tibble()\n\n# Preview\nglimpse(enntt_natives_ann_tbl)\n```\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 264,124\nColumns: 17\n$ doc_id        <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n$ paragraph_id  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ sentence_id   <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ sentence      <chr> \"It is extremely important that action is taken to ensur…\n$ start         <int> 1, 4, 7, 17, 27, 32, 39, 42, 48, 51, 58, 63, 66, 73, 77,…\n$ end           <int> 2, 5, 15, 25, 30, 37, 40, 46, 49, 56, 61, 64, 71, 75, 82…\n$ term_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ token_id      <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\",…\n$ token         <chr> \"It\", \"is\", \"extremely\", \"important\", \"that\", \"action\", …\n$ lemma         <chr> \"it\", \"be\", \"extremely\", \"important\", \"that\", \"action\", …\n$ upos          <chr> \"PRON\", \"AUX\", \"ADV\", \"ADJ\", \"SCONJ\", \"NOUN\", \"AUX\", \"VE…\n$ xpos          <chr> \"PRP\", \"VBZ\", \"RB\", \"JJ\", \"IN\", \"NN\", \"VBZ\", \"VBN\", \"TO\"…\n$ feats         <chr> \"Case=Nom|Gender=Neut|Number=Sing|Person=3|PronType=Prs\"…\n$ head_token_id <chr> \"4\", \"4\", \"4\", \"0\", \"8\", \"8\", \"8\", \"4\", \"10\", \"8\", \"13\",…\n$ dep_rel       <chr> \"expl\", \"cop\", \"advmod\", \"root\", \"mark\", \"nsubj:pass\", \"…\n$ deps          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ misc          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n```\n\n\n:::\n:::\n\n:::\n\n\n\n\n\nThere is quite a bit of information which is returned from `udpipe()`. Note that the input lines have been tokenized by word. Each token includes the `token`, `lemma`, part of speech (`upos` and `xpos`), morphological features (`feats`), and syntactic relationships (`head_token_id` and `dep_rel`). The `token_id` keeps track of the token's position in the sentence and the `sentence_id` keeps track of the sentence's position in the original text. Finally, the `doc_id` column and its values correspond to the `doc_id` in the `enntt_natives_tbl` dataset.\n\nThe number of variables in the `udpipe()` annotation output is quite overwhelming. However, these attributes come in handy for manipulating, extracting, and plotting information based on lexical and syntactic patterns. See the dependency tree in @fig-td-generation-udpipe-english-plot-tree for an example of the syntactic information that can be extracted from the `udpipe()` annotation output.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Plot of the syntactic tree for a sentence in the ENNTT natives dataset.](7_transform_files/figure-html/fig-td-generation-udpipe-english-plot-tree-1.png){#fig-td-generation-udpipe-english-plot-tree width=768}\n:::\n:::\n\n\n::: {.callout .halfsize}\n**{{< fa medal >}} Dive deeper**\n\nThe plot in @fig-td-generation-udpipe-english-plot-tree was created using the `rsyntax` package [@R-rsyntax]. In addition to creating dependency tree plots, the `rsyntax` package can be used to extract syntactic patterns from the `udpipe()` annotation output. [See the documentation for more information](https://github.com/vanatteveldt/rsyntax).\n:::\n\nIn @fig-td-generation-udpipe-english-plot-tree we see the syntactic tree for a sentence in the ENNTT natives dataset. Each node is labeled with the `token_id` which provides the linear ordering of the sentence. Above the nodes the `dep_relation`, or dependency relationship label is provided. These labels are based on the UD project's [dependency relations](https://universaldependencies.org/u/dep/index.html). We can see that the 'ROOT' relation is at the top of the tree and corresponds to the verb 'brought'. 'ROOT' relations mark predicates in the sentence. Not seen in the example tree, 'cop' relation is a copular, or non-verbal predicate and should be included. These are the key syntactic pattern we will use to identify main clauses for T-units.\n\n### Recoding {#sec-td-recoding}\n\nRecoding processes can be characterized by the creation of structural changes which are derived from values in variables effectively recasting values as new variables to enable more direct access in our analyses.\n\nSpecifically, we will need to identify and count the main clauses and their subordinate clauses to create a variable `t_units` from our natives and translations annotations objects. In the UD project's listings, the relations 'ccomp' (clausal complement), 'xcomp' (open clausal complement), and 'acl:relcl' (relative clause), as seen in @fig-td-generation-udpipe-english-plot-tree are subordinate clauses. Furthermore, we will also need to count the number of words in each sentence to create a variable `word_len`.\n\nTo calculate T-units and words per sentence we turn to the `dplyr` package. We will use the `group_by()` function to group the dataset by `doc_id` and `sentence_id` and then use the `summarize()` function to calculate the number of T-units and words per sentence, where a T-unit is the combination of the sum of main clauses and sum of subordinante clauses. The code is seen in @exm-td-generation-udpipe-natives-tunits-words.\n\n::: {#exm-td-generation-udpipe-natives-tunits-words}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the number of T-units and words per sentence\nenntt_natives_syn_comp_tbl <-\n  enntt_natives_ann_tbl |>\n  group_by(doc_id, sentence_id) |>\n  summarize(\n    main_clauses = sum(dep_rel %in% c(\"ROOT\", \"cop\")),\n    subord_clauses = sum(dep_rel %in% c(\"ccomp\", \"xcomp\", \"acl:relcl\")),\n    t_units = main_clauses + subord_clauses,\n    word_len = n()\n  ) |>\n  ungroup()\n\n# Preview\nglimpse(enntt_natives_syn_comp_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10,199\nColumns: 6\n$ doc_id         <chr> \"1\", \"10\", \"100\", \"1000\", \"10000\", \"1001\", \"1002\", \"100…\n$ sentence_id    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ main_clauses   <int> 1, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 1, 1, 0, 2, 0, 1, 1, 1…\n$ subord_clauses <int> 3, 2, 1, 0, 1, 2, 1, 1, 0, 2, 1, 0, 3, 2, 0, 4, 2, 1, 1…\n$ t_units        <int> 4, 2, 1, 0, 3, 2, 1, 2, 1, 2, 2, 1, 4, 2, 2, 4, 3, 2, 2…\n$ word_len       <int> 21, 25, 27, 15, 40, 43, 29, 23, 13, 30, 33, 9, 68, 35, …\n```\n\n\n:::\n:::\n\n:::\n\n\n\n\n\nA quick spot check of some sentences calculations `enntt_natives_syn_comp_tbl` dataset against the `enntt_natives_ann_tbl` is good to ensure that the calculation is working as expected. In @fig-td-generation-udpipe-natives-tunits we see a sentence that has a word length of 13 and a T-unit value of 5.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Sentence with a word length of 13 and a T-unit value of 5.](7_transform_files/figure-html/fig-td-generation-udpipe-natives-tunits-1.png){#fig-td-generation-udpipe-natives-tunits width=768}\n:::\n:::\n\n\nNow we can drop the intermediate columns we created to calculate our key syntactic complexity measures using `select()` to indicate those that we do want to keep, as seen in @exm-td-generation-udpipe-natives-tunits-words-select.\n\n::: {#exm-td-generation-udpipe-natives-tunits-words-select}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select columns\nenntt_natives_syn_comp_tbl <-\n  enntt_natives_syn_comp_tbl |>\n  select(doc_id, sentence_id, t_units, word_len)\n```\n:::\n\n:::\n\n\n\n\n\nNow we can repeat the process for the ENNTT translated dataset. I will assign the result to `enntt_translations_syn_comp_tbl`. The next step is to join the `sentences` from the annotated data frames into our datasets so that we have the information we set out to generate for both datasets. Then we will combine the native and translations datasets into a single dataset. These steps are part of the transformation process and will be covered in the next section.\n\n### Integration {#sec-td-integration}\n\nOne final class of transformations that can be applied to curated datasets to enhance their informativeness for a research project is the process of integrating two or more datasets. There are two primary types of integrations: joins and concatenation. **Joins** can be row- or column-wise operations that combine datasets based on a common attribute or set of attributes. **Concatenation** is exclusively a row-wise operation that combines datasets that share the same attributes.\n\nOf the two types, joins are the most powerful and sometimes more difficult to understand. When two datasets are joined at least one common variable must be shared between the two datasets. The common variable(s) are referred to as **keys**. The keys are used to match observations in one dataset with observations in another dataset by serving as an index.\n\nThere are a number of join types. The most common are left, full, semi, and anti. The type of join determines which observations are retained in the resulting dataset. Let's see this in practice. First, let's create two datasets to join with a common variable `key`, as seen in @exm-td-merging-join-dfs.\n\n::: {#exm-td-merging-join-dfs}\n\n::: {layout=\"[50, 50]\" layout-valign=\"top\"}\n\n::: {#first}\n\n::: {.cell}\n\n```{.r .cell-code}\na_tbl <-\n  tibble(\n    key = c(1, 2, 3, 5, 8),\n    a = letters[1:5]\n  )\n\na_tbl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 2\n    key a    \n  <dbl> <chr>\n1     1 a    \n2     2 b    \n3     3 c    \n4     5 d    \n5     8 e    \n```\n\n\n:::\n:::\n\n:::\n\n\n::: {#second}\n\n::: {.cell}\n\n```{.r .cell-code}\nb_tbl <-\n  tibble(\n    key = c(1, 2, 4, 6, 8),\n    b = letters[6:10]\n  )\n\nb_tbl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 2\n    key b    \n  <dbl> <chr>\n1     1 f    \n2     2 g    \n3     4 h    \n4     6 i    \n5     8 j    \n```\n\n\n:::\n:::\n\n:::\n\n:::\n:::\n\nThe `a_tbl` and the `b_tbl` datasets share the `key` variable, but the values in the `key` variable are not identical. The two datasets share values `1`, `2`, and `8`. The `a_tbl` dataset has values `3` and `5` in the `key` variable and the `b_tbl` dataset has values `4` and `6` in the `key` variable.\n\nIf we apply a left join to the `a_tbl` and `b_tbl` datasets, the result will be a dataset that retains all of the observations in the `a_tbl` dataset and only those observations in the `b_tbl` dataset that have a match in the `a_tbl` dataset. The result is seen in @exm-td-merging-join-left.\n\n::: {#exm-td-merging-join-left}\n\n::: {.cell}\n\n```{.r .cell-code}\nleft_join(x = a_tbl, y = b_tbl, by = \"key\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 3\n    key a     b    \n  <dbl> <chr> <chr>\n1     1 a     f    \n2     2 b     g    \n3     3 c     <NA> \n4     5 d     <NA> \n5     8 e     j    \n```\n\n\n:::\n:::\n\n:::\n\nNow, if the key variable has the same name, R will recognize and assume that this is the variable to join on and we don't need the `by = ` argument, but if there are multiple potential key variables, we use `by = ` to specify which one to use.\n\nA full join retains all observations in both datasets, as seen in @exm-td-merging-join-full.\n\n::: {#exm-td-merging-join-full}\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_join(x = a_tbl, y = b_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 3\n    key a     b    \n  <dbl> <chr> <chr>\n1     1 a     f    \n2     2 b     g    \n3     3 c     <NA> \n4     5 d     <NA> \n5     8 e     j    \n6     4 <NA>  h    \n7     6 <NA>  i    \n```\n\n\n:::\n:::\n\n:::\n\nLeft and full joins maintain or increase the number of observations. On the other hand, semi and anti joins aim to decrease the number of observations. A semi join retains only those observations in the left dataset that have a match in the right dataset, as seen in @exm-td-merging-join-semi.\n\n::: {#exm-td-merging-join-semi}\n\n::: {.cell}\n\n```{.r .cell-code}\nsemi_join(x = a_tbl, y = b_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n    key a    \n  <dbl> <chr>\n1     1 a    \n2     2 b    \n3     8 e    \n```\n\n\n:::\n:::\n\n:::\n\nAnd an anti join retains only those observations in the left dataset that do not have a match in the right dataset, as seen in @exm-td-merging-join-anti.\n\n::: {#exm-td-merging-join-anti}\n\n::: {.cell}\n\n```{.r .cell-code}\nanti_join(x = a_tbl, y = b_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n    key a    \n  <dbl> <chr>\n1     3 c    \n2     5 d    \n```\n\n\n:::\n:::\n\n:::\n\nOf these join types, the left join and the anti join are some of the most common to encounter in research projects.\n\n::: {.callout .halfsize}\n**{{< fa regular lightbulb >}} Consider this**\n\nIn addition to datasets that are part of an acquired resource or derived from a corpus resource, there are also a number of datasets that are included in R packages that are particularly relevant for text analysis. For example, the `tidytext` package includes `sentiments` and `stop_words` datasets. The `lexicon` package [@R-lexicon] includes large number of datasets that include sentiment lexicons, stopword lists, contractions, and more.\n:::\n\nWith this in mind, let's return to our syntactic simplification investigation. Recall that we started with two curated ENNTT datasets: the natives and translations. We manipulated these datasets subsetting them to 10,000 randomly selected lines, prepped them for annotation by adding a `doc_id` column and dropping all columns except `text`, and then annotated them using the `udpipe` package. We then calculated the number of T-units and words per sentence and created the variables `t_units` and `word_len` for each.\n\nThese steps produced two datasets for both the natives and for the translations. The first dataset for each is the annotated data frame. The second is the data frame with the sytactic complexity measures we calculated. The annotated data frames are named `enntt_natives_ann_tbl` and `enntt_translations_ann_tbl`. The data frames with the syntactic complexity measures are named `enntt_natives_syn_comp_tbl` and `enntt_translations_syn_comp_tbl`.\n\nIn the end, we want a dataset that looks something like @tbl-td-integration-idealized.\n\n| doc_id | type        | t_units | word_len | text                                                   |\n| ------ | ----------- | ------- | -------- | ------------------------------------------------------ |\n| 1      | natives     | 1       | 5        | I am happy right now.                                  |\n| 2      | translation | 3       | 11       | I think that John believes that Mary is a good person. |\n\n: Idealized integrated dataset for the syntactic simplification investigation. {#tbl-td-integration-idealized .striped}\n\nTo create this unified dataset, we will need to apply joins and concatenation. First, we will join the prepped datasets with the annotated datasets. Then, we will concatenate the two resulting datasets.\n\nLet's start by joining the annotated datasets (`enntt_natives_ann_tbl` and `enntt_translations_ann_tbl`) with the datasets with the syntactic complexity calculations (`enntt_natives_syn_comp_tbl` and `enntt_translations_syn_comp_tbl`). In these joins, we can see that the prepped and calculated datasets share a couple variables, `doc_id` and `sentence_id`, in @exm-td-merging-join-prepped-syn-comp.\n\n::: {#exm-td-merging-join-prepped-syn-comp}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Preview datasets to join\nenntt_natives_ann_tbl |>\n  slice_head(n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 17\n  doc_id paragraph_id sentence_id sentence    start   end term_id token_id token\n  <chr>         <int>       <int> <chr>       <int> <int>   <int> <chr>    <chr>\n1 1                 1           1 It is extr…     1     2       1 1        It   \n2 1                 1           1 It is extr…     4     5       2 2        is   \n3 1                 1           1 It is extr…     7    15       3 3        extr…\n# ℹ 8 more variables: lemma <chr>, upos <chr>, xpos <chr>, feats <chr>,\n#   head_token_id <chr>, dep_rel <chr>, deps <chr>, misc <chr>\n```\n\n\n:::\n\n```{.r .cell-code}\nenntt_natives_syn_comp_tbl |>\n  slice_head(n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  doc_id sentence_id t_units word_len\n  <chr>        <int>   <int>    <int>\n1 1                1       4       21\n2 10               1       2       25\n3 100              1       1       27\n```\n\n\n:::\n:::\n\n:::\n\nThe `doc_id` and `sentence_id` variables are both keys that we will use to join the datasets. The reason being that if we only use one of the two we will not align the two datasets at the sentence level. Only the combination of `doc_id` and `sentence_id` isolates the sentences for which we have syntactic complexity measures. Beyond a having common variable (or variables in our case), we must also ensure that join key variables are of the same vector type in both data frames and that we are aware of any differences in the values. From the output in @exm-td-merging-join-prepped-syn-comp, we can see that the `doc_id` and `sentence_id` variables aligned in terms of vector type; `doc_id` is character and `sentence_id` is integer in both data frames. If they happened not to be, their types would need to be adjusted.\n\nNow, we need to check for differences in the values. We can do this by using the `setequal()` function. This function returns `TRUE` if the two vectors are equal and `FALSE` if they are not. If the two vectors are not equal, the function will return the values that are in one vector but not the other. So if one has `10001` and the other doesn't we will get `FALSE`. Let's see this in practice, as seen in @exm-td-merging-join-prepped-syn-comp-check.\n\n::: {#exm-td-merging-join-prepped-syn-comp-check}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for differences in the values\nsetequal(\n  enntt_natives_ann_tbl$doc_id,\n  enntt_natives_syn_comp_tbl$doc_id\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nsetequal(\n  enntt_natives_ann_tbl$sentence_id,\n  enntt_natives_syn_comp_tbl$sentence_id\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n:::\n\nSo the values are the same. The final check is to see if the vectors are of the same length. We know the values are the same, but we don't know if the values are repeated. We do this by simply comparing the length of the vectors, as seen in @exm-td-merging-join-prepped-syn-comp-length.\n\n::: {#exm-td-merging-join-prepped-syn-comp-length}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for differences in the length\nlength(enntt_natives_ann_tbl$doc_id) ==\n  length(enntt_natives_syn_comp_tbl$doc_id)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n\n```{.r .cell-code}\nlength(enntt_natives_ann_tbl$sentence_id) ==\n  length(enntt_natives_syn_comp_tbl$sentence_id)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n:::\n\nSo they are not the same length. Using the `nrow()` function, I can see that the annotated dataset has 264,124 observations and the calculated dataset has 10,199 observations. The annotation data frames will have many more observations due to the fact that the unit of observations is word tokens. The recoded syntactic complexity data frames' unit of observation is the sentence.\n\nTo appreciate the difference in the number of observations, let's look at the first 10 observations of the natives annotated frame for just the columns of interest, as seen in @exm-td-merging-join-prepped-syn-comp-ann.\n\n::: {#exm-td-merging-join-prepped-syn-comp-ann}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Preview the annotated dataset\nenntt_natives_ann_tbl |>\n  select(doc_id, sentence_id, sentence, token) |>\n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 4\n   doc_id sentence_id sentence                                             token\n   <chr>        <int> <chr>                                                <chr>\n 1 1                1 It is extremely important that action is taken to e… It   \n 2 1                1 It is extremely important that action is taken to e… is   \n 3 1                1 It is extremely important that action is taken to e… extr…\n 4 1                1 It is extremely important that action is taken to e… impo…\n 5 1                1 It is extremely important that action is taken to e… that \n 6 1                1 It is extremely important that action is taken to e… acti…\n 7 1                1 It is extremely important that action is taken to e… is   \n 8 1                1 It is extremely important that action is taken to e… taken\n 9 1                1 It is extremely important that action is taken to e… to   \n10 1                1 It is extremely important that action is taken to e… ensu…\n```\n\n\n:::\n:::\n\n:::\n\nThe annotated data frames have a lot of redundancy in for the join variables and the `sentence` variable that we want to add to the calculated data frames. We can reduce the redundancy by using the `distinct()` function from the `dplyr` package. In this case we want all observations where `doc_id`, `sentence_id` and `sentence` are distinct. We then select these variables with `distinct()`, as seen in @exm-td-merging-annotation-distinct.\n\n::: {#exm-td-merging-annotation-distinct}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Reduce annotated data frames to unique sentences\nenntt_natives_ann_distinct <-\n  enntt_natives_ann_tbl |>\n  distinct(doc_id, sentence_id, sentence)\n\nenntt_translations_ann_distinct <-\n  enntt_translations_ann_tbl |>\n  distinct(doc_id, sentence_id, sentence)\n```\n:::\n\n:::\n\nWe now have two datasets that are ready to be joined with the recoded datasets. The next step is to join the two. We will employ a left join where the syntactic complexity data frames are on the left and the join variables will be both the `doc_id` and `sentence_id` variables. The code is seen in @exm-td-merging-join-left-syn-comp.\n\n::: {#exm-td-merging-join-left-syn-comp}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join the native datasets\nenntt_natives_transformed_tbl <-\n  left_join(\n    x = enntt_natives_syn_comp_tbl,\n    y = enntt_natives_ann_distinct,\n    by = c(\"doc_id\", \"sentence_id\")\n  )\n\n# Preview\nglimpse(enntt_natives_transformed_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10,199\nColumns: 5\n$ doc_id      <chr> \"1\", \"10\", \"100\", \"1000\", \"10000\", \"1001\", \"1002\", \"1003\",…\n$ sentence_id <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ t_units     <int> 4, 2, 1, 0, 3, 2, 1, 2, 1, 2, 2, 1, 4, 2, 2, 4, 3, 2, 2, 1…\n$ word_len    <int> 21, 25, 27, 15, 40, 43, 29, 23, 13, 30, 33, 9, 68, 35, 16,…\n$ sentence    <chr> \"It is extremely important that action is taken to ensure …\n```\n\n\n:::\n\n```{.r .cell-code}\n# Join the translations datasets\nenntt_translations_transformed_tbl <-\n  left_join(\n    x = enntt_translations_syn_comp_tbl,\n    y = enntt_translations_ann_distinct,\n    by = c(\"doc_id\", \"sentence_id\")\n  )\n\n# Preview\nglimpse(enntt_translations_transformed_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10,392\nColumns: 5\n$ doc_id      <chr> \"1\", \"10\", \"100\", \"1000\", \"10000\", \"1001\", \"1002\", \"1003\",…\n$ sentence_id <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ t_units     <int> 0, 2, 0, 1, 3, 0, 3, 2, 3, 3, 2, 0, 1, 0, 3, 1, 0, 1, 2, 2…\n$ word_len    <int> 24, 31, 5, 39, 44, 26, 67, 23, 46, 28, 24, 68, 19, 18, 36,…\n$ sentence    <chr> \"To my great surprise , on leaving the sitting , I found t…\n```\n\n\n:::\n:::\n\n:::\n\n<!-- Concatenation -->\n\nThe two data frames now have the same columns and we are closer to our final dataset. The next step is to move toward concatenating the two datasets. Before we do that, we need to do some preparation. First, and most important, we need to add a `type` column to each dataset. This column will indicate whether the sentence is a native or a translation. The second is that our `doc_id` does not serve as a unique identifier for the sentences. Only in combination with `sentence_id` can we uniquely identify a sentence.\n\nSo our plan will be to add a `type` column to each dataset specifying the values for all the observations in the respective dataset. Then we will concatenate the two datasets. Note, if we combine them before, distiguishing the type will be more difficult. After we concatenate the two datasets, we will add a `doc_id` column that will serve as a unique identifier for the sentences and drop the `sentence_id` column. OK, that's the plan. Let's execute it in @exm-td-merging-concatenation.\n\n::: {#exm-td-merging-concatenation}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add a type column\nenntt_natives_transformed_tbl <-\n  enntt_natives_transformed_tbl |>\n  mutate(type = \"natives\")\n\nenntt_translations_transformed_tbl <-\n  enntt_translations_transformed_tbl |>\n  mutate(type = \"translations\")\n\n# Concatenate the datasets\nenntt_transformed_tbl <-\n  bind_rows(\n    enntt_natives_transformed_tbl,\n    enntt_translations_transformed_tbl\n  )\n\n# Overwrite the doc_id column with a unique identifier\nenntt_transformed_tbl <-\n  enntt_transformed_tbl |>\n  mutate(doc_id = row_number()) |>\n  select(doc_id, type, t_units, word_len, text = sentence)\n\n# Preview\nglimpse(enntt_transformed_tbl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 20,591\nColumns: 5\n$ doc_id   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ type     <chr> \"natives\", \"natives\", \"natives\", \"natives\", \"natives\", \"nativ…\n$ t_units  <int> 4, 2, 1, 0, 3, 2, 1, 2, 1, 2, 2, 1, 4, 2, 2, 4, 3, 2, 2, 1, 1…\n$ word_len <int> 21, 25, 27, 15, 40, 43, 29, 23, 13, 30, 33, 9, 68, 35, 16, 53…\n$ text     <chr> \"It is extremely important that action is taken to ensure tha…\n```\n\n\n:::\n:::\n\n:::\n\nThe output of @exm-td-merging-concatenation now looks like @tbl-td-integration-idealized. We have a dataset that has the syntactic complexity measures for both the natives and the translations. We can now write this dataset to disk and document it in the data dictionary.\n\n## Activities {.unnumbered}\n\nIn the following activities, you will review the concept of transforming data to prepare it for analysis and working to implement these steps with R. This includes preparation and enrichment of curated datasets using normalization, tokenization, recoding, generation, and/ or integration strategies.\n\n::: {.callout}\n**{{< fa regular file-code >}} Recipe**\n\n**What**: Transforming and documenting datasets\\\n**How**: Read Recipe 7, complete comprehension check, and prepare for Lab 7.\\\n**Why**: To work with to primary types of transformations, tokenization and joins. Tokenization is the process of recasting textual units as smaller textual units. The process of joining datasets aims to incorporate other datasets to augment or filter the dataset of interest.\n:::\n\n::: {.callout}\n**{{< fa flask >}} Lab**\n\n**What**: Dataset alchemy\\\n**How**: Fork, clone, and complete the steps in Lab 7.\\\n**Why**: To gain experience working with coding strategies for transforming datasets using tidyverse functions and regular expressions, practice reading/ writing data from/ to disk, and implement organizational strategies for organizing and documenting a dataset in reproducible fashion.\n:::\n\n## Summary {.unnumbered}\n\nIn this chapter we covered the process of transforming datasets. The goal is to manipulate the curated dataset to make it align better for analysis.\nWe covered various types of transformation procedures from text normalization to data frame integrations. In any given research project some or all of these steps will be employed --but not necessarily in the order presented in this chapter. It is not uncommon to mix procedures as well. The etiology of the transformation is as unique as the data that you are working with.\n\nSince you are applying techniques that have a significant factor on the shape and contents of your dataset(s) it is important to perform data checks to ensure that the transformations are working as expected. You may not catch everything, and some things may not be caught until later in the analysis process, but it is important to do as much as you can as early as you can.\n\nIn line with the reproducible research principles, it is important to write the transformed dataset to disk and to document it in the data dictionary. This is especially important if you are working with multiple datasets. Good naming conventions also come into play. Choosing descriptive names is so easily overlooked by your present self but so welcomed by your future self.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}