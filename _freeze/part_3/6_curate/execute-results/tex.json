{
  "hash": "e0e5db38add69b2efcd006a8dcb29dcf",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexecute:\n  echo: true\n---\n\n\n\n::: {.content-visible when-format=\"pdf\"}\n\n\n```{=latex}\n\\setDOI{10.4324/9781003393764.6}\n\\thispagestyle{chapterfirstpage}\n```\n\n\n:::\n\n# Curate {#sec-curate-chapter}\n\n<!-- Data Curation in Text Analysis: Strategies for Structuring and Documenting Datasets -->\n\n\n\n\n\n\n\n\n\n::: {.callout}\n**{{< fa regular list-alt >}} Outcomes**\n\n- Describe the importance of data curation in text analysis\n- Recognize the different types of data formats\n- Associate the types data formats with the appropriate R programming techniques to curate the data\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n\nIn this chapter, we will now look at the next step in a text analysis project: data curation. That is, the process of converting the original data we acquire to a tidy dataset. Acquired data can come in a wide variety of formats. These formats tend to signal the richness of the metadata that is included in the file content. We will consider three general types of content formats: (1) unstructured data, (2) structured data, and (3) semi-structured data. Regardless of the file type and the structure of the data, it will be necessary to consider how to curate a dataset that such that the structure reflects the basic the unit of analysis that we wish to investigate. The resulting dataset will form the base from which we will work to further transform the dataset such that it aligns with the unit(s) of observation required for the analysis method that we will implement. Once the dataset is curated, we will create a data dictionary that describes the dataset and the variables that are included in the dataset for transparency and reproducibility.\n\n::: {.callout}\n**{{< fa terminal >}} Lessons**\n\n**What**: Pattern Matching, Tidy Datasets\\\n**How**: In an R console, load {swirl}, run `swirl()`, and follow prompts to select the lesson.\\\n**Why**: To familiarize yourself with the basics of using the pattern matching syntax Regular Expressions and the {dplyr} package to manipulate data into Tidy datasets.\n:::\n\n## Unstructured {#sec-unstructured}\n\nThe bulk of text ever created is of the unstructured variety. Unstructured data is data that has not been organized to make the information contained within machine-readable. Remember that text in itself is not information. Only when given explicit context does text become informative. The explicit contextual information that is included with data is called metadata. Metadata can be linguistic or non-linguistic in nature. So for unstructured data there is little to no metadata directly associated with the data.\n\n### Reading data\n\nSome of the common file formats which contain unstructured data include TXT, PDF, and DOCX. Although these formats are unstructured, they are not the same. Reading these files into R requires different techniques and tools.\n\nThere are many ways to read TXT files into R and many packages that can be used to do so. For example, using {readr}, we can choose to read the entire file into a single vector of character strings with `read_file()` or read the file by lines with `read_lines()` in which each line is a character string in a vector.\n\nLess commonly used in prepared data resources, PDF and DOCX files are more complex than TXT files as they contain formatting and embedded document metadata. However, these attributes are primarily for visual presentation and not for machine-readability. Needless to say, we need an alternate strategy to extract the text content from these files and potentially some of the metadata. For example, using {readtext} [@R-readtext], we can read the text content from PDF and DOCX files into a single vector of character strings with `readtext()`.\n\nWhether in TXT, PDF, or DOCX format, the resulting data structure will require further processing to convert the data into a tidy dataset.\n\n### Orientation\n\nAs an example of curating an unstructured source of corpus data, let's take a look at the [Europarl Parallel Corpus](https://www.statmt.org/europarl/) [@Koehn2005]. This corpus contains parallel texts (source and translated documents) from the European Parliamentary proceedings between 1996-2011 for some 21 European languages.\n\nLet's assume we selected this corpus because we are interested in researching Spanish to English translations. After consulting the corpus website, downloading the archive file, and inspecting the unarchived structure, we have the the file structure seen in @def-curate-europarl-file-structure.\n\n::: {#def-curate-europarl-file-structure}\nProject directory structure for the Europarl Parallel Corpus\n\n```{.bash}\nproject/\n├── process/\n│   ├── 1-acquire-data.qmd\n│   ├── 2-curate-data.qmd\n│   └── ...\n├── data/\n│   ├── analysis/\n│   ├── derived/\n│   └── original/\n│       │── europarl_do.csv\n│       └── europarl/\n│           ├── europarl-v7.es-en.en\n│           └── europarl-v7.es-en.es\n├── reports/\n├── DESCRIPTION\n├── Makefile\n└── README\n```\n\n:::\n\nThe *europarl_do.csv* file contains the data origin information documented as part of the acquisition process. The contents are seen in @tbl-curate-europarl-data-origin.\n\n\n\n::: {#tbl-curate-europarl-data-origin .cell tbl-cap='Data origin: Europarl Corpus' tbl-colwidths='[25,75]'}\n::: {.cell-output-display}\n\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n| attribute               | description                                                                                                                   |\n+=========================+===============================================================================================================================+\n| Resource name           | Europarl Parallel Corpus                                                                                                      |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n| Data source             | <https://www.statmt.org/europarl/>                                                                                            |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n| Data sampling frame     | Spanish transcripts from the European Parliament proceedings                                                                  |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n| Data collection date(s) | 1996-2011                                                                                                                     |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n| Data format             | TXT files with '.es' for source (Spanish) and '.en' for target (English) files.                                               |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n| Data schema             | Line-by-line unannotated parallel text                                                                                        |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n| License                 | See: <https://www.europarl.europa.eu/legal-notice/en/>                                                                        |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n| Attribution             | Please cite the paper: Koehn, P. 2005. 'Europarl: A Parallel Corpus for Statistical Machine Translation.' MT Summit X, 12-16. |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\nNow let's get familiar with the corpus directory structure and the files. In @def-curate-europarl-file-structure, we see that there are two corpus files, *europarl-v7.es-en.es* and *europarl-v7.es-en.en*, that contain the source and target language texts, respectively. The file names indicate that the files contain Spanish-English parallel texts. The *.es* and *.en* extensions indicate the language of the text.\n\nLooking at the beginning of the *.es* and *.en* files, in @def-curate-europarl-es and @def-curate-europarl-en, we see that the files contain a series of lines in either the source or target language.\n\n::: {#def-curate-europarl-es}\n*europarl-v7.es-en.es* file\n\n```{.xml}\nReanudación del período de sesiones\nDeclaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\nComo todos han podido comprobar, el gran \"efecto del año 2000\" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\nSus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\nA la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\n```\n\n:::\n\nWe can clearly appreciate that the data is unstructured. That is, there is no explicit metadata associated with the data. The data is just a series of character strings separated by lines. The only information that we can surmise from structure of the data is that the texts are line-aligned and that the data in each file corresponds to source and target languages.\n\n::: {#def-curate-europarl-en}\n*europarl-v7.es-en.en*\n\n```{.xml}\nResumption of the session\nI declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\nAlthough, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\nYou have requested a debate on this subject in the course of the next few days, during this part-session.\nIn the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n```\n\n:::\n\nNow, before embarking on a data curation process, it is recommendable to define the structure of the data that we want to create. I call this the \"**idealized structure**\" of the data. For a curated dataset, we want to reflect the contents of the original data, yet in a tidy format, to maintain the integrity of and connection with the data.\n\nGiven what we know about the data, we can define the idealized structure of the data as seen in @tbl-curate-europarl-structure-example.\n\n::: {#tbl-curate-europarl-structure-example tbl-colwidths=\"[10, 17, 19, 54]\"}\n\n| variable | name | type | description |\n|----------|------|---------------|-------------|\n| type | Document type | character | Contains the type of document, either 'Source' or 'Target' |\n| line | Line | character | Contains the text of each line in the document |\n\nIdealized structure for the curated Europarl Corpus datasets\n:::\n\nOur task now is to develop code that will read the original data and render the idealized structure as a curated dataset for each corpus file. We will then write the datasets to the *data/derived/* directory. The code we develop will be added to the *2-curate-data.qmd* file. And finally, the datasets will be documented with a data dictionary file.\n\n### Tidy the data\n\nTo create the idealized dataset structure in @tbl-curate-europarl-structure-example, lets's start by reading the files by lines into R. As the files are aligned by lines, we will use the `read_lines()` function to read the files into character vectors.\n\n::: {#exm-curate-europarl-readr}\n```r\n# Load package\nlibrary(readr)\n\n# Read Europarl files .es and .en\neuroparl_es_chr <-\n  read_lines(\"../data/original/europarl-v7.es-en.es\")\n\neuroparl_en_chr <-\n  read_lines(\"../data/original/europarl-v7.es-en.en\")\n```\n\n\n\n::: {.cell}\n\n:::\n\n\n:::\n\nUsing the `read_lines()` function, we read each line of the files into a character vector. Since the Europarl corpus is a parallel corpus, the lines in the source and target files are aligned. This means that the first line in the source file corresponds to the first line in the target file, the second line in the source file corresponds to the second line in the target file, and so on. This alignment is important for the analysis of parallel corpora, as it allows us to compare the source and target texts line by line.\n\nLet's inspect our character vectors to ensure that they are of the length and appear to be structured as we expect. We can use the `length()` function to get the number of lines in each file and the `head()` function to preview the first few lines of each file.\n\n::: {#exm-curate-europarl-inspect-chr}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Inspect Spanish character vector\nlength(europarl_es_chr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1965734\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(europarl_es_chr, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Reanudación del período de sesiones\"                                                                                                                                                                                                 \n[2] \"Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\"                                      \n[3] \"Como todos han podido comprobar, el gran \\\"efecto del año 2000\\\" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\"                    \n[4] \"Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\"                                                                                                                \n[5] \"A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Inspect English character vector\nlength(europarl_en_chr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1965734\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(europarl_en_chr, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Resumption of the session\"                                                                                                                                                                                                               \n[2] \"I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\"                         \n[3] \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\"                                         \n[4] \"You have requested a debate on this subject in the course of the next few days, during this part-session.\"                                                                                                                               \n[5] \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\"\n```\n\n\n:::\n:::\n\n\n:::\n\nThe output of @exm-curate-europarl-inspect-chr shows that the number of lines in each file is the same. This is good. If the number of lines in each file was different, we would need to figure out why and fix it. We also see that the content of the files is aligned as expected.\n\nLet's now create a dataset for each of the character vectors. We will use the `tibble()` function from {tibble} to create a data frame object with the character vectors as the `line` column and add a `type` column with the value 'Source' for the Spanish file and 'Target' for the English file. We will assign the output two new objects `europarl_source_df` and `europarl_target_df`, respectively, as seen in @exm-curate-europarl-df.\n\n::: {#exm-curate-europarl-df}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create source data frame\neuroparl_source_df <-\n  tibble(\n    type = \"Source\",\n    lines = europarl_es_chr\n  )\n# Create target data frame\neuroparl_target_df <-\n  tibble(\n    type = \"Target\",\n    lines = europarl_en_chr\n  )\n```\n:::\n\n\n:::\n\nInspecting these data frames with `glimpse()` in @exm-curate-europarl-glimpse, we can see if the data frames have the structure we expect.\n\n::: {#exm-curate-europarl-glimpse}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Preview source\nglimpse(europarl_source_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,965,734\nColumns: 2\n$ type  <chr> \"Source\", \"Source\", \"Source\", \"Source\", \"Source\", \"Source\", \"Sou~\n$ lines <chr> \"Reanudación del período de sesiones\", \"Declaro reanudado el per~\n```\n\n\n:::\n\n```{.r .cell-code}\n# Preview target\nglimpse(europarl_target_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,965,734\nColumns: 2\n$ type  <chr> \"Target\", \"Target\", \"Target\", \"Target\", \"Target\", \"Target\", \"Tar~\n$ lines <chr> \"Resumption of the session\", \"I declare resumed the session of t~\n```\n\n\n:::\n:::\n\n\n:::\n\nWe now have our `type` and `lines` columns and the associated observations for our idealized dataset, in @tbl-curate-europarl-structure-example. We can now write these datasets to the *data/derived/* directory using `write_csv()` and create corresponding data dictionary files.\n\n## Structured\n\nStructured data already reflects the physical and semantic structure of a tidy dataset. This means that the data is already in a tabular format and the relationships between columns and rows are already well-defined. Therefore the heavy lifting of curating the data is already done. There are two remaining questions, however, that need to be taken into account. One, logistical question, is what file format the dataset is in and how to read it into R. And the second, more research-based, is whether the data may benefit from some additional curation and documentation to make it more amenable to analysis and more understandable to others.\n\n### Reading datasets\n\nLet's consider some common formats for structured data, *i.e.* datasets, and how to read them into R. First, we will consider R-native formats, such as package datasets and RDS files. Then will consider non-native formats, such as relational databases and datasets produced by other software. Finally, we will consider software agnostic formats, such as CSV.\n\nR and some R packages provide structured datasets that are available for use directly within R. For example, {languageR} [@R-languageR] provides the `dative` dataset, which is a dataset containing the realization of the dative as NP or PP in the Switchboard corpus and the Treebank Wall Street Journal collection. {janeaustenr} [@R-janeaustenr] provides the `austen_books` dataset, which is a dataset of Jane Austen's novels. **Package datasets** are loaded into an R session using either the `data()` function, if the package is loaded, or the `::` operator, if the package is not loaded, `data(dative)` or `languageR::dative`, respectively.\n\n::: {.callout}\n**{{< fa medal >}} Dive deeper**\n\nTo explore the available datasets in a package, you can use the `data(package = \"package_name\")` function. For example, `data(package = \"languageR\")` will list the datasets available in {languageR}. You can also explore all the datasets available in the loaded packages with the `data()` function using no arguments. For example, `data()`.\n:::\n\nR also provides a native file format for storing R objects, the **RDS file**. Any R object, including data frames, can be written from an R session to disk by using the `write_rds()` function from `readr`. The *.rds* files will be written to disk in a binary format that is not human-readable, which is not ideal for transparent data sharing. However, the files and the R objects can be read back into an R session using the `read_rds()` function with all the attributes intact, such as vector types, factor levels, *etc.*.\n\nR provides a suite of tools for importing data from non-native structured sources such as databases and datasets from software such as SPSS, SAS, and Stata. For instance, if you are working with data stored in a **relational database** such as MySQL, PostgreSQL, or SQLite, you can use {DBI} [@R-DBI] to connect to the database and {dbplyr} [@R-dbplyr] to query the database using the SQL language. Files from SPSS (*.sav*), SAS (*.sas7bdat*), and Stata (*.dta*) can be read into R using {haven} [@R-haven].\n\nSoftware agnostic file formats include delimited files, such as CSV, TSV, *etc.*. These file formats lack the robust structural attributes of the other formats, but balance this shortcoming by storing structured data in more accessible, human-readable format. Delimited files are plain text files which use a delimiter, such as a comma (`,`), tab (`\\t`), or pipe (`|`), to separate the columns and rows. For example, a CSV file is a delimited file where the columns and rows are separated by commas, as seen in @exm-curate-csv-example.\n\n::: {#exm-curate-csv-example}\n```{.xml}\ncolumn_1,column_2,column_3\nrow 1 value 1,row 1 value 2,row 1 value 3\nrow 2 value 1,row 2 value 2,row 2 value 3\n```\n:::\n\nGiven the accessibility of delimited files, they are a common format for sharing structured data in reproducible research. It is not surprising, then, that this is the format which we have chosen for the derived datasets in this book.\n\n### Orientation\n\nWith an understanding of the various structured formats, we can now turn to considerations about how the original dataset is structured and how that structure is to be used for a given research project. As an example, we will work with the CABNC datasets acquired in @sec-acquire-chapter. The structure of the original dataset is shown in @def-curate-cabnc-structure.\n\n::: {#def-curate-cabnc-structure}\nDirectory structure for the CABNC datasets\n\n```{.bash}\ndata/\n├── analysis/\n├── derived/\n└── original/\n    ├── cabnc_do.csv\n    └── cabnc/\n        ├── participants.csv\n        ├── token_types.csv\n        ├── tokens.csv\n        ├── transcripts.csv\n        └── utterances.csv\n```\n\n:::\n\nIn addition to other important information, the data origin file *cabnc_do.csv* shown in @tbl-curate-cabnc-do informs us the the datasets are related by a common variable.\n\n\n\n::: {#tbl-curate-cabnc-do .cell tbl-cap='Data origin: CABNC datasets' tbl-colwidths='[25,75]'}\n::: {.cell-output-display}\n\n+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| attribute               | description                                                                                                                                                                  |\n+=========================+==============================================================================================================================================================================+\n| Resource name           | CABNC.                                                                                                                                                                       |\n+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Data source             | <https://ca.talkbank.org/access/CABNC.html>, <doi:10.21415/T55Q5R>                                                                                                           |\n+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Data sampling frame     | Over 400 British English Speakers from across the UK stratified age, gender, social group, and region, and recording their language output over a set period of time.        |\n+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Data collection date(s) | 1992.                                                                                                                                                                        |\n+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Data format             | CSV Files                                                                                                                                                                    |\n+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Data schema             | The recordings are linked by `filename` and the participants are linked by `who`.                                                                                            |\n+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| License                 | CC BY NC SA 3.0                                                                                                                                                              |\n+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Attribution             | Saul Albert, Laura E. de Ruiter, and J.P. de Ruiter (2015) CABNC: the Jeffersonian transcription of the Spoken British National Corpus. https://saulalbert.github.io/CABNC/. |\n+-------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n:::\n:::\n\n\n\nThe CABNC datasets are structured in a relational format, which means that the data is stored in multiple tables that are related to each other. The tables are related by a common column or set of columns, which are called a keys. A key is used to join the tables together to create a single dataset. There are two keys in the CABNC datasets, `filename` and `who`. Each variable corresponds to recording- and/ or participant-oriented datasets.\n\nNow, let's envision a scenario in which we are preparing our data for a study that aims to investigate the relationship between speaker demographics and utterances. In their original format, the CABNC datasets separate information about utterances and speakers in separate datasets, `cabnc_utterances` and `cabnc_participants`, respectively. Ideally, we would like to curate these datasets such that the information about the utterances and the speakers are ready to be joined as part of the dataset transformation process, while still retaining the relevant original structure. This usually involves removing redundant and/ or uninformative variables and/ or adjusting variable names and writing these datasets and their documentation files to disk.\n\n### Tidy the dataset\n\nWith these goals in mind, let's start the process of curation by reading the relevant datasets into an R session. Since we are working with CSV files will will use the `read_csv()` function, as seen in @exm-curate-cabnc-read.\n\n::: {#exm-curate-cabnc-read}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read the relevant datasets\ncabnc_utterances <-\n  read_csv(\"data/cabnc/original/utterances.csv\")\ncabnc_participants <-\n  read_csv(\"data/cabnc/original/participants.csv\")\n```\n:::\n\n\n:::\n\nThe next step is to inspect the structure of the datasets. We can use the `glimpse()` function for this task.\n\n::: {#exm-curate-cabnc-glimpse}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Preview the structure of the datasets\nglimpse(cabnc_utterances)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 235,901\nColumns: 10\n$ filename  <chr> \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", \"KB0RE000\", ~\n$ path      <chr> \"ca/CABNC/KB0/KB0RE000\", \"ca/CABNC/KB0/KB0RE000\", \"ca/CABNC/~\n$ utt_num   <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17~\n$ who       <chr> \"PS002\", \"PS006\", \"PS002\", \"PS006\", \"PS002\", \"PS006\", \"PS002~\n$ role      <chr> \"Unidentified\", \"Unidentified\", \"Unidentified\", \"Unidentifie~\n$ postcodes <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~\n$ gems      <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~\n$ utterance <chr> \"You enjoyed yourself in America\", \"Eh\", \"did you\", \"Oh I co~\n$ startTime <dbl> 0.208, 2.656, 2.896, 3.328, 5.088, 6.208, 8.320, 8.480, 10.2~\n$ endTime   <dbl> 2.67, 2.90, 3.33, 5.26, 6.02, 8.50, 9.31, 11.23, 14.34, 15.9~\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(cabnc_participants)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 6,190\nColumns: 13\n$ filename  <chr> \"KB0RE004\", \"KB0RE004\", \"KB0RE004\", \"KB0RE006\", \"KB0RE006\", ~\n$ path      <chr> \"ca/CABNC/0missing/KB0RE004\", \"ca/CABNC/0missing/KB0RE004\", ~\n$ who       <chr> \"PS008\", \"PS009\", \"KB0PSUN\", \"PS007\", \"PS008\", \"PS009\", \"KB0~\n$ name      <chr> \"John\", \"Gethyn\", \"Unknown_speaker\", \"Alan\", \"John\", \"Gethyn~\n$ role      <chr> \"Unidentified\", \"Unidentified\", \"Unidentified\", \"Unidentifie~\n$ language  <chr> \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng\", \"eng~\n$ monthage  <dbl> 481, 481, 13, 949, 481, 481, 13, 637, 565, 13, 637, 565, 13,~\n$ age       <chr> \"40;01.01\", \"40;01.01\", \"1;01.01\", \"79;01.01\", \"40;01.01\", \"~\n$ sex       <chr> \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"mal~\n$ numwords  <dbl> 28, 360, 156, 1610, 791, 184, 294, 93, 3, 0, 128, 24, 0, 150~\n$ numutts   <dbl> 1, 9, 27, 7, 5, 7, 6, 5, 1, 0, 11, 6, 0, 110, 74, 96, 12, 1,~\n$ avgutt    <dbl> 28.00, 40.00, 5.78, 230.00, 158.20, 26.29, 49.00, 18.60, 3.0~\n$ medianutt <dbl> 28, 39, 5, 84, 64, 9, 3, 15, 3, 0, 9, 3, 0, 7, 6, 4, 3, 12, ~\n```\n\n\n:::\n:::\n\n\n:::\n\nFrom visual inspection of the output of @exm-curate-cabnc-glimpse we can see that there are common variables in both datasets. In particular, we see the `filename` and `who` variables mentioned in the data origin file *cabnc_do.csv*.\n\nThe next step is to consider the variables that will be useful for future analysis. Since we are creating a curated dataset, the goal will be to retain as much information as possible from the original datasets. There are cases, however, in which there may be variables that are not informative and thus, will not prove useful for any analysis. These removable variables tend to be of one of two types: variables which show no variation across observations and variables where the information is redundant.\n\nAs an example case, let's look at the `cabnc_participants` data frame. We can use the `skim()` function from {skimr} to get a summary of the variables in the dataset. We can add the `yank()` function to look at variable types one at a time. We will start with the character variables, as seen in @exm-curate-cabnc-skim-character.\n\n::: {#exm-curate-cabnc-skim-character}\n```r\n# Load package\nlibrary(skimr)\n\n# Summarize character variables\ncabnc_participants |>\n  skim() |>\n  yank(\"character\")\n```\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n-- Variable type: character ----------------------------------------------------\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 filename              0             1   8   8     0     2020          0\n2 path                  0             1  21  26     0     2020          0\n3 who                   0             1   4   7     0      581          0\n4 name                  0             1   3  25     0      269          0\n5 role                  0             1  12  12     0        1          0\n6 language              0             1   3   3     0        1          0\n7 age                   0             1   7   8     0       83          0\n8 sex                   0             1   4   6     0        2          0\n```\n\n\n:::\n:::\n\n\n:::\n\n\nWe see from the output in @exm-curate-cabnc-skim-character, that the variables `role` and `language` have a single unique value. This means that these variables do not show any variation across observations. We will remove these variables from the dataset.\n\nContinuing on, let's look for redundant variables. We see that the variables `filename` and `path` have the same number unique values. And if we combine this with the visual summary in @exm-curate-cabnc-glimpse, we can see that the `path` variable is redundant. We will remove this variable from the dataset.\n\nAnother potentially redundant set of variables are `who` and `name` --both of which are speaker identifiers. The `who` variable is a unique identifier, but there may be some redundancy with the `name` variable, that is there may be two speakers with the same name. We can check this by looking at the number of unique values in the `who` and `name` variables from the `skim()` output in @exm-curate-cabnc-skim-character. `who` has 568 unique values and `name` has 269 unique values. This suggests that there are multiple speakers with the same name.\n\nAnother way to explore this is to look at the number of unique values in the `who` variable for each unique value in the `name` variable. We can do this using the `group_by()` and `summarize()` functions from {dplyr}. For each value of `name`, we will count the number of unique values in `who` with `n_distinct()` and then sort the results in descending order.\n\n::: {#exm-curate-cabnc-who-name}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncabnc_participants |>\n  group_by(name) |>\n  summarize(n = n_distinct(who)) |>\n  arrange(desc(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 269 x 2\n   name                          n\n   <chr>                     <int>\n 1 None                         59\n 2 Unknown_speaker              59\n 3 Group_of_unknown_speakers    21\n 4 Chris                         9\n 5 David                         9\n 6 Margaret                      8\n 7 Ann                           7\n 8 John                          7\n 9 Alan                          6\n10 Jackie                        5\n# i 259 more rows\n```\n\n\n:::\n:::\n\n\n:::\n\nIt is good that we performed the check in @exm-curate-cabnc-who-name beforehand. In addition to speakers with the same name, such as 'Chris' and 'David', we also have multiple speakers with generic codes, such as 'None' and 'Unknown_speaker'. It is clear that `name` is redundant and we can safely remove it from the dataset.\n\nWith this in mind, we can then safely remove the following variables from the dataset: `role`, `language`, `name`, and `path`. To drop variables from a data frame we can use the `select()` function in combination with the `-` operator. The `-` operator tells the `select()` function to drop the variables that follow it.\n\n::: {#exm-curate-cabnc-drop-vars}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Drop variables\ncabnc_participants <-\n  cabnc_participants |>\n  select(-role, -language, -name, -path)\n\n# Preview the dataset\nglimpse(cabnc_participants)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 6,190\nColumns: 9\n$ filename  <chr> \"KB0RE004\", \"KB0RE004\", \"KB0RE004\", \"KB0RE006\", \"KB0RE006\", ~\n$ who       <chr> \"PS008\", \"PS009\", \"KB0PSUN\", \"PS007\", \"PS008\", \"PS009\", \"KB0~\n$ monthage  <dbl> 481, 481, 13, 949, 481, 481, 13, 637, 565, 13, 637, 565, 13,~\n$ age       <chr> \"40;01.01\", \"40;01.01\", \"1;01.01\", \"79;01.01\", \"40;01.01\", \"~\n$ sex       <chr> \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"mal~\n$ numwords  <dbl> 28, 360, 156, 1610, 791, 184, 294, 93, 3, 0, 128, 24, 0, 150~\n$ numutts   <dbl> 1, 9, 27, 7, 5, 7, 6, 5, 1, 0, 11, 6, 0, 110, 74, 96, 12, 1,~\n$ avgutt    <dbl> 28.00, 40.00, 5.78, 230.00, 158.20, 26.29, 49.00, 18.60, 3.0~\n$ medianutt <dbl> 28, 39, 5, 84, 64, 9, 3, 15, 3, 0, 9, 3, 0, 7, 6, 4, 3, 12, ~\n```\n\n\n:::\n:::\n\n\n:::\n\n\nNow we have a frame with 9 more informative variables which describe the participants. We would then repeat this process for the `cabnc_utterances` dataset to remove redundant and uninformative variables.\n\nAnother, optional step, is to rename and/ or organize the order the variables to make the dataset more understandable. Let's organize the columns to read left to right from most general to most specific. Again, we turn to the `select()` function, this time including the variables in the order we want them to appear in the dataset. We will take this opportunity to rename some of the variable names so that they are more informative.\n\n::: {#exm-curate-cabnc-rename-vars}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Rename variables\ncabnc_participants <-\n  cabnc_participants |>\n  select(\n    doc_id = filename,\n    part_id = who,\n    part_age = monthage,\n    part_sex = sex,\n    num_words = numwords,\n    num_utts = numutts,\n    avg_utt_len = avgutt,\n    median_utt_len = medianutt\n  )\n\n# Preview the dataset\nglimpse(cabnc_participants)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 6,190\nColumns: 8\n$ doc_id         <chr> \"KB0RE004\", \"KB0RE004\", \"KB0RE004\", \"KB0RE006\", \"KB0RE0~\n$ part_id        <chr> \"PS008\", \"PS009\", \"KB0PSUN\", \"PS007\", \"PS008\", \"PS009\",~\n$ part_age       <dbl> 481, 481, 13, 949, 481, 481, 13, 637, 565, 13, 637, 565~\n$ part_sex       <chr> \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"male\",~\n$ num_words      <dbl> 28, 360, 156, 1610, 791, 184, 294, 93, 3, 0, 128, 24, 0~\n$ num_utts       <dbl> 1, 9, 27, 7, 5, 7, 6, 5, 1, 0, 11, 6, 0, 110, 74, 96, 1~\n$ avg_utt_len    <dbl> 28.00, 40.00, 5.78, 230.00, 158.20, 26.29, 49.00, 18.60~\n$ median_utt_len <dbl> 28, 39, 5, 84, 64, 9, 3, 15, 3, 0, 9, 3, 0, 7, 6, 4, 3,~\n```\n\n\n:::\n:::\n\n\n:::\n\nThe variable order is organized after running @exm-curate-cabnc-rename-vars. Now let's sort the rows by `doc_id` and `part_id` so that the dataset is sensibly organized. The `arrange()` function takes a data frame and a list of variables to sort by, in the order they are listed.\n\n<!-- [ ] tmp fmt -->\n\\pagebreak\n\n::: {#exm-curate-cabnc-sort-rows}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort rows\ncabnc_participants <-\n  cabnc_participants |>\n  arrange(doc_id, part_id)\n\n# Preview the dataset\ncabnc_participants |>\n  slice_head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 x 8\n   doc_id   part_id part_age part_sex num_words num_utts avg_utt_len\n   <chr>    <chr>      <dbl> <chr>        <dbl>    <dbl>       <dbl>\n 1 KB0RE000 KB0PSUN       13 male             2        2        1   \n 2 KB0RE000 PS002        721 female         759       74       10.3 \n 3 KB0RE000 PS006        601 male           399       64        6.23\n 4 KB0RE001 KB0PSUN       13 male             7        3        2.33\n 5 KB0RE001 PS005        481 female         257       32        8.03\n 6 KB0RE001 PS007        949 male           284       29        9.79\n 7 KB0RE002 KB0PSUN       13 male             0        0        0   \n 8 KB0RE002 PS003        601 female         379       30       12.6 \n 9 KB0RE002 PS007        949 male            98       29        3.38\n10 KB0RE003 KB0PSUN       13 male             0        0        0   \n# i 1 more variable: median_utt_len <dbl>\n```\n\n\n:::\n:::\n\n\n:::\n\nApplying the sorting in @exm-curate-cabnc-sort-rows, we can see that the utterances are now our desired order, a dataset that reads left to right from document to participant-oriented attributes and top to bottom by document and participant.\n\n## Semi-structured\n\nBetween unstructured and structured data falls semi-structured data. And as the name suggests, it is a hybrid data format. This means that there will be important structured metadata included with unstructured elements. The file formats and approaches to encoding the structured aspects of the data vary widely from resource to resource and therefore often requires more detailed attention to the structure of the data and often includes more sophisticated programming strategies to curate the data to produce a tidy dataset.\n\n### Reading data\n\nThe file formats associated with semi-structured data include a wide range. These include file formats conducive to more structured-leaning data, such as XML, HTML, and JSON, and file formats with more unstructured-leaning data, such as annotated TXT files. Annotated TXT files may in fact appear with the *.txt* extension, but may also appear with other, sometimes resource-specific, extensions, such as *.utt* for the Switchboard Dialog Act Corpus or *.cha* for the CHILDES corpus annotation files, for example.\n\nThe more structured file formats use standard conventions and therefore can be read into an R session with format-specific functions. Say, for example, we are working with data in a JSON file format. We can read the data into an R session with the `read_json()` function from {jsonlite} [@R-jsonlite]. For XML and HTML files, {rvest} [@R-rvest] provides the `read_xml()` and `read_html()` functions.\n\nSemi-structured data in TXT files can be read either as a file or by lines. The choice of which approach to take depends on the structure of the data. If the data structure is line-based, then `read_lines()` often makes more sense than `read_file()`. However, in some cases, the data may be structured in a way that requires the entire file to be read into an R session and then subsequently parsed.\n\n### Orientation {#sec-curate-semi-structured-orientation}\n\nTo provide an example of the curation process using semi-structured data, we will work with the ENNTT corpus [@Nisioi2016]. The ENNTT corpus contains native and translated English drawn from European Parliament proceedings. Let's look at the directory structure for the ENNTT corpus in @def-curate-enntt-structure.\n\n::: {#def-curate-enntt-structure}\nData directory structure for the ENNTT corpus\n\n```{.bash}\ndata/\n├── analysis/\n├── derived/\n└── original/\n    ├── enntt_do.csv\n    └── enntt/\n        ├── natives.dat\n        ├── natives.tok\n        ├── nonnatives.dat\n        ├── nonnatives.tok\n        ├── translations.dat\n        └── translations.tok\n```\n\n:::\n\nWe now inspect the data origin file for the ENNTT corpus, *enntt_do.csv*, in @tbl-curate-enntt-do.\n\n\n\n::: {#tbl-curate-enntt-do .cell tbl-cap='Data origin file for the ENNTT corpus.' tbl-colwidths='[25,75]'}\n::: {.cell-output-display}\n\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| attribute               | description                                                                                                                                                                                                             |\n+=========================+=========================================================================================================================================================================================================================+\n| Resource name           | Europarl corpus of Native, Non-native and Translated Texts - ENNTT                                                                                                                                                      |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Data source             | https://github.com/senisioi/enntt-release                                                                                                                                                                               |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Data sampling frame     | English, European Parliament texts, transcribed discourse, political genre                                                                                                                                              |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Data collection date(s) | Not specified in the repository                                                                                                                                                                                         |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Data format             | .tok, .dat                                                                                                                                                                                                              |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Data schema             | *.tok files contain the actual text; *.dat files contain the annotations corresponding to each line in the *.tok files.                                                                                                 |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| License                 | Not specified. Contact the authors for more information.                                                                                                                                                                |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Attribution             | Nisioi, S., Rabinovich, E., Dinu, L. P., & Wintner, S. (2016). A corpus of native, non-native and translated texts. Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016). |\n+-------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n:::\n:::\n\n\n\nAccording to the data origin file, there are two important file types, *.dat* and *.tok*. The *.dat* files contain annotations and the *.tok* files contain the actual text. Let's inspect the first couple of lines in the *.dat* file for the native speakers, *nonnatives.dat*, in @def-curate-enntt-nonnatives-dat.\n\n::: {#def-curate-enntt-nonnatives-dat}\nExample *.dat* file for the non-native speakers\n\n```{.xml}\n<LINE STATE=\"Poland\" MEPID=\"96779\" LANGUAGE=\"EN\" NAME=\"Danuta Hübner,\" SEQ_SPEAKER_ID=\"184\" SESSION_ID=\"ep-05-11-17\"/>\n<LINE STATE=\"Poland\" MEPID=\"96779\" LANGUAGE=\"EN\" NAME=\"Danuta Hübner,\" SEQ_SPEAKER_ID=\"184\" SESSION_ID=\"ep-05-11-17\"/>\n```\n:::\n\nWe see that the *.dat* file contains annotations for various session and speaker attributes. The format of the annotations is XML-like. XML is a form of markup language, such as YAML, JSON, *etc.* **Markup languages** are used to annotate text with additional information about the structure, meaning, and/ or presentation of text. In XML, structure is built up by nesting of nodes. The nodes are named with tags, which are enclosed in angle brackets, `<` and `>`. Nodes are opened with `<TAG>` and closed with `</TAG>`. In @def-curate-xml we see an example of a simple XML file structure.\n\n<!-- [ ] tmp fmt -->\n\\pagebreak\n\n::: {#def-curate-xml}\nExample *.xml* file structure\n\n```{.xml}\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<book category=\"fiction\">\n  <title lang=\"en\">The Catcher in the Rye</title>\n  <author>J.D. Salinger</author>\n  <year>1951</year>\n</book>\n```\n\n:::\n\nIn @def-curate-xml there are four nodes, three of which are nested inside of the `<book>` node. The `<book>` node in this example is the root node. XML files require a root node. Nodes can also have attributes, such as the `category` attribute in the `<book>` node, but they are not required. Furthermore, XML files also require a declaration, which is the first line in @def-curate-xml. The declaration specifies the version of XML used and the encoding.\n\nSo the *.dat* file is not strict XML, but is similar in that it contains nodes and attributes. An XML variant you a likely familiar with, HTML, has more relaxed rules than XML. HTML is a markup language used to annotate text with information about the organization and presentation of text on the web that does not require a root node or a declaration --much like our *.dat* file. So suffice it to say that the *.dat* file can safely be treated as HTML.\n\nAnd the *.tok* file for the native speakers, *nonnatives.tok*, in @def-curate-enntt-nonnatives-tok, shows the actual text for each line in the corpus.\n\n::: {#def-curate-enntt-nonnatives-tok}\nExample *.tok* file for the non-native speakers\n\n```{.xml}\nThe Commission is following with interest the planned construction of a nuclear power plant in Akkuyu , Turkey and recognises the importance of ensuring that the construction of the new plant follows the highest internationally accepted nuclear safety standards .\nAccording to our information , the decision on the selection of a bidder has not been taken yet .\n```\n\n:::\n\nIn a study in which we are interested in contrasting the language of natives and non-natives, we will want to combine the *.dat* and *.tok* files for these groups of speakers.\n\nThe question is what attributes we want to include in the curated dataset. Given the research focus, we will not need the `LANGUAGE` or `NAME` attributes. We may want to modify the attribute names so they are a bit more descriptive.\n\nAn idealized version of the curated dataset based on this criteria is shown in @tbl-curate-enntt-ideal.\n\n::: {#tbl-curate-enntt-ideal tbl-colwidths=\"[15, 18, 17, 50]\"}\n\n| variable | name | type | description |\n|----------|------|---------------|-------------|\n| session_id | Session ID | character | Unique identifier for each session. |\n| speaker_id | Speaker ID | integer | Unique identifier for each speaker. |\n| state | State | character | The political state of the speaker. |\n| type | Type | character | Indicates whether the text is native or non-native |\n| session_seq | Session Sequence | integer | The sequence of the text in the session. |\n| text | Text | character | Contains the text of the line, and maintains the structure of the original data. |\n\nIdealized structure for the curated ENNTT Corpus datasets\n:::\n\n### Tidy the data\n\nNow that we have a better understanding of the corpus data and our target curated dataset structure, let's work to extract and organize the data from the native and non-native files.\n\nThe general approach we will take is, for native and then non-natives, to read in the *.dat* file as an HTML file and then extract the line nodes and their attributes combining them into a data frame. Then we'll read in the *.tok* file as a text file and then combine the two into a single data frame.\n\nStarting with the natives, we use {rvest} to read in the *.dat* file as an XML file with the `read_html()` function and then extract the line nodes with the `html_elements()` function as in @exm-curate-enntt-read-xml.\n\n::: {#exm-curate-enntt-read-xml}\n```r\n# Load packages\nlibrary(rvest)\n\n# Read in *.dat* file as HTML\nns_dat_lines <-\n  read_html(\"../data/original/enntt/natives.dat\") |>\n  html_elements(\"line\")\n\n# Inspect\nclass(ns_dat_lines)\ntypeof(ns_dat_lines)\nlength(ns_dat_lines)\n```\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"xml_nodeset\"\n[1] \"list\"\n[1] 116341\n```\n\n\n:::\n:::\n\n\n:::\n\nWhen can see that the `ns_dat_lines` object is a special type of list, `xml_nodeset` which contains 116,341 line nodes. Let's now jump out of sequence and read in the *.tok* file as a text file, in @exm-curate-enntt-read-lines, again by lines using `read_lines()`, and compare the two to make sure that our approach will work.\n\n::: {#exm-curate-enntt-read-lines}\n```r\n# Read in *.tok* file by lines\nns_tok_lines <-\n  read_lines(\"../data/enntt/original/natives.tok\")\n\n# Inspect\nclass(ns_tok_lines)\ntypeof(ns_tok_lines)\nlength(ns_tok_lines)\n```\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"character\"\n[1] \"character\"\n[1] 116341\n```\n\n\n:::\n:::\n\n\n:::\n\nWe do, in fact, have the same number of lines in the *.dat* and *.tok* files. So we can proceed with extracting the attributes from the line nodes and combining them with the text from the *.tok* file.\n\nLet's start by listing the attributes of the first line node in the `ns_dat_lines` object. To do this we will draw on the `pluck()` function from {purrr} [@R-purrr] to extract the first line node. Then, we use the `html_attrs()` function to get the attribute names and the values, as in @exm-curate-enntt-list-attributes.\n\n::: {#exm-curate-enntt-list-attributes}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load package\nlibrary(purrr)\n\n# List attributes line node 1\nns_dat_lines |>\n  pluck(1) |>\n  html_attrs()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            state             mepid          language              name \n \"United Kingdom\"            \"2099\"              \"EN\" \"Evans, Robert J\" \n   seq_speaker_id        session_id \n              \"2\"     \"ep-00-01-17\" \n```\n\n\n:::\n:::\n\n\n:::\n\nNo surprise here, these are the same attributes we saw in the *.dat* file preview in @def-curate-enntt-nonnatives-dat. At this point, it's good to make a plan on how to associate the attribute names with the column names in our curated dataset.\n\n<!-- [ ] tmp fmt -->\n\\pagebreak\n\n- `session_id` = `session_id`\n- `speaker_id` = `MEPID`\n- `state` = `state`\n- `session_seq` = `seq_speaker_id`\n\nWe can do this one attribute at a time using the `html_attr()` function and then combine them into a data frame with the `tibble()` function as in @exm-curate-enntt-extract-attributes.\n\n::: {#exm-curate-enntt-extract-attributes}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract attributes from first line node\nsession_id <- ns_dat_lines |> pluck(1) |> html_attr(\"session_id\")\nspeaker_id <- ns_dat_lines |> pluck(1) |> html_attr(\"mepid\")\nstate <- ns_dat_lines |> pluck(1) |> html_attr(\"state\")\nsession_seq <- ns_dat_lines |> pluck(1) |> html_attr(\"seq_speaker_id\")\n\n# Combine into data frame\ntibble(session_id, speaker_id, state, session_seq)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 4\n  session_id  speaker_id state          session_seq\n  <chr>       <chr>      <chr>          <chr>      \n1 ep-00-01-17 2099       United Kingdom 2          \n```\n\n\n:::\n:::\n\n\n:::\n\nThe results from @exm-curate-enntt-extract-attributes show that the attributes have been extracted and mapped to our idealized column names, but this would be tedious to do for each line node. A function to extract attributes and values from a line and add them to a data frame would help simplify this process. The function in @exm-curate-enntt-extract-attributes-function does just that.\n\n::: {#exm-curate-enntt-extract-attributes-function}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to extract attributes from line node\nextract_dat_attrs <- function(line_node) {\n  session_id <- line_node |> html_attr(\"session_id\")\n  speaker_id <- line_node |> html_attr(\"mepid\")\n  state <- line_node |> html_attr(\"state\")\n  session_seq <- line_node |> html_attr(\"seq_speaker_id\")\n\n  tibble(session_id, speaker_id, state, session_seq)\n}\n```\n:::\n\n\n:::\n\nIt's a good idea to test out the function to verify that it works as expected. We can do this by passing the various indices to the `ns_dat_lines` object to the function as in @exm-curate-enntt-test-extract-attributes-function.\n\n<!-- [ ] tmp fmt -->\n\\pagebreak\n\n::: {#exm-curate-enntt-test-extract-attributes-function}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test function\nns_dat_lines |> pluck(1) |> extract_dat_attrs()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 4\n  session_id  speaker_id state          session_seq\n  <chr>       <chr>      <chr>          <chr>      \n1 ep-00-01-17 2099       United Kingdom 2          \n```\n\n\n:::\n\n```{.r .cell-code}\nns_dat_lines |> pluck(20) |> extract_dat_attrs()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 4\n  session_id  speaker_id state          session_seq\n  <chr>       <chr>      <chr>          <chr>      \n1 ep-00-01-17 1309       United Kingdom 40         \n```\n\n\n:::\n\n```{.r .cell-code}\nns_dat_lines |> pluck(100) |> extract_dat_attrs()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 4\n  session_id  speaker_id state          session_seq\n  <chr>       <chr>      <chr>          <chr>      \n1 ep-00-01-18 4549       United Kingdom 28         \n```\n\n\n:::\n:::\n\n\n:::\n\nLooks like the `extract_dat_attrs()` function is ready for prime-time. Let's now apply it to all of the line nodes in the `ns_dat_lines` object using the `map_dfr()` function from {purrr} as in @exm-curate-enntt-extract-attributes-all.\n\n::: {#exm-curate-enntt-extract-attributes-all}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract attributes from all line nodes\nns_dat_attrs <-\n  ns_dat_lines |>\n  map_dfr(extract_dat_attrs)\n\n# Inspect\nglimpse(ns_dat_attrs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 116,341\nColumns: 4\n$ session_id  <chr> \"ep-00-01-17\", \"ep-00-01-17\", \"ep-00-01-17\", \"ep-00-01-17\"~\n$ speaker_id  <chr> \"2099\", \"2099\", \"2099\", \"4548\", \"4548\", \"4541\", \"4541\", \"4~\n$ state       <chr> \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"Uni~\n$ session_seq <chr> \"2\", \"2\", \"2\", \"4\", \"4\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12~\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.callout}\n**{{< fa medal >}} Dive deeper**\n\nThe `map*()` functions from {purrr} are a family of functions that apply a function to each element of a vector, list, or data frame. The `map_dfr()` function is a variant of the `map()` function that returns a data frame that is the result of row-binding the results, hence `_dfr`.\n:::\n\nWe can see that the `ns_dat_attrs` object is a data frame with 116,341 rows and 4 columns, just has we expected. We can now combine the `ns_dat_attrs` data frame with the `ns_tok_lines` vector to create a single data frame with the attributes and the text. This is done with the `mutate()` function assigning the `ns_tok_lines` vector to a new column named `text` as in @exm-curate-enntt-combine-attributes-text.\n\n::: {#exm-curate-enntt-combine-attributes-text}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine attributes and text\nns_dat <-\n  ns_dat_attrs |>\n  mutate(text = ns_tok_lines)\n\n# Inspect\nglimpse(ns_dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 116,341\nColumns: 5\n$ session_id  <chr> \"ep-00-01-17\", \"ep-00-01-17\", \"ep-00-01-17\", \"ep-00-01-17\"~\n$ speaker_id  <chr> \"2099\", \"2099\", \"2099\", \"4548\", \"4548\", \"4541\", \"4541\", \"4~\n$ state       <chr> \"United Kingdom\", \"United Kingdom\", \"United Kingdom\", \"Uni~\n$ session_seq <chr> \"2\", \"2\", \"2\", \"4\", \"4\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12~\n$ text        <chr> \"You will be aware from the press and television that ther~\n```\n\n\n:::\n:::\n\n\n:::\n\nThis is the data for the native speakers. We can now repeat this process for the non-native speakers, or we can create a function to do it for us.\n\n::: {.callout .halfsize}\n**{{< fa regular lightbulb >}} Consider this**\n\nUsing the previous code as a guide, consider what steps you would need to take to create a function to combine the *.dat* and *.tok* files for the non-native speakers (and/ or the translations). What arguments would the function take? What would the function return? What would the processing steps be? In what order would the steps be executed?\n:::\n\nAfter applying the curation steps to both the native and non-native datasets, we will have two data frames, `enntt_ns_df` and `enntt_nns_df`, respectively that meet the idealized structure for the curated ENNTT Corpus datasets, as shown in @tbl-curate-enntt-ideal. The `enntt_ns_df` and `enntt_nns_df` data frames are ready to be written to disk and documented.\n\n## Documentation\n\nAfter applying the curation steps to our data, we will now want to write the dataset to disk and to do our best to document the process and the resulting dataset.\n\nSince data frames are a tabular, we will have various options for the file type to write. Many of these formats are software-specific, such as `*.xlsx` for Microsoft Excel, `*.sav` for SPSS, `*.dta` for Stata, and `*.rds` for R. We will use the `*.csv` format since it is a common format that can be read by many software packages. We will use the `write_csv()` function from {readr} to write the dataset to disk.\n\nNow the question is where to save our CSV file. Since our dataset is derived by our work, we will added it to the *derived/* directory. If you are working with multiple data sources within the same project, it is a good idea to create a subdirectory for each dataset. This will help keep the project organized and make it easier to find and access the datasets.\n\nThe final step, as always, is to provide documentation. For datasets the documentation is a data dictionary, as discussed in @sec-data-data-dictionaries. As with data origin files, you can use spreadsheet software to create and edit the data dictionary.\n\n::: {.callout .halfsize}\n**{{< fa regular hand-point-up >}} Tip**\n\nThe `create_data_dictionary()` function from {qtkit} provides a rudimentary data dictionary template by default. However, the `model` argument let's you take advantage of OpenAI's text generation models to generate a more detailed data dictionary for you to edit. See the function documentation for more information.\n:::\n\nIn {qtkit} we have a function, `create_data_dictionary()` that will generate the scaffolding for a data dictionary. The function takes two arguments, `data` and `file_path`. It reads the dataset columns and provides a template for the data dictionary.\n\nAn example of a data dictionary, a data dictionary for the `enntt_ns_df` dataset is shown in @tbl-curate-unstructured-data-dictionary-example.\n\n\n\n::: {#tbl-curate-unstructured-data-dictionary-example .cell tbl-cap='Data dictionary for the `enntt_ns_df` dataset' tbl-colwidths='[15,15,15,55]'}\n::: {.cell-output-display}\n\n+-------------+------------------+-------------+-------------------------------------------------------+\n| variable    | name             | type        | description                                           |\n+=============+==================+=============+=======================================================+\n| session_id  | Session ID       | categorical | Unique identifier for each session                    |\n+-------------+------------------+-------------+-------------------------------------------------------+\n| speaker_id  | Speaker ID       | categorical | Unique identifier for each speaker                    |\n+-------------+------------------+-------------+-------------------------------------------------------+\n| state       | State            | categorical | Name of the state or country the session is linked to |\n+-------------+------------------+-------------+-------------------------------------------------------+\n| session_seq | Session Sequence | ordinal     | Sequence number in the session                        |\n+-------------+------------------+-------------+-------------------------------------------------------+\n| text        | Text             | categorical | Text transcript of the session                        |\n+-------------+------------------+-------------+-------------------------------------------------------+\n| type        | Type             | categorical | The type of the speaker, whether native or nonnative  |\n+-------------+------------------+-------------+-------------------------------------------------------+\n:::\n:::\n\n\n\n## Activities {.unnumbered}\n\nThe following activities build on your skills and knowledge to use R to read, inspect, and write data and datasets in R. In these activities you will have an opportunity to learn and apply your skills and knowledge to the task of curating datasets. This is a vital component of text analysis research that uses unstructured and semi-structured data.\n\n::: {.callout}\n**{{< fa regular file-code >}} Recipe**\n\n**What**: Organizing and documenting datasets\\\n**How**: Read Recipe 6, complete comprehension check, and prepare for Lab 6.\\\n**Why**: To rehearse methods for deriving tidying datasets to use a the base for further project-specific purposes. We will explore how regular expressions are helpful in developing strategies for matching, extracting, and/ or replacing patterns in character sequences and how to organize datasets in rows and columns. We will also explore how to document datasets in a data dictionary.\n:::\n\n::: {.callout}\n**{{< fa flask >}} Lab**\n\n**What**: Taming data\\\n**How**: Fork, clone, and complete the steps in Lab 6.\\\n**Why**: To gain experience working with coding strategies to manipulate data using tidyverse functions and regular expressions, to practice reading/ writing data from/ to disk, and to implement organizational strategies for organizing and documenting a dataset in reproducible fashion.\n:::\n\n## Summary {.unnumbered}\n\nIn this chapter we looked at the process of structuring data into a dataset. This included a discussion on three main types of data --unstructured, structured, and semi-structured. The level of structure of the original data(set) will vary from resource to resource and by the same token so will the file format used to support the level of metadata included. The results from data curation results in a dataset that is saved separate from the original data to maintain modularity between what the data(set) looked like before we intervene and afterwards. Since there can be multiple analysis approaches applied the original data in a research project, this curated dataset serves as the point of departure for each of the subsequent datasets derived from the transformational steps. In addition to the code we use to derive the curated dataset's structure, we also include a data dictionary which documents the curated dataset.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{\"knit_meta_id\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]}},\"value\":[{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"booktabs\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"longtable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"array\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"multirow\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"wrapfig\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"colortbl\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"pdflscape\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabu\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttablex\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"ulem\"]},{\"type\":\"character\",\"attributes\":{},\"value\":[\"normalem\"]},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"makecell\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"xcolor\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabularray\"]},{\"type\":\"NULL\"},{\"type\":\"character\",\"attributes\":{},\"value\":[\"\\\\usepackage[normalem]{ulem}\",\"\\\\usepackage{graphicx}\",\"\\\\UseTblrLibrary{booktabs}\",\"\\\\UseTblrLibrary{siunitx}\",\"\\\\NewTableCommand{\\\\tinytableDefineColor}[3]{\\\\definecolor{#1}{#2}{#3}}\",\"\\\\newcommand{\\\\tinytableTabularrayUnderline}[1]{\\\\underline{#1}}\",\"\\\\newcommand{\\\\tinytableTabularrayStrikeout}[1]{\\\\sout{#1}}\"]}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabularray\"]},{\"type\":\"NULL\"},{\"type\":\"character\",\"attributes\":{},\"value\":[\"\\\\usepackage[normalem]{ulem}\",\"\\\\usepackage{graphicx}\",\"\\\\UseTblrLibrary{booktabs}\",\"\\\\UseTblrLibrary{siunitx}\",\"\\\\NewTableCommand{\\\\tinytableDefineColor}[3]{\\\\definecolor{#1}{#2}{#3}}\",\"\\\\newcommand{\\\\tinytableTabularrayUnderline}[1]{\\\\underline{#1}}\",\"\\\\newcommand{\\\\tinytableTabularrayStrikeout}[1]{\\\\sout{#1}}\"]}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabularray\"]},{\"type\":\"NULL\"},{\"type\":\"character\",\"attributes\":{},\"value\":[\"\\\\usepackage[normalem]{ulem}\",\"\\\\usepackage{graphicx}\",\"\\\\UseTblrLibrary{booktabs}\",\"\\\\UseTblrLibrary{siunitx}\",\"\\\\NewTableCommand{\\\\tinytableDefineColor}[3]{\\\\definecolor{#1}{#2}{#3}}\",\"\\\\newcommand{\\\\tinytableTabularrayUnderline}[1]{\\\\underline{#1}}\",\"\\\\newcommand{\\\\tinytableTabularrayStrikeout}[1]{\\\\sout{#1}}\"]}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabularray\"]},{\"type\":\"NULL\"},{\"type\":\"character\",\"attributes\":{},\"value\":[\"\\\\usepackage[normalem]{ulem}\",\"\\\\usepackage{graphicx}\",\"\\\\UseTblrLibrary{booktabs}\",\"\\\\UseTblrLibrary{siunitx}\",\"\\\\NewTableCommand{\\\\tinytableDefineColor}[3]{\\\\definecolor{#1}{#2}{#3}}\",\"\\\\newcommand{\\\\tinytableTabularrayUnderline}[1]{\\\\underline{#1}}\",\"\\\\newcommand{\\\\tinytableTabularrayStrikeout}[1]{\\\\sout{#1}}\"]}]}]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}